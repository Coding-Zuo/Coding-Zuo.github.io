<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Coding-Zuo - Coding And Studying</title><meta name="keywords" content="NLP zuoyuhui"><meta name="author" content="Coding-Zuo"><meta name="copyright" content="Coding-Zuo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="我的建议是看开点、、、">
<meta property="og:type" content="website">
<meta property="og:title" content="Coding-Zuo">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Coding-Zuo">
<meta property="og:description" content="我的建议是看开点、、、">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/03/22/YP2oqk7lOAfceTD.jpg">
<meta property="article:author" content="Coding-Zuo">
<meta property="article:tag" content="NLP zuoyuhui">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/03/22/YP2oqk7lOAfceTD.jpg"><link rel="shortcut icon" href="https://i.loli.net/2021/03/22/reFlcYOnP3dSuJX.png"><link rel="canonical" href="http://example.com/page/2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.json","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-03-01 14:43:28'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/css/footer.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css"><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="Coding-Zuo" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="https://i.loli.net/2021/03/22/YP2oqk7lOAfceTD.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">138</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https:www.baidu.com"><i class="fa-fw fas fa-heart"></i><span> 我的简历</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E9%9F%B3%E4%B9%90"><i class="fa-fw /music/"></i><span> 0</span></a></li><li><a class="site-page child" href="/%E7%94%B5%E5%BD%B1"><i class="fa-fw /movies/"></i><span> 1</span></a></li><li><a class="site-page child" href="/%E7%85%A7%E7%89%87"><i class="fa-fw /Gallery/"></i><span> 2</span></a></li></ul></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('https://i.loli.net/2021/03/22/Td23imqYWsLGcJA.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Coding-Zuo</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https:www.baidu.com"><i class="fa-fw fas fa-heart"></i><span> 我的简历</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E9%9F%B3%E4%B9%90"><i class="fa-fw /music/"></i><span> 0</span></a></li><li><a class="site-page child" href="/%E7%94%B5%E5%BD%B1"><i class="fa-fw /movies/"></i><span> 1</span></a></li><li><a class="site-page child" href="/%E7%85%A7%E7%89%87"><i class="fa-fw /Gallery/"></i><span> 2</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Coding-Zuo</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/Coding-Zuo" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/zzuuoo666@sina.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/12/27/Unsupervised-Domain-Adaptation-of-a-Pretrained-Cross-Lingual-Language-Model/" title="Unsupervised Domain Adaptation of a Pretrained Cross-Lingual Language Model">     <img class="post_bg" src="https://i.loli.net/2021/11/17/hMK8qF4Jne6sTYC.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Unsupervised Domain Adaptation of a Pretrained Cross-Lingual Language Model"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/12/27/Unsupervised-Domain-Adaptation-of-a-Pretrained-Cross-Lingual-Language-Model/" title="Unsupervised Domain Adaptation of a Pretrained Cross-Lingual Language Model">Unsupervised Domain Adaptation of a Pretrained Cross-Lingual Language Model</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-12-27T11:39:54.000Z" title="发表于 2021-12-27 19:39:54">2021-12-27</time></span></div><div class="content">Unsupervised Domain Adaptation of a Pretrained Cross-Lingual Language Model最近的研究表明，在大规模的无标签文本上预训练跨语言语言模型，可以在各种跨语言和低资源任务中产生明显的性能改进。通过对一百种语言和TB级文本的训练，跨语言模型已被证明能有效地利用高资源语言来提高低资源语言的处理能力，并超过了单语言模型。在本文中，我们进一步研究了当预训练的跨语言模型需要适应新领域时的跨语言和跨领域（CLCD）设置。具体来说，我们提出了一种新的无监督的特征分解方法，该方法可以从纠缠在一起的预训练的跨语言表征中自动提取特定领域的特征和领域不变的特征，给定源语言中未标记的原始文本。我们提出的模型利用相互信息估计，将跨语言模型计算的表征分解为领域变量和领域特定部分。实验结果表明，我们提出的方法比最先进的预训练的跨语言模型在CLCD环境中取得了明显的性能改进。本文的源代码可在https://github.com/lijuntaopku/UFD。
Introduction深度学习的最新进展使各种NLP任务受益，并在大规模注释数据集可用时 ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/12/22/Contrastive-Representation-Distillation/" title="Contrastive Representation Distillation">     <img class="post_bg" src="https://s2.loli.net/2021/12/22/aCcIO7GKrPo5DV3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Contrastive Representation Distillation"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/12/22/Contrastive-Representation-Distillation/" title="Contrastive Representation Distillation">Contrastive Representation Distillation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-12-22T15:50:39.000Z" title="发表于 2021-12-22 23:50:39">2021-12-22</time></span></div><div class="content">Contrastive Representation Distillation我们经常希望将表征性知识从一个神经网络转移到另一个神经网络。这方面的例子包括将一个大型网络提炼成一个较小的网络，将知识从一种感觉模式转移到另一种感觉模式，或者将一系列模型集合成一个单一的估计器。知识提炼是解决这些问题的标准方法，它使教师和学生网络的概率输出之间的KL散度最小。我们证明这个目标忽略了教师网络的重要结构知识。
这促使我们提出了另一个目标，即训练学生在教师的数据表述中捕捉到更多的信息。我们把这个目标表述为对比学习。
实验证明，我们所产生的新目标在各种知识迁移任务上优于知识蒸馏和其他尖端的蒸馏器，包括单一模型压缩、集合蒸馏和跨modal转移。我们的方法在许多迁移任务中创造了新的最先进的技术，当与知识蒸馏相结合时，有时甚至超过了教师网络。
INTRODUCTION知识蒸馏（KD）将知识从一个深度学习模型（教师）转移到另一个（学生）。最初由Hinton等人（2015）提出的目标是最小化教师和学生输出之间的KL散度。当输出是一个分布时，这种表述具有直观的意义，例如，在类上的概率质量函数。然而，我们经常希望迁 ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/12/22/Achieving-Forgetting-Prevention-and-Knowledge-Transfer-in-Continual-Learning/" title="Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning">     <img class="post_bg" src="https://s2.loli.net/2021/12/22/OG4tHAZ7uEoJTfh.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/12/22/Achieving-Forgetting-Prevention-and-Knowledge-Transfer-in-Continual-Learning/" title="Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning">Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-12-22T06:09:05.000Z" title="发表于 2021-12-22 14:09:05">2021-12-22</time></span></div><div class="content">Achieving Forgetting Prevention and Knowledge Transfer in Continual LearningAbstract持续学习（CL）是指逐步学习一连串的任务，目的是实现两个主要目标：克服灾难性遗忘（CF）和鼓励跨任务的知识转移（KT）。然而，大多数现有的技术只注重克服CF，没有鼓励KT的机制，因此在KT中表现不佳。尽管有几篇论文试图同时处理CF和KT，但我们的实验表明，当任务没有太多的共享知识时，它们受到严重的CF影响。
另一个观察结果是，目前大多数CL方法没有使用预训练的模型，但事实证明，这种模型可以大大改善最终的任务表现。例如，在自然语言处理中，对类似BERT的预训练语言模型进行微调是最有效的方法之一。
 然而，对于CL来说，这种方法受到了严重的CF的影响。一个有趣的问题是如何将预训练的模型最好地用于CL。本文提出了一个名为CTR的新模型来解决这些问题。我们的实验结果证明了CTR的有效性。
Introduction本文研究了在任务持续学习（Task-CL）环境下的自然语言处理（NLP）任务序列的持续学习（CL）。它的目的是 

( ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/12/21/On-Transferability-of-Prompt-Tuning-for-Natural-Language-Understanding/" title="On Transferability of Prompt Tuning for Natural Language Understanding">     <img class="post_bg" src="https://i.loli.net/2021/12/02/MVzbTiL5glWvtj4.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="On Transferability of Prompt Tuning for Natural Language Understanding"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/12/21/On-Transferability-of-Prompt-Tuning-for-Natural-Language-Understanding/" title="On Transferability of Prompt Tuning for Natural Language Understanding">On Transferability of Prompt Tuning for Natural Language Understanding</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-12-21T07:44:25.000Z" title="发表于 2021-12-21 15:44:25">2021-12-21</time></span></div><div class="content">On Transferability of Prompt Tuning for Natural Language UnderstandingPrompt tuning（PT）是一种很有前途的参数高效方法，可以利用极其庞大的预训练语言模型（PLM），只需 tuning 几个软提示，就可以达到与全参数微调相当的性能。
然而，与微调相比，经验上PT需要更多的训练步骤。为了探索是否可以通过重复使用训练好的 soft prompts 和分享学到的知识来提高 PT 的效率，我们从经验上研究了 soft prompts 在不同任务和模型中的可迁移性。

在跨任务迁移中，发现经过训练的  soft prompts  可以很好地迁移到类似的任务中，并为它们初始化PT，以加速训练和提高性能。此外，为了探索哪些因素会影响 prompts 的跨任务转移性，我们研究了如何测量 prompt 的相似性，发现激活的神经元的重叠率与迁移性高度相关。
在跨模型迁移中，我们探索了如何将一个PLM的 prompt 投射到另一个PLM上，并成功地训练了一种 projector，该projector 可以在类似的任务上实现非微 ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/12/19/TransPrompt-Towards-an-Automatic-Transferable-Prompting-Framework-for-Few-shot-Text-Classification/" title="TransPrompt Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification">     <img class="post_bg" src="https://s2.loli.net/2021/12/19/hTNkCw9jpLiZRWV.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="TransPrompt Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/12/19/TransPrompt-Towards-an-Automatic-Transferable-Prompting-Framework-for-Few-shot-Text-Classification/" title="TransPrompt Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification">TransPrompt Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-12-19T13:59:11.000Z" title="发表于 2021-12-19 21:59:11">2021-12-19</time></span></div><div class="content">TransPrompt: Towards an Automatic Transferable Prompting Framework for Few-shot Text ClassificationAbstract最近的研究表明，prompts 可以提高大型预训练语言模型在 few-shot 文本分类中的表现。然而，目前还不清楚如何在类似的NLP任务中迁移 prompts 知识以达到相互强化的目的。基于连续的 prompts 嵌入，我们提出了TransPrompt，一个可迁移的 prompt 框架，用于在类似的任务中进行 few-shot 的学习。
 在TransPrompt中，我们采用了一个多任务元知识获取程序来训练一个元学习者，以捕获跨任务的可迁移知识。我们进一步设计了两种去偏技术，使其对任何任务都更具有任务无关性和无偏性。
之后，元学习器可以以高精确度适应目标任务。大量的实验表明，TransPrompt在多个NLP任务和数据集上的表现优于单任务和跨任务的强基线。我们进一步表明，元学习器可以有效地提高以前未见过的任务的性能。当用完整的训练集学习时，TransPrompt也优于强大的 ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/12/07/Iterative-Network-Pruning-with-Uncertainty-Regularization-for-Lifelong-Sentiment-Classification/" title="Iterative Network Pruning with Uncertainty Regularization for Lifelong Sentiment Classification">     <img class="post_bg" src="https://s2.loli.net/2021/12/07/YiNu7IOeRfMjyUX.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Iterative Network Pruning with Uncertainty Regularization for Lifelong Sentiment Classification"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/12/07/Iterative-Network-Pruning-with-Uncertainty-Regularization-for-Lifelong-Sentiment-Classification/" title="Iterative Network Pruning with Uncertainty Regularization for Lifelong Sentiment Classification">Iterative Network Pruning with Uncertainty Regularization for Lifelong Sentiment Classification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-12-07T13:20:14.000Z" title="发表于 2021-12-07 21:20:14">2021-12-07</time></span></div><div class="content">Iterative Network Pruning with Uncertainty Regularization for Lifelong Sentiment Classification提出了一种新的具有不确定性正则化的迭代网络修剪方法，用于终身情感分类（IPRLS），它利用了网络修剪和权重正则化的原则。通过以迭代的方式进行网络修剪和不确定性正则化，IPRLS可以使一个单一的BERT模型适应来自多个领域的连续到达的数据，同时避免灾难性的遗忘和相互影响。
具体来说，利用迭代修剪方法来去除大型深度网络中的冗余参数，这样释放出来的空间就可以用来学习新的任务，解决灾难性遗忘的问题。
在学习新任务时，我们也使用基于贝叶斯在线学习框架的不确定性正则化来约束BERT中旧任务权重的更新，这使得正向转移成为可能，即学习新任务可以提高过去任务的表现，同时保护旧知识不被丢失。
此外，我们提出了一个与BERT各层并行的特定任务的低维残差函数，这使得IPRLS在学习新任务时不容易丢失保存在基础BERT网络中的知识。
INTRODUCTION随着网络上大量富含观点的文档的增加，人们对情感分类给予了极大的关注， ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/12/03/Continual-Learning-with-Hypernetworks/" title="Continual Learning with Hypernetworks">     <img class="post_bg" src="https://i.loli.net/2021/12/03/HXDsvWIFemhG37V.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Continual Learning with Hypernetworks"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/12/03/Continual-Learning-with-Hypernetworks/" title="Continual Learning with Hypernetworks">Continual Learning with Hypernetworks</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-12-03T12:18:00.000Z" title="发表于 2021-12-03 20:18:00">2021-12-03</time></span></div><div class="content">[TOC]
Continual Learning with Hypernetworks当人工神经网络在多个任务上进行顺序训练时，它们会遭受灾难性的遗忘。 为了克服这个问题，我们提出了一种基于任务条件超网络的新方法，即基于任务身份生成目标模型权重的网络。
由于一个简单的关键特征，此类模型的持续学习 (CL) 难度较小：任务条件超网络不需要回忆所有先前看到的数据的输入-输出关系，只需要排练特定于任务的权重实现，这可以 使用简单的正则化器在内存中维护。
除了在标准的CL基准上取得最先进的性能外，对长任务序列的额外实验显示，任务条件下的超网络显示出非常大的能力来保留以前的记忆。
值得注意的是，当可训练的超网络权重数量与目标网络大小相当或小于目标网络大小时，如此长的记忆寿命是在一个压缩制度下实现的。我们对低维任务嵌入空间（超网络的输入空间）的结构进行了深入研究，并表明任务条件下的超网络展示了迁移学习。最后，基于CIFAR-10/100图像数据集的挑战性CL基准的经验结果进一步支持了前向信息迁移。
INTRODUCTION我们假设一个具有可训练权重 $Θ$ 的神经网络 $f(x,Θ)$ 被赋予来自 ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/12/02/PromptBERT-Improving-BERT-Sentence-Embeddings-with-Prompts/" title="PromptBERT: Improving BERT Sentence Embeddings with Prompts">     <img class="post_bg" src="https://i.loli.net/2021/12/02/MVzbTiL5glWvtj4.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="PromptBERT: Improving BERT Sentence Embeddings with Prompts"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/12/02/PromptBERT-Improving-BERT-Sentence-Embeddings-with-Prompts/" title="PromptBERT: Improving BERT Sentence Embeddings with Prompts">PromptBERT: Improving BERT Sentence Embeddings with Prompts</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-12-02T12:31:29.000Z" title="发表于 2021-12-02 20:31:29">2021-12-02</time></span></div><div class="content">PromptBERT: Improving BERT Sentence Embeddings with Prompts在以前的工作中，原始的BERT在句子语义相似性方面的表现不佳，已经被广泛讨论。我们发现，不尽如人意的表现主要是由于静态 token 嵌入的偏差和无效的 BERT 层，而不是因为句子嵌入的高余弦相似度。
为此，我们提出了一种 prompt based 的句子嵌入方法，它可以减少 token 嵌入的偏差，使原来的BERT层更加有效。通过将句子嵌入任务重新表述为填空问题，我们的方法显著提高了原始BERT的性能。我们讨论了 prompt based 的句子嵌入的两种 prompt 表示方法和三种 prompt 搜索方法
此外，我们通过模板去噪技术提出了一个新的无监督训练目标，这大大缩短了有监督和无监督设置之间的性能差距。
在实验中，我们对我们的方法在非微调和微调的设置上进行评估。即使是一个非微调的方法，也可以在STS任务上超过微调的方法，如无监督的ConSERT。我们的微调方法在无监督和有监督的情况下都优于最先进的方法SimCSE。与SimCSE相比，在无监督设置下，我们在BE ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/11/30/Parameter-Efficient-Tuning-by-Manipulating-Hidden-States-of-Pretrained-Language-Models-For-Classification-Tasks/" title="Parameter-Efficient Tuning by Manipulating Hidden States of Pretrained Language Models For Classification Tasks">     <img class="post_bg" src="https://i.loli.net/2021/11/30/tzcKE3YZAfOvNkw.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Parameter-Efficient Tuning by Manipulating Hidden States of Pretrained Language Models For Classification Tasks"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/11/30/Parameter-Efficient-Tuning-by-Manipulating-Hidden-States-of-Pretrained-Language-Models-For-Classification-Tasks/" title="Parameter-Efficient Tuning by Manipulating Hidden States of Pretrained Language Models For Classification Tasks">Parameter-Efficient Tuning by Manipulating Hidden States of Pretrained Language Models For Classification Tasks</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-11-30T11:46:34.000Z" title="发表于 2021-11-30 19:46:34">2021-11-30</time></span></div><div class="content">Parameter-Efficient Tuning by Manipulating Hidden States of Pretrained Language Models For Classification TasksParameter-Efficient tuning旨在通过优化一些引入的参数的同时，冻结 PLMs 来提取下游任务的知识。
连续的 prompt tuning 在输入的嵌入中预先加入一些可训练向量。是其中的一种方法，由于其有效性和效率而受到广泛关注。这个系列的方法可以被理解为对PLM内部的隐藏状态进行了非线性转换。
然而，一个自然的问题被忽略了：隐藏状态能否直接用于分类而不改变它们？在本文中，我们旨在通过提出一种简单的 tuning 方法来回答这个问题，这种方法只引入了三个可训练的向量。
首先，我们使用引入的向量整合不同层的隐藏状态。然后，我们将整合后的隐藏状态输入到一个特定任务的线性分类器中，以预测类别。
这个方案类似于ELMo利用隐藏状态的方式，只是他们将隐藏状态反馈给基于LSTM的模型。 虽然我们提出的tuning 方案很简单，但它取得了与 P-tuning  ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/11/25/P-Tuning-v2-Prompt-Tuning-Can-BeComparable-to-Fine-tuning-Universally-Across-Scales-and-Tasks/" title="P-Tuning v2，Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks">     <img class="post_bg" src="https://i.loli.net/2021/11/23/cF1h7WGLEr6CegQ.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="P-Tuning v2，Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/11/25/P-Tuning-v2-Prompt-Tuning-Can-BeComparable-to-Fine-tuning-Universally-Across-Scales-and-Tasks/" title="P-Tuning v2，Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks">P-Tuning v2，Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-11-25T09:29:41.000Z" title="发表于 2021-11-25 17:29:41">2021-11-25</time></span></div><div class="content">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and TasksPrompt tuning 只用冻结的语言模型来调整连续的prompts，大大减少了训练时每个任务的存储和内存使用。但

prompt tuning 对于正常大小的预训练模型来说表现并不理想。

现有的 prompt tuning 方法不能处理困难的序列标记任务，缺乏普适性。


P-Tuning-v2 与微调的性能相匹配，并且只有0.1%-3%的调整参数。
Introduction微调，为目标任务更新整个模型参数集。虽然微调能获得良好的性能，但在训练过程中却很耗费内存，因为必须存储所有参数的梯度和优化器状态。
此外，微调需要在推理过程中为每个任务保留一份模型参数，这很不方便，因为预训练的模型通常很大。
Prompting 另一方面，冻结PLMs的所有参数并使用自然语言 prompts 来查询语言模型。例如，对于情感分析，我们可以将一个样本与一个 prompts 串联起来 “这部电影是[MASK]” ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/#content-inner">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/#content-inner">14</a><a class="extend next" rel="next" href="/page/3/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="https://i.loli.net/2021/03/22/YP2oqk7lOAfceTD.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Coding-Zuo</div><div class="author-info__description">我的建议是看开点、、、</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">138</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Coding-Zuo"><i class="fab fa-github"></i><span>Let's Github</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Coding-Zuo" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/zzuuoo666@sina.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">用力去思考🤔</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/02/21/%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0Active-Learning-%E5%9F%BA%E7%A1%80/" title="主动学习Active Learning 基础"><img src="https://s4.ax1x.com/2022/02/21/HX2g8e.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="主动学习Active Learning 基础"/></a><div class="content"><a class="title" href="/2022/02/21/%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0Active-Learning-%E5%9F%BA%E7%A1%80/" title="主动学习Active Learning 基础">主动学习Active Learning 基础</a><time datetime="2022-02-21T02:11:24.000Z" title="发表于 2022-02-21 10:11:24">2022-02-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/02/21/Prompt-tuning-%E5%9F%BA%E7%A1%80/" title="Prompt tuning 基础"><img src="https://s4.ax1x.com/2022/02/21/HX2kHP.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Prompt tuning 基础"/></a><div class="content"><a class="title" href="/2022/02/21/Prompt-tuning-%E5%9F%BA%E7%A1%80/" title="Prompt tuning 基础">Prompt tuning 基础</a><time datetime="2022-02-21T02:08:56.000Z" title="发表于 2022-02-21 10:08:56">2022-02-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/02/21/Auto-Encoding-Variational-Bayes/" title="Auto-Encoding Variational Bayes"><img src="https://s4.ax1x.com/2022/02/21/HXgjAK.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Auto-Encoding Variational Bayes"/></a><div class="content"><a class="title" href="/2022/02/21/Auto-Encoding-Variational-Bayes/" title="Auto-Encoding Variational Bayes">Auto-Encoding Variational Bayes</a><time datetime="2022-02-21T02:06:21.000Z" title="发表于 2022-02-21 10:06:21">2022-02-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/02/21/A-Survey-of-Active-Learning-for-Text-Classification-Using-Deep-Neural-Networks/" title="A Survey of Active Learning for Text Classification Using Deep Neural Networks"><img src="https://s4.ax1x.com/2022/02/21/HXgKw6.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="A Survey of Active Learning for Text Classification Using Deep Neural Networks"/></a><div class="content"><a class="title" href="/2022/02/21/A-Survey-of-Active-Learning-for-Text-Classification-Using-Deep-Neural-Networks/" title="A Survey of Active Learning for Text Classification Using Deep Neural Networks">A Survey of Active Learning for Text Classification Using Deep Neural Networks</a><time datetime="2022-02-21T02:03:15.000Z" title="发表于 2022-02-21 10:03:15">2022-02-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/02/21/ZeroPrompt-Scaling-Prompt-Based-Pretraining-to-1-000-Tasks-Improves-Zero-Shot-Generalization/" title="ZeroPrompt- Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization"><img src="https://s4.ax1x.com/2022/02/21/HXcFxA.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ZeroPrompt- Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization"/></a><div class="content"><a class="title" href="/2022/02/21/ZeroPrompt-Scaling-Prompt-Based-Pretraining-to-1-000-Tasks-Improves-Zero-Shot-Generalization/" title="ZeroPrompt- Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization">ZeroPrompt- Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization</a><time datetime="2022-02-21T01:56:43.000Z" title="发表于 2022-02-21 09:56:43">2022-02-21</time></div></div></div></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/Active-Learning/" style="font-size: 1.22em; color: #999ea4">Active Learning</a> <a href="/tags/Convex-optimization/" style="font-size: 1.1em; color: #999">Convex optimization</a> <a href="/tags/DP/" style="font-size: 1.26em; color: #999fa8">DP</a> <a href="/tags/DataGame/" style="font-size: 1.26em; color: #999fa8">DataGame</a> <a href="/tags/Docker/" style="font-size: 1.22em; color: #999ea4">Docker</a> <a href="/tags/GNN/" style="font-size: 1.34em; color: #99a3b0">GNN</a> <a href="/tags/GNN-cv/" style="font-size: 1.14em; color: #999b9d">GNN&cv</a> <a href="/tags/GNN-nlp/" style="font-size: 1.46em; color: #99a7bb">GNN&nlp</a> <a href="/tags/LeetCode/" style="font-size: 1.42em; color: #99a6b7">LeetCode</a> <a href="/tags/ML-DL/" style="font-size: 1.22em; color: #999ea4">ML&DL</a> <a href="/tags/NLP/" style="font-size: 1.1em; color: #999">NLP</a> <a href="/tags/context-detection/" style="font-size: 1.5em; color: #99a9bf">context detection</a> <a href="/tags/nlp/" style="font-size: 1.38em; color: #99a4b4">nlp</a> <a href="/tags/pytorch/" style="font-size: 1.1em; color: #999">pytorch</a> <a href="/tags/%E5%88%B7%E9%A2%98/" style="font-size: 1.3em; color: #99a1ac">刷题</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 1.14em; color: #999b9d">数据结构</a> <a href="/tags/%E9%85%8D%E7%BD%AE%E8%AE%B0%E5%BD%95/" style="font-size: 1.18em; color: #999ca1">配置记录</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/02/"><span class="card-archive-list-date">二月 2022</span><span class="card-archive-list-count">7</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/01/"><span class="card-archive-list-date">一月 2022</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/12/"><span class="card-archive-list-date">十二月 2021</span><span class="card-archive-list-count">8</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/11/"><span class="card-archive-list-date">十一月 2021</span><span class="card-archive-list-count">12</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/10/"><span class="card-archive-list-date">十月 2021</span><span class="card-archive-list-count">13</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/09/"><span class="card-archive-list-date">九月 2021</span><span class="card-archive-list-count">20</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/08/"><span class="card-archive-list-date">八月 2021</span><span class="card-archive-list-count">10</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/07/"><span class="card-archive-list-date">七月 2021</span><span class="card-archive-list-count">7</span></a></li></ul></div><div class="card-widget card-map"><div class="card-content"><div class="item-headline"><i class="fa fa-globe-asia" aria-hidden="true"></i><span>aside.card_map</span></div><script id="clstr_globe" type="text/javascript" defer="defer" src="//clustrmaps.com/globe.js?d=dw5ySwxQc209v5eDdMmMLhW4-aHUt-vE_XaC-6nRPAA"></script></div></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">138</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-03-01T06:43:27.632Z"></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Coding-Zuo</div><div class="footer_custom_text">Hi, welcome to my BLOG</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function subtitleType () {
  if (true) { 
    var typed = new Typed("#subtitle", {
      strings: "今日事&#44;今日畢,Never put off till tomorrow what you can do today".split(","),
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50
    })
  } else {
    document.getElementById("subtitle").innerHTML = '今日事&#44;今日畢'
  }
}

if (true) {
  if (typeof Typed === 'function') {
    subtitleType()
  } else {
    getScript('https://cdn.jsdelivr.net/npm/typed.js/lib/typed.min.js').then(subtitleType)
  }
} else {
  subtitleType()
}</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script></div><script src="/js/custom.js"></script><script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script src="//code.tidio.co/mak6nokafytw9mgrsuzglwzfxiy3fpdl.js" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>