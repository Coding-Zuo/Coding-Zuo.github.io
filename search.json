[{"title":"二叉树刷题1","url":"/2021/11/19/二叉树刷题1/","content":"\n# 二叉树刷题1\n\n## [102. 二叉树的层序遍历](https://leetcode-cn.com/problems/binary-tree-level-order-traversal/)\n\n```java\nclass Solution {\n    public List<List<Integer>> levelOrder(TreeNode root) {\n        List<List<Integer>> res = new ArrayList<>();\n        if(root == null) return res;\n\n        Queue<TreeNode> queue = new LinkedList<>();\n        queue.offer(root);\n        while(!queue.isEmpty()){\n            List<Integer> level = new ArrayList<>();\n            int currentLevelSize = queue.size();\n            for(int i=1; i<= currentLevelSize; ++i){\n                TreeNode node = queue.poll();\n                level.add(node.val);\n\n                if(node.left!=null){\n                    queue.offer(node.left);\n                }\n                if(node.right!=null){\n                    queue.offer(node.right);\n                }\n            }\n            res.add(level);\n        }\n        return res;\n    }\n}\n```\n\n## [107. 二叉树的层序遍历 II](https://leetcode-cn.com/problems/binary-tree-level-order-traversal-ii/)\n\n从根节点开始搜索，每次遍历同一层的全部节点，使用一个列表存储该层的节点值。\n\n如果要求从上到下输出每一层的节点值，做法很直观，在遍历完一层节点之后，将存储该层节点值的列表添加到结果列表的尾部。\n\n但这道题要求从下到上输出每一层，只要对上述操作稍作修改即可；在遍历完一层节点之后，将存储该节点值的列表添加到结果列表的头部。\n\n为了降低在结果列表的头部添加一层节点值的列表的时间复杂度，结果列表可以使用链表的结构，在链表头部添加一层节点值的列表的时间复杂度是 O(1)O(1)。在 Java 中，由于我们需要返回的 List 是一个接口，这里可以使用链表实现；而 C++ 或 Python 中，我们需要返回一个 vector 或 list，它不方便在头部插入元素（会增加时间开销），所以我们可以先用尾部插入的方法得到从上到下的层次遍历列表，然后再进行反转。\n\n```java\nclass Solution {\n    public List<List<Integer>> levelOrderBottom(TreeNode root) {\n        List<List<Integer>> res = new LinkedList<>();\n        if(root == null) return res;\n\n        Queue<TreeNode> queue = new LinkedList<>();\n        queue.offer(root);\n        while(!queue.isEmpty()){\n            List<Integer> level = new ArrayList<>();\n            int size = queue.size();\n            for(int i=0;i<size;i++){\n                TreeNode node = queue.poll();\n                level.add(node.val);\n                TreeNode left = node.left, right =node.right;\n                if(left!=null){\n                    queue.offer(left);\n                }\n                if(right!=null){\n                    queue.offer(right);\n                }\n            }\n            res.add(0, level);\n        }\n        \n        return res;\n    }\n    \n}\n```\n\n\n\n\n\n\n\n\n\n\n\n","tags":["LeetCode"]},{"title":"Towards a Unified View of Parameter-Efficient Transfer Learning","url":"/2021/11/17/Towards-a-Unified-View-of-Parameter-Efficient-Transfer-Learning/","content":"\n# Towards a Unified View of Parameter-Efficient Transfer Learning\n\nICLR2022高分文章\n\n这篇工作将最近提出的多种Parameter-Efficient的迁移学习方法联系在了一起，提出了一个统一的框架，并探索了这些方法成功的关键因素是什么。\n\n统一什么？把Adapter、prompt-tuning、LoRA都定义为预训练模型中添加可调整的特定的隐层状态，只是设计的参数维度、修改函数的计算和位置不同。定义成一个统一的框架，顺便还排列组合出几个小变体。\n\n## INTRODUCTION\n\n使通用PLM适应下游任务的最常见方法是微调所有模型参数。然而，这导致每个任务都有一份单独的微调模型参数，当为执行大量任务的模型提供服务时，其成本过高。\n\n为了缓解这个问题，已经提出了一些轻量级的替代方案，只更新少量的额外参数，同时保持大多数预训练参数的冻结，如：Adapters、prefix tuning 与 prompt tuning、LoRA 。（下文详细介绍他们）\n\n这些方法都在不同的任务集上表现出与完全微调相媲美的性能，通常是通过更新不到1%的原始模型参数。除了节省参数外，参数有效的调整使其有可能快速适应新的任务，而不会出现灾难性的遗忘（Pfeiffer等人，2021），并且在 out-of-distribution 上往往表现出卓越的稳健性。\n\n作者接下来针对上面这几种参数有效的方法提出了几个问题：\n\n- 这些方法是如何联系的？\n- 这些方法是否具有对其有效性至关重要的设计要素，这些要素是什么？\n- 每种方法的有效成分是否可以转移到其他方法中，以产生更有效的变体？\n\n## PRELIMINARIES\n\n首先看一下现有这些方法在Transformer里的结构是如何：\n\n![](https://i.loli.net/2021/11/20/BNFQM3GX546kyiL.png)\n\n- Adapters：在PLM的每一层插入称为适配器的小型神经模块，在微调时只对适配器进行训练。适配器层一般使用$W_{down}\\in R^{d×r}$的向下投影，将输入 $h$ 投影到瓶颈维度 $r$ 指定的低维空间，然后使用非线性激活函数 $f(\\cdot)$，再使用$W_{up}\\in R^{r×d}$的向上投影，还有一个残差连接。\n  $$\n  \\begin{equation}\\begin{split} \n   h \\leftarrow h + f(hW_{dnow})W_{up}\n      \\end{split}\\end{equation}\n  $$\n  将两个适配器依次放在变压器的一个层内，一个在多头关注之后，一个在FFN子层之后。\n\n- prefix tuning 与 prompt tuning ：受通prompt方法的启发，在输入层或隐藏层中预置了额外的 $l$ 个可调整的前缀tokens，在下游任务的微调时只训练这些 soft prompt。具体来说，两组prefix 向量 $P_k , P_v\\in R^{l×d}$ 与原始键 $K$ 和值$V$相连接，如图中所示。然后对新的 prefixed key 和值进行多头注意力计算：\n  $$\n  \\begin{equation}\\begin{split} \n   head_i = Attn(x W_{q}^{(i)} , concat(P_k^{(i)}, CW_{k}^{(i)}), concat(P_v^{(i)}, CW_v^{(i)}))\n      \\end{split}\\end{equation}\n  $$\n  这其实也于Graphormer等Graph Transformer模型有异曲同工之妙。$P_k$ 和 $P_v$ 分别被分成 $N_h$个头部向量。Prompt-tuning 简化了前缀调整，其只对第一层的输入词嵌入进行预处理；类似工作还包括P-tuning。\n\n- LoRA ：将可训练的低秩矩阵注入 transformer 层，以近似权重更新。对于一个预训练好的权重矩阵 $W\\in R^{d×k}$ LoRA用低秩分解 $W +\\Delta W = W +W_{down}W_{up}$ 表示其更新，其中$W_{down}\\in R^{d×r},W_{up}\\in R^{r×k} $ 是可调整的参数。LoRA将这种更新应用于多头注意子层中的 Query 和 Key 投影矩阵，如图1所示。对于多头注意力中的线性投影的特定输入$x$ ，LoRA将投影输出 $h$ 修改为:\n  $$\n  \\begin{equation}\\begin{split} \n   h \\leftarrow  h + s \\cdot x W_{down}W_{up}\n      \\end{split}\\end{equation}\n  $$\n  其中 $s≥1$ 是可调标量超参数。\n\n  \n\n其实还有一些参数有效的调整方法像：BitFit 只对预训练模型中的 bias 向量进行微调，以及上一篇文章提到的diff-pruning，它学习一个稀疏的参数更新向量。\n\n## 推导 prefix tuning \n\n上文关于 prefix tuning  在注意力 K 和 V上添加可学习的向量来改变注意力模块，这里提出另一种观点：\n$$\n\\begin{equation}\\begin{split} \n head &=Attn(x W_{q} , concat(P_k, CW_{k}, concat(P_v, CW_v) \\\\\n &= softmax(x W_q concat(P_k, CW_k)^T) \\begin{bmatrix} P_v \\\\ CW_v\\\\ \\end{bmatrix} \\\\\n &= (1-\\lambda(x)) softmax(xW_qW_k^TC^T) CW_v + \\lambda(x)softmax(xW_qP_k^T) P_v\\\\\n& = (1 - \\lambda(x)) \\underbrace{ \\text{Attn}(xW_q, CW_k, CW_v) }_{\\text{standard attention}} + \\lambda(x) \\underbrace{ \\text{Attn}(xW_q, P_k, P_v) }_{\\text{independent of } C},\n    \\end{split}\\end{equation}\n$$\n其中 $λ(X)$ 是标量，归一化注意力权重之和：\n$$\n\\begin{equation}\\begin{split} \n\\lambda(x) = \\frac{\\sum_i\\exp (xW_qP_k^T)_i}{\\sum_i \\exp (xW_qP_k^T)_i + \\sum_j \\exp(xW_qW_k^TC^T)_j}.\n    \\end{split}\\end{equation}\n$$\n\n\n## THE UNIFIED FRAMEWORK\n\n受 prefix tuning   和  Adapter 之间联系的启发，作者提出了一个总体框架，旨在统一几种最先进的参数有效的调谐方法。\n\n具体来说，作者把它们看作是学习一个向量 $∆h$，它被应用于各种隐藏表征。形式上，作者把要直接修改的隐藏表征表示为 $h$ ，把计算 $h$ 的PLM子模块的直接输入表示为 $x$。\n\n为了描述这个修改过程，作者定义了一组设计维度，不同的方法可以通过改变这些维度的值而被实例化。并在表1中说明了Adapters、prefix tuning 和LoRA在这些维度上的情况。\n\n![](https://i.loli.net/2021/11/20/45RtsBp9aCEZQlg.png)\n\n- 表中的 Functional Form :是指计算 $∆h$ 的具体函数。所有这些方法的函数形式都类似于proj down → nonlinear → proj up的架构。\n- Modified Representation : 指直接修改的隐藏表示形式。\n- Insertion Form : 指添加的模块如何插入到网络中。传统上适配器是以 sequential 方式插入某个位置的，其中输入和输出都是 $h$ 。prefix tuning和LoRA 相当于 parallel 插入。\n- Composition Function :指修改后的向量 $∆h$ 如何与原始隐藏表征 $h$ 计算，以形成新的隐藏表征。例如，适配器执行简单的加法组合，前缀调整使用门控加法组合，而LoRA通过一个恒定的因子对 $Δh$ 进行缩放，并将其添加到原始隐藏表示中。\n\n## 变体组合——通过在不同的方法之间转移设计元素而得到\n\n![](https://i.loli.net/2021/11/20/VpIxoeY5yRLZiOF.png)\n\n- Parallel Adapter 是通过将 prefix tuning 的 parallel 插入转移到 Adapter 的变体。\n- Multi-head Parallel Adapter 是使 Adapter 与 prefix tuning 更加相似的进一步措施：应用 Parallel Adapter 来修改头部注意力输出作为 prefix tuning 。这样，变体通过利用多头投影来提高能力\n- Scaled Parallel Adapter 是通过将LoRA的组成和插入形式转移到适配器的变体，如图3e所示。\n\n## EXPERIMENTS\n\n下图说明，在数据较为充沛、比较有挑战的任务中，现有的方法 距离 Full FIne-tuning 还有一定差距\n\n![](https://i.loli.net/2021/11/20/4kcyzS9rXLFgbNI.png)\n\n###  SEQUENTIAL OR PARALLEL?\n\nParallel 和  sequential 哪个方式好些？\n\n![](https://i.loli.net/2021/11/20/Kxwtri6pOHbU4SG.png)\n\n Parallel Adapter在所有情况下都能够击败 Sequential Adapter\n\n###  WHICH MODIFIED REPRESENTATION – ATTENTION OR FFN?\n\n适配修改放在Transformer哪里比较好？\n\n![](https://i.loli.net/2021/11/20/hlqXwRJOBMDVsWv.png)\n\n![](https://i.loli.net/2021/11/20/j7kGyYfVLsQRADE.png)\n\n### 哪个 COMPOSITION FUNCTION 比较好\n\n![](https://i.loli.net/2021/11/20/BGhesmZN2tREPAr.png)\n\n简单composition（Adapter）、门控composition（ prefix tuning ）和缩放composition（LoRA）。\n\n缩放的 composition 函数，同时也很容易适用。\n\n## 总结\n\n(1) Scaled parallel adapter 是修改FFN的最佳变体\n\n(2) FFN可以在更大的容量下更好地利用修改\n\n(3) 像 prefix tuning 这样修改头部注意力可以在只有0.1%的参数下实现强大的性能。\n\n","tags":["context detection"]},{"title":"Generalizing to Unseen Domains: A Survey on Domain Generalization","url":"/2021/11/14/Generalizing-to-Unseen-Domains-A-Survey-on-Domain-Generalization/","content":"\n# Generalizing to Unseen Domains: A Survey on Domain Generalization\n\n机器学习系统通常假定训练和测试分布是相同的。为此，一个关键的要求是开发能够泛化到未见过的分布的模型。近年来，Domain generalization 领域泛化（DG），即 out-of- distribution 泛化，吸引了越来越多的兴趣。\n\n领域泛化处理的是一个具有挑战性的环境，即给定一个或几个不同但相关的领域，目标是学习一个能够泛化到未见过的测试领域的模型。\n\n本文首次对该领域的最新进展进行了回顾。\n\n- 首先，提供了一个领域泛化的正式定义，并讨论了几个相关的领域。\n- 然后，彻底回顾了与领域泛化相关的理论，并仔细分析了泛化背后的理论。我们将最近的算法分为三类：数据操作、表征学习和学习策略，并对每一类算法详细介绍了几种流行的算法。\n- 第三，介绍了常用的数据集和应用。\n- 最后，我们总结了现有的文献并提出了一些未来的潜在研究课题。\n\n## INTRODUCTION\n\n传统的ML模型是基于 i.i.d. 假设进行训练的，即训练和测试数据是完全独立分布的。然而，这一假设在现实中并不总是成立。当训练数据和测试数据的概率分布不同时，由于领域分布的差距，ML模型的性能往往会恶化。收集所有可能领域的数据来训练ML模型是昂贵的，甚至是不可能的。因此，提高ML模型的泛化能力在工业和学术领域都很重要。\n\n有许多与泛化相关的研究课题，如领域适应、元学习、迁移学习、covariate shift等等。近年来，领域泛化（DG）受到广泛关注。\n\n![](https://i.loli.net/2021/11/14/CzTe6qKXc9StOPm.png)\n\n如图1所示，领域泛化的目标是从一个或几个不同但相关的领域（即不同的训练数据集）中学习一个模型，该模型将在未见过的测试领域中具有良好的泛化能力。\n\n> 例如，给定一个由素描、卡通图像和绘画组成的训练集，领域泛化要求训练一个好的机器学习模型，该模型在对来自自然图像或照片的分类中具有最小的预测误差，这些图像显然与训练集中的图像具有不同的分布。\n>\n> 在过去的几年里，领域泛化在计算机视觉和自然语言处理等各个领域都取得了重大进展。尽管取得了这些进展，但在这一领域还没有一份全面介绍和总结其主要思想、学习算法和其他相关问题的调查报告，以提供对未来的研究见解。\n>\n> 在本文中，我们提出了第一份关于领域泛化的调查报告，介绍了它的最新进展，特别关注它的公式、理论、算法、数据集、应用和未来研究方向。我们希望这个调查能够为感兴趣的研究者提供一个全面的回顾，并激发在这个领域和相关领域的更多研究。\n\n- 提出了关于领域泛化和相关领域适应的理论分析。\n\n- 通过增加新的类别：特征分解的生成模型、不变的风险最小化、基于梯度运算的方法和其他学习策略来全面总结这些泛化方法。\n\n- 对于所有的类别，我们通过包括更多的相关算法、比较和讨论，扩大了对不同类别方法的分析。\n\n- 扩展了数据集和应用的范围，同时我们也探索了领域通用的评价标准。\n\n本文的组织结构如下。在第二节中提出了领域泛化的问题并讨论了它与现有研究领域的关系。第三节介绍了领域泛化的相关理论。在第四节中，我们详细描述了一些有代表性的DG方法。第五节介绍了应用，第六节介绍了DG的基准数据集。我们在第七节中总结了现有工作的启示，并提出了一些可能的未来方向。最后，我们在第八节中对本文进行总结。\n\n## BACKGROUND\n\n### *A. Formalization of domain generalization*\n\n在本节中，我们介绍本文中使用的符号和定义。\n\n**Definition 1 (Domain) :** X 表示一个非空的输入空间，Y 表示一个输出空间。一个域是由从分布中取样的数据组成的。我们把它表示为 $S = \\{(x_i, y_i)\\}^n_{i=1} \\sim P_{XY}$ , 其中 $x\\in X \\subset R^d, y\\in Y \\subset R$ 表示标签，而 $P_{XY}$ 表示输入样本和输出标签的联合分布。X、Y表示相应的随机变量。\n\n**Definition 2 (Domain generalization) :**\n\n![](https://i.loli.net/2021/11/14/QGw1vmbkEZcjSHD.png)\n\n在领域泛化中，我们得到了M个训练（源）的领域 $S_{train} = \\{S^i| i = 1,...,M\\}$ 其中 $S^i=\\{(x^i_j,y^i_j)\\}_{j=1}^{n_i}$ 定义为第 $i$ 个域。每对域的联合分布是不同的：$P_{XY}^i \\neq P_{XY}^j, 1\\le i \\neq j \\le M$ 。域泛化的目标是在 $M$ 个训练域中学习一个稳健的、可泛化的预测函数 $h : X → Y$，以实现对未见过的测试域 $S_{test}$ 的最小预测误差（即在训练中不能访问 $S_{test}$，$P^{test}_{XY}\\neq P^i_{XY}  i∈\\{1,...,M\\}$）:\n$$\nmin_h E_{(x,y)\\in S_{test}} [l(h(x),y)]\n$$\n其中 $E$ 是期望， $l(\\cdot,\\cdot)$ 是loss 。我们在表1中列出了常用的记号。\n\n![](https://i.loli.net/2021/11/14/FiE4rGMZsNjq6pb.png)\n\n\n\n### *B. Related research areas*\n\n有几个研究领域与领域泛化密切相关，包括但不限于：迁移学习、领域适应、多任务学习、多领域学习、元学习、终身学习和 zero-shot 学习。我们在表二中总结了它们与领域泛化的区别，并在下文中简要介绍了它们。\n\n![](https://i.loli.net/2021/11/14/MPWAwsXG1t7dBH5.png)\n\n- Multi-task learning ：联合优化了几个相关任务的模型。通过在这些任务之间共享表征，我们可以使模型在原来的任务上有更好的泛化能力。请注意，多任务学习的目的不是为了加强对新的（未见过的）任务的泛化。特别是，多领域学习是一种多任务学习，它在多个相关领域进行训练，为每个原始领域而不是新的测试领域学习好的模型。\n- Transfer learning：在一个源任务上训练一个模型，旨在提高该模型在不同但相关的目标领域/任务上的性能。预训练-微调是转移学习的常用策略，在这种情况下，源域和目标域有不同的任务，目标域在训练中被访问。在DG中，目标域不能被访问，训练和测试任务往往是相同的，而它们的分布是不同的。\n- Domain adaptation（DA）：在最近几年也很流行。DA的目的是利用现有的训练源域在给定的目标域上实现性能最大化。DA和DG的区别在于，DA可以接触到目标域的数据，而DG在训练过程中无法看到这些数据。这使得DG比DA更具挑战性，但在实际应用中更现实和有利。\n- Meta-learning ：旨在通过学习以前的经验或任务来学习学习者本身，即学会学习。虽然元学习中的学习任务是不同的，但在领域泛化中的学习任务是相同的。元学习是一种通用的学习策略，可以用于DG，通过模拟训练领域中的元训练和元测试任务来提高DG的性能。\n- 终身学习，或持续学习：关注的是多个连续领域/任务之间的学习能力。它要求模型通过容纳新的知识，同时保留以前学到的经验，随着时间的推移不断地学习。这也与DG不同，因为它可以在每个时间步骤中访问目标域，而且它没有明确处理跨域的不同分布。\n- Zero-shot learning：旨在从已看到的类别中学习模型，并对在训练中未看到的类别的样本进行分类。与此相反，一般来说，领域概括研究的问题是训练和测试数据来自相同的类别，但分布不同。\n\n此外，领域泛化还与 tributionally robust optimization（DRO）有关，其目标是在最坏的分布情况下学习一个模型，希望它能够很好地泛化到测试数据。DRO关注的是优化过程，在领域泛化研究中也可以利用。此外，DG也可以通过数据操作或表征学习方法来完成，这与DRO方法不同。\n\n## THEORY\n\n在本节中，我们将回顾一些与领域泛化相关的理论。由于领域适应与领域泛化密切相关，我们从领域适应的理论开始。\n\n### *A. Domain adaptation*\n\n对于一个二元分类问题，我们把源域上的真实标签函数表示为 $h^{∗s} : X → [0,1]$，目标域上的真标签函数为 $h^{∗t}$。让 $h:X →[0,1]$ 是来自假设空间 $H$ 的任何分类器。然后，源域上两个分类器 $h$ 和 $h'$之间的分类误差可以通过以下方式测量 :\n$$\n\\epsilon^s(h,h') = E_{x\\sim P^s_X} [h(x) \\neq h'(x)] = E_{x\\sim P^s_X} [|h(x) - h'(x)|]\n$$\n同样地，我们可以定义 $ε^t$，当取 $x∼P^t_X$ 的期望时。定义 $\\epsilon^s(h) := \\epsilon^s(h,h^{*s})$ 并且 $\\epsilon^t(h):=\\epsilon^t(h,h^{*t})$ 作为分类器 $h$ 在源域和目标域上的风险。\n\nDG/DA 的目标是使目标风险 $ε^t(h)$ 最小。但由于我们没有任何关于 $h^{∗t}$ 的信息，所以不容易达到这个目标。因此，人们试图用可操作的源风险 $ε^s(h)$来约束目标风险 $ε^t(h)$。 Ben-David等人给出两种风险的界限：\n$$\n\\epsilon^t(h) \\leq \\epsilon^s(h) +2d_1(P_X^s,P_X^t)+min_{P_{X}\\in \\{P^s_X,P^t_X\\}} E_{x\\sim P_X}[|h^{*s}(x) - h^{*t}(x)|]\n$$\n其中 $d_1 (P_X^s , P_X^t ) := sup_{A∈X} |P_X^s [A] - P_X^t [A]| $ 是两个分布之间的总变化，$\\mathbf{X}$ 表示 $X$ 上的 σ场。r.h.s 上的第二项衡量的是两个领域中分布的差异，第三项代表的是标签函数的差异。\n\n然而，总变化是一个强距离（即，它倾向于非常大），可能会使约束（1）松动，并且很难使用有限样本来估计。为了解决这个问题，Ben-David等人开发了另一个约束 ：\n$$\n\\epsilon^t(h) \\le \\epsilon^s(h) + d_{H\\Delta H}(P^s_X,P^t_X) + \\lambda_H\n$$\n其中，H∆H-divergence 定义为 $d_{H∆H}(P_X^s , P_X^t ) := sup_{h,h'∈H} |ε^s (h, h' ) - ε^t (h, h')|$，取代总变异d1来衡量分布差异，理想联合风险 $λ_H : = inf_{h∈H} [ε^s (h) + ε^t (h)]$ 衡量 $H$ 在两个领域的预测任务中的复杂性。\n\n### *B. Do main generalization*\n\n- 数据操作，指的是通过对数据的增强和变化使训练数据得到增强。这一类包括数据增强和数据生成两大部分。\n- 表征学习，指的是学习领域不变特征（Domain-invariant representation learning）以使得模型对不同领域都能进行很好地适配。领域不变特征学习方面主要包括四大部分：核方法、显式特征对齐、领域对抗训练、以及不变风险最小化（Invariant Risk Minimiation, IRM）。特征解耦与领域不变特征学习的目标一致、但学习方法不一致，我们将其单独作为一大类进行介绍。\n- 学习策略，指的是将机器学习中成熟的学习模式引入多领域训练中使得模型泛化性更强。这一部分主要包括基于集成学习和元学习的方法。同时，我们还会介绍其他方法，例如自监督方法在领域泛化中的应用。\n\n\n\n## METHODOLOGY\n\n![](https://i.loli.net/2021/11/14/o2gSpyE89cRQkd5.png)\n\n\n\n## Future research challenges\n\n总结了未来在领域泛化方面的一些研究挑战。\n\n1）***Continuous domain generalization* 连续的领域泛化**。在许多实际应用中，系统消耗的是具有非平稳统计数据的流媒体数据。在这种情况下，进行连续的领域泛化是非常重要的，它可以有效地更新DG模型以克服灾难性的遗忘并适应新的数据。虽然有一些专注于连续学习的领域适应方法[166]，但只有很少的关于连续DG的调查[167]，而这在实际场景中是有利的。\n\n2）**对新类别的领域泛化**。现有的DG算法通常假定不同领域的标签空间是相同的。一个更实际、更普遍的设定是支持对新类别的泛化，也就是领域和任务的泛化。这在概念上类似于元学习和 zero-shot 学习的目标。一些工作[65,168]提出了  zero-shot DG，我们期望在这个领域有更多的工作。\n\n3）**可解释的领域概括**。基于Disentanglement的DG方法将一个特征分解为领域不变/共享和领域特定的部分，为DG提供一些解释。对于其他类别的方法，目前仍然缺乏对DG模型中学习到的特征的语义或特征的深入理解。因果关系[103]可能是理解领域泛化网络并提供解释的一个有前途的工具。\n\n4）**大规模预训练/自学和DG**：近年来，我们见证了大规模预训练/自学的快速发展，如BERT[169]、GPT- 3[170]和Wav2vec[171]。在大规模数据集上进行预训练，然后根据下游任务对模型进行微调，可以提高其性能，其中预训练有利于学习一般表征。因此，如何设计有用和高效的DG方法来帮助大规模的预训练/自我学习是值得研究的。\n\n5）**DG的性能评估**：最近的工作[71]指出，在几个数据集上，一些DG方法的性能几乎与基线方法（即经验风险最小化）相同。我们不认为这就是DG在实际应用中没有用处的全部证据。相反，我们认为这可能是由于现在使用的评估方案不合适，或者是领域的差距没有那么大。在更现实的情况下，比如存在明显领域差距的人际关系[63]，DG的改进是巨大的。因此，我们对DG的价值保持肯定，并希望研究人员也能找到更合适的设置和数据集进行研究。\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"EANN: Event Adversarial Neural Networks for Multi-Modal Fake News Detection","url":"/2021/11/12/EANN-Event-Adversarial-Neural-Networks-for-Multi-Modal-Fake-News-Detection/","content":"\n# EANN: Event Adversarial Neural Networks for Multi-Modal Fake News Detection\n\n社交媒体上的假新闻检测的独特挑战之一是如何识别新出现的事件的假新闻。\n\n大多数现有的方法很难应对这一挑战，因为它们倾向于学习特定于事件的特征，这些特征不能迁移到看不见的事件。\n\n事件对抗神经网络(EANN)，它可以提取事件不变的特征，从而有利于对新到达的事件进行假新闻的检测。包括三个主要部分：\n\n- 多模态特征提取器：负责从帖子中提取文本和视觉特征\n- 假新闻检测器：学习用于检测假新闻的可判别表示\n- 事件鉴别器：去除事件的特定特征，并保留事件间的共享特征\n\n## Introduction\n\n最近，社交媒体的激增大大改变了人们获取信息的方式。如今，通过社交媒体消费新闻的人越来越多，社交媒体可以为世界各地发生的事件提供及时、全面的多媒体信息。与传统的文字新闻相比，带有图片和视频的新闻可以提供更好的故事性，吸引更多读者的关注。不幸的是，这也被假新闻所利用，它们通常包含错误的甚至是伪造的图片，以误导读者并获得快速传播。\n\n假新闻的传播可能造成大规模的负面影响，有时会影响甚至操纵重要的公共事件。例如，在2016年美国总统大选的最后三个月内，为支持两位提名人中的任何一位而产生的假新闻被很多人相信，在Facebook上的分享次数超过3700万次。因此，非常需要一个自动检测器来减轻假新闻造成的严重负面影响。\n\n到目前为止，各种假新闻检测方法，包括传统学习[6, 15, 29]和基于深度学习的模型[21, 25]，都被利用来识别假新闻。在对不同事件进行充分验证的情况下，现有的深度学习模型由于其卓越的特征提取能力，已经取得了比传统模型更好的性能。**然而，它们仍然无法处理假新闻检测的独特挑战，即检测新出现的和时间关键的事件上的假新闻[27]**。由**于缺乏相应的先验知识，关于这类事件的经过验证的帖子很难及时获得，这导致现有模型的性能不尽人意。**事实上，**现有的模型倾向于捕捉许多事件的特定特征，这些特征在不同的事件中并不共享。**这些**特定的事件特征，虽然能够帮助对已验证的事件进行分类，但会影响对新出现的事件的检测**。出于这个原因，我们**认为学习所有事件中的共享特征将有助于我们从未经核实的帖子中检测出假新闻，而不是捕捉事件的具体特征**。因此，这项工作的目标是设计一个有效的模型，**去除不可转移的特定事件特征，保留所有事件中的共享特征**，以完成识别假新闻的任务。\n\n**要删除事件的具体特征，第一步是要识别它们。对于不同事件的帖子，它们有自己独特的或特定的特征，是不可共享的。这种特征可以通过测量对应于不同事件的帖子之间的差异来检测。**在这里，**帖子可以用学到的特征来表示。因此，识别事件的特定特征等同于测量不同事件的学习特征之间的差异。**然而，这是一个在技术上具有挑战性的问题。首先，由于帖子的学习特征表示是高维的，像平方误差这样的简单指标可能无法估计这种复杂特征表示之间的差异。其次，在训练阶段，特征表示不断变化。这就要求所提出的测量机制能够捕捉到特征表征的变化并持续提供准确的测量。尽管这非常具有挑战性，但**有效估计不同事件上所学特征的差异性是去除事件特定特征的前提。**因此，如何在这种条件下有效地估计异同性是我们必须解决的挑战。\n\n为了应对上述挑战，提出了一个端到端的框架，称为事件对抗神经网络（EANN），用于基于多模态特征的假新闻检测。受对抗网络的启发，我们在训练阶段加入了**事件判别器来预测事件的辅助标签，而相应的损失可以用来估计不同事件之间特征表示的不相似性。损失越大，不相似性越低。**由于假新闻利用多媒体内容来误导读者并得到传播，我们的模型需要处理多模态的输入。多模态特征表示仍然高度依赖于数据集中的特定事件，不能很好地泛化为识别新来事件的假新闻。\n\n受到对抗网络的启发。现有的对抗网络通常用于生成能够与观察到的样本相匹配的图像，通过最小化博弈框架。对抗性学习框架已被用于一些任务，如半监督学习的表征[23]、鉴别性的图像特征[20]和领域适应[8, 9]。\n\n**模型还在事件判别器和多模态特征提取器之间建立了一个最小化博弈。特别是，多模态特征提取器被强制要求学习一个事件不变的表征来欺骗判别器。通过这种方式，它消除了对所收集的数据集中特定事件的严格依赖，并对未见过的事件实现了更好的概括能力。**\n\n\n\n## METHODOLOGY\n\n![](https://i.loli.net/2021/11/12/wZ1o2DQRTXMnylm.png)\n\n目标是为假新闻检测学习可迁移和可鉴别的特征表示。假新闻检测器和事件判别器都是建立在多模式特征提取器之上的。\n\n- 假新闻检测器将学到的特征表示作为输入，预测帖子是假的还是真的。\n- 事件判别器根据这个潜在的表征来识别每个帖子的事件标签。\n\n### Multi-Modal Feature Extractor\n\n文本表征 $R_T$  和 视觉表征 $R_V$， 多模态特征为：$R_F = R_T \\oplus R_V \\in R^{2p}$\n\n### Fake News Detector\n\n它部署了一个带有softmax的全连接层来预测帖子是假的还是真的。假新闻检测器是建立在多模态特征提取器之上的，因此将多模态特征表 $R_F$ 作为输入。我们将假新闻检测器表示为 $G_d(\\cdot;θ_d)$，其中 $θ_d$ 代表所有包含的参数。对于第 $i$ 个多媒体帖子，假新闻检测器的输出表示为 $m_i$，是这个帖子是假的概率:\n$$\nP_{\\theta}(m_i) = G_d(G_f(m_i;\\theta_f); \\theta_d)\n$$\n假新闻检测器的目标是识别一个特定的帖子是否是假新闻。我们使用 $Y_d$ 来表示标签集，并采用交叉熵来计算检测损失。\n$$\nL_d(\\theta_f,\\theta_d) = -E_{(m,y)\\sim(M,Y_d)}[ylog(P_{\\theta}(m))+ (1-y)(log(1-P_{\\theta}(m)))]\n$$\n我们通过寻求最佳参数 $\\hat θ_f$和 $\\hat θ_d $ 来最小化检测损失函数，这个过程可以表示为：\n$$\n(\\hat\\theta_f,\\hat\\theta_d) = argmin_{\\theta_f,\\theta_d} L_d(\\theta_f,\\theta_d)\n$$\n如前所述，**假新闻检测的主要挑战之一是训练数据集没有涵盖的事件**。**这就要求我们能够为新出现的事件学习可迁移的特征表示**。**检测损失的直接最小化只有助于检测训练数据集中所包含的事件的假新闻，因为这只捕捉到了特定事件的知识（如关键词）或模式，不能很好地进行推广。**\n\n因此，我们需要使模型能够学习更多的一般特征表示，**以捕捉所有事件中的共同特征**。这样的表征应该是事件不变的，不包括任何事件的特定特征。**为了实现这一目标，我们需要消除每个事件的独特性**。特别是，**我们测量不同事件中特征表征的不相似性，并将其去除，以捕获事件不变的特征表征。**\n\n### Event Discriminator\n\n事件判别器是一个神经网络，由两个具有相应激活函数的全连接层组成。它的**目的是根据多模态特征表示，将帖子正确地分类为K个事件之一**。我们将事件判别器表示为 $G_e (R_F; θ_e)$，其中 $θ_e$ 代表其参数。我们用交叉熵来定义事件判别器的损失，并使用 $Y_e$ 来表示事件标签的集合:\n$$\nL_e(\\theta_f, \\theta_e) = - E_{(m,y)\\sim (M,Y_e)} [\\sum_{k=1}^K 1_{[k=y]} log(G_e(G_f(m;\\theta_f)); \\theta_e)]\n$$\n损失最小化的事件判别器的参数 $L_e (-, -)$被写成:\n$$\n\\hat \\theta_e = argmin_{\\theta_e} L_e(\\theta_f,\\theta_e)\n$$\n上述损失 $L_e (θ_f , \\hat θ_e )$**可以用来估计不同事件分布的不相似性**。**大的损失意味着不同事件的分布表征是相似的**，而且学到的特征是事件不变量。因此，为了消除每个事件的唯一性，我们需要通过寻求最佳参数来最大化判别损失。\n\n上述想法激发了多模态特征提取器和事件判别器之间的最小值博弈。\n\n**一方面，多模态特征提取器试图愚弄事件判别器，使判别损失最大化；另一方面，事件判别器旨在发现包含在特征表示中的事件特定信息，以识别事件。**下一小节将介绍三个部分的整合过程和最终的目标函数。\n\n### Model Integration\n\n在训练阶段，多模态特征提取器 $G_f(-;θ_f)$ 需要与假新闻检测器 $G_d(-;θ_d)$ 合作，使检测损失 $L_d(θ_f , θ_d)$ 最小，从而提高假新闻检测任务的性能。同时，多模态特征提取器 $G_f (-;θ_f)$试图欺骗事件判别器 $G_e (-; θˆe)$，通过最大化事件鉴别损失 $L_e (θ_f, θ_e)$ 来实现事件不变的表示。事件判别器 $G_e (R_F;θ_e)$试图通过最小化事件判别损失来识别基于多模态特征表示的每个事件。我们可以将这个三人游戏的最终损失定义为:\n$$\nL_{final} (\\theta_f,\\theta_d,\\theta_e) = L_d(\\theta_f, \\theta_d) - \\lambda L_e(\\theta_f,\\theta_e)\n$$\n其中，$λ$ 控制着 【假新闻检测】 和 【事件识别】 的目标函数之间的权衡。在本文中，我们简单地将 $λ$ 设置为1，而不对权衡参数进行调整。对于最小化博弈，我们寻求的参数集是最终目标函数的鞍点:\n$$\n(\\hat \\theta_f,\\hat \\theta_d) = argmin_{\\theta_f,\\theta_d} L_{final} (\\theta_f,\\theta_d,\\hat \\theta_e)\n$$\n\n$$\n\\hat \\theta_e = argmax_{\\theta_e} L_{final} (\\hat \\theta_f,\\theta_e)\n$$\n\n我们使用随机梯度下降法来解决上述问题。$θ_f$ 根据\n$$\n\\theta_f \\leftarrow \\theta_f - \\eta (\\frac{\\partial L_d}{\\partial \\theta_f} - \\lambda \\frac{\\partial L_e}{\\partial \\theta_f})\n$$\n\n\n进行更新。这里我们采用[8]中介绍的梯度反转层（GRL）。梯度反转层在前向阶段作为一个识别函数，它将梯度与 $-λ$ 相乘，并在反推阶段将结果传递给前层。GRL可以很容易地加在 多模态特征提取器和事件判别器 之间。\n\n为了稳定训练过程，我们采用了[8]中的方法来衰减学习率η。\n$$\n\\eta'=\\frac{\\eta}{(1+\\alpha\\cdot p)^{\\beta}}\n$$\n其中 $α=10，β=0.75$，p从0到1线性变化，对应于训练进度。所提出的事件对抗性神经网络（EANN）的详细步骤在算法1中进行了总结。\n\n![](https://i.loli.net/2021/11/12/XNKMlT6p2G8ngQE.png)\n\n## EXPERIMENTS\n\n\n\n![](https://i.loli.net/2021/11/12/PUrRayJ4wCjKsTQ.png)\n\n为了进一步分析事件判别器的有效性，我们将EANN-和EANN在微博测试集上用t-SNE[22]学习的文本特征RT定性为图4所示。每个帖子的标签是真实或虚假的。\n从图4中，我们可以观察到，对于EANN-的方法，它可以学习到可分辨的特征 ，但学到的特征仍然是扭曲在一起的，特别是对于图4a的左边部分。相比之下，EANN模型学习到的特征表征更具可辨识性，而且在图4b所示的不同标签的样本之间有更大的隔离区域。这是因为在训练阶段，事件判别器试图消除特征表征与特定事件之间的依赖关系。在最小化博弈的帮助下，多模态特征提取器可以针对不同的事件学习不变的特征表征，并获得更强大的转移能力来检测新事件的假新闻。EANN-和EANN的比较证明，所提出的方法在事件判别器的作用下可以学习到更好的特征表征，从而取得更好的性能。\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer","url":"/2021/11/09/SPoT-Better-Frozen-Model-Adaptation-through-Soft-Prompt-Transfer/","content":"\n# SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer\n\n在 《The Power of Scale for Parameter-Efficient Prompt Tuning》的 PROMPTTUNING 方法（学习特定任务的软 prompt，以调节冻结的语言模型来执行下游任务）的基础上，提出了一种新的基于Prompt 的迁移学习方法，称为SPOT：Soft Prompt Transfer。\n\nSPOT首先在一个或多个源任务上学习 prompt，然后用它来初始化目标任务的 prompt。\n\n更重要的是，SPOT 大于等于 model-tuning ，同时参数效率更高（最多可减少27,000倍的特定任务参数）。\n\n进一步对26个NLP任务和160个源-目标任务的组合进行了大规模的任务迁移性研究，并证明了多任务往往可以通过 Prompt Transfer 而相互受益。\n\n最后，提出了一种简单有效的检索方法，将任务 prompts 解释为任务 embeddings，以识别任务之间的相似性，并预测最可迁移的源任务用于新目标任务。\n\n## Introduction\n\n越来越大的预先训练的语言模型是获得最佳性能的关键因素。虽然这一趋势不断推动着各种NLP基准的可能性，但这些模型的巨大规模对实际应用提出了重大挑战。对于100B以上的参数模型，为每个下游任务微调和部署一个单独的模型实例将是非常昂贵的。\n\n为了绕过微调的不可行性，GPT-3 提出了 Prompt design，其中每一个下游任务都被铸成一个语言建模任务，冻结的预训练模型通过对推理时提供的手动文本 prompts 调节来执行不同的任务。\n\nBrown等人（2020）用一个冻结的GPT-3模型展示了令人印象深刻的 few-shot 性能，尽管其性能高度依赖于 prompt 的选择，并且仍然远远落后于最先进的微调结果。\n\n最近的工作探索了学习软prompt的方法（\n\n- 《Gpt understands, too》\n- 《Learning how to ask: Querying LMs with mixtures of soft prompts》\n- 《Prefix-tuning: Optimizing continuous prompts for generation》\n- 《The power of scale for parameter-efficient prompt tuning》\n\n），这可以被视为注入语言模型的额外可学习参数。\n\nPROMPTTUNING 在适应过程中为每个下游任务学习一个小的特定任务 prompt（一个可调整的标记序列，预置在每个样本中），以调节冻结的语言模型来执行该任务。\n\n引人注目的是，随着模型容量的增加，PROMPTTUNING 与 model-tuning 比较起来，后者在每个下游任务上对整个模型进行微调。然而，在小规模和中等规模的模型（小于11B参数）中，PROMPTTUNING 和 model tuning 之间仍有很大差距。\n\n例如，对于T5 BASE（220M参数）和T5 XXL（11B参数）模型，在SuperGLUE基准上分别获得了+10.1和+2.4点的平均精度改进。SPOT在所有模型规模上的表现都比model tuning有竞争力或明显更好\n\n![](https://i.loli.net/2021/11/09/78b2YLcQWw3xOVg.png)\n\n在这些结果的激励下，通过任务 prompts 的视角来研究任务之间的可迁移性。目标是回答以下问题：\n\n- (a) 对于一个给定的目标任务，何时将 prompt 初始化为源任务的 prompt 有助于提高性能？\n\n  为了解决(a)，作者使用26个NLP任务和160个源-目标任务的组合对T5模型进行了系统研究。结果表明，任务往往可以通过prompt transfer 而相互受益。\n\n- (b) 能不能利用任务 prompt，对给定的新目标任务使用哪些源任务做出更有原则的选择？\n\n  为了解决(b)，把学到的任务 prompt 解释为任务嵌入，以构建一个任务的语义空间，并规范任务之间的相似性。设计了一种高效的检索算法，用来测量任务嵌入的相似性，使能够识别那些有可能对给定的新目标任务产生积极迁移性的源任务。\n\n## Improving PROMPTTUNING with SPOT\n\n为了提高PROMPTTUNING的性能，SPOT引入了源 prompt tuning，这是语言模型预训练和目标 prompt tuning 之间的一个中间训练阶段（如图左），在一个或多个源任务上学习prompt（同时仍保持基础模型冻结），然后用来初始化目标任务的prompt。\n\n![](https://i.loli.net/2021/11/09/vBU7Mqmlt6JP5Ie.png)\n\n### Experimental setup\n\n冷冻模型是建立在预先训练好的各种尺寸的T5 checkpoints 之上的。SMALL、BASE、LARGE、XL、XXL，参数分别为60M、220M、770M、3B和11B。在对SPOT的实验中，利用了T5的LM adapted（[prefix LM](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md)）版本，发现它更容易为PROMPTTUNING优化。\n\n### Baselines\n\n- PROMPTTUNING : vanilla prompt tuning approach , 其中针对每个目标任务直接训练独立提示。\n- MODELTUNING & MULTI-TASKMODEL TUNING: 将 prompt tuning 方法与MODELTUNING--标准的微调方法进行比较，其中所有的预训练参数都在每个目标任务上分别进行微调。为了进行 apples-to-apples 的比较，我们还包括MULTI-TASKMODEL TUNING，这是一个更有竞争力的基线，首先在SPOT使用的相同混合源任务上微调整个模型，然后在每个目标任务上单独微调。\n\n### Evaluation datasets\n\n在GLUE 和SuperGLUE 基准（每个基准都有8个数据集）的不同任务集上研究下游的性能。\n\n### Data for source prompt tuning\n\n与语言模型的预训练一样，训练数据的选择对于成功的 prompt transfer至关重要。为了研究源训练数据对下游性能的影响，我们比较了一系列不同的源任务。\n\n- **A single unsupervised learning task :** 首先考虑在C4数据集上使用 \"prefix LM\" 目标训练一个 prompt。虽然这个任务是用来预先训练冷冻T5模型的，但它仍然可以帮助学习一个通用的提示。\n- **A single supervised learning task :** 另外，我们可以使用监督任务来训练 prompt。我们使用MNLI或SQuAD作为单源任务。MNLI被证明对许多句子级别的分类任务有帮助，而SQuAD被发现对QA任务有很好的概括性。\n- **A multi-task mixture :** 到目前为止，我们一直在对单一来源的任务进行训练提示。另一种方法是多任务训练。在T5的统一文本到文本框架内，这只是相当于将不同的数据集混合在一起。我们探索混合来自不同NLP基准或任务系列的数据集，包括GLUE、SuperGLUE、自然语言推理（NLI）、转述/语义相似性、情感分析、MRQA的问题回答、RAINBOW的常识推理。 我们使用Raffel等人（2020）的例子--比例混合策略，从上述每个NLP基准/任务家族中创建了一个源任务的mixture，人工数据集大小限制为K = 219个训练例子。最后，我们包括C4和上述NLP基准/任务族中的所有标记数据集的混合（55个数据集）。\n\n\n\n遵循 PROMPTTUNING 的观点。，我们使用 CLASS-LABEL 方案(其中 prompt tokens用表示输出类的合并的嵌入来初始化)来初始化提示，并退回到SAMPLEDVOCAB方案以填充任何剩余的提示位置)。\n\n\n\n### Effect of SPOT\n\n![](https://i.loli.net/2021/11/09/P79bgkXEKDuZArh.png)\n\n我们在图1和表1中比较了SPOT和其他方法的结果。下面，我们对每个发现进行详细的总结和分析。\n\n- **SPOT significantly improves performance and stability of PROMPTTUNING:**  表1显示了我们在T5-BASE glue 和SuperGlue基准上的结果。总体而言，结果表明，prompt transfer 为 PROMPTTUNING 提供了一种提高性能的有效手段。我们的烧蚀研究表明，Longer tuning 也是实现我们最佳性能的一个重要因素，并且是对 prompt transfer 的补充。此外，当Longer tuning 被省略时，我们观察到SPOT在不同的运行中提高了稳定性。\n- **Different source mixtures can lead to performance gains:**  在SPOT方法中，我们可以比较不同来源混合的有效性（见表1）。在GLUE和SuperGLUE上的源 prompt tuning 表现最好，分别获得82.8和73.2的平均分数。有趣的是，C4上的无监督源 prompt tuning（与预训练我们的冻结模型的任务相同）仍然产生了相当大的改进，甚至在SuperGLUE任务中超过了SuperGLUE的源 prompt tuning。此外，使用MNLI或SQuAD作为单一源数据集对GLUE和SuperGLUE都特别有帮助。最后，其他来源的混合也能带来明显的收益，一些NLP基准/任务家族（如NNLI和转述/语义相似性）比其他任务更有利。\n- **SPOT helps close the gap with MODELTUNING across all model sizes:**  最后，SPOT产生了与强大的MULTI-TASKMODEL TUNING基线相竞争的性能，同时在多任务源 tuning 和目标 tuning 方面参数效率更高；在XXL尺寸下，SPOT获得了91.2的最佳平均得分，**比MULTI-TASKMODELTUNING好+1.1分，尽管其特定任务参数少27000倍。**\n\n\n\n## Investigating task transferability\n\n在确定了 prompt transfer 对 prompt tuning 有帮助之后，我们现在将重点转移到通过任务prompts 的视角来研究任务迁移性。\n\n为了阐明不同任务之间的可迁移性，我们对26个NLP任务（包括一个无监督的任务）和160个源-目标任务的组合进行了大规模的实证研究。我们证明，在各种情况下，任务可以通过 prompt transfer 来互相帮助，而任务的相似性在决定迁移性方面起着重要作用。\n\n此外，我们表明，通过将任务prompts解释为任务嵌入，我们可以构建一个任务的语义空间，并制定一个更严格的任务相似性概念。最后，我们提出了一种检索算法，该算法测量任务嵌入的相似性，以选择哪些源任务用于给定的新目标任务（图2，右）。\n\n![](https://i.loli.net/2021/11/09/lzYAeRdUOH6rfPh.png)\n\n我们学习源任务的prompts，并将早期检查点作为任务嵌入，将最佳检查点保存为源prompts。这些构成了我们prompts库的键和值。给定一个新的目标任务。用户：(i) 计算一个任务嵌入，(ii) 检索一个最佳的源prompt，(iii) 训练一个目标prompt，该prompt以源prompt为初始化。\n\n### Experimental setup\n\n我们研究了16个源数据集和10个目标数据集的不同集合（见表2）。我们考虑了所有160对可能的源和目标数据集，并从每个源任务转移到每个目标任务。\n\n**Source and target tasks**\n\n 源任务包括一个无监督任务（C4）和15个监督任务，涵盖自然语言推理（NLI）、转述/语义相似性、情感分析、问题回答（QA）和常识推理。所有的源任务都是数据丰富的，或者在以前的工作中已经被证明产生了积极的转移。为了模拟一个真实的场景，我们使用低资源任务（少于1万个训练实例）作为目标任务。这些任务涵盖了上述类型的任务，此外还包括语法可接受性、词义消歧和核心推理的解决。\n\n**Training details**\n\n为了限制计算成本，我们在所有的任务迁移性实验中使用T5 BASE。我们在每个源任务上执行262,144个 prompt tuning 步骤。选择具有最高源任务验证性能的 prompt checkpoint 来初始化不同目标任务的 prompt。由于目标数据集较小，我们只对每个目标任务进行100K的 prompt tuning 步骤。\n\n**Constructing a semantic space of tasks**\n\n由于在具体任务的 prompt tuning 过程中只有 prompt 参数被更新，任务prompt很可能编码了特定的任务知识。这表明，它们可以被用来推理任务的性质及其关系。为了测试这个想法，我们将任务 prompt 解释为任务嵌入，并构建一个任务语义空间。请注意，虽然我们使用源任务的最佳 prompt checkpoint 来迁移到目标任务中，但我们使用早期的 prompt checkpoint 作为我们的任务嵌入。这使得新的目标任务的任务嵌入可以快速计算。在我们的实验中，任务嵌入来自一个固定的prompt checkpoint，即在10K步，为每个任务。我们通过测量它们对应的任务嵌入 $e^1$, $e^2$之间的相似性来估计两个任务 $t^1$,  $t^2$ 之间的相似性，使用以下指标:\n\n- COSINE SIMILARITY OF AVERAGE TOKENS: 计算 prompt tokens 的平均集合表示之间的余弦相似度。\n\n  $sim(t^1,t^2) = cos(\\frac{1}{L} \\sum_i e^1_i, \\frac{1}{L}\\sum_j e_j^2)$ , 其中 $e^1_i,e^2_j$ 定义为各自的 prompt tokens\n\n- PER-TOKEN AVERAGE COSINE SIMILARITY: 计算每个 prompt token 对之间的平均余弦相似度 $(e_i^1,e^2_j)$:\n\n  $sim(t^1,t^2) = \\frac{1}{L^2} \\sum_i\\sum_j cos(e_i^1,e_j^2)$\n\n### Predicting and exploiting transferability\n\n利用任务嵌入来预测和利用任务迁移性。具体来说，我们探索了预测对给定目标任务最有利的源任务的方法，然后利用其prompt 来提高目标任务的表现。\n\n为了扩大我们的源 prompts 集，我们使用了每个源任务上所有三个不同的 prompt tuning运行的 prompt，从而产生了48个源 prompts。给定一个具有任务嵌入的目标任务 $t$ ，我们将所有的源prompts $ρ^s$按其对应的任务嵌入$e^s$ 和目标嵌入$e^t$之间的相似度从高到低排序。\n\n我们将源prompts 的排序列表表示为 $ρ^{s_r}$，其中r表示排序（r=1,2,...,48）。我们用以下方法进行实验:\n\n- BEST OF TOP-k : 选择前 k 个源prompts，并分别使用它们来初始化目标prompt。这个过程需要对目标任务 t 进行 k 次prompt tuning，每个源prompt一次。然后，最好的单个结果被用来评估这个方法的有效性。\n- TOP-k WEIGHTED AVERAGE: 用前k个源prompt 的加权平均数初始化目标prompt $\\sum_{r=1}^k \\alpha_r ρ^{s_r}$，这样我们只对目标任务 $t$ 进行一次提示调整。 权重 $\\alpha_r = \\frac{sim(e^{s_r, e^t})}{\\sum_{l=1}^k sim(e^{s_l, e^t})}$， $e^{s_r}$表示相应的任务嵌入 $ρ^{s_r}$\n- TOP-k MULTI-TASK MIXTURE: 首先确定 prompt 在前k个 prompt 中的源任务，并将其数据集和目标数据集混合在一起，使用Raffel等人（2020）的例子-比例混合策略。然后，我们在这个多任务混合物上进行源prompt tuning，并使用最后的 prompt checkpoint 来为目标 prompt tuning 进行初始化。\n\n**Evaluation**\n\n我们报告了通过使用上述每一种方法在目标任务中取得的平均分数。对于每个目标任务 $t$ ，我们衡量三个不同的 prompt tuning 运行（导致不同的任务嵌入等）的平均和标准偏差。为了进行比较，我们报告了在对每个目标任务从头开始进行prompt tuning（即没有任何 prompt Transfer）时，比基线的绝对和相对改进。此外，我们还包括通过使用暴力搜索来确定每个目标任务的48个源 prompt 中的最佳 prompt 所取得的谕示结果。\n\n### Effect of prompt-based task embeddings\n\n在这一部分中，我们首先分析我们的任务可迁移性结果。然后，我们论证了使用基于 prompt 的任务嵌入来表示任务、预测和开发任务可迁移性的有效性。\n\n![](https://i.loli.net/2021/11/09/bBqas3ylQuUZndf.png)\n\n**Tasks can help each other via prompt transfer in various scenarios:**  实验结果表明，在许多情况下，将 prompt 从源任务转移到目标任务（SOURCE → TARGET）可以在目标任务上提供显著的增益。\n\n![](https://i.loli.net/2021/11/09/e1y6ipCjg5OfaLn.png)\n\n**Task embeddings capture task relationships :** 图3显示了研究的26个NLP任务的任务嵌入之间的余弦相似性的分层聚类热图，使用的是平均托肯斯的余弦相似性指标。具体来说，类似的任务被归为几个集群，包括问题回答（SQuAD、ReCoRD和DROP；MultiRC和BoolQ）、情感分析（Yelp-2、SST-2和CR）、NLI（MNLI和CB；DocNLI和RTE）、语义相似性（STS-B和CxC）、副词（MRPC和QQP）和常识推理（WinoGrande、HellaSWAG和CosmosQA）。我们注意到，QNLI是由SQuAD数据集建立的NLI任务，与SQuAD没有密切联系；这表明我们的任务模型对任务类型比领域相似性更敏感。有趣的是，它们也捕捉到了ReCoRD对WSC的高可转移性这一非直观的情况。此外，来自同一任务的不同提示的任务嵌入具有很高的相似性分数\n\n![](https://i.loli.net/2021/11/09/eEAGvyxmXN8nzlh.png)\n\n**Correlation between task embedding similarity and task transferability:** 图4显示了目标任务上的相对误差减少是如何作为源和目标任务嵌入之间的相似性的函数而变化的。总的来说，我们发现，在我们研究的四个（10个）目标任务上，任务嵌入的相似性和任务可转移性之间存在明显的正相关，包括STS-B（p < 0.001），CB（p < 0.001，未显示），WSC（p < 0.01），和RTE（p < 0.05），而在其他任务上则不太明显。\n\n![](https://i.loli.net/2021/11/09/eVcs79ZDtWYK2JM.png)\n\n**Task embeddings can be used to predict and exploit task transferability:** 我们在表3中比较了不同方法的结果，以确定哪些源提示可能对给定的目标任务有益。我们发现，对于小的k值(≤9)，使用每键平均COSINE SIMILARITY指标比使用AV-ERAGE TOKENS的COSINE SIMILARITY指标产生更好的结果。我们的结果还表明，BEST OF TOP-k提供了一个预测和利用任务转移性的有效手段。简单地选择源提示，其相关的任务嵌入与目标嵌入具有最高的相似性，使用每键平均COSMINE SIMILARITY度量，比基线有很大的改善（从平均得分74.7到76.7，平均相对误差减少12.1%）。为每个目标任务尝试所有前三名（共48个）的源提示，得到的平均分数为77.5分。随着k值的增大，我们可以保留神谕选择源提示的大部分好处（k=9时平均得分的80%，k=15时平均得分的90%），同时仍然可以消除2/3以上的候选源提示。尽管这种方法需要对目标任务进行K次提示调谐，但与模型调谐相比，提示调谐的成本相对低廉。在k=1的情况下，TOP-k加权平均法与BEST OF TOP-k的平均性能相似，但实现的方差较小。因此，在禁止对tar-get任务进行多次调谐的情况下，这可能是对BEST OF TOP-k的一个有吸引力的替代方案。最后，TOP-k MULTI-TASK MIXTURE也提供了一种获得强大性能的方法，其平均得分为77.8，甚至在k≤3的情况下超过了BEST OF TOP-k。\n\n\n\n\n\n## Related Work\n\n**Parameter-efficient transfer learning & lan- guage model prompting**\n\n预训练的语言模型已被证明是改善许多NLP基准的最先进结果的有效手段（Devlin等人，2019；Liu等人，2019b；Yang等人，2019；Lan等人，2020；Raffel等人，2020；Brown等人，2020；He等人，2021）。然而，MODELTUNING（又称微调）--目前将这些模型应用于下游任务的主流方法--可能变得不切实际，因为为每项任务微调所有预训练的参数可能过于昂贵，特别是随着模型规模的不断扩大。\n\n为了解决这个问题，早期的工作使用了compression技术，如知识分散（Sanh等人，2019；Jiao等人，2020；Sun等人，2020）和模型修剪（Fan等人，2020；Sanh等人，2020；Chen等人，2020），以获得轻型预训练模型。其他工作只更新语言模型的小部分（Zaken等人，2021）或训练特定的任务模块，如适配器（Houlsby等人，2019；Karimi Mahabadi等人，2021）和/或低等级结构（Mahabadi等人，2021；Hu等人，2021），同时保持大部分或全部预训练参数固定。值得注意的是，Brown等人（2020年）使用PROMPTDESIGN的单一冻结的GPT-3模型展示了显著的几次学习性能，其中每个任务都是在推理时间向模型提供手动文本提示，要求它产生一些输出文本。\n\n此后，一些努力集中在开发基于提示的学习方法，包括精心手工制作的提示（Schick和Schütze，2021）、提示挖掘和转述（Jiang等人，2020b）、基于梯度搜索的改进提示（Shin等人，2020）和自动提示生成（Gao等人，2021）。然而，使用硬性提示被发现是次优的和敏感的，即下游表现和提示格式之间没有明显的相关性，提示的微小变化会导致下游表现的显著差异（Liu等人，2021b）。因此，最近的工作已经转向学习软提示（Liu et al., 2021b; Qin and Eisner, 2021; Li and Liang, 2021; Lester et al., 2021），这可以被看作是注入语言模型的一些额外的可学习参数。我们请读者参考Liu等人（2021a）对基于提示的学习研究的最新调查。\n\n同时进行的工作（Gu等人，2021）也探讨了 prompt 预训练的有效性。他们的方法使用手工制作的预训练任务，为不同类型的下游任务量身定做，这限制了其对新型下游任务的应用。相比之下，我们使用现有的任务作为源任务，并表明即使在源任务和目标任务之间存在不匹配（如任务类型、输入/输出格式）的情况下，prompt transfer也能带来好处。他们的工作也集中在 few-shot 的设置上，而我们是在较大的数据集背景下工作。此外，我们研究了任务的可迁移性，并证明任务往往可以通过 prompt transfer 来互相帮助，而任务 prompt 可以被解释为任务嵌入，以正式确定任务的相似性，从而确定哪些任务可以互相受益。\n\n**Task transferability**\n\n我们还建立在现有的关于NLP的任务迁移性的工作上（Phang等人，2019；Wang等人，2019a；Liu等人，2019a；Talmor和Berant，2019；Pruksachatkun等人，2020；Vu等人，2020；Poth等人，2021）和计算机视觉（Zamir等人，2018；Achille等人，2019；Yan等人，2020）。之前的工作表明，从数据丰富的源任务（Phang等人，2019年）、需要复杂推理和推理的任务（Pruksachatkun等人，2020年）或与目标任务相似的任务（Vu等人，2020年）中有效转移。也有人努力预测任务之间的可转移性（Bingel和Søgaard，2017；Vu等人，2020；Poth等人，2021）。Vu等人（2020）使用来自输入文本或语言模型对角线Fisher信息矩阵的任务嵌入，而Poth等人（2021）探索基于适配器的方法。在这里，我们对T5的使用使我们能够更好地对任务空间进行建模，因为每个任务都被投到一个统一的文本到文本的格式中，并且在不同的任务中使用相同的模型（没有特定的任务成分）。此外，基于提示的任务嵌入相对来说更容易获得。\n\n\n\n## Conclusion\n\n在本文中，我们研究了在提示调谐背景下的转移学习。我们表明，规模对于PROMPTTUNING与MODEL-TUNING的性能相匹配是没有必要的。我们的SPOT方法在不同的模型规模下与MODEL-TUNING的性能相匹配，甚至超过了MODEL-TUNING的性能，同时参数效率更高（最多可减少27,000倍的特定任务参数）。我们对任务转移性的大规模研究表明，在各种情况下，任务可以通过提示转移而相互受益。最后，我们证明，任务提示可以被解释为任务嵌入，以正式确定任务之间的相似性。我们提出了一种简单而有效的检索方法，以衡量任务的相似性，从而确定哪些源任务可以给一个新的目标任务带来好处。从整体上看，我们希望我们的工作能够促进对基于提示的迁移学习的更多研究。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"The Power of Scale for Parameter-Efficient Prompt Tuning","url":"/2021/11/08/The-Power-of-Scale-for-Parameter-Efficient-Prompt-Tuning/","content":"\n# The Power of Scale for Parameter-Efficient Prompt Tuning\n\nhttps://zhuanlan.zhihu.com/p/428512183\n\n在这项工作中，作者探索了 \"prompt tuning\"，这是一种简单而有效的机制，用于学习 \"软提示\"，以调节冻结的语言模型来执行特定的下游任务。\n\n与GPT-3使用的离散文本提示不同，软提示是通过反向传播来学习的，并且可以进行调整以合并来自任何数量的标注样本的信号。\n\n此端到端学习方法在很大程度上超过了GPT-3的 few-shot learning。更值得注意的是，通过使用T5对模型规模的消减，表明，随着规模的扩大，prompt tuning变得更有竞争力：当模型超过数十亿个参数时，该方法 \"缩小了差距\"，与model tuning （其中调整了所有模型权重）的强大性能相匹配。\n\n这一发现尤其重要，因为共享和服务大型模型的成本很高，而将一个冻结模型重用于多个下游任务的能力可以减轻这一负担。\n\n此方法可以看作是对最近提出的  \"prefix tuning\"，以软提示为条件的冻结模型对领域转移的鲁棒性有好处，并能实现有效的 \"prompt ensembling\"。\n\n## 1 Introduction\n\n随着预训练的大型语言模型的广泛成功，出现了一系列的技术来适应这些通用模型的下游任务。ELMo提出冻结预训练的模型，并学习其每层表征的特定任务加权。\n\n然而，自GPT和BERT以来，主流的适应技术是模型调整（或 \"微调\"），即在适应期间调整所有的模型参数。\n\n最近，GPT3 表明，prompt design （或 \"priming\"）在通过文本提示来调节冻结的GPT-3模型的行为方面是惊人的有效。提示通常由一个任务描述和/或几个典型的例子组成。这种回到 \"冻结 \"预训练模型的做法很有吸引力，尤其是在模型规模不断扩大的情况下。与其说每个下游任务都需要一个单独的模型副本，不如说一个通用模型可以同时为许多不同的任务服务。\n\n不幸的是，基于提示的适应有几个关键的缺点。任务描述容易出错，需要人的参与，而且提示的有效性受限于模型输入中能容纳多少条件文本。\n\n因此，下游任务的质量仍然远远落后于 model tuning。例如，GPT-3 175B 在SuperGLUE上的几率性能比微调的T5-XXL低17.5分（71.8比89.3），尽管使用了16倍的参数。\n\n最近提出了几项自动化 prompt 设计。\n\n《 AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts》\n\n虽然这种技术优于人工提示设计，但相对于 model tuning 而言，仍有差距。\n\n《Prefix-tuning: Optimizing continuous prompts for generation》 提出 “prefix tuning” 并 在生成性任务中表现出很强的效果。这种方法冻结了模型参数，并将调整过程中的误差反向传播到预置在编码器堆栈中每一层的前缀激活，包括输入层。\n\n《WARP: Word-level Adversarial ReProgramming》 通过将可训练的参数限制在 masked 语言模型的输入和输出子网络上，简化了这个方法，并在分类任务上显示了合理的结果。\n\n本文提出了 prompt tuning 作为适应语言模型的进一步简化方法。我们冻结了整个预训练的模型，只允许每个下游任务有额外的 $k$ 个可调整的标记被预加到输入文本中。这种 \"软提示 \"是端到端的训练，可以浓缩来自完整的标记数据集的信号，使我们的方法能够胜过  few-shot prompts，并缩小与模型调整的质量差距。如图1所示，随着规模的扩大，提示调谐变得更具竞争力。\n\n![](https://i.loli.net/2021/11/08/Ws5EYPU7JOTMItl.png)\n\n同时，由于一个预训练模型被循环用于所有下游任务，因此我们保留了冻结模型的有效服务优势\n\n![](https://i.loli.net/2021/11/08/VGpNzwImrZFR7oE.png)\n\n与Prefix-tuning 和 AutoPrompt 虽然都使用 “软提示”，但本文是第一个表明单独的 prompt tuning（没有中间层前缀或特定任务的输出层）就足以与 model tuning 相媲美。\n\n主要贡献：\n\n- 提出 prompt tuning，并证明其在大型语言模型体系中与模型调谐的竞争力。\n- 消融了许多设计选择，并证明质量和稳健性随着规模的扩大而提高。\n- 表明在领域转移问题上，prompt tuning 优于 model tuning。\n- 提出 \"prompt ensembling\"并显示其有效性。\n\n## 2 Prompt Tuning\n\n按照T5的 \"text-to-text\"的方法，将所有的任务作为文本生成。我们现在不是将分类建模为给定某种输入的输出类别的概率$Pr(y|X)$ ，其中 $X$ 是一系列 tokens，$y$ 是一个单一的类别标签，而是将其建模为条件生成，其中 $Y$ 是代表一个类别标签的一系列 tokens。T5将分类建模为 $Pr_θ(Y|X)$，参数由构成其编码器和解码器的 transformer 的权重 $θ$ 决定。\n\nPrompting 是指在生成 $Y$ 的过程中为模型添加额外的信息作为条件的方法。\n\n通常情况下，提示是通过在输入的 $X$ 上预置一系列标记 $P$ 来完成的，这样模型就能最大限度地提高正确 $Y$ 的可能性，即$Pr_θ(Y |[P ; X ])$，同时保持模型参数 $θ$ 的固定。在GPT-3中，prompt tokens的表示，$P=\\{p_1, p_2, ..., p_n\\}$，是模型嵌入表的一部分，由冻结的 $θ$ 作为参数。\n\n 因此，寻找最佳 prompt 需要通过人工搜索或无差别搜索方法来选择提示符。\n\nPrompt tuning 消除了提示 $P$ 被 $θ$ 参数化的限制；相反，提示有它自己的专用参数 $θ_P$，可以被更新。prompt design 涉及从固定的冻结嵌入词汇中选择提示标记，而 Prompt tuning 可以被认为是使用固定的特殊标记的提示，其中只有这些提示标记的嵌入可以被更新。\n\n新的条件生成现在是 $Pr_{\\theta;\\theta_P} (Y| [P;X])$，可以通过反向传播使 $Y$ 的可能性最大化来训练，同时只对 $θ_P$ 应用梯度更新。\n\n给定一系列的 n个 tokens，$\\{x_1, x_2,..., x_n\\}$，T5做的第一件事就是嵌入这些tokens，形成一个矩阵 $X_e\\in R^{n\\times e}$，其中 $e$ 是嵌入空间的维度。\n\nsoft-prompts 被表达为一个参数 $P_e\\in R^{p\\times e}$ 其中 $p$ 是prompt长度。\n\n然后，prompt 与嵌入的输入相连接，形成一个单一的矩阵 $[P_e; X_e]\\in R^{(p+n)×e}$，然后像平常一样流经编码器-解码器。我们的模型被训练为最大化 $Y$ 的概率，但只有 prompt 参数 $P_e$被更新。\n\n### Design Decisions\n\n有许多可能的方法来初始化 prompt 表征。最简单的是从头开始训练，使用随机初始化。一个更复杂的选择是将每个 prompt token 初始化为一个从模型词汇中提取的嵌入。\n\n从概念上讲，soft-prompt  以与输入前的文本相同的方式调节冻结网络的行为，因此，类似于单词的表述可能作为一个好的初始化点。对于分类任务，第三个选择是用列举输出类别的嵌入来初始化 prompt，类似于 \"verbalizers\"。\n\n由于我们希望模型在输出中产生这些 tokens，用有效的目标 tokens 的嵌入来初始化prompt ，应该使模型将其输出限制在合法的输出类别中。\n\n另一个设计考虑是 prompt 的长度。我们方法的参数成本是 $EP$ ，其中E是 token 嵌入维度，P是 prompt 长度。prompt越短，必须调整的新参数就越少，所以我们的目标是找到一个表现良好的最小长度。\n\n### Unlearning Span Corruption\n\nT5 模型的预训练任务是 Span Corruption，模型被要求去重构被打乱的句子\n\n与GPT-3等自回归语言模型不同，我们试验的 T5 模型使用编码器-解码器架构，并对 Span Corruption 目标进行预训练。具体来说，T5的任务是 \"重建 \"输入文本中被屏蔽的span ，这些跨度被 tokens 为独特的哨兵符号。目标输出文本由所有被屏蔽的跨度组成，用哨兵标记分开，再加上最后一个哨兵标记。\n\n例如。从文本 \"Thank you for inviting me to your party last week\"中，我们可以构建一个预训练的例子，其中输入是 \"Thank you ⟨X⟩ me to your party ⟨Y⟩ week\"，目标输出是\"⟨X⟩ for inviting ⟨Y⟩ last ⟨Z⟩\"。\n\n虽然Raffel等人（2020年）发现这种架构和预训练目标比传统的语言建模更有效，但我们假设这种设置并不适合产生一个可以通过prompt tuning而随时控制的冻结模型。特别是，一个专门针对 Span Corruption 进行预训练的T5模型，如T5.1.1，从未见过真正自然的输入文本（不含哨兵标记），也从未被要求预测真正自然的目标。\n\n事实上，由于T5的 Span Corruption 预处理的细节，每个预训练目标都会以哨兵开始。虽然这种输出哨兵的 \"非自然 \"倾向很容易通过微调来克服，但我们怀疑，由于解码器的先验因素无法调整，仅通过提示就很难推翻它。\n\n考虑到这些问题，我们在三种情况下试验了T5模型。\n\n- (1) \"Span Corruption\"。我们使用预先训练好的现成的T5作为我们的冻结模型，并测试其为下游任务输出预期文本的能力。\n- (2) \"Span Corruption + Sentinel\"。我们使用相同的模型，但在所有的下游目标中预置一个哨兵，以便更接近于预训练中看到的目标。\n- (3) \"LM Adaptation\"。我们继续T5的自我监督训练，进行少量的附加步骤，但使用Raffel等人（2020）所讨论的 \"LM \"目标；给定一个自然文本 prefix 作为输入，该模型必须产生自然文本的延续作为输出。\n\n最重要的是，这种适应性只发生一次，产生一个单一的冻结模型，我们可以在任何数量的下游任务中重复使用，进行prompt tuning。\n\n通过LM adaptation，我们希望将T5 \"快速 \"转变为一个与GPT-3更相似的模型，GPT-3总是输出真实的文本，并且作为一个 \"few-shot learner\"，对提示有良好的反应。与从头开始的预训练相比，这种后期转变的成功率并不明显，而且据我们所知，以前也没有人研究过这种情况。因此，我们对各种长度的适应进行了实验，最高可达10万步。\n\n## 3 Results\n\n实验设定如下：\n\n**预训练模型：**T5 v1.1 from small to XXL。\n\n**默认设置：**采用经过额外 100k steps 的 LM Adaption 以后的 T5 模型 + 100 tokens 的 prompt。\n\n**评测数据集：**采用 SuperGLUE 基准的全量数据，将数据集重定义为 text-to-text 的形式（但是并不会加上 task name 的前缀），每一个 task 单独训练一个 prompt，训练步数为 30K，最后报告 SuperGLUE 的 dev set 的结果。\n\n**基线模型：**1）Model Tuning：每个 task 分别微调一个 T5 模型；2） Model Tuning（Multi-Task）：多个 task 一起训练，为了区分每个 task，会加上 task name 的前缀。\n\n实验结果如下：\n\n![](https://i.loli.net/2021/11/08/gyFpB84RMnjEA3I.png)\n\n随着模型参数的增加，Prompt Tuning 的效果越来越好，当 T5 模型参数达到 XXL 时，Prompt Tuning 的效果追平了 Model Tuning 和 Model Tuning（Multi-Task）。同时，Prompt Tuning 的效果远远超过了与 T5 同参数级别的 GPT-3 in context learning 的效果。\n\n**prompt tokens 对 prompt tuning 的影响：**在一般模型大小情况下，prompt tokens 越多，确实效果越好，但是当 token 超过 20 以后，增益就越来越小，对于超大模型的情况，即使是单个 prompt token，也能达到和 20 个 token 以上的 prompt 相近的效果。\n\n**prompt token 的初始化：**1. 随机初始化；2. 从 T5 词表中 5000 个常见单词中采样；3. 用类标签来初始化，标签是多个 token 时，则取均值，当类标签都用完后，剩下的 prompt token 用方法 2 初始化。类标签初始化在各种尺寸的模型上都表现最好，但是不同初始化策略在各种尺寸模型上表现差异很大，当尺寸变 XXL 后，这种差异就会消失。\n\n**预训练任务对 prompt tuning 的影响：**Span Corruption 任务导致了 prompt tuning 表现很差，即使加了 Sentinel 也没法缓解，而 LM Adaptation 设定下随着模型尺寸增大则 prompt tuning 表现越来越好。当然，当尺寸变为 XXL 后，这种影响也会消失。\n\n**LM Adaptation steps 对 prompt tuning 的影响：**LM Adaptation steps 越多，效果越好，这也说明 T5 需要进一步预训练才行。当然，当尺寸变为 XXL 后，这种影响也会消失。\n\n\n\n## 4 Comparison to Similar Approaches\n\n比较的一个重要轴是每种方法所需的特定任务参数的数量，如图4所示。在具有可学习参数的方法中，prompt tuning是参数效率最高的，对于超过10亿个参数的模型，需要不到0.01%的特定任务参数。\n\n![](https://i.loli.net/2021/11/08/NHCwWmDvGKdOY9o.png)\n\n**\"prefix tuning\"**：学习在每个 transformer 层预置的前缀序列。这类似于学习 transformer 的激活，这些激活在每个网络层都是固定的。与此相反，Prompt Tuning 使用单一的提示表示，被预置到嵌入式输入。除了重新引用较少的参数外，我们的方法允许 transformer 更新中间层的任务表征，就像输入实例的背景一样。他们的工作建立在GPT-2 和 BART 的基础上，而我们的工作则以T5为基础，研究了随着模型大小的增加，设计选择的性能和稳健性的变化。当使用BART时，prefix tuning 包括编码器和解码器网络上的前缀，而 Prompt Tuning 只需要编码器上的提示。\n\n**与 P-tuning 的区别：**P-tuning 的 soft tokens 需要考虑插入位置，同时采取的策略是 LM+Prompt Tuning，而 Prompt Tuning 则是直接插入在 prefix 位置，同时固定了 LM。同时 Prompt Tuning 相比 Model Tuning 的好处在于不会太过拟合在目标任务上，拥有更好的泛化性。\n\n\n\n**\"adapters\"**，即在冻结的预训练网络层之间插入小型瓶颈层。adapters 提供了另一种减少特定任务参数的手段，Houlsby等人（2019年）在冻结BERT-Large并只增加2-4%的额外参数时，实现了接近全模型调谐的GLUE性能。Pfeiffer等人（2020年）在一个多语言文本中使用多个适配器，明确地将语言理解与任务规范分开，与我们的方法类似。\n\nadapters 和 Prompt Tuning 之间的一个核心区别是这些方法如何改变模型行为。\n\nadapters 通过允许重写任何给定层的激活来修改作用于输入表征的实际函数，该功能由神经网络参数化。\n\nPrompt Tuning  通过保留固定的函数和增加新的输入表示来修改行为，这些表示会影响后续输入的处理。\n\n\n\n## 5 Resilience to Domain Shift\n\n**通过冻结核心语言模型参数，prompt tuning可防止模型修改其对语言的一般理解。相反，prompt表示间接调整输入的表示。**\n\n**这减少了模型通过记忆特定的词汇线索和虚假的相关关系来 overfit 数据集的能力。**这一限制表明，prompt tuning可能会提高对domain shifts 的稳健性，在这种情况下，输入的分布在训练和评估之间有所不同。\n\n我们在两个任务上研究了  zero-shot  的领域转移：问题回答（QA）和转述检测（paraphrase detection）。对于问答，使用MRQA 2019关于泛化的共享任务。这项任务以统一的格式收集提取的QA数据集，并测试在 \"域内 \"数据集上训练的模型在评估 \"域外 \"数据集时的表现。在我们的实验中，我们在SQuAD上训练，并在每个域外数据集上进行评估。\n\n![](https://i.loli.net/2021/11/08/v2mIghVcnt6FCUk.png)\n\n作为对结构域转移稳健性的第二个测试，我们探索了来自GLUE的两个释义检测任务之间的迁移。第一个任务是QQP(Iyer等人，2017年)，它询问来自社区问答网站Quora的两个问题是否是“重复的”。\n\n![](https://i.loli.net/2021/11/08/odZXCSlN2ceIExk.png)\n\n## 6 Prompt Ensembling\n\n**Prompt 集成：**prompt tuning 的另一个好处在于可以在保存一份 LM 模型拷贝情况下，同时训练多个 prompt，并实现集成。作者在 SuperGLUE 上训练了 5 个 prompt，并用多数投票法进行集成，表现优于单一 prompt。\n\n\n\n\n\n**总结**\n\nPrompt Tuning 的做法是添加可训练的 prefix，同时固定 LM，只训练 prefix，采用 Prompt Tuning 的方式可以在 T5 超大模型和全量数据的情况下，追平 fine-tuning 的效果。\n\n实验发现采用 prompt tuning 的方式在小模型的情况容易受到 prompt 长度，初始化策略，预训练任务等影响，并不稳定，也没法超过 fine-tuning 的效果。\n\n作者没有探索少量数据 + 超大模型情况下和 fine-tuning 的效果比较。\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"二分查找典型问题(二、三)","url":"/2021/11/08/二分查找典型问题-二、三/","content":"\n# 二分查找典型问题(二、三)\n\nhttps://leetcode-cn.com/leetbook/read/learning-algorithms-with-leetcode/x4e1p5/\n\n## 二、二分答案\n\n二分答案意思是：题目要我们找的是一个整数，并且这个整数我们知道它可能的最小值和最大值。此时可以考虑用二分查找算法找到这个的目标值\n\n### [69. Sqrt(x)](https://leetcode-cn.com/problems/sqrtx/)\n\n分析：这个题要求计算一个非负整数的平方根，返回值是一个整数。当平方根是浮点数事，需向下取整\n\n#### 法一：暴力解法\n\n输入 8 返回的是 2 ， 因为 3 的平方等于 9 大于 8，因此【结果只保留整数的部分，小数部分将被舍去】。要求我们从1开始找，找到最后一个平方以后小于等于 x 的那个数。\n\n假设 s 表示从 1 开始的那个数：\n\n- 如果 s 平方以后小于 x， 暂时放过\n- 如果 s 平方以后等于 x ，直接返回\n- 如果 s 平方以后大于 x，说明 s-1 是题目要求，返回 s-1\n\n```java\n\tpublic int mySqrt(int x) {\n        // 特判\n        if (x <= 1) {\n            return x;\n        }\n        for (int i = 1; i <= x; ++i) {\n            if (i == x / i) {\n                return i;\n            } else if (i > x / i) {\n                return i - 1;\n            }\n        }\n        throw new IllegalArgumentException(\"参数出错\");\n    }\n```\n\n注意：如果判别条件写成 `s * s == x` ，会发生整型溢出，应该写成 `s = x / s` ，判别条件 `s * s > x` 也是类似这样写。\n\n复杂度分析：\n\n时间复杂度：O(x)，最坏情况下暴力解法会一直尝试到 x/2，O(x/2)=O(x)；\n空间复杂度：O(1)，只使用到常数个变量。\n\n#### 法二，二分查找\n\n如果一个数的平方大于 `x` ，这个数就一定不是我们要找的平方根。于是，可以通过逼近的方式找到平方根。\n\n```java\n\tpublic int mySqrt(int x) {\n        if (x == 0) {\n            return 0;\n        }\n        int left = 1;\n        int right = x / 2;\n        while (left < right){\n            // 写完分支以后调整为向上取整\n            int mid = (left + right + 1) / 2;     \n            if (mid > x / mid) {\n                // mid 以及大于 mid 的数一定不是解，下一轮搜索的区间为 [left, mid - 1]\n                right = mid - 1;\n            } else {\n                left = mid;\n            }\n        }\n        return left;\n    }\n```\n\n### [287. 寻找重复数](https://leetcode-cn.com/problems/find-the-duplicate-number/)\n\n- 要找 一个整数，这个整数有明确的范围 (1到n)之间，因此可以使用 二分查找。\n- 每一次猜一个数，然后遍历整个数组，进而缩小搜索区间，然后确定重复的是哪个数。\n- 不是在输入数组上直接使用二分查找，而是在数组 $[1,...,n]$ （有序数组）上使用二分查找。\n\n理解题意：\n\n- n + 1个整数，放在长度为 n 的数组里，根据抽屉原则，至少有一个数组重复\n- 找重复，最容易想到的是哈希表\n- 但题目要求，0(1)空间\n- 找找一个有范围的整数可以用二分查找\n- 快慢指针\n\n二分查找的思路是先猜一个数（有效范围  $[left,...,right]$ 里位于中间的数 mid），然后统计原始数组中，小于等于mid 的元素个数cnt\n\n- 如果cnt严格大于mid，根据抽屉原则，重复元素在 $[left, mid]$ 里\n- 否则，重复元素就在  $[mid+1,..right]$\n\n```java\nclass Solution {\n    public int findDuplicate(int[] nums) {\n        int len = nums.length;\n\n        int left = 0;\n        int right = len - 1;\n        while(left < right){\n            int mid = left + (right - left) / 2;\n\n            int cnt = 0;\n            for(int num : nums){\n                if(num <= mid){\n                    cnt+=1;\n                }\n            }\n\n            // 根据抽屉原理，小于等于4的个数如果严格大于4个，此时重复元素一定出现在 [1...4]里\n            if(cnt > mid){\n                right = mid;\n            } else{\n                left = mid + 1;\n            }\n\n        }\n        return left;\n    }\n}\n```\n\n### 1300. [转变数组后最接近目标值的数组和](https://leetcode-cn.com/problems/sum-of-mutated-array-closest-to-target/)\n\n一句话解题：\n\n- 使用二分法确定一个整数 threshold （就是提中说的value），使得这个 threshold下，【转变后的数组】的和最接近目标值 target\n- 转变的规则是：严格大于threshold的元素变成 threshold，那么 threshold 越大，【转变后的数组】的和越大，这是单调性。（注意说得具体一点是：单调不减，因为有些情况下，阈值扩大后，和可能不变）\n\n这道题比较麻烦的是求和以后可能不等于 target，所以让我们求【最接近的方案】，这个烦人的根源是 value 的取值一定得是整数，正是因为题目说 value 的整数，并且【答案不一定是arr中的数字】，因此依然可以使用二分查找法确定这个整数值。\n\n![](https://i.loli.net/2021/11/11/9czC2Jl61mZ7RaS.png)\n\n做题的时候，会发现判别条件很不好写，因为【怎么衡量接近】，度量这个【最接近】的量不好选，因为此需要考虑别的方案；\n\n最接近的情况是：选定了一个value求和以后，恰恰好等于target。不过更有可能出现的情况是：value选得小了，接近程度变大，而value选得大了，接近程度变小。\n\n![](https://i.loli.net/2021/11/11/dpaIjifs3KPLk8C.png)\n\n代码一\n\n如果选择一个阈值 value，使得它对应的 sum 是第 1 个大于等于target的，那么目标值可能在 value 也可能在 value - 1\n\n```java\nclass Solution {\n    public int findBestValue(int[] arr, int target) {\n        int len=arr.length;\n        int left = 0;\n        int right = 0;\n\n        for(int num: arr){\n            right = Math.max(right, num);\n        }\n\n        while(left < right){\n            int mid = left + (right - left) / 2;\n            \n            int sum = 0;\n            for(int num : arr){\n                sum += Math.min(num, mid);\n            }\n            // 计算第一个使得转变后数组的和大于等于target的阈值 threshold\n            if(sum < target){\n                // 严格小于的不一定是解\n                left = mid + 1;\n            }else{\n                right = mid;\n            }\n\n        }\n\n        // 比较阈值线分别定在 left-1 和left 的时候与target的接近程度\n        int sum1 = 0;\n        int sum2 = 0;\n        for(int num:arr){\n            sum1 += Math.min(num, left-1);\n            sum2 += Math.min(num, left);\n        }\n\n        if(target - sum1 <= sum2 - target){\n            return left - 1;\n        }\n        return left;\n    }\n  \t/*\n  \t如果选择一个阈值 value ，使得它对应的 sum 是最后 1 个小于等于 target 的阈值，那么目标值可能在 value 也可能在 value + 1。\n  \t*/\n  \tpublic int findBestValue(int[] arr, int target) {\n        int left = 0;\n        int right = 0;\n        // 注意：\n        for (int num : arr) {\n            right = Math.max(right, num);\n        }\n\n        while (left < right) {\n            int mid = left + (right - left + 1) / 2;\n            int sum = calculateSum(arr, mid);\n            // 计算最后 1 个使得转变以后数组的和小于等于 target 的阈值 threshold\n            if (sum > target) {\n                // 大于等于的就不是解，threshold 太大了，下一轮搜索区间是 [left, mid - 1]\n                right = mid - 1;\n            } else {\n                // 下一轮搜索区间是 [mid, right]\n                left = mid;\n            }\n        }\n\n        // 比较阈值线分别定在 left 和 left + 1 的时候与 target 的接近程度\n        int sum1 = calculateSum(arr, left);\n        int sum2 = calculateSum(arr, left + 1);\n        // 注意：这里必须加绝对值，因为有可能出现 sum1 == sum2 < target 的情况\n        if (Math.abs(target - sum1) <= Math.abs(sum2 - target)) {\n            return left;\n        }\n        return left + 1;\n    }\n\n    private int calculateSum(int[] arr, int threshold) {\n        int sum = 0;\n        for (int num : arr) {\n            sum += Math.min(num, threshold);\n        }\n        return sum;\n    }\n}\n```\n\n## 三、判别条件复杂的二分查找问题\n\n在上一节【二分答案】中看到的问题是，根据目标变量具有的单调性质编写判别函数。\n\n还有一类问题是这样的：目标变量和另一个变量有相关关系（一般而言是线性关系），目标变量的性质不好推测，但是另一个变量的性质相对容易推测。\n\n这样的问题的判别函数通常会写成一个函数的形式。\n\n### [875. 爱吃香蕉的珂珂](https://leetcode-cn.com/problems/koko-eating-bananas/)\n\n思路分析：\n\n- 根据题意可以知道：珂珂吃香蕉的速度越小，耗时越多。反之，速度越大，耗时越小，这是题目的单调性\n- 我们要找的速度，因为题目限制了一小时内只能选择一堆香蕉吃，因此速度最大值就是这几堆香蕉中，数量最多的那一堆。速度的最小值是1，其实还可以再分析一下下界是多少，由于二分搜素的时间复杂度很低，严格的Fenix不是很有必要。\n- 还是因为一小时内只能选择一堆香蕉吃，因此：**每堆香蕉吃完的耗时 = 这堆香蕉的数量/ 一小时吃香蕉的数量**。根据题意，这里的 / 在不能整除的时候需向上取整\n\n注意：当二分查找算法猜测的速度恰好使得珂珂在规定的实际内吃完香蕉的时候，还应该去尝试更小的速度是不是还可以保证在规定的时间内吃完香蕉\n\n```java\nclass Solution {\n    public int minEatingSpeed(int[] piles, int h) {\n        int n = piles.length;\n\n        int maxVal = 1;\n        for(int pile: piles){\n            maxVal = Math.max(maxVal, pile);\n        }\n\n        // 速度最小的时候，耗时最长\n        int left = 1;\n        // 速度最大的时候，耗时最短\n        int right = maxVal;\n\n        while(left < right){\n            int mid = left + (right - left) / 2;\n            if(calculateSum(piles, mid) > h){\n                //耗时太多，说明速度太慢，下一轮搜索区间是 [mid+1, right]\n                left = mid +1;\n            }else{\n                right = mid;\n            }\n        }\n        return left;\n    }\n\n    private int calculateSum(int[] piles, int speed){\n        int sum=0;\n        for(int pile : piles){\n            // 向上取整可以这样写\n            sum += (pile + speed -1) / speed;\n            // 还可以这样\n            // if (pile % speed == 0){\n            //     sum += pile / speed;\n            // }else{\n            //     sum += pile / speed + 1;\n            // }\n        }\n        return sum;\n    }\n}\n```\n\n时间复杂度：\n\n- 时间复杂度： $O(Nlog \\ max(piles))$, 这里 N 表示数组 piles 的长度、我们在 $[1,max\\ piles]$ 里使用二分查找，定位最小速度，而每一次执行判别函数的时间复杂度是 $O(N)$\n\n### [410. 分割数组的最大值](https://leetcode-cn.com/problems/split-array-largest-sum/)\n\n- 动态规划的写法其实是穷举：按照长度、前缀、枚举最后一个划分，记录每一步结果。细节比较多，需要作图+仔细讨论边界情况，并且属性二维数组、三层for循环的写法；\n- 本题的二分查找思路：查找一个有范围的整数，关于在利用单调性逼近这个整数。\n\n题意分析：各自和的最大值最小：\n\n- 由于数组是确定的，其中一组得分多，相应的另一组分到的值就少，所以对于任意一种拆分（切成 m 段），这 m 段可以取最大值 val\n- 需要找一种拆分，使得这个最大值 val 的值是所有分成 m 段拆分里值最小的那个；\n\n#### 方法1：动态规划\n\n枚举所有的分割情况，例如题目中的输入数组【7，2，5，10，8】分割成两个非空连续子数组，可以有以下四种方式：\n\n- `[7, | 2, 5, 10, 8]`；\n- `[7, 2, | 5, 10, 8]`；\n- `[7, 2, 5, | 10, 8]`；\n- `[7, 2, 5, 10, | 8]`。\n\n比较容易想到的递归结构是：\n\n- 找到最后一个分割，求出这个分割的连续子数组的和，与之前的分割取最大值。\n- 枚举最后一个分割，找出所有最大值中最小的那一个\n\n整个过程稍微有一些繁琐，但是思想是直接的：对于所有长度的 前缀区间（题目中的关键信息是子数组连续，所以考虑前缀区间），枚举所有可能的分割，并记录每一步的结果，递推完成计算。以题目中的示例 [7, 2, 5, 10, 8] 为例：\n\n先考虑 所有前缀区间 分割成 1 个非空连续子数组的情况：\n\n- [7] 分割成 1 个非空连续子数组的和，就是整个数组的和 7，下同；\n- [7, 2] 分割成 1 个非空连续子数组的和，就是整个数组的和 9；\n- [7, 2, 5] 分割成 1 个非空连续子数组的和，就是整个数组的和 14；\n- [7, 2, 5, 10] 分割成 1 个非空连续子数组的和，就是整个数组的和 24；\n- [7, 2, 5, 10, 8] 分割成 1 个非空连续子数组的和，就是整个数组的和 32；\n\n再考虑 所有前缀区间 分割成 2 个非空连续子数组的情况：\n\n- [7] 不能分割成 2 个非空连续子数组的和；\n- [7, 2] 分割成 2 个非空连续子数组，只有 1 种分割情况：[7, | 2] ，其中「[7] 分割成 1 个非空连续子数组」的情况我们在第 1 步计算过；\n- [7, 2, 8] 分割成 2 个非空连续子数组，有 2 种分割情况：\n- [7, | 2, 8] ，其中「[7] 分割成 1 个非空连续子数组」的情况我们在第 1 步计算过；\n- [7, 2, | 8] ，其中「[7, 2] 分割成 1 个非空连续子数组」的情况我们在第 2 步计算过；\n\n分析到这里，可以把递推结构形式化描述成如下：\n\n**第一步：定义状态**\n\n$dp[i][k]$ 表示：将前缀区间 $[0, i]$ 被分成 k 段的各自和的最大值的最小值记为 $dp[i][k]$，那么前缀区间 $[0, j]$ （这里 j < i） 被分成 k - 1 段各自和的最大值的最小值为 $dp[j][k - 1]$。\n\n即：第一维是第 `k` 个分割的最后一个元素的下标 `i` ，第二维是分割的总数 `i`。\n\n**第二步：推导状态转移方程**\n$$\ndp[i][k] = max(dp[j][k-1],rangeSum(j+1,i))\n$$\n这里 $rangeSum(j + 1, i)$ 表示数组 $nums[j + 1..i] $的区间和，它可以先计算出所有前缀和，然后以 O(1) 的方式计算出区间和。\n\n上面的状态转移方程中，j 的值需要枚举。我们画图分析：\n\n![](https://i.loli.net/2021/11/14/XpFodj7RkH4G8nC.png)\n\n- 由于区间 [0, j] 一定要分成 k - 1 个非空连续子数组；\n- j 的意义是：第 k - 1 个分割的最后一个元素的下标；\n- 而下标 k - 1 的前面（不包括 k - 1），一共有 k - 1 个元素（这一条只要是下标从 0 开始均成立）；\n- 故 j 的枚举从 k - 2 开始，到 i - 1 结束，因为第 k 个分割至少要有 1 个元素。\n\n**第三步：思考初始化**\n\n- 由于要找最小值，初值赋值成为一个不可能达到的很大的值；\n- 分割数为 1 ，即不分割的情况，所有的前缀和就是依次的状态值。\n\n**第 4 步：思考输出**\n\n$dp[len][k]$，根据状态定义，这是显然的。\n\n下面给出题目中的示例 [7, 2, 5, 10, 8] 的状态计算表，为了更突出一般性，把 m 设置成为数组的长度 5：\n\n![](https://i.loli.net/2021/11/14/HvYxSOThwA7slU1.png)\n\n编码的思考路径：\n\n我们按照阶段、状态和选择进行分析，依次把三层循环写下来：\n\n- 阶段：依次计算长度为 1 的区间、长度为 2 的区间，直到题目要求的长度为 m 的区间；\n- 状态：前缀区间 [0, i] 的状态值，由于 i 要被分成 k 份，前缀区间里至少要有 k 个元素，最小前缀区间 k 个元素的最后一个元素的下标为 k - 1，故 i 从 k - 1 开始到 len - 1；\n- 选择：枚举第 k - 1 个分割的最后一个元素的下标，根据上面的分析，从 k - 2 到 i - 1。\n\n\n\n#### 方法2：二分查找\n\n题目关键字：【非负整数数组】和【连续】这两个信息\n\n与69题、287题：可以用于查找一个有范围的整数，就能想到是不是可以使用二分查找去解决\n\n挖掘单调性：使用二分查找的一个前提是【数组具有单调性】，我们就去想想有没有单调性可以挖掘，不难发现：\n\n- 如果设置【数组各自和的最大值】很大，那么必然导致分割数很小\n- 如果设置【数组各自和的最大值】很小，那么必然导致分割数很大\n\n可以通过调整【数组各自和的最大值】来达到：使得分割数恰好为 m 的效果。这里要注意一个问题：\n\n如果某个数组各自的最大值恰好使得分割数为 m，此时不能放弃搜索，因为我们要使得这个最大值 最小化，此时还应该继续尝试缩小这个数组各自和的最大值，使得分割数超过 m，超过 m 的最后一个使得分割数为 m 的数组各自和的最大值就是我们要找的最小值。\n\n举个例子：\n\n例如：（题目中给出的示例）输入数组为【7，2，5，10，8】，m=2。如果设置数组各自和的最大值为 21那么每个的是【7，2，5，|10，8】，此时 m =  2，此时这个值太大，尝试一点一点缩小：\n\n- 设置 数组各自和的最大值 为 20，此时分割依然是 [7, 2, 5, | 10, 8]，m = 2；\n- 设置 数组各自和的最大值 为 19，此时分割依然是 [7, 2, 5, | 10, 8]，m = 2；\n- 设置 数组各自和的最大值 为 18，此时分割依然是 [7, 2, 5, | 10, 8]，m = 2；\n- 设置 数组各自和的最大值 为 17，此时分割就变成了 [7, 2, 5, | 10, | 8]，这时 m = 3。\n\n`m` 变成 `3` 之前的值 **数组各自和的最大值** `18` 是这个问题的最小值，所以输出 `18`。\n\n```java\nclass Solution {\n    public int splitArray(int[] nums, int m) {\n        int max = 0;\n        int sum = 0;\n\n         // 计算「子数组各自的和的最大值」的上下界\n        for(int num :nums){\n            max = Math.max(max, num);\n            sum += num;\n        }\n\n        // 使用【二分查找】确定一个恰当的【子数组各自的和的最大值】\n        int left = max;\n        int right = sum;\n        while(left < right){\n            int mid = left + (right - left) / 2;\n\n            int splits = split(nums, mid);\n            if(splits > m){\n                // 如果分割数太多，说明【子数组各自的和的最大值】太小，此时需要将【子数组各自的和的最大值】调大\n                // 下一轮搜索的区间是【mid+1，right】\n                left = mid+1;\n            }else{\n                right = mid;\n            }\n        }\n        return left;\n\n    }\n\n    private int split(int[] nums, int maxIntervalSum){\n        // 至少是一个分割\n        int splits = 1;\n        // 当前区间的和\n        int curIntervalSum = 0;\n        for(int num : nums){\n            // 尝试加上当前遍历的这个数，如果加上去超过了【子数组各自的和的最大值】，就不加这个数，另起炉灶\n            if (curIntervalSum + num > maxIntervalSum){\n                curIntervalSum = 0;\n                splits++;\n            }\n            curIntervalSum += num;\n        }\n        return splits;\n    }\n}\n```\n\n\n\n\n\n### [1011. 在 D 天内送达包裹的能力](https://leetcode-cn.com/problems/capacity-to-ship-packages-within-d-days/)\n\n对于左边界而言，由于我们不能「拆分」一个包裹，因此船的运载能力不能小于所有包裹中最重的那个的重量，即左边界为数组 weights 中元素的最大值。\n\n对于右边界而言，船的运载能力也不会大于所有包裹的重量之和，即右边界为数组 weights 中元素的和。\n\n```java\nclass Solution {\n    public int shipWithinDays(int[] weights, int days) {\n        \n        int maxVal = 0;\n        int sum = 0;\n        for(int w : weights){\n            maxVal = Math.max(maxVal, w);\n            sum+=w;\n        }\n\n        int left = maxVal;\n        int right = sum;\n        // int left = Arrays.stream(weights).max().getAsInt(), right = Arrays.stream(weights).sum();\n\n        while(left < right){\n            int mid = left + (right - left) / 2;\n\n            int cur = cal(weights, mid);\n            if (cur <= days) {\n                right = mid;\n            } else {\n                left = mid + 1;\n            }\n\n        }\n        return left;\n\n    }\n    private int cal(int[] weights, int mid){\n        int need = 1, cur = 0;\n        for (int weight : weights) {\n            if (cur + weight > mid) {\n                ++need;\n                cur = 0;\n            }\n            cur += weight;\n        }\n        return need;\n    }\n}\n```\n\n\n\n### [1482. 制作 m 束花所需的最少天数](https://leetcode-cn.com/problems/minimum-number-of-days-to-make-m-bouquets/)\n\n```java\nclass Solution {\n    public int minDays(int[] bloomDay, int m, int k) {\n\n        int len = bloomDay.length;\n\n        if(m * k > len){\n            return -1;\n        }\n\n        int maxVal = 0;\n        int minVal = 0;\n        for(int b : bloomDay){\n            maxVal = Math.max(b, maxVal);\n            minVal = Math.min(b, minVal);\n        }\n\n        int left = minVal;\n        int right = maxVal;\n\n        while(left < right){\n            int mid = left + (right - left) / 2;\n\n            int cur = cal(bloomDay ,mid, m, k);\n            if(cur >= m){\n                right = mid;\n            }else{\n                left = mid + 1;\n            }\n        }\n        return left;\n\n    }\n\n\n    private int cal(int[] bloomDay, int mid, int m, int k){\n        int curNum = 0;\n        int f = 0;\n        for(int b:bloomDay){\n            if(b <= mid){\n                f++;\n                if(f==k){\n                    curNum++;\n                    f=0;\n                }\n            }else{\n                f=0;\n            }\n        }\n        return curNum;\n    }\n}\n```\n\n\n\n### [LCP 12. 小张刷题计划](https://leetcode-cn.com/problems/xiao-zhang-shua-ti-ji-hua/)\n\n```java\nclass Solution {\n    public int minTime(int[] time, int m) {\n        if(time.length < m){\n            return 0;\n        }\n\n        int left = 0;\n        int right = Integer.MAX_VALUE;\n\n        while(left < right){\n            int mid = left + (right - left) / 2;\n\n            if(check(time, m, mid)){\n                right = mid;\n            }else{\n                left = mid + 1;\n            }\n            \n        }\n        return left;\n    }\n\n    public boolean check(int[] time, int m, int limit){\n        int cur = 0, sum = 0, max = 0, day = 1; // 当前遍历的题目，当前组的总耗时，当前组的最大耗时，需要的天数\n        while(cur < time.length){\n            sum += time[cur];\n            max = Math.max(max, time[cur]);\n            if(sum - max > limit){// 当前组总耗时减去组内最大耗时仍超出限制，则需开启额外一天\n                day ++;\n                if(day > m){// 超出总天数m，无法完成分配\n                    return false;\n                }\n                sum = time[cur]; // sum和max更新为新组的值\n                max = time[cur];\n            }\n            cur++;\n        }\n        return true; // 能遍历完所有题目即完成了分配\n    }\n}\n```\n\n","tags":["LeetCode"]},{"title":"Parameter-Efficient Transfer Learning with Diff Pruning","url":"/2021/11/05/Parameter-Efficient-Transfer-Learning-with-Diff-Pruning/","content":"\n# Parameter-Efficient Transfer Learning with Diff Pruning\n\nDiff pruning 使参数有效的迁移学习在新任务中得到良好的扩展。\n\n该方法学习了一个特定于任务的 \"diff \"向量，该向量对原始预训练的参数进行了调整。在训练过程中，这个 diff 向量通过对L0-norm惩罚的可微调近似来适应性地修剪，以鼓励稀疏性。\n\n随着任务数量的增加，diff pruning仍然具有参数有效，因为它只需要为每个任务存储一个小的diff向量。由于它不需要在训练期间访问所有任务，因此它在任务以流形式设置中很有吸引力。\n\n在GLUE基准测试中，diff pruning可以与微调基线的性能相媲美，而每个任务只需修改0.5%的预训练模型参数，与流行的修剪方法相比，其扩展性更强。\n\n## Introduction\n\n针对特定任务对预训练的深度网络进行微调是当代NLP的主流模式，在一系列自然语言理解任务中取得了最先进的结果。\n\n虽然这种方法简单明了，在经验上也很有效，但很难扩展到多任务、内存受限的情况下（例如设备上的应用），因为它需要为每个任务运送和存储一整套模型参数。\n\n由于这些模型是通过自监督的预训练来学习可推广的、与任务无关的语言表征，因此为每个任务微调整个模型似乎特别浪费。\n\n提高参数有效的一种流行方法：\n\n- 《Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning》\n- 《Poor Man’s BERT: Smaller and Faster Transformer Models》\n- 《Masking as an Efficient Alternative to Finetuning for Pretrained Language Models》\n- 《Adaptive Sparsity by Fine-Tuning》\n\n是为每个任务学习较小的压缩模型。这种方法面临着严重的稀疏性/性能权衡，并在每个任务中保留大量非零参数（例如10%-30%）。多任务学习和基于特征的迁移允许每个任务进行更有效的参数迁移学习。\n\n多任务学习和 feature-based 的迁移允许每个任务的参数效率更高的迁移学习：\n\n- 《Multi-Task Deep Neural Networks for Natural Language Understanding》\n- 《BAM! Born-Again Multi-Task Networks for Natural Language Understanding》\n- 《BERT and PALs: Projected attention layers for efficient adaptation in multi-task learning》\n- 《Sentence- BERT: Sentence Embeddings using Siamese BERT-Networks》\n\n这些方法在共享模型的基础上训练少量附加参数（例如线性层）。然而，多任务学习通常需要在训练期间访问所有任务，以防止灾难性遗忘，而 feature-based 的迁移学习（例如，基于任务不可知的句子表示）通常通过微调表现得更好\n\n一个有吸引力的中间地带是为特定任务 finetune 基础模型的扩展。这种方法在保持基于特征的转移的任务模块化的同时，还能抓住 finetune 的训练优势。\n\n例如，Adapters 使用较小的特定任务模块，在模型的层之间插入这种方法不需要在训练期间访问所有的任务，针对现实环境，随着新的任务流到达（\n\n《Parameter-efficient transfer learning for nlp》\n\n《Adapter- Fusion: Non-Destructive Task Composition for Transfer Learning》\n\n）发现，适配器层可以在GLUE基准上匹配完全微调的BERT的性能，而每个任务需要3.6%的额外参数（平均）。\n\nDiff pruning 是对预训练模型的一个新的扩展，目的是为了更参数有效的迁移学习。Diff pruning 不是修改模型的结构，而是通过一个特定任务的 diff 向量扩展基础模型。\n\n为了学习这个向量，我们将特定任务的模型参数重新参数化为 $θ_{task} = θ_{pretrained} + δ_{task}$，其中预训练的参数向量 $θ_{pretrained}$是固定的，特定任务的 diff 向量 $δ_{task}$ 是微调的。差异向量用L0-norm惩罚的可微调近似值进行重构，以鼓励稀疏性。\n\nDiff pruning 可以变得非常有效的参数，因为它只需要为每个任务存储 diff 向量的非零位置和权重。存储共享预训练模型的成本保持不变，并在多个任务中分摊。在GLUE基准上，Diff pruning 可以匹配完全微调的BERT基线的性能，而每个任务只微调0.5%的预训练参数。随着任务数量的增加，diff pruning在所需的存储量方面优于流行的基于剪枝的方法。\n\n## Background: Transfer Learning\n\nNLP中的迁移学习大多使用 pretrain finetune 范式，它从预训练的模型中为所有任务初始化一个模型参数子集，然后根据特定的任务目标进行调整。预训练目标包括上下文预测、自动编码、机器翻译，以及最近的语言建模的变种目标。\n\n这里我们考虑将转移学习应用于多个任务。 我们考虑的是具有潜在的未知任务集（可能以流的形式到达）的设置，其中每个任务 $τ∈T$ 有一个相关的训练集 $D_{\\tau} = \\{x_{\\tau}^{(n)}, y_{\\tau}^{(n)}\\}_{n=1}^{N}$。对于所有任务，目标是产生（可能是捆绑的）模型参数 $θ_τ$，使经验风险最小化，\n$$\nmin_{\\theta_{\\tau}} \\frac{1}{N} \\sum_{n=1}^N C(f_{\\tau}(x_{\\tau}^{(n)}; \\theta_{\\tau}), y_{\\tau}^{(n)}) + \\lambda R(\\theta_{\\tau})\n$$\n其中，$f_τ(\\cdot;θ_τ)$ 是一个关于输入的参数化函数（例如神经网络），$C(\\cdot,\\cdot)$是一个损失函数（例如交叉熵）1，R（-）是一个具有超参数 $λ$ 的操作性正则器。\n\n我们可以通过简单地学习每个任务的独立参数来使用 pretrain finetune 方法。然而，预训练模型的巨大规模使得这种方法的参数非常不方便。例如，广泛采用的模型，如BERT-BASE 和 BERT-LARGE，分别有1.1亿和3.4亿个参数，而他们同代的模型有数十亿的参数数。 解决这种参数效率低下的经典方法是通过联合训练，针对多个任务训练一个共享模型（连同特定任务的输出层）。然而，多任务学习的通常表述要求事先知道任务集 $T$，以防止灾难性的遗忘，这使得它不适合于任务集未知或任务流到来的应用。\n\n## Diff Pruning\n\nDiff pruning 将特定任务的微调表述为学习一个 diff 向量 $δ_τ$，该向量被添加到预先训练的模型参数 $θ$ 中，该参数保持固定。我们首先对特定任务的模型参数进行重新参数化，\n$$\n\\theta_{\\tau} = \\theta + \\delta_{\\tau}\n$$\n这导致了下面的经验风险最小化问题，\n$$\nmin_{\\delta_{\\tau}} L(D_{\\tau}, f_{\\tau} , \\theta+\\delta_{\\tau}) + \\lambda R(\\theta+\\delta_{\\tau})\n$$\n为了简洁起见，我们将 $L(D_τ, f_τ, θ_τ)$ 定义为:\n$$\nL(D_{\\tau} , f_{\\tau},\\theta_{\\tau}) = \\frac{1}{N} \\sum_{n=1}^N C(f_{\\tau}(x_{\\tau}^{(n)};\\theta_{\\tau}), y_{\\tau}^{(n)})\n$$\n这种微不足道的重新参数化表明，存储预训练参数 $θ$ 的成本在不同的任务中被分摊，而新任务的唯一边际成本是 diff向量。如果我们能将 $δ$ 正则化，使其稀疏，从而使  $||\\delta_{\\tau}||_0 << ||\\theta||_0$，那么随着任务数量的增加，这种方法可以变得更具有参数效率。我们可以用差值向量的L0-norm惩罚来指定这一目标,\n$$\nR(\\theta + \\delta_{\\tau}) = ||\\delta_{\\tau}||_0 = \\sum_{i=1}^d 1 \\ \\ \\{\\delta_{\\tau,i} \\neq 0\\}\n$$\n\n### Differentiable approximation to the L0-norm\n\n这个正则器很难优化，因为它是不可微分的。为了近似这个L0目标，我们采用了一种基于梯度的学习方法，即使用一个宽松的掩码向量进行L0稀疏度学习《Learning Sparse Neural Networks through L0 Regularization》\n\n这种方法包括将 binary vector 放宽到连续空间，然后与密集的权重向量相乘，以确定在训练中应用多少权重向量。训练结束后，掩码被制成确定性的，并且很大一部分 diff 向量为零。\n\n为了应用这种方法，我们首先将 $δ_τ$ 分解成一个二进制掩码向量，再乘以一个密集向量。\n$$\n\\delta_{\\tau} = z_{\\tau} \\odot w_{\\tau} , \\ \\ \\ z_{\\tau} \\in \\{0,1\\}, \\ w_{\\tau}\\in R^d\n$$\n我们现在对真实目标进行下限，并对关于 $z_τ$ 的期望进行优化，其分布 $p(z_τ; α_τ)$ 初始是伯努利，并引入参数 $α_τ$。\n$$\nmin_{\\alpha_{\\tau}, w_{\\tau}} E_{z_{\\tau} \\sim p(z_{\\tau};\\alpha_{\\tau})} [L(D_{\\tau}, f_{\\tau},\\theta + \\delta_{\\tau}) + \\lambda||\\delta_{\\tau}||_0]\n$$\n这个目标仍然因为 $z_τ$ 的离散性而变得复杂，但是这个期望为经验上有效松弛提供了一些指导。\n\n遵循先前的工作《Learning Sparse Neural Networks through L0 Regularization》，将 $z_τ$ 放宽到连续空间 $[0, 1]^d$，并采用拉伸的  Hard-Concrete 分布，这样就可以使用路径梯度估计器。具体来说，$z_τ$ 现在被定义为来自均匀分布的样本 $u$ 的一个确定性和（次）可微函数。\n$$\n    \\begin{equation}\\begin{split} \n u &\\sim U(0, 1)\\\\\n s_{\\tau} &= \\sigma(logu - log(1-u) + \\alpha_{\\tau}) \\\\\n \\hat s_{\\tau} &= s_{\\tau} \\times (r - l ) + l \\\\\n z_{\\tau} &= min (1,max(0, \\hat s_{\\tau}))\n    \\end{split}\\end{equation}\n$$\n这里 $l<0, r>1$是两个常数，用来将 $s_τ$ 拉伸到区间 $(l,r)^d$，然后用 $min(1, max(0, \\cdot))$ 操作将它夹在 $[0, 1]^d$中。在这种情况下，我们有一个预期L0-norm的可微闭式表达。\n$$\nE[||\\delta_{\\tau}||_0] = \\sum_{i=1}^d \\sigma(\\alpha_{\\tau,i} - log\\frac{-l}{r})\n$$\n因此，最终的优化问题由以下方式给出，\n$$\nmin_{\\alpha_{\\tau}, w_{\\tau}} E_{u\\sim U[0,1]} [L(D_{\\tau}, f_{\\tau}, \\theta + z_{\\tau} \\odot w_{\\tau})] + \\lambda \\sum_{i=1}^d \\sigma(\\alpha_{\\tau} - log\\frac{-l}{r})\n$$\n为了减少符号的混乱，我们把没有经过预训练的特定任务输出层的参数归入θ。我们现在可以利用路径梯度估计器来优化关于 $α_τ$ 的第一项，因为期望不再依赖于它。 训练后，我们通过对 $u$ 采样一次以获得 $z_τ$（即 不一定是二进制向量，但由于钳位函数的原因，其维数非常多，恰好为零），然后设置 $δ_τ = z_τ \\odot w_τ $\n\n### L0-ball projection with magnitude pruning for sparsity control\n\n微分 L0 正则化使我们能够实现高稀疏率。然而，最理想的是设置一个精确的稀疏率，特别是考虑到需要参数预算的应用。由于正则化系数 $λ$ 是某个 $η$ 的约束条件 $E[||δ_τ||_0]< η$ 的拉格朗日乘数，原则上可以通过搜索不同的 $λ$ 值来实现。 然而，我们发现通过训练后投影到目标 L0-ball 上实现精确的稀疏率更有效率，而且经验上也更有效。\n\n《Structured Pruning of Large Language Models》\n\n具体来说，我们对 diff 向量 $δ_τ$ 使用 magnitude pruning 幅度修剪，通过在 $δ_τ$ 中只保留前  $t \\%\\times d$ 的值来达到稀疏率 $t\\%$。注意，与标准的 magnitude pruning 不同，这是基于diff向量值的幅度而不是模型参数。我们发现，在固定非零掩码的情况下进一步微调 $δ_τ$ 以保持良好的性能是很重要的，这也是 magnitude pruning 中经常出现的情况。由于这种通过投射到L0-ball 上的参数效率可以在没有自适应 diff puning的情况下应用，这样的方法将作为我们在实证研究中的基线之一。\n\n### Structured Diff Pruning\n\n为了使diff pruning能够适应模型结构，我们考虑了一个结构化的扩展，其中包括维度之间的依赖性。假设，这种方法可以让模型学会在局部区域修改参数，而不是独立处理每个参数。修改正则器，首先将参数索引分为G组 $ \\{g(1),...,g(G)\\}$，其中 $g(j)$ 是由组 $g(j)$ 支配的参数指数的子集。\n\n然后，为每个组 $g(j)$ 引入一个标量 $z^j_τ$（及相关参数 $α^j_τ$），并将索引 $i\\in g(j)$的特定任务参数分解为 $δ_j = z_{τ,i} - z^j_τ - w_{τ,i}$\n\n然后，期望的L0范数由下式给出:\n$$\n    \\begin{equation}\\begin{split} \n \tE[||\\delta_{\\tau}||_0] &= \\sum_{j=1}^G \\sum_{i\\in g(j)} E[1\\ \\{z_{\\tau,i}\\ \\cdot z_{\\tau}^g > 0\\}] \\\\\n \t&= \\sum_{j=1}^G \\sum_{i\\in g(j)} \\sigma(\\alpha_{\\tau,i} - log\\frac{-l}{r}) \\cdot \\sigma(\\alpha_{\\tau}^j - log\\frac{-l}{r})\n    \\end{split}\\end{equation}\n$$\n我们可以像以前一样用基于梯度的优化训练。一个组中的参数被正则器所鼓励，共同被移除。\n\n## Experiments\n\n### Model and datasets\n\n为了评估，使用GLUE基准以及SQuAD抽取式问题回答数据集。按照Adapters，在GLUE任务的以下子集上测试。\n\n- 多类型自然语言推理（MNLI），目标是预测两个句子之间的关系是包含关系、矛盾关系还是中性关系（我们在 $MNLI_m$ 和 $MNLI_{mm}$上进行测试，分别对匹配/不匹配的领域进行测试）；\n- Quora问题对（QQP），一个分类任务，预测两个问题是否语义等同；\n- 问题自然语言推理（QNLI），必须预测一个句子是否是问题的正确答案。\n- Stanford Sentiment Treebank (SST-2)，一个预测电影评论情绪的句子分类任务；\n- Corpus of Linguistic Acceptability (CoLA)，其目标是预测一个句子在语言上是否可以接受。\n- 语义文本相似性基准（STS-B），必须预测两个句子之间的相似性等级；\n- 微软研究院转述语料库（MRPC），目标是预测两个句子是否在语义上等同；\n- 识别文本关联（RTE），必须预测第二个句子是否被第一个句子所包含。\n\n该基准对CoLA使用Matthew's correlation，对STS-B使用Spearman，对MRPC/QQP使用F1 score，对MNLI/QNLI/SST- 2/RTE使用accuracy。\n\n### Baselines\n\n将结构化和非结构化的 diff pruning 变体与以下基线进行比较：\n\n- Full finetuning：像往常一样对BERT-LARGE进行完全微调\n- Last layer finetuning：仅微调倒数第二层(连同最终输出层)\n- Adapters：该研究在预训练模型的每一层之间训练特定任务的瓶颈层，通过改变瓶颈层的大小，可以对参数效率进行控制。\n- Non-adaptive diff pruning：在magnitude pruning的基础上进行diff pruning（即，通过通常的微调获得$\\theta_{\\tau}$，设置 $\\delta_{\\tau} = \\theta_{\\tau} - \\theta$，然后应用magnitude pruning，再对 $\\delta_{\\tau}$ 进行额外的微调）。对于diff pruning，我们将目标稀疏率设置为0.5%\n\n![](https://i.loli.net/2021/11/07/pa6jXZ2JvkinyCO.png)\n\n### Structured vs. Non-structured Diff Pruning\n\n![](https://i.loli.net/2021/11/07/rGCJ4qX1cn9d2R8.png)\n\n结构化Diff Pruning 为每个组引入了一个额外的掩码，这鼓励了对整个组进行 pruning。这比传统的组稀疏技术的限制性要小，这些技术被用于L0-norm松弛，迫使一个组中的所有参数共享同一个掩码。然而，我们仍然期望整个组更经常地被pruning 掉，这可能会使学习过程偏向于完全消除或将非零 diff 聚在一起。在表3中，我们确实发现，结构化的差异修剪导致的微调模型更有可能使整个组与它们的预训练值（零差异）没有变化。\n\n### Task-specific Sparsity\n\n![](https://i.loli.net/2021/11/07/bP5n9iavNcFOJqB.png)\n\n预训练模型的不同层被认为是对不同信息的编码。鉴于每个任务可能会招募不同种类的语言现象嵌入到隐藏层中，我们假设 diff pruning 将通过特定任务的微调来修改预训练模型的不同部分。图2显示了每个任务中不同层的非零 diff 参数的百分比。我们发现，不同的任务确实修改了网络的不同部分，尽管有些任务之间存在一些质量上的相似性，例如QNLI和QQP（都必须对问题进行编码），以及MRPC和STS-B（都必须预测句子间的相似性）。嵌入层对所有任务的修改都很稀疏。虽然稀疏性分布的一些变化是由于简单的随机性造成的，但我们确实观察到在同一任务的多次运行中存在一定程度的一致性。\n\n\n\n### Effect of L0-ball projection\n\n![](https://i.loli.net/2021/11/07/8MiDzmJnkHr1TGp.png)\n\n应用 magnitude pruning 幅度修剪来投影到L0-ball上是实现精确稀疏目标的关键。如表4所示，我们观察到通过此方法在性能上几乎没有损失。我们重申，使用固定掩码进行微调至关重要，即使对于不应用幅度修剪的方法也是如此。\n\n![](https://i.loli.net/2021/11/07/YsyAIgbN5el4njO.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"谣言、虚假信息综述","url":"/2021/11/03/A-Survey-on-Natural-Language-Processing-for-Fake-News-Detection/","content":"\n[TOC]\n\n# 谣言、虚假信息综述\n\n------\n\n# A Survey on Natural Language Processing for Fake News Detection\n\n## Abstract\n\n虚假新闻检测是自然语言处理（NLP）中的一个关键但具有挑战性的问题。**社交网络平台的迅速崛起不仅带来了信息可及性的大幅提高，而且也加速了假新闻的传播。因此，假新闻的影响越来越大，有时甚至延伸到线下世界，威胁到公共安全。鉴于海量的网络内容，自动检测假新闻是一个实用的NLP问题，对所有在线内容提供商都有用，以减少人类检测和防止假新闻传播的时间和精力。**在本文中，我们描述了假新闻检测所涉及的挑战，也描述了相关任务。我们系统地回顾和比较了为该任务开发的任务描述、数据集和NLP解决方案，还讨论了它们的潜力和局限性。基于我们的见解，我们概述了有希望的研究方向，包括更精细、详细、公平和实用的检测模型。我们还强调了假新闻检测和其他相关任务之间的区别，以及NLP解决方案对假新闻检测的重要性。\n\n## 1 Introduction\n\n自动假新闻检测是评估新闻中 claims（声明，主张） 的真实性的任务。这是一个新的但关键的NLP问题，因为传统的新闻媒体和社交媒体对社会中的每个人都有巨大的社会政治影响。例如，对假新闻的曝光会导致对某些政治候选人的无效、疏远和嘲讽（Balmas，2014）。假新闻甚至与威胁公共安全的真实世界的暴力事件有关（例如，比萨门（Kang和Goldman，2016））。检测假新闻是NLP可以帮助的一个重要的应用，因为它也对技术如何在教育公众的同时促进验证 claims 的真实性产生了更广泛的影响。\n\n**这项任务的传统解决方案是请专业人员，如记者，根据以前说过的或写过的事实，对照证据来检查 claims。然而，这样做既费时又费力。**例如，PolitiFact 需要三位编辑来判断一条新闻的真伪。随着互联网社区和信息传播速度的快速增长，互联网内容的自动假新闻检测已经引起了人工智能研究界的兴趣。**自动假新闻检测的目标是减少人类检测假新闻的时间和精力，帮助我们停止传播假新闻。随着计算机科学子领域的发展，如机器学习（ML）、数据挖掘（DM）和NLP，假新闻检测的任务已从不同角度得到研究。**\n\n在本文中，我们从NLP的角度调查了自动假新闻检测。概括地说，我们介绍了假新闻检测的技术挑战，以及研究人员如何定义不同的任务并制定ML解决方案来解决这个问题。我们讨论了每项任务的优点和缺点，以及潜在的陷阱和弊端。更具体地说，我们对假新闻检测的研究工作进行了概述，并对其任务定义、数据集、模型构建和性能进行了系统的比较。我们还讨论了这个方向上的未来研究的指导方针。本文还包括一些其他方面，如社会参与分析。我们的贡献有三个方面。\n\n- 对用于自动检测假新闻的自然语言处理解决方案进行了首次全面调查。\n- 系统地分析了假新闻检测如何与现有的NLP任务保持一致，并讨论了问题的不同公式的假设和值得注意的问题。\n- 对现有的数据集、NLP方法和结果进行了分类和总结，为对这个问题感兴趣的新研究人员提供了第一手的经验和易懂的介绍。\n\n## 2 Related Problems\n\n### 2.1. Fact-Checking\n\n事实核查的任务是评估政治家、专家学者等公众人物提出的主张的真实性。许多研究者并不区分假新闻检测和事实核查，因为它们都是为了评估 claims主张 的真实性。一般来说，**假新闻检测通常专注于新闻事件，而事实核查则更广泛。** Thorne和Vlachos（2018）对这一主题进行了全面的回顾。\n\n### 2.2. Rumor Detection\n\n**谣言检测并没有一个一致的定义**。最近的一项调查（Zubiaga等人，2018）将谣言检测定义为将个人主张分为谣言和非谣言，其中谣言被**定义为在发布时由未经核实的信息片段组成的声明**。**换句话说，谣言必须包含可以验证的信息，而不是主观的意见或感觉。**\n\n### 2.3. Stance Detection\n\n立场检测是指从文本中评估作者在辩论中站在哪一边的任务。它与假新闻检测不同，因为它不是针对真实性，而是针对一致性。**立场检测可以是假新闻检测的一个子任务，因为它可以应用于搜索文本的证据**（Ferreira和Vlachos，2016）。PHEME，假新闻数据集之一，有与新闻相关的推文，捕捉到用户信任或不信任的行为。\n\n### 2.4. Sentiment Analysis\n\n情感分析是一项提取情感的工作，例如顾客对一家餐厅的好感或负面印象。与谣言检测和假新闻检测不同的是，情感分析不是为了对主张进行客观验证，而是为了分析个人情感。\n\n## 3. Task Formulations\n\n在第2节中，我们比较了与假新闻检测有关的问题，以确定本调查的范围。在本调查中，**假新闻检测的一般目标是识别假新闻，定义为看似新闻的虚假故事，包括在谣言检测中被判断为可以验证的信息的谣言。**特别是，我们专注于文本内容的假新闻检测。输入可以是文本，从简短的声明到整个文章。输入与使用的数据集有关（见第4节），而且还可以附加附加信息，如发言人的身份。\n有不同类型的标签或评分策略用于假新闻检测。在大多数研究中，假新闻检测被表述为一个分类或回归问题，但分类的使用更为频繁。\n\n### 3.1. Classification\n\n最常见的方法是将假新闻的检测制定为一个二元分类问题。**然而，将所有的新闻分为两类（假的或真的）是很困难的，因为存在着新闻部分是真的和部分是假的情况。**为了解决这个问题，增加额外的类别是常见的做法。主要是为既不完全真实也不完全虚假的新闻设置一个类别，或者设置两个以上的真实度作为附加类别。当使用这些数据集时，预期的输出是多类标签，而这些标签是作为独立的标签学习的，具有i.i.d的假设（Rashkin等人，2017；Wang，2017）。\n假新闻分类器取得良好性能的条件之一是有足够的标签数据。然而，要获得可靠的标签需要大量的时间和人力。因此，人们提出了半/弱监督和无监督的方法（Rubin和Vashchilko，2012；Bhattacharjee等人，2017）。\n\n### 3.2. Regression\n\n虚假新闻检测也可以被表述为一项回归任务，其输出是真实性的数字分数。Nakashole和Mitchell（2014）采用了这种方法。通常情况下，评估是通过计算预测分数和地面真实分数之间的差异或使用Pearson/Spearman Correlations来完成。然而，由于可用的数据集有离散的地面真实分数，这里的挑战是如何将离散的标签转换成数字分数。\n\n## 4. Datasets\n\n数据集地址集合：\n\n https://www.sohu.com/a/377489976_787107\n\nhttps://www.zhihu.com/question/264356019/answer/1327236489\n\n![](https://i.loli.net/2021/11/03/bJIfvLDMoyYBFsc.png)\n\n自动假新闻检测的一个重要挑战是数据集的可用性和质量。我们将公共假新闻数据集分为三类：\n\n- claims :  是一个或几个句子，包括值得验证的信息（表2中有一个样本）\n- 整篇文章 : 是由许多相互关联的句子组成，构成信息的整体。\n- 社交网络服务（SNS）数据，在长度上与 claims 相似，但以账户和帖子的结构化数据为特征，包括大量的非文本数据。\n\n### 4.1. Claims\n\nPOLITIFACT、CHANNEL4.COM、 SNOPES 是三个来源的新闻中的人工标注的短文，这些短文是人工收集和标注的。编辑们从各种场合，如辩论、竞选、Facebook、Twitter、采访、广告等，精心挑选了这些说法。许多数据集是基于这些网站创建的。\n\nVlachos和Riedel（2014）发布了第一个公开的假新闻检测数据集，收集了来自POLITIFACT和CHANNEL4.COM的数据。这个数据集有221条声明，其中有制作日期、说话人和URL，以及五分制的真实性标签。EMERGENT（Ferreira和Vlachos，2016）也是早期的声称-验证数据集的工作。它是在事实核查的背景下进行立场分类，包括带有一些支持或反对文本的主张。这个数据集可以改善事实核查，条件是提供一些与 Claims 有关的文章。\n\nVlachos只包括221项索赔，Emergent只包括300项索赔，因此将其用于基于机器学习的评估是不切实际的。这些天来，有许多索赔的数据集被公布，它们可以作为前两者的改进版使用。\n\n最近一个用于假新闻检测的基准数据集是LIAR（Wang，2017）。这个数据集与Vlachos和Riedel（2014）一样从Politifact收集数据，但包括12,836个真实世界的短文，每个声明都被标记为六级真实性。该数据集中还包括关于主题、政党、背景和发言人的信息。对于来自Politifact文章的数据集，Rashkin等人（2017）也发表了大型数据集。他们也收集了来自PunditFact（Politifact的衍生网站）的文章。\n\nFever 是一个为事实核查提供相关证据的数据集。在这一点上，它与EMERGENT相似。Fever包含185,445个由维基百科数据生成的说法。每个声明都被标记为支持、反驳或信息不足。他们还标注了他们使用维基百科中的哪些感性内容作为证据。Fever使我们有可能开发出一个能够与证据一起预测主张的真实性的系统，尽管来自维基百科的事实和证据的类型可能仍然表现出与现实世界的政治运动的一些主要风格差异。\n\n### 4.2. Entire-Article Datasets\n\n有几个假新闻检测的数据集可以预测------整个文章是真的还是假的。例如，FAKENEWSNET（Shu等人，2017a；Shu等人，2017b；Shu等人，2018）是一个正在进行的假新闻研究的数据收集项目。它包括基于BuzzFeed和PolitiFact的假新闻文章的标题和正文。它还收集了来自Twitter的这些文章的社会参与信息。\nBS DETECTOR4是从一个名为BS Detector的浏览器扩展中收集的，表明其标签是BS Detector的结果，而不是人类注释者。BS Detec- tor通过检查人工编制的不可靠域名列表，搜索有问题的网页上的所有链接，以寻找不可靠来源的参考。\n\n### 4.3. Posts On Social Networking Services\n\nBUZZFEEDNEWS收集了9家新闻机构在Facebook上的2282个帖子。每个帖子都由5名BuzzFeed记者进行事实核查。这个数据集的优势在于，文章是从左倾和右倾组织的两边收集的。BUZZFEEDNEWS有两个丰富的版本。Potthast等人（2017）通过添加链接文章等数据对其进行了丰富，而BUZZFACE（Santia和Williams，2018）则通过Facebook上与新闻文章相关的160万条评论来扩展BuzzFeed数据集。\n\nSOME-LIKE-IT-HOAX（Tacchini等人，2017）由32个Facebook页面的15500个帖子组成，也就是组织的公开资料（14个阴谋论和18个科学组织）。这个数据集是根据发布者的身份而不是帖子级别的注释来标注的。这种数据集的一个潜在隐患是，这种标签策略可能导致模型学习每个发布者的特征，而不是假新闻的特征。\n\nPHEME（Zubiaga等人，2016）和CREDBANK(Mitra and Gilbert, 2015)是两篇文章。PHEME包含9个有新闻价值的事件的330条twitter线程（一个人的一系列连接tweet），标记为真或假。CREDBANK包含覆盖96天的6000万条推文，被分组为1049个事件，有一个30维的真实性标签向量。每个事件都由30名人类注释者以5分的李克特量表对其真实性进行评分。他们将30个评分串联起来作为一个向量，因为他们发现很难将其简化为一个一维的分数。\n\n如上所述，这些数据集是为验证推文的真实性而创建的。因此，它们只限于少数主题，并且可能包括与新闻没有关系的推文。因此，这两个数据集对于假新闻的检测并不理想，它们更多地被用于谣言检测。\n\n## 5. Methods\n\n我们介绍假新闻的检测方法。像往常一样，我们首先将输入文本预处理成合适的形式（5.1.）。如果数据集有整个文章的长度，可以使用修辞学方法作为手工制作的特征提取之一（5.3.）。如果数据集有EMERGENT或FEVER这样的证据，我们可以使用5.4.中的方法来收集输出的证据。\n\n### 5.1. Preprocessing\n\n预处理通常包括标记化、词干化和概括化或加权词。为了将标记化的文本转换为特征，经常使用术语频率-反向文档频率（TF-IDF）和语言学查询和单词计数（LIWC）。对于单词序列，通常使用预先学习的单词嵌入向量，如word2vec（Mikolov等人，2013）和GloVe（Pennington等人，2014）。\n\n当使用整个文章作为输入时，一个额外的预处理步骤是从原始文本中识别中心主张。Thorne等人（2018）使用TF- IDF和DrQA系统（Chen等人，2017）对句子进行排名。这些操作与子任务密切相关，如单词嵌入、命名实体识别、消歧义或核心参考解析。\n\n### 5.2. Machine Learning Models\n\n如第3节所述，现有的研究大多使用监督方法，而半监督或无监督的方法则较少使用。在本节中，我们主要通过几个实际的例子来描述分类模型。\n\n#### 5.2.1. Non-Neural Network Models\n\nSupport Vector Machine (SVM) 和 Naive Bayes Clas- sifier (NBC) 是经常使用的分类模型（Conroy等人，2015；Khurana和Intelligentie，2017；Shu等人，2018）。这两种模型在结构上有很大不同，它们通常都被用作基线模型。Logistic回归（LR）（Khurana和Intelligentie，2017；Bhattacharjee等人，2017）和决策树，如Ran- dom Forest Classifier（RFC）（Hassan等人，2017）也被偶尔使用。\n\n#### 5.2.2. Neural Network Models\n\n循环神经网络（RNN）在自然语言处理中非常流行，特别是长短时记忆（LSTM），它解决了梯度消失的问题，因此它可以捕获较长期的依赖关系。在第6节中，许多基于LSTM的模型在LIAR和FEVER上都有很高的准确性。此外，Rashkin等人（2017）建立了两个LSTM模型，将文本作为简单的词嵌入输入到一边，并作为LIWC特征向量输入到另一边。在这两种情况下，它们都比NBC和MaxEntropy(MaxEnt)模型更准确，尽管只是轻微的。\n\n卷积神经网络（CNN）也被广泛使用，因为它们在许多文本分类任务中都很成功。Wang（2017）使用了一个基于Kim的CNN（Kim，2014）的模型，将最大池的文本代表与双向LSTM的元数据代表连接起来。CNN也被用于提取具有各种元数据的特征。例如，Deligiannis等人（2018）将新闻和出版商之间的关系图样数据作为CNN的输入，并用它们评估新闻。\n\nKarimi等人（2018）提出了多源多类假新闻检测框架（MMFD），其中CNN分析索赔中每个文本的局部模式，LSTM分析整个文本的时间依赖性，然后通过全连接网络传递所有最后的隐藏输出的连接。 这个模型利用了两种模型的特点，因为LSTM对长句子的效果更好。\n\n注意力机制经常被纳入神经网络以获得更好的性能。Long等人（2017）使用了一个注意力模型，该模型结合了说话人的名字和语句的主题，首先关注特征，然后将加权向量送入LSTM。这样做使准确率提高了约3%（表3）。Kirilin和Strube（2018）使用了一个非常类似的注意机制。Pham（2018）使用了记忆网络，它是一种基于注意力的神经网络，也分享了注意力机制的想法。\n\n### 5.3. Rhetorical Approach\n\n修辞结构理论（RST），有时与矢量空间模型（VSM）相结合，也被用于假新闻检测（Rubin等人，2015b；Della Vedova等人，2018；Shu等人，2017b）。RST是一个故事连贯性的分析框架。通过定义文本单元的语义作用（例如，一个句子代表环境、证据和目的），这个框架可以系统地识别基本思想，并分析输入文本的特点。然后根据其连贯性和结构来识别假新闻。为了用RST解释结果，VSM被用来将新闻文本转换成向量，在高维RST空间中与真新闻和假新闻的中心进行比较。向量空间的每个维度表示新闻文本中修辞关系的数量。\n\n### 5.4. Collecting Evidence\n\n基于RTE（识别文本蕴涵）（Dagan等人，2010）的方法经常被用来收集和利用证据。RTE是识别句子之间关系的任务。通过使用RTE方法从数据源（如新闻文章）收集支持或反对输入的句子，我们可以预测输入是否正确。基于RTE的模型需要文本证据进行事实核查，因此这种方法只有在数据集包括证据时才能使用，如FEVER和Emergent。\n\n--------\n\n# The Future of False Information Detection on Social Media: New Perspectives and Trends\n\n社交媒体上虚假信息的大量传播已经成为一种全球性的风险，隐性地影响着公众舆论，威胁着社会/政治发展。因此，虚假信息检测（FID）已成为近年来风起云涌的研究课题。作为一个前景广阔、发展迅速的研究领域，我们发现很多人已经为FID的新研究问题和方法付出了努力。因此，有必要对FID的新研究趋势做一个全面的回顾。我们首先简要回顾了FID的文献历史，在此基础上，我们提出了几个新的研究挑战和技术，包括**早期检测、多模态数据融合检测和解释式检测**。我们进一步研究了FID中各种人群智能的提取和使用，这为解决FID的挑战铺平了道路。最后，我们对FID的开放性问题和未来的研究方向提出了自己的看法，如**模型对新事件的适应性/通用性、对新型机器学习模型的接纳、人群智慧的聚合、检测模型中的对抗性攻击和防御等等**。                         \n\n## INTRODUCTION\n\n> 社会化媒体平台（如Twitter1、Facebook2、新浪微博3）彻底改变了信息的传播模式，大大提高了信息传播的速度、数量和种类。然而，社交媒体为事实和虚假信息的快速传播提供了便利。根据奈特基金会最近的一项调查4，美国人估计，他们在社交媒体上看到的新闻有65%是假新闻。此外，虚假信息通常在社交网络中传播得更快、更深、更广。\n\n利用社交媒体传播误导性信息的敌对行为构成了一种政治威胁[8]。例如，在2016年美国总统大选期间，有多达529种不同的低可信度言论在推特上传播[73]，约有1900万个恶意机器人账户发布或转发了支持特朗普或克林顿的帖子，这有可能影响了选举。2018年，《科学》杂志发表了关于 \"假新闻 \"的主题期刊，他们报道说，假 statements 声明可以引起人们的恐惧和惊讶的感觉，这有助于社会恐慌。例如，一段名为索马里人 \"被推入浅坟 \"埃塞俄比亚的虚假视频，引起了埃塞俄比亚两个种族之间的暴力冲突；一条网上的虚假信息，暗示希腊已经取消了转机限制，导致希腊警察与移民发生了冲突。上述例子表明，虚假信息的泛滥对社会信息传播的生态构成了严重威胁[91]。社交媒体用户每天都会接触到大量关于各种主题的信息。对用户来说，判断每条信息的可信度是不现实的，也是不可行的[140]。因此，检测社交媒体上的虚假信息是非常迫切的。\n\n随着新媒体时代的到来，多模态的社交媒体帖子已经逐渐成为社交媒体的主流。因此，随着人工智能（AI）的快速发展，未来的网络虚假信息将超越文字，大规模地扩展到高质量和可操控的信息材料，如图像、视频和音频[8]。例如，DeepFakes[44, 56]利用深度学习模型创建了真实人物的音频和视频，说和做他们从未说过或做过的事情，这使得虚假信息越来越逼真，越来越难以辨别。虽然自动虚假信息检测不是一个新现象，但目前它已经引起了越来越多的公众关注。\n\n为了便于理解和解释网络和社交媒体上的虚假信息，Kumar等人[89]根据其意图和知识对虚假信息进行总结和分类。\n\n![](https://i.loli.net/2021/11/03/kq1zrQMWtJ9jx2U.png)\n\n按照意图，false information 可以分为错误信息 misinformation 和虚假信息 disinformation，错误信息是指在事件演变过程中产生的虚假信息，或者是在知识更新过程中产生的虚假信息，没有误导的目的[87，150]；虚假信息是指为了某种目的而故意误导他人的虚假信息[36，166]。根据知识，虚假信息可以被认为是基于意见的，它表达了用户的主观意见，描述了一些没有独特基础真相的情况，而基于事实的，是捏造或违背绝对基础真相的信息[172]。此外，相关文献中还有一些类似的术语，**如谣言、假新闻。谣言一词通常指的是在发布时未经核实的信息**[204]。**因此，谣言可能会被证明是真的或假的。与谣言不同的是，假新闻一词被广泛用于指那些故意的、可验证的虚假新闻文章**[162]。我们根据其意图对这些术语进行分类，如图1所示。尽管上述术语之间有区别，但它们都涉及到虚假信息的传播，并有能力或意图影响一些用户。因此，本调查坚持这些术语的定义，并从技术角度回顾了社交媒体上虚假信息检测（FID）的发展。\n\n> 近年来，在FID方面有很多努力。根据现有FID方法中使用的特征类型，我们将其分为**四类：基于内容的方法、基于社会环境的方法、基于特征融合的方法和基于深度学习的方法**。**基于内容的检测方法**主要利用从社交帖子中提取的文本或视觉特征进行二元分类（真实或虚假）。**基于社会环境的方法**一般依赖于丰富的用户之间的互动特征，如评论、转贴、关注等。**基于特征融合的方法**综合利用了内容特征和社会环境特征。此外，**基于深度学习的方法**主要通过神经网络学习信息的潜在深度表示。\n\n尽管过去几年对FID做了很多研究，但仍有许多遗留问题需要解决。**首先，现有的FID方法大多利用内容或传播特征，并且通常在虚假信息的整个生命周期中工作良好，这可能导致早期检测的性能不佳。**由于虚假信息可能在短短几分钟内产生严重影响，因此在早期阶段检测它们是至关重要的。第二，随着多模态帖子在社交网络上传播的增加，传统的基于文本的检测方法已不再可行，在更复杂的情况下，利用图像或视频进行FID是有益的。第三，目前的检测方法只给出了声明是否为假的最终结果，但缺乏做出决定的理由。对于揭穿不准确的信息并防止其进一步传播，给出一个令人信服的解释是非常重要的。\n\n本文旨在深入调查与FID方法有关的最新发展。目前已经有一些关于FID的调查[39, 162, 201, 204]。Zhou等人[201]从**基于知识、基于风格、基于传播和基于可信度**等四个角度研究假新闻，并总结了心理学和社会科学的相关检测方法。Zubiaga等人[204]专注于谣言分类系统，研究了现有的识别疑似谣言、收集谣言相关帖子、检测帖子立场和评估目标事件可信度的方法。同样，Fernandez等人[39]将错误**信息检测分为四个阶段：错误信息识别、传播、验证和驳斥。**他们相应地组织了现有的在线错误信息检测系统。Shu等人[162]从数据挖掘的角度将检测模型分为基于新闻内容的模型和基于社会背景的模型，并总结了虚假新闻检测算法的评估测量方法。我们的调查与其他相关调查的区别如下：\n\n- 上述调查对基于深度学习的虚假信息检测方法关注甚少。然而，在过去的三年里，深度学习模型已经被广泛地应用于FID。为了给检测方法提供一个最新的全面调查，我们调查并交叉比较了最近基于深度学习的方法。\n- 本文回顾了近年来在FID领域出现的新问题和新技术，如早期检测、多模态数据融合检测和解释式检测等。此外，我们的论文从人群智能的角度调查了这些新问题和有前途的工作，研究了利用人群智能促进FID的潜力。\n- 人工智能的发展提高了FID模型的性能，因此数据集已经变得和算法一样重要。本文为未来的研究人员梳理了自2015年以来广泛使用的开放数据集，这些数据集被现有的调查所忽视。\n\n与现有研究大多使用帖子内容不同，基于人群智能的方法旨在检测基于聚合的用户意见、猜想和证据的虚假信息，这是人类与帖子互动过程中注入的隐性知识（如帖子的发布、评论和转贴）。最重要的是，我们工作的主要贡献包括：\n\n- 基于对FID的简要文献回顾，我们集中讨论了它的最新研究趋势，包括对新事件的模型通用性、早期检测、基于多模态融合的检测和解释式检测。\n- 我们对基于人群智能的FID方法进行了调查，包括FID中人群智能的范围，基于人群智能的检测模型，以及人机混合融合模型。\n- 我们进一步讨论了FID的开放性问题和有前途的研究方向，如模型对新事件的适应性/通用性，拥抱新型机器学习模型，以及FID模型中的对抗性攻击和防御。\n\n本文的其余部分组织如下。我们在第2节中对现有的FID工作进行了简要的文献回顾。然后，我们在第3节调查了FID的几个新的研究趋势。在第4节中，我们强调了基于人群智能的检测，然后在第5节中介绍了FID的开放问题和未来方向。最后，我们在第6节中总结了本文。\n\n## 2 A BRIEF LITERATURE REVIEW\n\n本调查主要关注检测在社交网络上传播的虚假或不准确的说法，因此我们首先给出虚假信息检测问题的一般定义。\n\n- 对于一个具体的声明 $s$ ，它包含一组相关的 $n$ 个帖子 $P=\\{p_1, p_2, ..., p_n \\}$ 和一组相关的用户 $U=\\{u_1,u_2,...,u_m\\}$.每个 $p_i$ 由一系列代表帖子的属性组成，包括文字、图片、评论数量等。每个 $u_i$ 由一系列描述用户的属性组成，包括姓名、注册时间、职业等。\n- 让 $E = \\{e1,e2,...,en\\}$ 指的是 $m$ 个用户和 $n$ 个帖子之间的互动。每个 $e_i$ 被定义为 $e_i = \\{p_i,u_j,a,t\\}$，代表一个用户 $u_j$ 在时间 $t$ 通过行动 $a$ （发帖、转帖或评论）与帖子 $p_i$互动。\n\n定义2.1。false information错误信息检测：给定具有帖子集 $P$、用户集 $U$ 和参与集 $E$ 的语句 $s$，错误信息检测任务是学习预测函数 $F(S)\\to \\{0，1\\}$\n\n在下文中，我们对现有的FID技术进行了简要的文献回顾，分为四大类型，即基于内容、基于社会环境、基于特征融合和基于深度学习的方法，如表1（前三种类型）和表2（最后一种类型）所总结的。此外，我们还对现有的几个在线FID工具进行了总结，这些工具对于减轻虚假信息的影响和防止其进一步传播具有重要意义。\n\n### 2.1 Content-based Methods\n\n对于一个具体的事件，其微博一般是由一段文字来描述，往往与几张图片或视频相关。基于内容的方法主要是基于特定的写作风格或虚假文章的耸人听闻的标题，如词汇特征、句法特征和主题特征[143]。例如，Castillo等人[16, 17]发现，高可信度的推文有更多的URL，而且文本内容长度通常比低可信度的推文长。\n\n许多研究利用词法和句法特征来检测虚假信息。例如，Qazvinian等人[136]发现，语篇（POS）是FID的一个可区分的特征。Kwon等人[90]发现一些类型的情感是机器学习分类器的明显特征，包括积极的情感词（如爱、好、甜）、否定词（如不、不、永不）、认知行动词（如原因、知道）和推断行动词（如可能、也许）。然后，他们提出了一个周期性的时间序列模型来识别真实推文和虚假推文之间的关键语言差异。此外，Pérez-Rosas等人[128]总结了真实和虚假内容的语言学特征的差异，可以分为五类。\"Ngrams\"、\"标点符号\" \"心理语言学特征\"、\"可读性 \"和 \"句法\"。基于上述特征，使用线性SVM来识别虚假信息。Rashkin等人[141]总结了不可信的新闻内容的语言风格。具体来说，他们发现第一/第二人称代词在低可信度信息中使用的频率更高，夸张的词汇也是如此。\n\n词汇特征有时不能完全反映虚假信息的特征，因为它的位置性。因此，许多研究为FID引入了语义特征，如话题、情感和写作风格。例如，Potthast等人[134]利用不同的写作风格来检测虚假声明。同样地，Horne等人根据假新闻文章在标题风格上与真实新闻文章有很大不同的观察，提出了一个FID模型。Hu等人[68]提出了一个利用情感信息检测低可信度社交帖子的框架。Ito等人[70]将Latent Dirichlet Allocation（LDA）主题模型引入到推文可信度的评估中，他们提出了推文主题特征和用户主题特征，用于检测虚假信息。\n\n### 2.2 Social Context-based Methods\n\n传统的基于内容的方法是孤立地分析单个微博或主张的可信度，忽略了不同微博和事件之间的高度关联性。此外，大量的人类内容互动数据（发帖、评论、转帖、评级和标签等）为FID提供了丰富的参考信息。具体来说，基于社会环境的方法可以进一步分为基于帖子和基于传播的方法。\n\n(1) Post-based features\n\n基于帖子的方法主要依靠用户的帖子来表达他们对特定事件的情绪或意见。许多研究通过分析用户的可信度[95, 118]或立场[63, 116]来检测虚假信息。例如，Shu等人[164]从用户档案中探索出对FID真正有用的特征，从而减少检测过程中特征提取的负担。具体来说，他们发现性格外向和随和的用户不太可能受到虚假信息的影响。此外，Guess等人[57]指出，保守派更倾向于在Facebook中分享虚假帖子。Long等人[104]发现，在基于内容的检测方法中应用用户档案（如党派、验证信息和位置）可以提高其在FID上的表现。他们提出了一个混合检测模型，分别提取新闻内容的主题特征和用户属性特征。此外，Tacchini等人[170]发现，有不准确信息的社交帖子通常比真实的事实有更多的赞。因此，他们使用逻辑回归（LR）模型和众包算法，在用户喜欢的基础上检测虚假信息。\n\n(2) Propagation-based features\n\n基于传播的方法将帖子和事件的可信度作为一个整体进行评估[14]，这些方法通常关注信息传播网络的构建和可信度的传播。\n一些研究通过分析其传播模式来检测虚假信息。例如，Ma等人[107]发现，社会环境的特征会随着时间的推移而逐渐改变。因此，他们提出了一个DSTS模型来描述FID的社会背景特征的时间模式，该模型将信息传播序列划分为固定长度的片段，然后从每段帖子中提取基于内容和社会背景的特征，最后用SVM进行分类。Liu等人[102]构建了基于异质用户特定属性的信息传播网络，用于识别虚假信息的特殊传播结构。Kim等人[79]提出了一个贝叶斯非参数模型来描述新闻文章的传播特征，该模型联合利用文章的主题和用户兴趣来进行FID。此外，Wu等人[186]观察到虚假信息通常先由普通用户发布，然后由一些意见领袖转发，最后由大量的普通用户传播。然而，真相往往是由一些意见领袖发布，然后由大量用户直接传播。基于这一观察，他们提出了一个用于FID的混合SVM分类器，该分类器对信息传播结构、主题信息、用户属性等共同建模。\n此外，许多研究还通过构建特定的树状或网络结构来检测虚假信息。例如，Ma等人[108]将谣言相关的微博传播建模为传播树，他们提出了一种基于内核的方法来捕捉这些传播树之间的模式，以实现FID。此外，Gupta等人[62]构建了一个包含用户、帖子和事件的可信度传播网络来模拟虚假信息的传播过程。Jin等人[74]提出了一个连接微博、子事件和事件的三层可信度传播网络，用于信息可信度验证。\n\n### 2.3 Feature Fusion-based Methods\n\n基于内容的检测方法主要从写作风格、词汇和句法特征方面来识别真实和非真实的主张之间的差异，而基于社会背景的检测方法主要利用从信息传播过程中提取的特征。由于两类方法应用的特征可以互补[145]，最近许多研究者开始研究基于特征融合的新方法。例如，Vedova等人[30]利用了用户和帖子之间的互动信息，以及帖子的文本信息。具体来说，他们对社交帖子进行词干分析，并将每个帖子表示为单词的TF-IDF向量。之后，他们利用用户的喜欢行为来描述社会背景特征，与Tacchini等人的工作类似[170]，最后通过整合这两种信号来识别虚假信息。为了利用传统的内容特征（如词汇或句法特征），Volkova等人[176]将来自新闻内容的心理语言学信号和来自社会环境的作者观点作为FID中不同分类器的输入数据。此外，Shu等人[165]进一步探讨了出版商、新闻作品和用户之间的社会关系。他们提出了一个名为TriFN的通用检测框架，通过非负矩阵分解（NMF）算法对新闻内容、社会互动和新闻发布者之间的内在关系进行建模，用于识别低可信度信息。\n\n### 2.4 Deep Learning-based Methods\n\n基于深度学习的方法旨在自动抽象出虚假信息数据的高层表示。目前，大多数工作主要利用递归神经网络[106]和卷积神经网络[195]进行FID，如表2所示。在下文中，我们首先总结了广泛使用的深度学习模型，主要包括。\n\n![](https://i.loli.net/2021/11/03/stFAr7x1CKzyXME.png)\n\n- 卷积神经网络（CNN）。CNN是典型的前馈神经网络之一，有三种层，即卷积层、池化层和全连接层[135]。在卷积层中，多个滤波器（核）与输入向量卷积，生成特征图。之后，池化层降低特征图的维度，以加速网络的训练过程。通过多次卷积和池化操作，CNN可以从输入中捕获局部和全局特征。最后，CNN通过全连接层（如Softmax）输出分类结果。可以看出，FID模型可以通过调整过滤器的大小来捕捉词与词、短语与短语之间的内容特征。\n- 图卷积网络（Graph Convolutional Network，GCN）。GCN是一种处理图数据的神经网络，由卷积层和全连接层组成，可以有效捕捉图的结构特征[29, 83]。每个卷积层的隐藏状态矩阵由一个特殊矩阵的非线性变化得到，该矩阵是该图的相邻矩阵与其上一层的隐藏状态矩阵和权重矩阵的乘积。\n- 递归神经网络（RNN）。RNN可以有效地捕捉连续数据的特征，通过同一隐含层的神经元之间的信息传输，节省了以前的计算。社交网络帖子显然具有时间性特征，因此FID模型可以将帖子的交互数据划分为连续的片段，并通过RNN捕获其顺序性特征。然而，Glorot等人[49]发现，RNN可能存在梯度消失的问题，这使得它不具备长期记忆。因此，长短时记忆（LSTM）[65]和门控递归单元（GRU）[25]，一种具有门控机制的RNN，被广泛用于NLP中。LSTM增加了一个存储单元来存储当前网络状态，然后通过输入门、遗忘门和输出门的协调来控制信息流。虽然GRU没有引入额外的存储单元，但它可以通过一个复位门和一个更新门控制当前的存储。\n- 递归神经网络（RvNN）。RvNN与RNN类似，它将数据结构展开，可用于分析数据的分层结构[135]，如语法分析树。该模型由根节点、左叶节点和右叶节点组成。此外，每个节点都从直接的左、右子节点中学习它的表示方法，该方法是递归计算的，直到所有节点都被遍历。\n- 自动编码器（AE）。AE是一个无监督的学习模型，包括编码和解码阶段[64]。在编码阶段，输入数据通过多个隐藏层转化为潜伏向量，在解码阶段将其重构为原始数据。通过最小化重建误差，AE尽可能地学习了输入的表示。与AE相比，变异自动编码器（VAE）约束了编码阶段，成为一个生成模型[82]。编码阶段的隐藏层通过从特定的分布（如高斯分布）中取样来学习潜变量，然后将其输入到解码阶段以生成现实的样本。\n- 生成式对抗网络（GAN）。GAN是一种生成性神经网络，由生成器和鉴别器组成[51]。在反向传播的迭代过程中，鉴别器区分其输入是来自真实的数据集还是由生成器生成的虚假样本，而生成器则根据数据集的抽样分布生成真实的样本以混淆鉴别器。他们最终实现了纳什均衡，也就是说，生成器和鉴别器的性能不能再有任何提高。\n- 注意机制。注意机制通常被用来描述神经网络对输入序列的注意分布[7]。它计算当前输入序列和输出向量之间的匹配度，目的是捕捉输入的关键信息。匹配度越高，注意力分数就越高。因此，检测方法可以利用注意力机制来找到这些对FID贡献较大的词或短语。\n\n许多现有的研究利用深度神经网络，通过将相关帖子建模为时间序列数据来学习虚假信息的潜在文本表征。例如，Ma等人[106]提出了一个基于RNN的检测模型，该模型捕获了连续的用户评论流的时间-语言特征。Li等人[94]认为帖子流的前向和后向序列都传达了丰富的交互信息，因此他们提出了用于FID的双向GRU方法。Liu等人[101]认为假新闻和真新闻的传播模式存在差异，他们利用CNN和GRU对传播路径进行分类，以识别低可信度信息。Yu等人[194]认为帖子的时间序列特征有助于对事件进行准确建模，他们提出了FID的ACAMI模型。该模型使用event2vec（建议学习事件相关表征）和注意力机制来提取事件的时间和语义表征，然后使用CNN来提取高层次特征，用于对虚假微博帖子进行分类。\n\n一些方法将文本信息和社会背景信息（如用户回应、用户或网站简介）结合起来作为深度神经网络的数据输入。例如，Guo等人[60]提出了一个分层的神经网络，将用户、帖子和传播网络的信息作为数据输入。此外，他们利用注意力机制来估计FID中特征的不同贡献。Ruchansky等人[144]的工作提出了一个基于RNN的检测模型，该模型结合了新闻内容、用户反应和源用户的特征来促进FID的性能。Ma等人[111]提出了一个基于GAN的检测模型，其目的是捕捉低频但有效的假推文迹象。生成器（基于GRU的seq2seq模型）试图生成有争议的观点，使推文的观点分布更加复杂，而判别器（基于RNN）试图从增强的样本中识别虚假信息的稳健特征。\n\n也有一些使用图神经网络进行FID的工作，如GCNs。他们通常利用神经网络来分析社交帖子的传播结构，然后为分类器提取信息传播模式的高级表示。例如，Monti等人[117]提出了一个基于GCN的FID模型，它整合了推文内容、传播结构、用户资料和用户社会关系（关注和被关注）。考虑到原始推文和所有相关推文，即评论和转发，检测模型将每条推文作为节点，推文传播路径和用户关系作为边，建立一个特定事件图。之后，他们使用GCN来识别那些低可信度的推文，其中包含两个卷积层和两个全连接层。此外，Dong等人[33]提出了一个基于GCN的检测模型，名为GCNSI，它利用图卷积网络来检测多个虚假信息源。\n\n\n\n## 3 NEW TRENDS IN FALSE INFORMATION DETECTION\n\n在回顾了关于FID的传统研究后，本节调查了这一领域的几个新的研究趋势，包括早期检测、通过多模态数据融合检测和解释性检测。\n\n### 3.1 Early Detection\n\n虚假信息很容易被社交网络上的大量用户传播，在很短的时间内造成严重影响[14, 46]。因此，对虚假信息的早期检测成为一个重要的研究课题。然而，大多数现有的研究（基于内容和社会背景的方法）通过假设他们拥有所有的生命周期数据来检测虚假信息。他们依赖于几个聚合特征，如内容特征和传播模式，这需要一定数量的帖子来训练强大的分类器。虚假信息开始时的可用数据非常有限，以至于在早期阶段检测它很有挑战性。最近，有一些针对早期FID的努力。\n\n传统的机器学习方法通常会分析帖子早期传播中的用户交互信息，手动提取大量的特征，最后用分类器（如SVM、随机森林）来评估其中的可信度。例如，Liu等人[100]发现，在少量的数据中，来源的可靠性、用户的多样性和证据信号，如 \"我看到 \"和 \"我听到\"，对FID有很大的影响。此外，Qazvinian等人[136]观察到，在推文传播的早期阶段，用户倾向于表达自己的信念（如支持或质疑）。因此，合理利用信息中的用户信念，对早期发现虚假信息大有裨益。为了解决数据缺乏的问题，从相关事件中借用知识进行FID将是另一种有用的方法。例如，Sampson等人[149]提出了一种通过利用隐性链接（如标签链接、网络链接）从相关事件中获取额外信息的突发性FID方法。实验结果表明，当可用的文本或互动数据较少时，这种隐性链接明显有助于正确识别突发的不真实声明。\n\n许多检测方法利用深度学习模型对虚假信息进行早期检测。基于深度学习的检测方法通常使用神经网络来自动提取社会环境特征，并通过利用注意力机制找到FID的关键特征。例如，Liu等人[99]观察到只有少数帖子对FID有很大贡献。为了选择这些关键内容，他们提出了一个基于注意力的检测模型，该模型通过每个帖子的注意力值来评估其重要性。此外，实验结果表明，正确使用注意力机制有利于早期发现虚假信息。同样，Chen等人[20]发现，在信息传播的不同时期，用户倾向于对不同的内容进行评论（例如，从惊讶到质疑）。基于这一观察，提出了一个基于RNN的深度注意模型，有选择地学习连续帖子的时间性隐藏表征，以实现早期FID。Yu等人[195]利用一个基于CNN的模型从帖子序列中提取关键特征，并学习它们之间的高层次互动，这有利于用相对较少的互动数据识别虚假推文。Nguyen等人[123]也利用CNN来学习每条推文的潜在表征，相应地获得推文的可信度。然后，他们通过汇总事件开始时所有相关推文的预测，来评估目标事件是否是一条虚假信息。更重要的是，Liu等人[101]发现大多数用户在信息传播的早期过程中没有评论就转发源推文，这隐含着利用用户评论进行早期FID的一些延迟。因此，他们提出了一个传播路径分类模型，名为PPC，该模型联合使用CNN和GRU来提取转发路径中用户的局部和全局特征。\n\n### 3.2 Detection by Multimodal Data Fusion\n\n传统的FID方法专注于文本内容和传播结构。然而，社交媒体帖子也包含丰富的视觉数据，如图片和视频，而这种多模态数据往往被忽视。图片和视频比纯文本信息更吸引用户，因为它们可以生动地描述目标事件。\n图像处理的巨大进步，如AE、VAE和GAN（如第2.4节所述），证明了图像可以很容易地被编辑和修改，使假图像的生成更加容易。因此，分析多模态数据之间的关系并开发基于融合的模型可以成为FID的一个有前途的方法[14]。社交媒体上的虚假信息中主要有三种假图像，包括图像篡改、图像不匹配和图像混合。\n\n\n\n### 3.3 Explanatory False Information Detection\n\n大多数基于深度学习的FID方法在输出决策结果时，往往不会呈现做出决策的原因，它们利用预先训练好的分类器来识别测试集中的可疑事件[14]。然而，找到支持决策的证据碎片将有利于揭穿虚假信息并防止其进一步扩散。因此，解释型FID已经成为另一个趋势性的研究课题。现有的解释性FID研究主要集中在两个方面：一是探索实用的可解释性检测模型（模型的解释），二是解释其结果（结果的解释）。\n\n####  Interpretation of models\n\n关于可解释的FID模型的研究主要集中在利用概率图模型（PGM）和知识图（KG）。\n\n- 概率图模型（PGM）。PGM使用图来表示相关变量（节点）的联合概率分布，由贝叶斯网络和马尔科夫网络组成，前者使用有向无环图来模拟变量之间的因果关系，后者使用无向图来模拟变量之间的互动[38]。节点的关系可以通过条件独立性来解释。基于概率图的检测模型可以同时描述用户、社交帖子和人际交往内容的特征，并根据显性交互数据近似推断出隐性信息的可信度。此外，广泛使用的近似推理算法是变分推理、信念传播和蒙特卡洛抽样[84]。\n- 知识图谱（KG）。KGs以图的形式描述现实世界中的实体以及它们之间的关系。具体来说，KG包含各种领域的知识，定义了实体的可能类别和关系，并允许任何实体之间有潜在的关联[127]。此外，还有一些权威的知识库，如Freebase17、Wikidata18、DBpedia19、谷歌的知识图谱20。这些知识库包含了数以百万计的实体和声明，为FID提供了参考。检测方法可以通过知识提取、融合和完成来检查社交媒体帖子的事实。\n\n具体来说，Shi等人[157]提出了一种基于KG的事实核查方法。它首先通过从知识图中提取类似实体的元路径来分析帖子的语义信息。之后，该方法在收集到的事实状态中挖掘出异质连接模式，用于事实核查。此外，Gad-Elrab等人[47]提出了ExFaKT，为候选事实提供人类可理解的解释，它结合了来自文本内容和知识图谱的语义证据。ExFaKT使用Horn规则（一阶谓词逻辑的一个子集）将目标事实重写为多个易于解释的事实，以便进一步进行FID。Popat等人[131]的工作提出了一个概率模型，将FID的内容感知和趋势感知评估算法统一起来。具体来说，他们对事件相关文章之间的相互作用进行建模，以产生适当的用户可解释的解释，包括语言特征、立场和来源的可靠性。Yang等人[191]提出了一种无监督的FID方法，称为UFD。该方法利用贝叶斯网络来模拟真相和用户意见的完整生成过程。UFD将新闻文章的真实性和用户的声誉视为潜在变量，然后利用用户之间的社交活动来提取他们对新闻可信度的观点。\n\n#### Interpretation of results\n\n对结果的解释主要是指决策过程的可视化，或对事实的分析。虽然基于深度学习的方法极大地提高了FID的性能，但深度模型的内在机制并不能很好地解释。因此，研究人员利用其他辅助信息进行解释性的FID。\n\n由于注意机制中的注意程度可以表征输入的每一部分的重要性[19]，几个基于深度学习的检测方法通过注意程度的可视化来解释其分类。例如，Chen等人[20]将他们的模型识别的一些虚假索赔的注意力分布可视化，发现大多数与事件相关的词被赋予的程度低于表达用户怀疑、愤怒和其他情绪的词。Dong等人[32]提出了一个名为DUAL的基于注意力的FID模型，该模型分别使用GRU来提取文本特征和DNN来提取社会环境特征。他们将两个隐藏层的注意力矩阵可视化，有效地描述了识别真假帖子时每个隐藏层的注意力程度分布。同样地，Popat等人[133]使用双向LSTM分别提取源主张和外部相关帖子的特征，然后结合注意力机制来学习虚假信息的表示。他们还将其关注度可视化，显示许多信号词如 \"勉强真实\"、\"证据 \"和 \"揭示 \"被赋予较高的关注度。此外，他们使用主成分分析（PCA）来可视化他们的模型所学习的文本特征向量，发现真实和虚假说法的文本表示可以被适当地分开。\n\nClaimVerif[200]是一个在线解释信息可信度评价系统，它将给定主张的立场、观点、来源可信度等因素考虑在内，以提供有效证据。在识别网上的虚假主张时，ClaimVerif使用谷歌搜索抓取相关文章，分析原始信息和转帖信息的文本特征，最后输出源信息的可信度，以及人类可理解的证据。类似地，CredEye[132]通过分析在线相关文章来确定一个给定的说法是否是假的。解释的依据是这些文章的语言风格、立场和来源声誉。此外，Yang等人[190]提出了一个可解释的FID框架，名为XFake，它全面分析了声明的属性（如主题、说话人和背景）、语义特征和语言特征。XFake通过可视化界面显示几个支持性的例子，并以集合树的形式显示推理过程。\n\n\n\n## 4 CROWD INTELLIGENCE-BASED DETECTION\n\n现有的研究表明，帖子的内容特征仍然是FID的首要任务。由于社交帖子是由用户产生、互动和消费的，它将在帖子的编辑、评论和转发中摄入各种人类智能（如意见、立场、质疑、证据提供）。在社交媒体帖子的传播过程中，所谓的人群智慧[58, 96, 185]也会以集体的方式被聚集起来。正如Castillo等人[16]所说，一个有希望的假设是，在社交媒体环境中存在一些内在的信号，有助于评估信息的可信度。Ma等人[110]也发现，Twitter支持基于聚合的用户意见、猜想和证据碎片的虚假信息的 \"自我检测\"。虽然，如何在FID中利用人群智能仍然是一个开放的问题。在第4节中，我们试图通过提炼和介绍人群智能在FID系统中的几种不同使用形式来解决这个问题，如图2所示。\n\n![](https://i.loli.net/2021/11/03/oeDcFSqNtObuY1J.png)\n\n### 4.1 Crowd Intelligence in False Information\n\n在FID中，人群智能是指在信息产生和传播过程中，来自社交媒体用户智慧的聚合线索或社会信号。在本小节中，我们总结了FID中人群智能的含义和使用方式。\n我们从社会背景、集体知识和集体行为等三个方面来描述人群智能。\n\n- Social contexts. 源用户和传播者之间的社会关系和互动有助于理解信息的确定性。例如，Kim等人[80]认为用户的标记可以间接反映推文的可信度，所以他们使用PGM来生成人与内容的互动过程，推断推文的真实性。Zhao等人[199]发现群众在评论中对真实性的质疑或询问是低可信度信息的指示性信号，他们使用正则表达式从用户评论中提取上述信号进行FID。此外，Wu等人[188]认为类似的话题可能会在类似的人群中传播，所以他们对传播者进行编码，以捕捉他们的社会接近性，从而识别虚假信息。\n- Collective knowledge. 群众提供的收集的证据对推断信息的可信度很有用。例如，Lim等人[97]利用用户对目标事件在线证据的支持或反对来检测不准确的言论。Rayana等人[142]认为用户的评分和评论是对帖子可信度的真实评价，所以他们提出了一个名为SpEagle的检测框架，从集体线索和关系数据（信息传播网络）中提取特征。此外，Qian等人[138]提出了一种用于FID的人群知识转移方法，其中利用了历史上真/假说法中的人群反应知识（如背景特征和行为特征）。\n- Collective behaviors. 在很多情况下，虽然个人行为不能很好地描述信息的可信度，但一群用户的聚合行为往往能揭示更多信息。这可能是指人群互动模式，行为或意见偏离多数[88]，观点冲突，等等。例如，经常参与低可信度信息的生产和传播的用户有行为偏差，例如，在短时间内发布几个意见，或在一个固定的时间间隔后与内容互动。基于上述观察，Kumar等人[88]通过贝叶斯模型推断出回复者及其评论的可信度。此外，Jin等人[76]发现同一事件下的相关推文包含支持和反对的意见（通过LDA主题模型分析），他们利用这些冲突的观点来建立FID的可信度传播网络。\n\n在调查了现有的FID研究后，我们提炼出四种不同的人群智能使用方式，如下所示。\n\n- 群体学习模型。它主要使用特征工程和代表学习将人群智能融入到FID模型中。\n- 人群行为建模。它使用图或概率模型对人群行为和互动进行建模，以推断信息的可信度。\n- 群众的知识转移。学习到的FID模型通常在新事件上不能很好地发挥作用。这种方式解决了如何将人群知识从现有的事件转移到新的事件。\n- 人机混合模型。考虑到人类智能和机器智能的互补性，这种方式集中于开发用于FID的混合人机模型。\n\n前面三种方式的一个共同特点是，人群智能是以隐性方式使用的，没有明确的人类输入。具体来说，人群智能被表示为统计学上的人类行为模式，作为学习模型的特征或参数使用。然而，最后一种方式是基于明确的人类输入，例如使用众包进行数据标记。\n\n![](https://i.loli.net/2021/11/03/WmdofhkMb9NeV2p.png)\n\n\n\n\n\n### 4.2 Implicit Crowd Intelligence Models\n\n在本节中，我们介绍了关于将隐式人群智能用于FID的开创性研究，特别关注4.1节中描述的前三种方式，如表4中总结的那样。\n\n(1) 群体学习模型。在该模型中，群体智能被表示为训练分类器以检测虚假信息的特征。这已被证明对早期的 FID 很有用。例如，刘等人。 [100] 尝试使用来自 Twitter 数据的人群线索来解决实时虚假索赔揭穿的问题，包括人们的意见、证人账户的统计数据、对事件的聚合信念、网络传播等。赵等人。 [199] 观察到，在决定是否相信此消息之前，有些人愿意质疑或询问 Twitter 中声明的真实性。特别是，他们发现使用探究思维有助于及早发现虚假信息。\n社会关系和交互也是 FID 特征学习中广泛使用的群体智能。例如，吴等人。 [188] 假设相似的消息通常会导致相似的信息传播轨迹。他们提出了一种社交媒体用户嵌入方法来捕捉社交接近度和社交网络结构的特征，在此基础上利用 LSTM 模型对信息传播路径进行分类并识别其真实性。拉亚娜等人。 [142] 应用集体意见线索和相关数据来检测虚假信息。\n通过利用发布虚假帖子的用户行为与发布真实事实的用户行为不同的人群情报来识别虚假信息也很有帮助。陈等人。 [22] 提出了一种无监督学习模型，该模型结合了 RNN 和自动编码器，以将低可信度信息与其他真实声明区分开来。此外，谢等人。 [189] 观察到垃圾评论攻击与其评分模式密切相关，这与正常评论者的行为模式不同。因此，他们提出了一种基于其时间行为模式的垃圾评论检测方法，为群体学习模型的 FID 提供了参考。\n\n(2) 人群行为建模。在这个模型中，集体的人群行为（人群智能的一种类型）被建模为图或概率模型来推断信息的可信度。Hooi等人[66]发现，欺诈性账户经常在短时间内呈现他们的评级（评级分数满足偏斜分布）。群众智慧的特点是贝叶斯推理模型，它可以估计一个用户的行为与相关社区的行为有多大偏差。他们通过测量行为偏差的程度来推断用户评级的可信度。同样，Kumar等人[88]提出了一个贝叶斯检测模型，该模型结合了聚合的人群智慧，如用户的行为属性、评级的可靠性和产品的优良性。通过对异常行为的惩罚，它可以推断出评级平台的信息可信度。\n一些研究利用聚合的人群行为建模来促进虚假信息的早期检测。例如，Ma等人[110]假设回复者倾向于询问谁支持或否认给定的事件，并表达他们对更多证据的渴望。因此，他们提出了两个树状结构的递归神经网络（RvNN），用于有效的虚假推文表征学习和早期检测，可以对用户回复结构进行建模，并学习捕捉FID的聚合信号。\n\n(3) 群众的知识转移。**现有的FID模型在新出现的和时间紧迫的事件上仍然表现不佳。换句话说，现有的FID模型通常捕捉到许多与事件相关的特征，而这些特征在其他事件中并不常见。**因此，**有必要学习并将从现有众包数据中获得的共享知识转移到新的事件中**。Wang等人[182]的工作提出了一个利用可转移特征识别新产生的虚假事件的检测模型，名为事件对抗神经网络（EANN ），它包括三个部分，即 \"特征提取器\"、\"事件判别器 \"和 \"假新闻检测器\"。**EANN使用事件判别器来学习与事件无关的共享特征，并在模型训练中减少事件特定特征的影响。**\n群众知识转移模型也有助于早期FID。例如，Qian等人[138]提出了一个生成性条件变异自动编码器，从历史上用户对真实和虚假新闻文章的评论中捕捉用户反应模式。换句话说，当虚假信息传播的早期阶段没有社会互动数据时，人群智能被利用来产生对新文章的反应，以提高模型的检测能力。Wu等人[187]还探讨了历史众包数据中的知识是否能对新出现的虚假社交媒体帖子的检测有所帮助。他们观察到，内容相似的社交帖子往往会导致类似的行为模式（如好奇心、询问）。因此，他们建立了一个稀疏表示模型来选择共享特征并训练与事件无关的分类器。\n\n## 5 OPEN ISSUES AND FUTURE DIRECTIONS\n\n尽管研究人员已经为解决FID系统的上述挑战做出了越来越多的努力，但仍有一些开放性的问题需要在未来进行研究，如下所述。\n\n### (1) Cognitive mechanisms of false information. \n\n人们对虚假信息的认知机制的研究对于虚假社交媒体帖子的检测和反驳具有很好的指导作用[87]，尤其是基于群体智能的检测方法。几部作品对社交媒体平台上的低可信帖子进行了分析，以研究虚假信息能够快速广泛传播的原因。莱万多夫斯基等人。 [92] 认为打击虚假信息需要在技术和心理学的背景下进行科学研究，因此他们提出了一种称为“技术认知”的跨学科解决方案。此外，他们将用户面对虚假信息的认知问题分为影响效应、熟悉度逆火效应、矫枉过正逆火效应和世界观逆火效应四类，为研究用户对虚假信息的感知奠定了基础。 93]。正如 Acerbi [1] 总结的那样，不准确信息的快速传播在于它们包含满足用户认知偏好的特定内容。为了探索虚假信息的认知特征，他们通过将认知偏好编码为“威胁”、“厌恶”、“社交”、“名人”等部分，进一步分析了真假新闻文章的偏好分布。未来一个有价值的研究点是将虚假和真实的信息与具有认知吸引力的特征进行比较，或者评估与认知偏好相关的特征如何促进信息病毒式传播。\n\n除了在数据分析层面研究认知机制外，我们还可以从人脑认知功能的角度来学习这种机制。神经科学的进步为研究虚假信息的认知机制提供了一个很好的途径。正如Poldrack等人[130]所说，利用脑电图（EEG）、脑磁图（MEG）、功能性磁共振成像（fMRI）和其他脑成像工具可以推动我们了解人脑如何形成社会行为。此外，Adolphs[3]已经确定了参与社会认知调控的神经结构，如扣带皮层、海马体和基底前脑。Arapakis等人[6]利用脑电图记录来测量用户对新闻文章的兴趣，实验结果显示，额叶α不对称性（FFA）可以客观地评价用户对媒体内容的偏好。为了解释信息病毒的机制，Scholz等人[151]提出了一个基于fMRI数据的神经认知框架来评估用户在Facebook上分享信息的意愿。如果我们能够理解虚假信息的认知机制，就可以把更多的精力放在探索揭穿信息最大化的方法上，从而找到针对虚假信息的有力对策。\n\n### (2) Lack of standard datasets and benchmarks.\n\n尽管研究人员在FID方面做了大量的工作，但仍然缺乏像ImageNet[31]这样的视觉对象识别基准数据集。数据集作为一种资源，与FID的算法同样重要。然而，收集虚假信息是一个耗时耗力的过程，这导致了权威基准的缺乏。\n我们总结了2015年以来的公开数据集，如表6所示，其数据收集自新浪微博（如RUMDECT，Meida_Weibo）、Twitter（如。MediaEval、PHEME、RUMOUREVAL）和其他社交平台，以及snopes.com、politifact.com（例如Emergent、BuzzFeedWebis、LIAR、Declare、FakeNewsNet）和其他事实核查网站。然而，这些数据集的注解方法、数据维度以及真假陈述的比例都不一样，这给研究人员公平评估其模型性能带来了一定的挑战。Shu等人[162]总结了广泛使用的FID的评价指标，现有的评价指标仍然是精度、召回率、F1得分、准确率等机器学习模型评价指标。在FID中，我们需要定义一些更实用的评价指标。例如，在政治选举中，我们会更关注虚假声明是否被更全面地识别出来（即更关注召回率而不是精度），所以用F1得分来评价检测模型的性能并不是很合适。在未来的研究中，需要标准的数据集和实用的评价指标来比较各种FID算法，促进FID方法的发展。\n\n### (3) Model adaptivity/generality to new events\n\n**FID方法应该识别未见过的、新出现的事件，因为系统的现有数据可能与新出现的事件的内容不同。然而，现有的方法倾向于提取事件的特定特征，而这些特征很难与新事件共享[**204]。正如Tolosi等人[173]所说，**基于特征工程的检测方法很难检测到不同领域（如政治、犯罪、自然灾害）的虚假信息，因为不同事件的特征变化很大。因此，模型的通用性或适应性对于提高FID模型的稳健性相当重要。**Zubiaga等人[206]指出，**依赖于领域的特征分布可能会限制模型的泛化能力。**由于大多数特征的分布直接对应于事件，FID模型的性能将受到影响。尽管我们在第4.2节中讨论了一些人群知识转移模型[138, 182, 187]，但还有更多的东西需要研究。在其他领域（如情感分类[50]和图像识别[103]）成功使用的转移学习模型[59, 126]，可以被用来设计领域适应性的FID模型。使用基于GAN的判别器[182]是另一种有前途的方法，以建立具有共享特征的通用FID模型。\n\n### (4) Embracing of novel machine learning models. \n\nFID过程从本质上讲就是学习分类器，以识别给定主张的可信度。我们发现，许多研究建立了深度学习模型[20, 72, 101, 106, 123, 144, 195]来提高自动事实核查的性能。然而，仍有更多可以探索的地方。在下文中，我们将介绍几个有代表性的例子，它们利用先进的机器学习技术来进行FID。\n\n-  Multi-task learning.  多任务学习[109]旨在通过使用相关任务中包含的领域知识来提高模型的泛化性能。现有的方法通过对任务的相关性进行建模，如特征共享、子空间共享和参数共享等，来寻找多个任务之间的共同点，作为促进每个任务学习效果的一些补充知识。例如，Ma等人[109]认为FID任务与立场分类任务高度相关，所以他们提出了一个神经多任务学习框架，以更好地进行事实核查。在权重共享的机制下，他们提出了两个基于RNN的多任务结构来联合训练这两个任务，这可以为谣言表征提取普通以及特定任务的特征。在这项工作的启发下，我们可以研究FID和其他任务之间的联系和协作，并进一步设计基于多任务学习的算法来提高FID模型性能。\n- Few-shot learning.  [183]致力于解决数据稀缺的问题，利用少数监督信息来识别未见过的类的样本。现有的少量学习方法通常将其训练程序分解为多个元任务学习程序，类似于元学习[43]，从不同任务的数据中提取可转移的知识。因此，这允许只用少量的标记数据对新类进行分类。据我们所知，在FID中应用的少量学习方法较少，因此我们可以从其他相关领域学习，如文本分类。为了提高分类器的归纳和泛化能力，Geng等人[48]提出了一个基于动态路由算法的分类架构，称为归纳网络，它从少数样本中学习泛化的类级表示。归纳网络主要包含一个编码器模块、一个归纳模块和一个关系模块。具体来说，编码器模块生成样本和查询表征，然后归纳模块利用一个转换矩阵将样本级表征映射到类级表征。最后，关系模块计算出查询和每个类别之间的匹配度。这项工作表明，Few-shot learning在NLP中有很大的潜力，我们可以继续研究基于Few-shot learning的FID方法。\n- Semi-supervised models.大多数现有的FID工作集中在监督分类上，他们通常通过大量的标记数据（例如，假的或不假的）来训练分类器识别虚假信息。然而，在很多情况下，我们只有少量的标记数据。半监督模型经常被用来处理标签稀少的问题。例如，Guacho等人[55]提出了一种半监督的FID方法，它利用基于张量分解的文本嵌入来捕捉社交帖子的全局和局部特征。在构建所有帖子的K-近邻（K-NN）图后，他们使用信念传播算法将已知的标签传播到图中，以获得事件的最终可信度。此外，图神经网络的发展也为半监督检测模型的研究提供了机会。GNN，如DeepWalk[129]、LINE[171]和node2vec[54]，利用不同的采样算法来生成节点序列，然后通过跳格模型学习每个节点或传播路径的表示。他们在损失函数中引入了一阶接近性（两个相邻节点之间相似性的表征）和二阶接近性（两个节点之间结构相似性的表征），以确保神经网络能够充分提取图的特征。特别是GCNs[83]，如第2节所讨论的，在相邻的卷积层中通过图形的拉普拉斯矩阵的非线性变换来传递信息。每个卷积层只计算一阶接近度，所以GCN可以通过多个卷积层学习节点或传播路径的高级特征表示。特别是，GNN能够通过明确的图正则化方法[184]平滑标签信息，用于图的半监督学习。因此，FID模型可以建立信息传播图，并结合GNNs来检测虚假信息。\n- Unsupervised models. 如果能够直接建立可靠的无监督检测模型，对于快速驳斥虚假信息具有重要意义。无监督模型可以从人与内容的互动（如发布或转发社交媒体帖子）和人与人的互动（如关注或提及某些用户）来评估帖子的可信度。一方面，GAN和VAE的进步为无监督的FID模型带来了新的可能性。另一方面，PGMs仍然可以在FID中发挥重要作用。例如，Chen等人[22]从用户的发帖行为中判断一个帖子是否是假的。这种无监督的方法利用AE来学习一个人最近的发帖和他们的评论的潜在代表。当其重建误差收敛时，该模型可用于评估新帖子的可信度。如果模型的重建误差超过一定的阈值，这个帖子可能是一个假消息。Yang等人[191]将新闻真实性和用户可信度视为潜在变量，并利用用户评论来推断他们对新闻真实性的看法。换句话说，新闻的真实性取决于用户意见的可信度，而意见的可信度则依赖于用户的声誉。他们利用贝叶斯网络对互动过程进行建模，在没有任何标记数据的情况下推断出新闻文章的真实性。实际上，用户的意见可能会受到其他用户的影响，而且他们对不同主题的虚假信息的识别能力也是不同的。在使用PGM时可以进一步考虑这些条件。\n- Hybrid learning models. 混合学习模型的发展，结合了线性模型和深度学习模型，已经成为人工智能领域新的研究趋势，即显性特征和潜在特征的结合使用。它利用了两类学习模型的互补性。例如，Wide & Deep[24]是一个表现良好的推荐系统框架，其中Wide部分提取显性特征，Deep部分学习非线性的潜性特征。在FID中也有初步的混合学习模型。Yang等人[192]提出了用于检测虚假信息的TI-CNN模型，该模型在融合显性和隐性特征空间的基础上，对文本和视觉信息进行整体训练。此外，Zhang等人[197]提出了一个基于贝叶斯深度学习的FID模型，该模型使用LSTM来编码索赔和用户评论，并利用贝叶斯模型来推断分类结果。由于混合学习模型仍处于早期阶段，在这个方向上还需要进一步的研究，如概率图模型和深度学习模型的融合。\n\n\n\n### (5) Adversarial attack and defense in FID models.\n\n基于深度学习的FID模型有助于有效提高事实核查的性能。然而，Szegedy等人[169]已经证明，训练有素的神经网络可能无法抵御对抗性攻击，这意味着在输入向量中添加一些小的扰动会使模型得到错误的结果[4]。现有的FID研究很少强调深度模型的鲁棒性，这些模型可能被对抗性攻击所欺骗。\n\n虽然很少有关于FID模型中对抗性攻击和防御的研究，但关于其他任务（如图像分类[52，169]、语音识别[15]、文本分类[86]和强化学习[10]）的相关工作已经被调查。有几项工作侧重于对抗性攻击对模型的影响。例如，Dai等人[28]提出了一种基于强化学习（RL）的图数据的对抗性攻击方法，该方法通过增加或减少图中的边的数量来学习最佳攻击策略。为了生成通用的文本对抗性扰动，Behjati等人[9]提出了一种基于梯度投影的攻击方法。Jia等人[71]通过在问题中添加不会对人类理解造成困难的句子或短语来攻击问答系统。\n\n以上攻击研究可以指导FID模型的对抗性攻击防御研究。Zhou等人[202]进一步将FID模型的对抗性攻击分为事实失真、主客体交换和原因混淆。为了抵御对抗性攻击，他们进一步提出了一个众包知识图谱来及时收集新闻事件的事实。Qiu等人[139]将防御方法分为三类，包括修改数据（如对抗性训练、梯度隐藏）、修改模型（如正则化、防御性蒸馏）和使用辅助工具（如防御-GAN[148]）。无论是对模型的攻击还是对数据的操作，都对FID系统的稳健性提出了更高的要求。因此，在FID的对抗性攻击和防御方面仍有更多的工作要做。\n\n\n\n### (6) Explanatory detection models.\n\n提供决策结果的证据或解释可以增加用户对检测模型的信任。尽管关于解释型FID模型的工作很少，但在其他相关领域，如推荐系统，解释的应用已经被研究过。\n\n可解释的推荐，提供关于为什么推荐一个项目的解释，在最近几年引起了越来越多的关注[198]。它可以提高用户对推荐系统的接受度、信任度和满意度，增强系统的说服力。例如，Chen等人[23]提出了一种基于atten- tive神经网络的可视觉解释的推荐方法，以模拟用户对图像的注意力。用户可以通过提供个性化和直观的视觉亮点来理解产品被推荐的原因。Catherine等人[18]研究了如何在外部知识图谱的支持下产生可解释的推荐，他们提出了一个个性化的PageRank程序，将项目和知识图谱实体一起排名。Wang等人[181]的工作提出了一个基于强化学习（RL）的模型诊断性解释推荐系统，它可以灵活地控制解释的呈现质量。最重要的是，这种可解释推荐系统所使用的方法可以启发我们设计更好的可解释FID系统。\n\n从更高的角度来看，机器学习模型已经在不同的应用领域（超越了推荐系统和FID）提供了突破性进展。尽管取得了巨大的成功，我们仍然缺乏对其固有行为的理解，例如分类器是如何得出一个特定的决定的。这导致了可解释机器学习（IML）研究方向的激增。IML使机器学习模型有能力以人类可理解的术语进行解释或呈现[2, 34]。Du等人[35]定义了两种类型的可解释性：模型级解释和预测级解释。模型级解释，为增加模型本身的透明度，可以阐明机器学习模型的内部工作机制。预测层面的解释有助于揭示特定输入和模型输出之间的关系。对于FID来说，它更关注预测层面的解释，它可以说明一个决定是如何得出的（使用来源的可靠性、证据和立场等要素）。构建预测级可解释模型的一个代表性方案是采用注意力机制，它被广泛用于解释序列模型（如RNN）做出的决策结果。我们还应该研究植根于IML的其他方法，以提高FID系统的可解释性。\n\n### (7) Aggregation of crowd wisdom.\n\n如何聚合人群智慧对FID系统来说非常重要，因为人群贡献的数据往往有噪音。大多数用户的意见可以有效地用于识别虚假信息，但也存在真理掌握在少数人手中的情况。因此，未来仍有必要探索FID的人群智慧的聚合和优化方法。\n\n我们可以从真相发现系统中学习。随着利用人类智慧从相互冲突的多源数据中提取可靠信息的能力，真相发现已经成为一个越来越重要的研究课题。对于FID，我们也有关于一个事件的多个帖子，目标是识别这个事件的真相。因此，这两个研究问题有相似之处，我们可以借用真相发现系统的知识来促进FID的研究。例如，Liu等人[98]提出了一种专家验证辅助的图像标签真相发现方法，旨在尽可能地从嘈杂的众包标签中推导出正确的标签。特别是，它以人机协作的方式利用了一种半监督学习算法，可以最大限度地发挥专家标签的影响，减少专家的努力。Zhang等人[196]提出了一个名为 \"TextTruth \"的基于概率图的真相发现模型，它通过全面学习关键因素（一组关键词）的可信度来选择高度可信的问题答案。TextTruth以无监督的方式将答案提供者的可信度和答案因素的可信度一起推断出来。Yin等人[193]提出了一个以无监督的方式进行人群智慧聚合的模型，称为标签感知自动编码器（Label-Aware Autoencoders，LAA），它提取了多源标签的基本特征和模式，并通过一个分类器和一个重构器推断出可信的标签。为了解决同一信息源在不同主题上具有不同可信度的挑战，Ma等人[105]提出了一种名为FaitCrowd的众包数据聚合方法。FaitCrowd通过在概率贝叶斯模型上对问题内容和发布者的答案进行建模，共同学习问题的主题分布、答案提供者的基于主题的知识和真实答案。\n\n### (8) Propagation by social bots.\n\n现有的FID研究集中在索赔的内容和发布模式上。然而，对发布和传播帖子的 \"账户 \"的特征并没有很好的调查。最近，人们已经做出了一些努力来研究虚假信息像病毒一样迅速传播的根本原因。例如，Shao等人[155]对2016年美国总统选举期间的1400万条推文进行了详细分析，他们观察到\n\"社交机器人 \"显然促进了虚假信息的快速传播。社交机器人通常指的是一种计算机算法或软件程序，为了某种目的而模仿人类的互动行为（例如，生产内容、关注其他账户、转发帖子等）[40]。这些恶意的机器人账户在虚假推文传播的早期阶段异常活跃。此外，在对社交机器人的社会互动和情感互动进行建模后，Stella等人[167]发现，他们增加了负面和暴力内容在社交网络上的曝光。\n\n以上发现表明，抑制社交机器人可以成为缓解虚假信息传播的一个有前景的方法。一些研究者分析了社交机器人的行为模式并提出了一些检测方法。例如，Ferrara等人[40]将现有的社交机器人检测方法分为四类，包括基于图的模型、众包、基于特征的模型和混合模型。Almaatoug等人[5]设计了一种社交机器人检测方法，该方法结合了内容属性、社交互动和个人资料属性。同样，Minnich等人[115]提出了BotWalk检测方法，该方法利用几个特征来区分用户和机器人账户，如元数据、内容、时间信息和网络互动。Cresci等人[27]对社交机器人的集体行为进行了穿透性分析，并介绍了一种用于垃圾邮件检测的社会指纹技术。特别是，他们利用数字DNA技术来描述所有账户的集体行为，然后他们提出了一种受DNA启发的方法来识别真实账户和垃圾邮件。Cresci等人[26]也利用集体账户的特征来检测恶意的机器人。由于社交机器人促进了低可信度声明的传播和负面内容的曝光[155, 167]，未来的工作可以将FID与社交机器人检测相结合，为快速驳斥虚假信息提供新的解决方案。\n\n### (9) False Information Mitigation.\n\n 有效的FID是预防虚假信息的一部分，也需要科学研究来减少虚假信息的影响，这属于虚假信息缓解的研究范畴。一些著作对虚假信息缓解和干预的方法进行了回顾。例如，Sharma等人[156]从信息扩散的角度总结了三种缓解方法，即 \"去污\" \"竞争级联 \"和 \"多阶段干扰\"。Shu等人[159]将现有的缓解策略分为 \"用户识别\"、\"网络规模估计 \"和 \"网络干预\"。由于每个用户在虚假信息的传播中扮演着不同的角色，如意见领袖、监护人、恶意传播者和旁观者，因此有必要采取灵活的缓解措施。例如，意见领袖和监护人适合被推荐使用事实信息，以帮助传播真相[175]，而恶意账户或机器人应被遏制[122]。正如Ozturk等人[125]曾经说过的，在Twitter上用事实核查信息展示虚假信息，有助于减少虚假信息的持续传播。基于这一观察，Budak等人[13]提出了多运动独立级联模型，它包含一个虚假信息的运动和一个真实信息的运动。此外，我们还可以利用多变量霍克斯过程[37]来模拟外部干预影响下的虚假信息的传播动态。\n\n在未来的研究中，FID可以与上述缓解策略相结合，在防止社交网络上的虚假信息传播方面探索出更多有前景的工作。此外，Sundar[168]曾经证实，社交帖子中存在的来源归属改善了用户对在线信息的可信度和质量的看法。因此，来源归属和因果推理[158]也可以用来指导社交媒体上虚假信息的检测。\n\n\n\n-----\n\n# Detection and Resolution of Rumours in Social Media: A Survey\n\n**尽管人们越来越多地使用社交媒体平台来收集信息和新闻，但其未经审核的性质往往导致谣言的出现和传播，即在发布时未经核实的信息项目。**同时，**社交媒体平台的开放性提供了研究用户如何分享和讨论谣言的机会**，并探索如何利用自然语言处理和数据挖掘技术自动评估其真实性。在这篇文章中，我们介绍并讨论了两种在社交媒体上流传的谣言：一种是长期流传的谣言，另一种是**在突发事件等快节奏事件中催生的新出现的谣言**，这些报道是零散发布的，在早期阶段往往是未经核实的状态。我们概述了对社交媒体谣言的研究，最终目标是开发一个由四个部分组成的谣言分类系统**：谣言检测、谣言跟踪、谣言立场分类和谣言真实性分类**。我们深入研究了科学文献中提出的开发这四个组成部分的方法。我们总结了迄今为止在开发谣言分类系统方面所做的努力和取得的成就，并在结论中对未来在社会媒体挖掘中检测和解决谣言的研究途径提出建议。\n\n## INTRODUCTION\n\n社会媒体平台越来越多地被用作收集信息的工具，例如，社会问题（Lazer等人，2009年），以及在突发新闻事件中了解最新进展（Phuvipadawat和Murata，2010年）。之所以能做到这一点，是因为这些平台使任何拥有互联网连接设备的人都能实时分享他们的想法和/或发布他们可能目睹的正在发生的事件的最新情况。因此，社交媒体已经成为记者（Diakopoulos等人，2012；Tolmie等人，2017）以及普通公民（Hermida，2010）的有力工具。然而，虽然社交媒体提供了前所未有的信息来源，但由于平台缺乏系统性的努力来调节帖子，也导致了错误信息的传播（Procter等人，2013b；Webb等人，2016），然后需要额外的努力来确定其来源和真实性。与突发新闻故事相关的更新往往是零散发布的，这就造成了这些更新中很大一部分在发布时未经核实，其中一些可能后来被证明是错误的（Silverman 2015a）。在没有权威声明证实或驳斥一个正在进行的谣言的情况下，据观察，社交媒体用户往往会通过一个集体的、主观间的感觉制造过程来分享他们自己对谣言真实性的想法（Tolmie等人，2018），这可能会导致谣言背后的真相曝光（Procter等人，2013a；Li和Sakamoto，2015）。\n\n然而，尽管社交媒体具有这种明显的稳健性**，但其日益增长的产生谣言的趋势促使人们开发一些系统，这些系统通过收集和分析用户的集体判断**（Lukasik等人，2016），能够通过加速感知过程来减少谣言的传播（Derczynski和Bontcheva 2014）。**谣言检测系统可以在早期阶段识别出真实性不确定的帖子，可以有效地用来警告用户，其中的信息可能是虚假的**（Zhao等人，2015）。同样，一个汇总了用户发布的不断变化的集体判断的谣言分类系统可以帮助跟踪谣言的真实性状态，因为它被暴露在这个集体感知的过程中（Metaxas等人，2015）。在这篇文章中，我们概述了开发这样一个谣言分类系统所需的组件，并讨论了到目前为止为建立该系统所做的努力的成功。\n\n### 1.1 Defining and Characterising Rumours\n\n谣言的定义。最近研究文献中的出版物使用了彼此不同的谣言的定义。例如，最近的一些工作**将谣言错误地定义为被认为是虚假的信息**（如Cai等人（2014）和Liang等人（2015）），而大多数文献将谣言定义为 \"**流通中的未经核实的、工具性的信息声明**\"（DiFonzo和Bordia，2007）。在我们的文章中，**我们采用了谣言的定义特征，即它们在发布时是未经核实的**，这与主要词典给出的定义是一致的，比如《牛津英语词典》将谣言定义为 \"目前流传的不确定或可疑的故事或报告 \"1，或者《梅里亚姆-韦伯斯特词典》将其定义为 \"目前没有已知权威机构证明其真实性的声明或报告 \"2。 这种未经核实的信息可能被证明是真的，或部分或完全错误；或者，它也可能仍未解决。因此，在这篇文章中，我们坚持这个流行的谣言定义，将其归类为 \"**在发布时其真实性尚未得到验证的流通信息**\"。这个定义的选择与近期社会媒体研究的一些文献不同；但是，它与主要的字典和社会科学的一个长期研究领域相一致（Allport和Postman 1946；Donovan 2007）。**谣言可以被理解为一个尚未被验证的信息，因此它的真实价值在流传过程中仍未得到解决。当没有证据支持它，或者没有来自权威来源（如那些有信誉的人）或在特定背景下可能有可信度的来源（如目击者）的正式确认时，谣言就被定义为未经证实。**\n\n谣言类型。许多不同的因素可用于按类型对谣言进行分类，包括其最终的真实性价值（真实、虚假或未解决）（Zubiaga等人，2016c）或其可信度（例如，高或低）（Jaeger等人，1980）。另一个按类型对谣言进行分类的尝试是Knapp（1944），他提出了三种类型的谣言的分类法。(1) \"白日梦 \"谣言：即导致一厢情愿的谣言；(2) \"无聊 \"谣言：即增加焦虑或恐惧的谣言；以及(3) \"楔子驱动 \"谣言：即产生仇恨的谣言。当涉及到开发一个谣言分类系统时，主要决定要利用的方法的因素是它们的时间特征：\n\n- 突发新闻中出现的新谣言。在突发新闻中出现的谣言通常是以前没有被观察到的。因此，谣言需要被自动检测出来，而且考虑到系统可用的训练数据可能与后来观察到的数据不同，谣言分类系统需要能够处理新的、未见过的谣言。在这些情况下，早期检测和解决谣言是至关重要的，需要实时处理帖子流。在突发新闻中出现的谣言的一个例子是，当一个可疑的恐怖分子的身份被报道时。谣言分类系统可能已经观察到其他类似的疑似恐怖分子的案件，但案件和涉及的名字很可能会有所不同。因此，在这些情况下，谣言分类器的设计需要考虑新案例的出现，以及它们可能带来的新词汇。\n- 长时间讨论的长期谣言。有些谣言可能流传了很长时间，但其真实性却没有得到确定的证实。尽管（或可能是因为）很难确定实际的真相，这些谣言还是会引起人们巨大的、持续的兴趣。例如，关于奥巴马是穆斯林的传言就是如此。虽然这个说法没有证据，但似乎没有任何证据能让大家满意地推翻它。3 对于像这样的谣言，一个谣言分类系统可能不需要检测谣言，因为它可能是先验的。此外，该系统可以利用历史上关于该谣言的讨论来对正在进行的讨论进行分类，其中词汇的差异性要小得多，因此建立在旧数据上的分类器仍然可以用于新数据。与新出现的谣言相比，对于长期存在的谣言，处理通常是回顾性的，所以帖子不一定需要实时处理。\n\n在整篇文章中，我们提到了这两种类型的谣言，描述了不同的访问者如何处理每一种谣言。\n\n### 1.2 Studying Rumours: From Early Studies to Social Media\n\n简史。谣言和相关现象已经被从许多不同的角度进行了研究（Donovan 2007），从心理学研究（Rosnow和Foster 2005）到计算分析（Qazvinian等人2011）。传统上，研究人们对谣言的反应是非常困难的，因为这将涉及到在谣言展开时实时收集反应，假设参与者已经被招募了。为了克服这一障碍，All-port（Allport and Postman 1946, 1947）在战时谣言的背景下进行了早期调查。他提出了研究谣言的重要性，强调 \"有新闻价值的事件很可能会滋生谣言\"，\"流通中的谣言数量会随着主题对相关个人的重要性而变化，同时与相关主题有关的证据的模糊性。这使他提出了一个尚待回答的动机问题。\"谣言可以被科学地理解和控制吗？\" (Allport and Postman 1946)。他在1947年的实验（Allport and Postman 1947）揭示了一个关于谣言流通和信仰的有趣事实。他研究了美国总统富兰克林-D-罗斯福如何消除关于美国海军在1941年日本袭击珍珠港时遭受损失的谣言。研究表明，在总统发表讲话之前，69%的本科生认为损失比官方公布的要大；但五天后，在总统发表讲话的同时，只有46%的同等学生认为这一说法是真的。这项研究揭示了一个有声望的人发表的官方声明在影响社会对谣言准确性的看法方面的重要性。\n\n早期的研究集中在不同的目标上。一些工作研究了决定谣言传播的因素，例如，包括谣言的可信度对其后续传播的影响，其中可信度是指谣言可能被视为真实的程度。Prasad(1935)和Sinha(1952)的早期研究认为，在自然灾害的背景下，可信度不是影响造谣的一个因素。然而，最近，Jaeger等人（1980）发现，当可信度较高时，谣言的传播更为频繁。此外，Jaeger等人（1980年）和Scanlon（1977年）发现，接受者认为谣言的重要性是决定它是否被传播的一个因素，最不重要的谣言被传播得更多。\n\n互联网上的流言。互联网的广泛采用使自然环境下的谣言研究进入了一个新的阶段（Bordia 1996），并且随着社交媒体的出现而显得尤为重要，它不仅为分享信息提供了强大的新工具，而且也便于从大量的参与者那里收集数据。例如，Takayasu等人（2015）利用社交媒体研究了2011年日本地震期间流传的谣言的扩散情况，该谣言称地震后的雨水可能包括有害的化学物质，并导致人们被警告要携带雨伞。作者研究了早期报道该谣言的推文以及后来报道该谣言的推文的转发（RTs）情况。虽然他们的研究显示，后来的更正推文的出现减少了报告虚假谣言的推文的传播，但分析仅限于一个谣言，并没有为理解社交媒体中谣言的性质提供足够的洞察力。然而，他们的案例研究确实展示了一个对社会有重要影响的谣言的例子，因为市民们都在关注有关地震的最新动态，以保持安全。\n\n社会媒体中的谣言。近年来，社交媒体作为研究谣言的一个来源已经得到了重视，这是因为它是收集与谣言相关的大型数据集的一个有趣的来源，而且，除其他因素外，其巨大的用户群和分享的便利性使其成为谣言滋生的沃土。研究普遍发现，由于用户在分享意见、猜想和证据时具有众包的自我修正特性，Twitter在驳斥不准确信息方面表现良好。例如，Castillo等人（2013）发现，在2010年智利地震的案例中，支持和驳斥虚假谣言的推文比例为1：1（每条支持的推文对应一条驳斥的推文）。Procter等人（2013b）在分析2011年英格兰骚乱期间的虚假谣言时得出了类似的结论，但他们指出，任何自我纠正的效果都很缓慢。相反，在他们对2013年波士顿马拉松爆炸案的研究中，Starbird等人（2014）发现Twitter用户在区分真相和骗局方面做得并不好。在研究三种不同的谣言时，他们发现支持虚假谣言的推文所占比例分别为44：1、18：1和5：1。Zubiaga等人（2016c）进一步深入研究了谣言传播和支持的时间方面，描述了对九个突发新闻事件中的谣言的分析。这项研究的结论是，虽然总体趋势是用户在早期阶段支持未经核实的谣言，但随着时间的推移，会转向支持真实的谣言和驳斥虚假的谣言。因此，社交媒体聚合大量用户社区的判断的能力（Li和Sakamoto 2015）促使人们进一步研究机器学习方法，以改善谣言分类系统。尽管谣言和错误信息的传播给此类系统的开发带来了挑战，但将开发过程分解成更小的组成部分并利用合适的技术，在开发有效系统方面取得了令人鼓舞的进展，这些系统可以帮助人们在评估从社交媒体收集的信息的真实性方面做出决定。\n\n### 1.3 Scope and Organisation\n\n这篇调查文章的起因是**人们越来越多地使用Facebook或Twitter等社交媒体平台来发布和发现信息。虽然我们承认它们在收集独家信息方面的作用毋庸置疑，但它们的开放性、缺乏节制以及信息可以随时随地发布的便利性，无疑给信息质量保障带来了很大的问题。考虑到谣言的传播可能带来的不安和潜在的危害，近年来，开发处理谣言的数据挖掘工具的动机越来越强烈。**这篇调查文章旨在深入研究谣言对开发用于收集社交媒体信息的数据挖掘应用所带来的这些挑战，并总结迄今为止在这个方向上的努力。\n\n我们在第2节继续这一调查，研究社交媒体给众多领域带来的机会，同时也引入了必须处理谣言的新挑战。接着是对谣言分类系统的分析，我们首先描述了将谣言数据集放在一起的不同方法，以便进行进一步的实验；第3节描述了数据集的生成，首先是访问社交媒体API的方法，然后概述了收集和注释从社交媒体收集的数据的方法。我们在第4节中总结了对社交媒体中谣言的扩散和动态的特征和理解的研究结果。之后，我们在第5节中描述了构成谣言分类系统的组件。然后，在随后的章节中进一步描述这些组件并讨论现有的方法；第6节中的谣言检测系统、第7节中的谣言追踪系统、第8节中的谣言立场分类以及第9节中的真实性分类。我们在第10节中继续列举并描述了现有的处理谣言分类的应用和相关应用。最后，我们在第11节中总结了到目前为止的成就，并概述了未来的研究方向。\n\n## 2 SOCIAL MEDIA AS AN INFORMATION SOURCE: CHALLENGES POSED BY RUMOURS\n\n社会媒体越来越多地被一系列专业人士和公众所利用，成为了解最新发展和时事的信息来源（Van Dijck 2013；Fuchs 2013）。社交媒体的使用已经在许多不同的领域被发现是有用的；我们在下面描述一些最值得注意的使用。\n\n新闻收集。社交媒体平台在新闻传播方面显示出巨大的潜力，在突发新闻报道方面有时甚至超过了专业新闻机构（Kwak等人，2010）。除其他外，这使人们能够从目击者和广泛的用户那里获得最新信息，这些用户可以获得潜在的独家信息（Diakopoulos等人，2012；Starbird等人，2012）。为了利用社交媒体平台的这一特点，研究人员研究了新闻收集工具的发展（Zubiaga等人，2013年；Diakopoulos等人，2012年；Marcus等人，2011年），分析了用户生成内容（UGC）在新闻报道中的使用（Hermida和Thurman，2008年；Tolmie等人，2017年），并探索了社交媒体催生合作和公民新闻的潜力，包括对社交媒体上发布的报道进行合作核查（Hermida，2012年；Spangenberg和Heise，2014年）。\n\n突发事件和危机。近年来，社会媒体在紧急情况和危机中的使用也大幅增加（Imran等人，2015；Castillo，2016；Procter等人，2013a），其应用包括从目击者那里获得报告或找到寻求帮助的人。人们发现社交媒体对不同情况下的信息收集和协调非常有用，包括紧急情况（Yates和Paquette 2011；Yin等人2012；Procter等人2013a）、抗议活动（Trottier和Fuchs 2014；Agarwal等人2014）和自然灾害（Vieweg等人2010；Middleton等人2014）。\n\n公共舆论。研究人员也在利用社交媒体来收集用户对一系列社会问题的看法，然后将其汇总以衡量公众意见（Murphy等人，2014）。研究人员试图清理社交媒体数据（Gao等人，2014），并试图摆脱人口偏见（Olteanu等人，2016），以了解社交媒体如何塑造社会对问题、产品、人物等的看法。古德曼等人（2011）。人们发现，社交媒体对于衡量选举期间的民意（Anstead和O'Loughlin 2015），以及网上意见对组织声誉（Sung和Lee 2015）或对健康项目的态度（Shi等人2014）等方面的影响都很有用。\n\n金融/股票市场。社交媒体也已经成为了解金融界和股票市场最新发展的重要信息来源。例如，推文中表达的情绪被用来预测股市反应（Azar和Lo 2016），收集投资者在社交媒体上发布的意见（Chen等人，2014）或分析社交媒体帖子对品牌和产品的影响（李等人，2015）。\n\n由于社交媒体作为信息来源的潜力越来越大，其传播错误信息和未经证实的主张的倾向已经引起了许多研究。研究考察了用户的可信度认知（Westerman等人，2014），也评估了用户依赖社交媒体收集新闻等信息的程度（Gottfried and Shearer，2016）。因此，社交媒体中存在的谣言和有问题的说法所带来的困难导致了人们对建立谣言分类系统的技术的兴趣，并通过促进用户收集准确的信息来缓解这一问题。谈到谣言分类系统的发展，有两个主要用例需要考虑。\n\n- 处理长期存在的流言。在这种情况下，被追踪的谣言是预先知道的，并且社交媒体被作为收集意见的来源而加以挖掘。这个用例可能适用，例如，当想要追踪公众意见，或者当诸如潜在的收购等谣言在金融领域被长期讨论时。\n- **处理新出现的传言。当某些事件或话题被追踪时，新的谣言突然出现。这种用例可能适用于新闻收集和紧急情况，在这种情况下，信息被零散地发布并需要被核实，或者其他突然出现的谣言，例如那些预计会对股票市场产生影响的政治决定。**\n\n## 3 DATA COLLECTION AND ANNOTATION\n\n本节介绍了用于收集社交媒体数据的不同策略，这些数据能够研究谣言，以及收集数据注释的方法。\n\n### 3.1 Access to Social Media APIs\n\n访问、收集和存储社交媒体平台数据的最佳方式通常是通过应用编程接口（API）（Lomborg和Bechmann，2014年）。API是易于使用的界面，通常伴随着描述如何请求感兴趣的数据的文档。它们被设计成可以被其他应用程序访问，而不是为人设计的网络接口；API提供了一套定义明确的方法，应用程序可以调用这些方法来请求数据。例如，在一个社交媒体平台上，可能需要检索某个特定用户发布的所有数据或包含某个关键词的所有帖子。\n\n在使用API之前，关键的第一步是阅读其文档，了解其方法和限制。事实上，每个社交媒体平台都有自己的局限性，当想要开发一个利用社交媒体数据的谣言分类系统时，这是关键。用于研究谣言的三个关键平台是Twitter、新浪微博和Facebook；这里我们简要讨论一下这三个平台的特点和局限。\n\n- Twitter提供了使用其API的详细文档4，它可以访问REST API以从其数据库中获取数据，也可以访问流式API以实时获取数据。在注册了一个Twitter应用程序5后，该程序将生成一组密钥，用于通过OAuth认证访问API，然后开发人员将有机会使用一系列方法（\"端点\"）来收集Twitter数据。这些端点中最慷慨的是可以访问整个推文流中随机抽样的1%；要获得更大比例的数据通常需要付费。为了确保收集到全面的推文，最好是通过流媒体API实时收集推文；同样，从这个API免费收集的推文数量有1%的限制。使用Twitter的API的主要优点是它是最开放的，这可能部分解释了为什么它被最广泛地用于研究；主要的注意事项是它主要被设计用来收集实时或最近的数据，因此收集比过去几周更早的数据更具挑战性。推特在收集每条推文时都会提供一系列元数据，包括推文语言、地点（如有）等，以及发布推文的用户的详细信息。\n- 新浪微博是中国最流行的微型博客平台，它提供的API与Twitter有许多相似之处。然而，对它的一些方法的访问是不公开的。例如，搜索API需要先与管理员联系以获得批准。此外，新浪微博提供的一系列方法只能通过其REST API访问，它缺乏一个官方的流媒体API来检索实时数据。要通过新浪微博的流媒体API检索实时数据，必须使用第三方供应商，如Socialgist。7,8 与Twitter一样，新浪微博为每个帖子提供一组元数据，包括帖子的信息和用户的详细信息。\n-  Facebook提供了一个记录在案的API，以及一套适用于多种编程语言和平台的软件开发工具包，使得利用其数据开发应用程序变得容易。与Twitter的API类似，Facebook也需要注册一个应用程序10来生成访问API所需的密钥。与Twitter相比，Facebook用户发布的大部分内容都是私密的，因此无法访问发布的具体内容，除非用户是认证账户的 \"朋友\"。获取Facebook上的帖子的变通方法通常是从所谓的Facebook页面收集数据，这些页面是由组织、政府、团体或协会创建的公开页面。与Twitter不同的是，从这些Facebook页面获取历史数据是可能的；但是，访问仅限于在这些页面上发布的内容。脸谱网提供的每个帖子的元数据更加有限，需要向API提出额外的请求才能获得这些数据。\n\n\n\n## 4 CHARACTERISING RUMOURS: UNDERSTANDING RUMOUR DIFFUSION AND FEATURES\n\n最近的许多研究都关注了社会媒体中谣言的出现和传播的特点。从这些研究中得到的启示反过来也可以为谣言分类系统的发展提供参考。其中一些研究集中于对某一特定谣言的广泛分析，而另一些研究则是对较大的谣言集进行更广泛的分析。\n\n对围绕谣言的话语进行研究是为了考察围绕谣言的讨论以及谣言随时间的演变。一些研究着眼于定义一个方案，以对谣言的反应类型进行分类。Maddock等人（2015年）研究了谣言的起源和随时间的变化，从而确定了对谣言的七种行为反应：错误信息、猜测、纠正、质疑、对冲、不相关或中立/其他。同样，Procter等人（2013b）提出，对谣言的反应可以分为四种类型，即支持、否认、呼吁提供更多信息和评论。还有人研究了谣言，以了解人们对谣言的反应。通过研究在中国微博平台上传播的谣言，Liao和Shi（2013）确定了七种类型的用户（名人、认证、大众媒体、组织、网站、网络明星和普通人）的干预，他们以七种不同的方式（提供信息、发表意见、情感状态、感性陈述、询问性陈述、指导性陈述和离题陈述）做出贡献。在另一项研究中，Zubiaga等人（2016c）研究了由Twitter上的谣言报道引发的 \"对话\"（即由回复关系连接的一系列推文），发现社交媒体用户的普遍倾向是支持和传播谣言，而不考虑其真实性价值。这包括声誉高的用户，如新闻机构，他们倾向于在谣言的早期阶段支持谣言，稍后在需要时发布更正声明。在早期的研究中，Mendoza等人（2010）发现了谣言支持和真实性之间的强烈关联，表明大多数用户支持真实的谣言，而更多的用户否认虚假的谣言。尽管这些研究之间存在明显的矛盾，但值得注意的是，Mendoza等人（2010年）研究了谣言的整个生命周期，因此汇总导致了良好的相关性；相反，Zubiaga等人（2016c）专注于谣言的早期再行动，表明用户在谣言的早期阶段确定真实性方面存在问题。利用Reddit的谣言数据，也发现了不同用户之间的差异，表明有三个不同的用户群体：一般支持虚假谣言的用户，一般反驳虚假谣言的用户，以及一般对虚假谣言开玩笑的用户（Dang等人，2016a）。也有人认为，更正通常是由新闻机构发布的，它们有时会被广泛传播（Takayasu等人，2015；Arif等人，2016；Andrews等人，2016），特别是如果这些更正来自志同道合的账户（Hannak等人，2014），偶尔甚至会导致原始帖子的删除或取消分享（Frias-Martinez等人，2012）。然而，更正并不总是具有与原始谣言相同的效果（Lewandowsky等人，2012；Shin等人，2016；Starbird等人，2014），这加强了开发处理新出现的谣言分类系统的必要性。\n\n其他研究也关注了促使谣言传播的因素。谣言的传播通常取决于用户之间的关系强度，谣言更有可能在网络中的强关系中传播（Cheng等人，2013）。其他对谣言时间模式的研究表明，在社交媒体（Kwon等人，2013年；Kwon和Cha，2014年；Lukasik等人，2015年b）和互联网上的其他平台（Jo，2002年），谣言的流行度往往会随着时间的推移而波动，但在谣言流行度消退后又有可能被重新讨论。\n\n研究还考察了谣言的出现。通过使用谣言理论方法来研究导致表达对追踪谣言的兴趣的因素，Oh等人（2013）认为缺乏官方来源和个人参与是最重要的因素，而其他因素，如焦虑，则不那么重要。海报的可信度和谣言的吸引力也被认为是促成谣言传播的因素（Petty和Cacioppo 2012）。Liu等人（2014）强化了这些发现，认为个人参与是最重要的因素。Chua等人（2016）分析了Twitter上的具体谣言信息，发现拥有较大粉丝网络的成熟用户的推文传播最广。\n\n虽然许多研究都探讨了谣言的传播，但对这些研究的详尽分析并不在本调查文章的范围内，而是侧重于有关检测和解决谣言的方法的发展研究。要阅读更多关于研究谣言扩散的研究，我们推荐Serrano等人（2015）和Walia和Bhatia（2016）的调查。\n\n## 5 RUMOUR CLASSIFICATION: SYSTEM ARCHITECTURE\n\n谣言分类系统的结构可以有轻微的变化，这取决于具体的使用情况。这里我们定义了一个典型的谣言分类系统的架构，它包括一个完整系统所需的所有组件；然而，正如我们在下面的描述中指出的，根据需求，其中一些组件可以省略。谣言分类系统通常从确定某条信息未被证实开始（即谣言检测），最后确定该条信息的估计可信度值（即可信度分类）。从谣言检测到真实性分类的整个过程是通过以下四个部分进行的（见图1）。\n\n![](https://i.loli.net/2021/11/04/Z9NnoAdsbS8t1EX.png)\n\n- 谣言检测。首先，一个谣言分类系统必须确定一条信息是否构成谣言。谣言检测组件的典型输入可以是社交媒体的帖子流，然后一个二元分类器必须确定每个帖子是被视为谣言还是非谣言。这个组件的输出是帖子流，其中每个帖子都被标记为谣言或非谣言。这个组件对于识别新出现的谣言很有用；但是，在处理先验已知的谣言时，它就没有必要了。\n- 谣言追踪。一旦确定了一个谣言，或者因为它是先验的，或者因为它是由谣言检测组件确定的，谣言跟踪组件就会收集和过滤讨论该谣言的帖子。谣言的输入可以是一个帖子或描述它的句子，也可以是一组关键词，这个组件监测社交媒体以找到讨论该谣言的帖子，同时剔除不相关的帖子。该组件的输出是讨论该谣言的帖子的集合。\n- 立场分类。当谣言追踪组件检索与谣言相关的帖子时，立场分类组件确定每个帖子对谣言的真实性的定位。将一组与同一谣言相关的帖子作为输入，它为每一个帖子输出一个标签，这些标签一般从预定义的立场类型集合中选择。这个组件对于促进后续处理真实性分类的组件的任务很有用。但是，如果公众的立场被认为是没有用的，例如，仅仅依靠专家的输入或权威来源的验证的情况下，它可以被省略。\n- 真实性分类。最后的真实性分类组件试图确定谣言的实际真相价值。它可以使用在谣言跟踪组件中收集到的帖子集，以及在立场分类组件中产生的立场标签作为输入。它可以选择从其他来源，如新闻媒体，或其他网站和数据库中收集额外的数据。该组件的输出可以只是预测的真值，但它也可以包括上下文，如URL或其他数据源，以帮助最终用户通过与相关来源的双重检查来评估分类器的可靠性。\n\n在下面的章节中，我们将更详细地探讨这四个组成部分，到目前为止用于实施这些组成部分的方法以及迄今取得的成就。\n\n## 6 RUMOUR DETECTION\n\n### 6.1 Definition of the Task and Evaluation\n\n谣言检测任务是指系统必须从一组社交媒体帖子中确定哪些帖子是报告谣言的，因此是在传播有待核实的信息。请注意，一条推文构成谣言的事实并不意味着它以后会被认为是真的或假的，而是意味着它在发布的时候是未经核实的。从形式上看，该任务将社交媒体帖子的时间线TL={t1,...,t|TL|}作为输入，分类器必须确定这些帖子中的每一个，ti，是谣言还是非谣言，从Y={R,NR}中分配标签。因此，该任务通常被表述为一个二元分类问题，其性能通过计算目标类别（即谣言）的精度、召回率和F1分数来评估。\n\n### 6.2 Datasets\n\n唯一公开的数据集是PHEME的谣言和非谣言数据集，其中包括与5个突发新闻故事相关的1972个谣言和3830个非谣言的集合（Zubiaga等人，2016b）。\n\n### 6.3 Approaches to Rumour Detection\n\n尽管人们对分析社交媒体中的谣言和建立工具来处理之前已经确定的谣言越来越感兴趣（Seo等人，2012；Takahashi和Igata，2012），但在自动谣言检测方面的工作却很少。谣言检测方面的一些工作（Qazvinian等人，2011年；Hamidian和Diab，2015年，2016年）仅限于寻找先验的谣言。他们用一组预定义的谣言（例如，奥巴马是穆斯林）来喂养分类器，将新的推文分类为与已知的谣言之一有关或无关（例如，我认为奥巴马不是穆斯林将与谣言有关，而奥巴马正在与一群穆斯林交谈则不是）。像这样的方法对于长期存在的谣言是很有用的，在这种情况下，需要的是识别与追踪已经确定的谣言有关的推文；在这篇调查文章中，我们把这项任务称为谣言追踪，因为被监测的谣言是已知的，但帖子流需要被过滤。仅仅依靠谣言追踪是不够的，因为在快节奏的背景下，**如突发新闻，会出现新的、未见过的谣言，而与尚未被发现的谣言相关的具体关键词并不是预先知道的。为了处理这个问题，分类器将需要学习可概括的模式，以帮助在新出现的事件中识别谣言。**\n\n第一个解决新谣言检测的工作是Zhao等人（2015）的工作。他们的方法建立在这样的假设上：谣言会引起怀疑论者的推文，他们会质疑或询问谣言的真实性；如果一条信息有一些相关的询问推文，那么就意味着该信息是谣言的。作者创建了一个由五个正则表达式（例如\"（那个|这个|它）是真的吗\"）组成的人工策划列表，用于识别询问性推文。然后，这些询问的推文按相似度进行聚类，每个聚类最终被视为一个候选谣言。他们用召回率来评估是不可行的，而只用精确度来评估。\n\n相比之下，Zubiaga等人（2016b，2017）提出了另一种方法，在整个突发新闻故事中学习上下文，以确定一条推文是否构成谣言。他们的假设是，由于缺乏上下文，单单一条推文可能不足以知道其背后的故事是否是谣言。此外，他们避免了对询问性推文的依赖，他们认为并非所有的谣言都会引发，因此可能导致低召回率，因为没有引发询问性推文的谣言会被遗漏。他们的上下文学习方法依靠条件随机场（CRF）作为顺序分类器，学习事件中的报道动态，这样分类器就可以根据事件中迄今为止的情况，对每条新推文确定其是否是谣言。他们的方法导致了比Zhao等人（2015）的基线分类器更高的性能，也改善了一些作为基线的非序列分类器。在这种情况下，该分类器也被评估为召回率，取得了最先进的结果。\n\nTolosi等人（2016年）对不同事件中的谣言进行特征分析，发现很难区分谣言和非谣言，因为不同事件的特征变化很大。Zubiaga等人（2016b）解决了推特层面的这些发现，表明通过利用事件的背景，可以实现普遍性。\nMcCreadie等人（2015年）研究了使用众包平台来识别社交媒体中的谣言和非谣言的可行性，发现注释者取得了很高的注释者之间的一致性。他们还将谣言分为六个不同的类型。未经证实的信息、有争议的信息、错误的信息/虚假的信息、报道、有关联的争议和有意见的。然而，他们的工作仅限于对谣言和非谣言的众包注解，他们没有研究自动谣言检测系统的发展。这项研究的数据集没有公开提供。\n然而，其他的工作被贴上了谣言检测的标签，专注于确定社交媒体上发布的信息是真的还是假的，而不是早期检测未经核实的信息，因此我们在第9节关于真实性分类中讨论。\n技术现状。谣言检测的最先进方法是Zubiaga等人（2017）提出的方法，它利用与特定事件相关的早期帖子的上下文来确定一条推文是否构成谣言。\n\n## 11 DISCUSSION: SUMMARY AND FUTURE RESEARCH DIRECTIONS\n\n随着社交媒体渗透率的提高，关于开发谣言检测和验证工具的研究变得越来越受欢迎，它使普通用户和专业从业人员能够实时收集新闻和事实，但也带来了未经核实的信息传播的副作用。这篇调查文章总结了科学文献中关于发展谣言分类系统的研究，对社会媒体谣言进行了定义和定性，并描述了发展其四个主要组成部分的不同方法。(1) 谣言检测，(2) 谣言追踪，(3) 谣言立场分类，以及(4) 谣言真实性分类。在这样做的过程中，该调查为这些组件的开发提供了一个技术现状的指导。该调查特别关注在社交媒体上流传的谣言的分类。大多数一般方面，如谣言的定义和分类架构，都是可以推广到新闻文章等类型的。然而，为四个部分中的每一部分描述的具体方法通常是为社交媒体设计的，不一定直接适用于其他体裁。在下文中，我们将回顾迄今为止所取得的进展，现有系统的缺点，概述对未来研究的建议，并评论谣言分类系统对其他类型的误导性信息的适用性和通用性，这些信息也在社交媒体中传播。\n\n自从社交媒体作为信息和新闻收集的平台激增以来，检测和解决谣言的研究有了很大进展。一系列的研究采取了非常不同的方法来理解和描述社会谣言，而这种多样性有助于阐明谣言分类系统的未来发展。在构成谣言分类系统的所有四个组成部分中，已经进行了重新搜索，尽管大多数都集中在管道的最后两个组成部分，即谣言立场分类和真实性分类。尽管如本调查所示，该研究领域取得了实质性进展，但我们也表明，这仍然是一个需要进一步研究的开放性研究问题。我们在下一节中研究主要的开放性研究挑战。\n\n### 11.1 Open Challenges and Future Research Directions\n\n近年来，谣言分类的研究主要集中在管道的后期阶段，即谣言的立场分类和真实性分类。这些都是至关重要的阶段；然而，如果不执行前面的检测谣言和跟踪与这些谣言相关的帖子的任务，它们就不能被使用。后者在以前的工作中通常被跳过，要么把这些组件的开发留给未来的工作，要么假设谣言和相关帖子是由人输入的。我们认为，未来的研究应该集中在谣言的检测和跟踪上，以避免完全依赖人在回路中的情况，从而减轻这些初始任务。在这个方向上的进一步研究将能够开发出完全自动化的谣言分类系统。\n\n谣言检测的研究应该从在谣言的特定背景下测试最先进的事件检测技术开始。除了事件检测系统所做的，谣言检测系统还需要确定检测到的事件是否构成谣言。如果只使用其内容，确定一个单独的社交媒体帖子是否报告了一个谣言是具有挑战性的。最近的研究表明，使用上下文（Zubiaga等人，2017年）和互动（Zhao等人，2015年）可以提供帮助，这些都是值得详细探索的方向。\n\n对谣言追踪系统的研究是有限的，而且研究人员经常假设用于收集与谣言有关的帖子的关键词是先验的。社交媒体的一个明显的问题是用户之间使用不一致的词汇，例如，用户可能不明确地使用杀戮或射击来指称同一事件。在扩大数据收集方面的研究仍处于起步阶段，通过技术（如伪相关性反馈）使用查询扩展方法，还有待详细探讨，但初步研究显示了其潜力\n\n谣言分类系统发展的一个重要限制是缺乏公开可用的数据集。除了我们在本调查中列出的最近发表的数据集，我们鼓励研究人员发布他们自己的数据集，以便对不同的数据集进行进一步研究，从而使科学界能够相互比较他们的方法。\n\n虽然许多人试图自动确定谣言的真实性价值，但鉴于分类器不可避免地会出现错误，仅仅输出真实性的最终决定的系统可能并不总是足够的。为了使真实性分类器的输出更加可靠，我们认为系统需要提供更丰富的输出，其中还包括决策的原因（Procter等人，2013b）。真实性分类器不仅输出自动确定的真实性分数，而且还链接到可以证实这一决定的来源，这将更加稳健，因为它将使用户能够评估分类器决定的可靠性，并且--如果发现想要忽略它。例如，可以通过使用立场分类器的输出来丰富真实性分类器的输出，选择一些支持和反对的观点，作为摘要呈现给用户。鉴于实现完全准确的真实性分类器是一个不太可能的目标，我们认为这个方向的研究应该特别关注寻找信息源，以促进终端用户对谣言的真实性做出自己的判断。\n\n现有真实性分类系统的另一个注意事项是，它们侧重于确定真实性，而不考虑谣言是否得到解决。在谣言尚未解决的情况下，真实性分类任务就变成了预测任务，由于缺乏支持系统决策的证据，这对终端用户来说可能并不可靠。由于谣言具有未经证实的立场，确定其真实性很难，或者需要权威来源的参与，未来的研究应该研究谣言真实性确定的时间性，可能会尝试在找到证据后很快确定真实性。\n谈到立场分类，最近的工作表明，利用社交媒体流和对话中的上下文来开发个人帖子立场的最先进分类器是有效的。然而，这个方向的研究仍处于起步阶段，还需要更多的研究来最好地利用这种背景来最大化立场分类器的性能。谣言分类的研究主要依赖于社交媒体帖子的内容，而从用户元数据和互动中提取的进一步信息可能有助于提高分类器的性能。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"Learn Continually, Generalize Rapidly, Lifelong Knowledge Accumulation for Few-shot Learning","url":"/2021/10/30/Learn-Continually-Generalize-Rapidly-Lifelong-Knowledge-Accumulation-for-Few-shot-Learning/","content":"\n# Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning\n\n随着时间的推移不断扩展知识，并利用这些知识迅速归纳到新的任务中，这是人类语言智能的一个关键特征。\n\n现有的追求快速泛化到新任务的模型（如few-shot learning），大多是在固定的数据集上进行单次训练，无法动态地扩展其知识；而持续学习算法则不是专门为快速泛化设计的。\n\n作者提出了一个新的学习设置，即 \" Continual Learning of Few-Shot Learners\"（CLIF），以在一个统一的设置中解决这两种学习设置的挑战。\n\nCLIF假设一个模型从一连串不同的NLP任务中依次学习，积累知识以提高对新任务的概括能力，同时也保留了之前学习的任务的性能。\n\n本文研究了在持续学习设置中泛化能力是如何受到影响的，评估了一些持续学习算法，并提出了一种新颖的**带有正则化的Adapter的双级超网络**。\n\n![](https://i.loli.net/2021/10/30/3o5X2RKGhSDivAP.png)\n\n挑战：模型在一连串的NLP任务中学习（逐一到达；不重复访问），然后在以下方面进行评估：（1）对新的（few-shot learning）任务的泛化；以及（2）保留其在解决已见任务上的性能。\n\n> 作者认为此类任务与LifeLong的区别:\n>\n> 此任务研究了NLP模型是否可以在一连串的任务中不断积累可归纳的知识，并迅速学习归纳到新的任务。\n>\n> 相关的工作是希望从连续到达的任务中学习，被称为持续学习（CL），主要关注的是当模型在新任务中被持续更新时，保留在所见任务中的表现。在后续的分析中，发现，现有的大多数CL方法几乎不利于模型的泛化能力，即使它们被证明可以缓解灾难性遗忘。\n\n---\n\n## Problem Formulation\n\n### The CLIF Problem\n\n我们假设有一个NLP模型 $f$ 随着时间的推移在不同的任务上不断地训练（即持续学习），然后通过少量的例子迅速概括到许多未见过的任务（即few-shot适应）\n\n在持续学习阶段，模型遇到一个有序的 $N_u$ 上游任务列表 : $[T_u^1,...,T^{N_u}_u]$ ，其中每个任务有自己的训练集和测试集。\n\n为了测试连续选了的模型 $f$ 的 few-shot 学习能力，在一组单独的 $N_v$ 少量任务 $\\{T_v^i\\}_{i=1}^{N_v} $ 上对其进行adapt ，其中每个未见的任务只有几个训练样本。\n\n在CLIF中，除了传统的CL目标是保持在所见任务上的性能外，在CLIF中，保持可概括的知识以在训练结束时获得更好的few-shot learning性能也是至关重要的。 \n\n### Evaluation Protocol\n\n如图所示，针对CLIF设置评估方法有三个主要方面：few-shot性能、最终性能和即时性能。\n\n![](https://i.loli.net/2021/10/30/b2W3lNhOngXmQv9.png)\n\n-  *Few-shot Performance*: 首先，在一组未见过的任务上苹果持续训练的模型 $f$,  在上游任务 $T^1_u,...,T_u^{N_u}$ 训练结束后，用几个标注的样本对每个任务 $T_v^i$ 进行微调。因此，我们可以评估 few-shot 的泛化能力。把一个任务 $T_v^i$ 的 few-shot accuracy 记为 $s_{FS}^i = F(Y_v^i, \\hat Y_v^i)$, 其中 $\\hat Y_v^i$ 是对任务 $T_{v}^i$ 的测试样本进行预测， $Y_v^i$ 是真实标签。$F$ 是度量函数如accuracy。 记录所有few-shot 任务，例如：$s_{FS}= \\frac{1}{N_v} \\sum_{i=1}^{N_v} s_{FS}^i$ 。 还计算了在每个 few-shot 任务上单独训练的模型的相对改进 $\\Delta_{FS}$\n- *Instant Performance* : 在模型完成对上游任务 $T_u^i$ 的学习后，立即评估其性能，在模型$f$ 将任务 $j$ 学习为 $\\hat Y_{u}^{i,j}$ 之后，记录在任务 $T_u^i$ 的测试集上的预测。 Instant performance 在任务 $T_u^i$ 上被定义为 $s_{inst.}^i = F(Y_u^i,\\hat Y_u^{i,i})$ 。例如，模型 $f$ 在 $T_u^1$ 和 $T_u^2 $ 的数据上训练之后，在 $T_u^3$ 上进一步训练之前评估 $f$ 在 $T_u^2$ 上的性能。因此，$f$ 在 $T_u^2 $ 上的表现可以告诉我们，模型将其知识从学习 $T_u^1 $ 转移到学习 $T_u^2 $  的情况 —— 使用 $f$ 仅只在 $T_u^2 $ 上训练时的表现作为参考。我们计算所有上游任务的 Instant performance，$s_{inst.} = \\frac {1}{N_u} \\sum_{i=1}^{N_u} s_{inst.}^i $  ，此外还计算了相对于在每个上游任务上单独训练的改进 $\\Delta_{inst.}$， 以表明上学学习的好处。\n- *Final Performance* ：评估 $f$ 在对上游任务的持续学习结束时的表现，以了解模型 $f$ 在学习解决更多任务后对任务知识的遗忘程度。一个任务 $T_u^i $ 的最终 accuracy 被定义为 $F(Y_u^i,\\hat Y_u^{i,N_u})$ 。同样地，我们报告了所有任务的平均最终准确度，记为 $s_{final} = \\frac{1}{N} \\sum_{i=1}^{N_u} s_{final}^i$。遗忘可以被量化为 $s_{inst} - s_{final}$ 。\n\n### Challenges\n\nCLIF 的设置对现有的 few-shot learning 方法来首特别具有挑战，大多数 few-shot 学习方法假定所有任务的上游数据集总是可用的，并且没有按时序去学习。因此，上游的任务可以在多任务学习的环境下共同学习。然而，CLIF问题采用的是持续学习的设置，即任务是按顺序访问的，没有重新访问。因此，依靠从任务分布中随机抽样的方法并不适用。\n\n### Tasks and Data Streams\n\n为了将CLIF挑战推向一个更实际的设置，考虑了一组多样化的NLP任务来进行CL和few-shot learning。我们考虑了两个数据集的组合，被称为CLIF-26和CLIF-55任务：\n\n![](https://i.loli.net/2021/10/30/kdeHY4IvltMXyBw.png)\n\n将CLIF-26中每个GLUE任务中的训练样本数量限制在10,000个，以避免数据集过度失衡。对于CLIF-55，每类使用90个样本进行连续学习。\n\n在CLIF-26和CLIF-55的 few-shot 学习任务中，如果没有指定的话，每类使用 k=16 个样本，并在实验中包括更多的 k 的设置。由于GLUE的测试标签没有公开，仅报告了验证集的性能。\n\n-----\n\n## Method\n\n首先介绍我们研究中的 baseline。然后，我们介绍一些现有的持续学习和持续元学习的方法。最后，提出了一个新颖的正则化双级适配器生成框架，以更好地解决CLIF问题。\n\n### Base NLP Models\n\n#### BART and BART-Adapter \n\n由于将CLIF问题中的NLP任务制定为统一的文本到文本格式，我们使用预先训练好的语言模型（LM）作为模型f的架构，并在训练期间对整个模型进行微调。\n\n我们还包括Adapter训练，作为对整个BART模型进行微调的一种改变。这里，适配器是插在BART每层之后的两层MLPs。\n\n给出 transformer的第 $l$ 层的输出 $h_l$ , adapter的输出被计算为 $h_l' = h_l + f_l^a(h_l)$， 其中 $f_l^a$ 是在 $l$ 层的adapter。只有adapter在训练中被学习，BART模型被frozen。\n\n#### Hyper-Networks for Adapter Generation\n\n除了BART和BART适配器之外，还使用考虑HyperNetwork（HNet）架构。HyperNetwork 记为 $g$ ，将任务表示 $z$ 作为输入，并生成另一个预测模型的模型参数，记为 $f$ 来解决该任务。在 few-shot learning 中，$z$ 通常被计算为任务的训练实例的平均表示，即 任务的平均表示: $z = \\frac{1}{|D_{tr}^i|} \\sum_{(x_j,y_j)\\in D_{tr}^i} f_e(x_j, y_j) $ ，其中 $D_{tr}^i$ 是任务 $T^i$ 的训练集，$f_e$ 是encoder。\n\n我们使用一个BART编码器作为 $f_e$，并将 $x$ 和标签 $y$ 的文本格式串联起来，得到任务表示 $z$。\n\n### Baseline Learning Algorithms\n\n#### Single Task Learning\n\n为了了解基础模型在没有任何知识转移的情况下对上游任务的参考性能，应用了单一任务学习（STL）方法，该方法在每个任务的数据集上单独地训练和测试模型 $f$。\n\n在这种情况下，我们忽略了CLIF问题的顺序性，所以我们可以用这个STL的性能来评估不同的持续方法（下面介绍）的有效性。理想情况下，一个有效的 CL 算法应该具有比 STL 结果更好的几率准确性，这意味着它积累了并有效地迁移了知识，用于学习。\n\n同样地，为了了解 few-shot 任务的参考性能，我们在没有任何上游训练的情况下，为每个 few-shot 任务学习一个模型 $f$ ，这样我们就可以用这种性能来评估CLIF方法对泛化能力的改善程度。\n\n#### Continual Learning Algorithms\n\n作为一种简单的基线方法，我们使用 Vanilla 表示简单地在上游任务上按顺序训练模型 $f$。\n\n具体来说，它在 $T_u^i$ 上训练模型 $f$，直到其性能收敛，然后在 $T_u^{i+1}$ 的数据上不断训练 $f$。\n\n请注意，CL 中不允许访问先前任务的数据，还考虑在实验中考虑 CL 算法，例如 EWC、MbPA++和 meta-MbPA。\n\nEWC 正则化了训练过程中重要模型参数的变化，MbPA++ 方法对存储在内存中的几个训练样本执行测试 test-time 调整。 meta-MbPA 方法包括快速适应元学习目标。\n\n#### Hyper-Networks for CL\n\n《Continual learning with hypernetworks》 提出了 hypernetwork-based continual learning。其中减轻灾难性遗忘的高级想法是惩罚超网络在其学习新任务时为先前任务生成的模型权重的改变。虽然原始工作生成模型的整个参数，但我们仅通过生成适配器的权重来使其适应 PTLMs。 将这种方法记为 HNet-Reg。\n\n具体来说，当模型刚刚完成学习任务 $T_{u}^{i-1}$ 并且在持续学习阶段学习任务 $T_u^i$ 之前，我们存储当前超网络为所有先前任务 $T_u^1...T_u^{i=1}$ 生成的适配器权重，记为 $\\{\\hat\\theta_1^{i-1},\\hat\\theta_2^{i-1},...,\\hat\\theta_{i-1}^{i-1}\\}$ ，其中生成是通过超网络 $h$ 应用于先前任务 $1,..,{i-1}$ 的存储任务表示来控制的，记为 $M = \\{z_h^1,...,z_h^{i-1}\\}$ 。在这里，任务 $T_u^i$ 的任务表示 $z_i$ 在学习任务之前随机初始化，并在学习任务时联合优化。\n\n然后，在学习 $T_u^i$  的每一步中，我们随机抽样一个先验任务 $T_u^j \\ \\ (j < i)$ 来规范超网络学习。 它惩罚在当前步骤 $\\theta_j$ 生成的适配器权重与预先计算的权重之间的 $l_2$ 距离，例如 $||\\theta_j-\\hat \\theta_j^{i-1}||_2^2$\n\n因此，避免了超网络 g 在持续学习阶段过多地改变其先前任务的输出，从而更好地保证学习模型的知识积累。\n\n#### Limitations\n\nEWC 和 HNET-Reg 不是为 CLIF 问题精心设计的，CLIF还试图在持续学习后改进对未知任务的 few-shot 泛化。 虽然 MbPA 和 meta-MbPA 中的 test-time 适应可能有利于 few-shot learning，但这些工作并未研究这种能力。 此外，由于这两种算法存储了先前训练任务的真实数据，因此不适用于无法再访问来自早期任务的数据的隐私敏感应用，这是持续学习中的典型场景。\n\n### Our Extension: Bi-level Hypernetworks for Adapters with Regularization\n\n受用于 few-shot 和 CL的超网络方法的启发，我们将基于超网络的CL方法扩展到CLIF。我们提出了一种新的方法，即带有正则化的双级超网络Adapters（BiHNet+Reg），该方法学习使用双级任务表示来生成Adapters权重，以便在一连串的任务中学习快速适应模型，同时通过正则化来减轻遗忘效应。\n\n方法由三个组件组成：\n\n![](https://i.loli.net/2021/10/30/usniAeI3GEyHXL7.png)\n\n- Context Predictor 上下文预测器，从训练实例中生成双级任务表征（即高资源和few-shot表征）\n- Adapter-Wise Hypernetworks 超网络，根据任务表征生成适配器的权重；\n- Regularization 正则化项，阻止所见任务的权重变化以避免遗忘\n\n#### Context Predictor\n\n为每个任务 $t$ 生成两个任务表征，分别在高资源和 few-shot 的情况下为其建模，表示为 $z_h^t$ 和 $z_f^t$，用frozne BART编码器。高资源表征用于鼓励持续学习过程中的知识转移；few-shot 任务表征帮助我们在 few-shot learning 模仿 few-shot任务，以获得更好的泛化，类似于元学习。\n\n然后，高资源任务表示被计算为任务 $t$ 中所有样本的上下文向量的平均值。 记为：$z_h^t = \\frac{1}{|D_t|} \\sum_{(x_i,y_i)\\in D_t} R(x_i, y_i)$\n\n然而，few-shot 任务表示 $z_f^t$ 使用有限数量 K 个采样样本 $z_f^t = \\frac{1}{k} \\sum_{(x_i,y_i)\\in \\Tau(D_t, K)} R(x_i, y_i)$， 其中$\\Tau(D_t,K)$ 是在 $D_t$ 中采样K 个样本。请注意，在不断的学习过程中，上游任务的高资源表征被长期储存在一个记忆模块中，$M=\\{z_h^t| t\\in \\{\\Tau_u^i\\}_{i=1}^{N_u}\\}$ 。在few-shot的学习阶段，我们设定 K为给定的样本的数量，因此对于任何任务，$z_h=z_f$。\n\n#### Adapter-Wise Hypernetworks\n\n使用超网络 $g$ 来生成frozen BART模型 $f$ 的各层之间的适配器的权重。\n\n在训练过程中，使用高资源和采样的任务表征 $z_h^t$ 和 $z_f^t$ 来产生适配器权重 分别记为 $\\theta_t^h$ 和 $\\theta_t^f$。我们对这两个适配器的预测损失进行了优化。\n\n#### Regularization\n\nHyperNetwork是模型中唯一可训练的部分，对生成的适配器施加正则化以减轻遗忘。\n\n虽然 BiHNet 被训练为从高资源和低资源任务表示生成适配器，但发现仅存储和正则化来自高资源任务表示的输出就足够了。\n\n#### Summary and Highlights\n\n总而言之，提出的方法首先生成了双级任务表征，用于训练具有正则化项的适配超网络，以避免随时间推移而遗忘。\n\n与基于重放记忆的CL方法（例如MbPA）不同，我们的方法不存储任何真实的训练实例。相反，它使用任务表示来存储记忆，因此允许该方法应用于对隐私敏感的场景中。\n\n-----\n\n## Results and Analysis\n\n在本节中，将讨论两个主要研究问题：\n\n- 考虑到潜在的灾难性遗忘，与离线设置相比，模型如何在CL设置中长期积累可推广的知识？\n- 持续学习的方法是否能减少对所见任务的表现和可归纳知识的灾难性遗忘。\n\n作者在试验了各种模型架构的组合和学习算法。通过其模型结构和应用的CL算法来说明一种方法，例如BART-Vanilla, BiHNet-EWC。\n\n![](https://i.loli.net/2021/10/30/M9NOlJoS64AYhGy.png)\n\n### Examining Knowledge Accumulation\n\n在这一节中，提出了对模型在离线和CL设置中获得可归纳知识的能力的分析。\n\n我们注意到BiHNet方法，对应于学习生成适配器，应与BiHNet-Single和BART-Adapter-Single进行比较，后者是零知识基线，其学习生成或从随机初始化中学习适配器；\n\n同样，BART方法应与BART-Single进行比较。重点是确定CLIF的挑战，并将方法论的讨论留在下一小节。\n\n#### 问题1：来自上游任务的知识是否有助于模型在在线学习和持续学习设置中的few-shot泛化？\n\n看表2，在CLIF-26和CLIF-55数据集上，我们看到BiHNet-MTL在 few-shot 情况下的表现比零知识基线要好0.4%和1.0%，这意味着在标准的离线学习设置中，上游任务对few-shot 情况下的泛化有帮助。\n\n对于BART模型，我们注意到BART-MTL在Clif-55数据集上比BART-Single提高了2.5%。然而，我们注意到CLIF-26的情况正好相反。鉴于在这些模型中整个BART参数都被优化了，我们假设BART-MTL可能受到了预训练的BART模型本身的知识遗忘的影响；而在适配器和BiHNet模型中，BART模型被冻结了。\n\n因此，在本节的其余部分，我们更关注 侧重于BiHNet方法。\n\n#### 问题2：模型的泛化能力是如何随时间变化的？\n\n![](https://i.loli.net/2021/10/30/GJhbKFcu3sAHVYj.png)\n\n\n\n#### 问题3：模型的灾难性遗忘是否阻碍了其知识积累？\n\n\n\n### Effect of Continual Learning Algorithms\n\n\n\n#### 问题1：持续学习算法能缓解灾难性遗忘吗？\n\n\n\n#### 问题2：缓解灾难性遗忘能更好地保留泛化能力吗？\n\n\n\n#### 问题3：BiHNet-REG比HNet-REG有改进吗？\n\n\n\n#### 问题4：敏感度分析：模型如何在不同数量的few-shot训练样本下执行。\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"Pattern Exploiting Training (PET)","url":"/2021/10/28/Pattern-Exploiting-Training-PET/","content":"\n# Pattern Exploiting Training (PET)\n\n介绍PET范式，可用于半监督或无监督训练。\n\n这篇主要关注两篇相同作者的文章：\n\n《Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference》\n\n《It's Not Just Size That Matters：Small Language Models Are Also Few-Shot Learners》\n\n首先看到一个问题比较好：**BERT在预训练时学习到的知识或者说参数我们在fine-tunning的时候都有用到吗？**\n\n答案是不是的。\n\nBERT的预训练其中一个任务是MLM，就是去预测被 【MASK】掉的token，采用的是拿bert的最后一个encoder（base版本，就是第12层的encoder输出，下图左图蓝色框）作为输入，然后接全连接层，做一个全词表的softmax分类（这部分就是左图的红色框）。但在finetuing的时候，我们是把MLM任务的全连接层抛弃掉，在最后一层encoder后接的初始化层来做具体下游任务。\n\n![](https://i.loli.net/2021/10/28/c31HAsXB5QbPklt.png)\n\nMLM目标是预测 输入时被挑选的15%的单词，所以在BERT的最后一层（如BERT-base版本就是第12层）的token的embedding后会接一个【embedding维度，词表大小】的全连接矩阵，做token的预测，这个全连接矩阵就是MLM层参数\n\n问题是，**能不能通过某些巧妙的设计，把MLM层学习到的参数也利用上？**\n\n> 注意，Prompt设计的这种完形填空和MLM任务是有区别的，二者虽然都是都是词分类，但是候选集不同，MLM的候选词是整个词库，prompt是verbalizer里的词。Prompt使用MLM层把其他的词给忽略掉。\n\n答案当然是可以的，请继续往下看。\n\n现在举一个二分类的例子，输入一条汽车论坛的评论，输出这个评论是属于【积极】or【消极】。但问题是现在我每个类别只有10个labeled数据，1K条unlabeled数据。怎么训练model？\n\n直接做有监督训练?样本量太少，会过拟合。应该优先采用半监督学习的方法，如UDA、MixText这种，而PET采用的是另外一种巧妙的设计思想。\n\n对于\"I love this movie\"这句输入，可以在后面加上Prompt也就是Pattern：\"the movie is ___\"，组成如下这样一句话：\n\n```\nI love this movie, the movie is ___\n```\n\n然后让预训练模型用表示情感的答案（例如\"great\"、\"terrible\"等）做完形填空，最后再将该答案转换为情感分类的标签。这里定义一个**verblizer**作为映射函数，把label【great】映射为+，把label【terrible】映射为- 。\n\n这样一来，我们就可以通过构造合适的「模板」，控制模型的输出空间，从而训练一个完全无监督的预训练模型来解决各种各样的下游任务。**BERT预训练时的MLM层的参数能利用上**。而且，**即使model没有进行fine tunning，这个model其实就会含有一定的准确率**！\n\nPattern和verblizer，就是一个PVP（pattern-verbalizer pairs）。\n\n## Prompt Notation\n\n设 $M$ 是被mask的语言模型，$V$ 是它的词表，$[MASK]$ 也包含在词表中。令 $L$ 为目标分类任务 A 的一组标签。\n\n我们把任务 A 的输入写成一串短语 $x=(s_1,...,s_k)$，其中 $s_i\\in V$.\n\n例如如果 $A$ 是文本推理（两个话虽然句子），$k=2$\n\n我们将pattern定义为一个函数 $P$, 它以 $x$ 为输入，输出一个短语或句子 $P(x)\\in V^*$, 其中正好包含一个MASK标记，也就是说，它的输出可以被看作是一个完形填空问题。\n\n此外，将 verbalizer 定义为一个注入函数 $v:L\\to V$, 它将每个标签映射到 $M$ 的词表中的一个词。\n\n令 $p=(P,v)$  是 PVP（pattern-verbalizer pairs）\n\n我们假设可以访问较小的训练集 $T$ 和 通常大得多的一组无标签数据 $D$ 。\n\n对于每个恰好包含一个MASK标签和 $w\\in V$ 的序列 $z\\in V^*$, 用 $M(w|z)$ 表示语言模型在掩码位置赋予$w$ 的非标准化分数。给定某个输入 $x$，我们将标签 $l\\in L$的得分定义为:\n$$\ns_p(l|x) = M(v(l) | P(x))\n$$\n并使用Softmax获得标签上的概率分布 :\n$$\nq_p(l|x) = \\frac{e^{s_p(l|x)}}{\\sum_{l'\\in L} e^{s_p(l'|x)}}\n$$\n\n\n## Auxiliary Language Modeling\n\n只有几个训练示例可用，可能会发生灾难性的遗忘。\n\n由于现在是用MLM做分类任务，所以可以引入无标注数据一起训练！\n\n举个简单的例子，下图样例1是labeled数据，我们利用pattern把它改写后，对 __ 部分做完形填空预测（即MLM任务）。\n\n样例2是一个unlabeled数据，我们就不对  __ 部分做预测，而是对被【MASK】做预测。这里的【MASK】可以采用BERT的方法，随机对句子的15%token进行【MASK】。\n\n![](https://i.loli.net/2021/10/28/daK2yEYcXFSoN9u.png)\n\n训练时两个损失联合训练：\n$$\nL = (1-\\alpha) \\cdot L_{CE} + \\alpha \\cdot L_{MLM}\n$$\n由于 $L_{MLM} $ 通常比 $L_{CE}$ 大得多，在初步实验中，发现$α=10^{-4}$的值能给出良好的结果\n\n这样做的好处是，能让model更适应于当前的任务，有点像**在预训练模型上继续根据任务的domain和task继续做预训练，然后再做fine-tunning呢？**\n\n## Combining PVPs\n\n引入一个问题，**怎么评价我们的pattern定义得好不好？**\n\n我们可以造两个pattern，又可以造两个verblizer。其实一共有4个PVP。我们怎么衡量哪一个PVP训练完后在测试集上的效果最好？\n\n答案是我们也不知道，因为**我们不能站在上帝视角从一开头就选出最佳的PVP，同样由于是小样本学习，也没有足够的验证集让我们挑选最佳的PVP**。既然如此，解决方式就是**知识蒸馏**。\n\n具体的，我们用20个labeled数据训练4个PVP模型，然后拿这四个PVP模型对1K条unlabeled数据进行预测，预测的结果用下式进行平均。\n$$\ns_M(l|x) = \\frac{1}{Z} \\sum_{p\\in P} w(p) \\cdot s_p(l|x)\n$$\n其中 $Z$ 保持概率和为1， $s_p(l|x)$ 就是单个PVP模型对样本预测的概率分布，$w(p)$ 就是PVP的权重。\n\n有uniform和weighted两种方式，uniform就是所有PVP的权重都为1，weighted就是把每个PVP的权重设置为它们在训练集上的准确率。最后还要对上式进行**temperature=2**的软化。\n\n这就是在做知识的蒸馏。**何谓知识的蒸馏？**经过这样处理后，噪声减少了，利用多个PVP平均的思想把某些本来单个PVP预测偏差比较大的进行平均后修正。\n\n这样子，利用训练好的PVPs所有1K条unlabeled数据打上soft label，再用这1K条打上软标签的数据进行传统的有监督训练，训练完的model应用于下游任务的model。\n\n> 注意哦，这里就可以用**轻量的模型**来做fine tuning了哦，因为从20条labeled数据扩充到1K条有带有soft label的数据，labeled数据量大大增加，这时候轻量级的模型也能取得不错的结果，而且轻量模型对轻量部署、高并发等场景更加友好。\n\n下图就是所有的流程，再总结一下步骤就是\n\n![](https://i.loli.net/2021/10/28/iyWJmp32kv8uQeU.png)\n\n- 第一步先定义PVPs，然后对每对PVP用labeled数据进行单独的训练，该步可以加入上面提到的Auxiliary Language Modeling一起训练\n- 第二步：用训练好的PVPs，对unlabled数据进行预测，并知识蒸馏，得到大量的soft label；\n- 第三步：用第二步得到的带有soft label的data，用传统的fine tuning方法训练model。\n\n## IPET\n\n将所有单个模型的知识提炼到单个分类器C中意味着它们不能相互学习。由于一些 pattern 的表现(明显地)比其他模式差，因此最终模型的训练集 $T_C$可能包含许多标记错误的示例。\n\n在每个PVP训练的过程中，互相之间是没有耦合的，就是没有互相交换信息，IPET的意思就是想通过迭代，不断扩充上面训练PVP的数据集。\n\n这里简单举个例子，现在有20个labeled数据，1K个unlabeled数据，定义5个PVP，\n\n第一轮，利用20个labeled数据分别训练PVP，第二轮，用第2~4个PVP来预测这1K unlabeled数据，然后选一些模型预测概率比较高的加入到第一个PVP的训练集上，同样用第1、3、4、5个PVP来训练这1K条，然后也将这部分加入到第2个PVP的训练集中，然后再训练一轮，训练后，重复，这样每一轮每个PVP的训练样本不断增多，而且PVP之间的信息也发生了交互。\n\n![](https://i.loli.net/2021/10/28/lzncwRWb5ovF93e.png)\n\n## 实验\n\n![](https://i.loli.net/2021/10/28/faoicY2BVnuyI9l.png)\n\n\n\n\n\n## 分析\n\n### Combining PVPs\n\n作者发现不同PVP之间可能有很大的性能差别，如下图min就是最差的PVP，max就是最好的PVP，可以观察到它们之间的差别就很大。但是又不能站在上帝视角从一开始就选择最好的PVP，所以办法就是做commind PVPs，即上面所提到的知识蒸馏，而且发现蒸馏后会比采用单个最好的PVP效果还要好，并且发现uniform和weighted两个方法效果差不多。\n\n![](https://i.loli.net/2021/10/28/UkDHrpN1St9q8hR.png)\n\n### Auxiliary Language Modeling\n\nlabeled数据越少，auxiliary task的提升效果越明显。\n\n![](https://i.loli.net/2021/10/28/JtsafNYc1BxU9V2.png)\n\n### Iterative PER\n\niPET的效果，因为iPET是迭代多轮，每一轮每个PVP的训练集都会增大，从图可以看到每一轮的模型效果都是越来越好的。\n\n![](https://i.loli.net/2021/10/28/hVICaP1rKk62fdZ.png)\n\n### In-Domain Pretraining\n\n这里讨论了一个问题：PET效果比有监督训练好，是不是因为PET在大量无标签上打上软标签，扩大了有标签数据集？\n\n然后作者做了一个实验，有监督训练时，先在所有数据集上进行继续预训练（这一步作者认为相当于把无标签数据也加进来了），然后再fine funing。实验结果表明，即使这样，有监督效果也离PET有一定距离。\n\n![](https://i.loli.net/2021/10/28/Ftm58XIxSDTbj9Z.png)\n\n\n\n## It's Not Just Size That Matters：Small Language Models Are Also Few-Shot Learners\n\n这篇论文是上篇论文的延伸，其实没有太多新的工作，主要是下面提到的处理多个token的mask，这篇论文主要PK GPT3，不断diss GPT3有多少的不环保。\n\n## PET with Multiple Masks\n\nPET要定义pattern和verblizer，还拿那汽车评论场景举例，我们能不能定义一个verbilzer，它把不同label映射到长度不一的token，如\n\n```\n定义 Pattern: s。真__!  这里s代表原始输入。  \n定义Verbilzer,  v(积极)=好,   V(消极)=不好\n样例 x:   保养贵，配件贵，小毛病多，还有烧机油风险。(label为消极)\npattern(x) = 保养贵,配件贵,小毛病多,还有烧机油风险，真__!\n```\n\n因为verbilzer把标签映射到长度不一致的token，那我们究竟定义长度为多少的下划线___，来让model进行完形填空。答案是用最长的那个，例如这里最长的是\"不好\"，长度为2，所以就挖空两个下划线来让模型做完形填空预测。\n\n做Inference时，\n\n- $p(\\text{积极}|x) =$ 第一个下划线__ 模型预测到token为好的概率。\n- $p(\\text{消极}|x)= $  就麻烦一些，先让模型对两个下划线，进行预测，看是第一个下划线预测token为不，还是第二个下划线预测token为好的概率高一些，把高的那个token先填上去，再重新预测剩下的。举个例子，假如模型预测第一个下划线token为不的概率是0.5，第二个下划线token为好的概率为0.4，即先把不填上第一个下划线，然后再用模型重新预测第二个token为好的概率，假如为0.8，即  $p(\\text{消极}|x)= 0.5*0.8=0.4$  \n\n做train时，就不考虑这么细致了，具体的，取上面的例子为例，\n\n- $p(\\text{积极}|x) =$ 第一个下划线__，模型预测到token为好的概率，跟inference是一样的\n- $p(\\text{消极}|x)= 0.5*0.4 = 0.2$ ，这里就不分成两步，一步KO，目的是一次前向计算就算完，避免训练过慢。  \n\n最后，采用的损失函数也跟第一篇的不一样，这里用的是hinge loss，详细的请看论文。\n$$\n\\sum_{y'\\in Y_x} max(0; 1-log\\hat q_p (y|x) + log\\hat q_p(y'|x))\n$$\n\n### Unlabeled Data Usage\n\n还是下面这幅图，这里讨论了unlabeled数据的利用。\n\n在PET利用到unlabeled数据的有三个地方：\n\n- 第一处：PET的第二步，用PVPs对unlabeled数据进行知识蒸馏，给数据打上soft label，然后第三步利用这些软标签训练一个模型；\n- 第二处：PET的第一步，假如用的是iPET的话，每一个generation都会把部分的无标签数据打上标签，加入到PVP的训练集；\n- 第三处：PET的第一步，假如采用的是Auxiliary Language Modelling辅助训练，也会引入无标签数据。\n\n首先，讨论上面的第一点，究竟能不能直接用PET训练的第一步的PVPs来做预测，这样就不用给unlabeled数据打软标签了（因为虽然说unlabeled数据比labled数据容易获得，但某些场景下unlabeled数据也有可能是拿不到的），答案是可以的，大家看下表的倒数两列，发现不用PET训练的第二、第三步，直接采用第一步训练好的PVPs来做下游应用的预测，效果也是OK的。\n\n**只不过，这样做的话，你应用于下游任务的时候就是一堆PVP模型，而不是单一个模型了，这样对轻量部署不是很友好**。\n\n![](https://i.loli.net/2021/10/28/SwGde9gKBF5t1xb.png)\n\n还讨论了上面的第二处，发现iPET训练过程中，每一个generation从unlabeled数据中挑选部分加入到PVP的训练集，能让PVP收敛更快，减少不稳定性。\n\n![](https://i.loli.net/2021/10/28/SeqEI7XiB2rgCVl.png)\n\n### Model Type\n\n不同预训练模型的影响，像BERT这种双向的语言模型会比GPT这种单向的要好，因为假如采用的是单向的语言模型，那么pattern的下划线__部分只能放在句子末尾进行预测。\n\n![](https://i.loli.net/2021/10/28/73phCBVNdwszAtf.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"Parameter-Efficient Transfer Learning for NLP","url":"/2021/10/23/Parameter-Efficient-Transfer-Learning-for-NLP/","content":"\n# Parameter-Efficient Transfer Learning for NLP\n\n微调大型预训练模型是 NLP 中一种有效的传输机制。 但是，在存在很多下游任务的情况下，微调参数效率低下：每项任务都需要一个全新的模型。\n\n作为替代方案，作者建议使用Adapter进行迁移。原始网络的参数保持不变，实现了高度的参数共享。\n\nAdapter提供紧凑且可扩展的模型；它们只为每个任务添加几个可训练的参数，并且可以添加新任务，而无需重新访问以前的任务。\n\n- 紧凑型模型：在每个任务中使用少量附加参数解决多个任务的模型。\n\n- 可扩展模型：可以增量训练以解决新任务，而不会忘记以前的任务。\n\n\n\n自然语言处理中最常见的两种迁移学习技术是 feature-based 和 fine-tuning。\n\n- feature-based的转移涉及预训练实值嵌入向量。 这些嵌入可能在单词、句子或段落级别。 然后将嵌入提供给自定义的下游模型。\n-  fine-tuning 涉及从预先训练的网络复制权重并在下游任务上调整它们\n\nfeature-based 和 fine-tuning 都需要为每个任务设置一组新的权重。 如果网络的较低层在任务之间共享，则fine-tuning参数效率更高。 然而，提出的adapter tuning方法的参数效率更高。\n\n![](https://i.loli.net/2021/10/24/rQJmoCbIKV8hFN3.png)\n\nx 轴显示每个任务训练的参数数量； 这对应于解决每个额外任务所需的模型大小的边际增加。\n\nadapter tuning 训练少两个数量级的参数来，同时获得与fine-tuning 相似的性能。\n\nadapter 是在预训练网络层之间添加的新模块。 基于adapter tuning与 feature-based/fine-tuning在以下方面有所不同。\n\n考虑参数为$w$ 的函数 $\\phi_w(x)$ (神经网络)。\n\nFeature-based 将 $\\phi_w$ 与新函数组合 $X_v$ 在一起: $X_v(\\phi_w(x))$ , 然后，仅训练新的、特定于任务的参数 $v$\n\nFine-tuning 为每个新任务调整原始参数 $w$ , 限制紧凑性。\n\n对于adapter tuning，定义了新函数 $\\psi_{w,v}(x)$，其中参数 $w$ 从预训练中复制过来。初始参数 $v_0$ 设置为使新函数类似于原始函数：$\\psi_{w,v_0}\\approx \\phi_w(x) $ 。 在训练期间，只有 $v$ 被调整。 定义 $\\psi_{w,v}$ 通常涉及向原始网络添加新层 $\\phi_{w}$\n\n如果选择  $|v|≪|w|$，结果模型需要 $∼|w|$ 多任务的参数。 由于 w 是固定的，模型可以扩展到新任务而不影响以前的任务。\n\nadapter tuning 几乎与完全Fine-tuning 的BERT的性能相当，但仅使用3%的特定于任务的参数，而微调使用100%的特定于任务的参数。\n\n## Method\n\n为了实现这些特性，提出了一个新的瓶颈适配器模块。 当执行深层网络的普通fine-tuning时，对网络的顶层进行修改。这是必需的，因为上游和下游任务的标签空间和损失不同。\n\nAdapter模块执行更通用的架构修改，以将预先训练的网络重新用于下游任务。\n\n 特别是，Adapter调整策略涉及将新层注入原始网络。 原始网络的权重不变，而新的Adapter层是随机初始化的。 在标准Fine-tuning中，新的顶层和原始权重是共同训练的。 相比之下，在适配器调整中，原始网络的参数被冻结，因此可能被许多任务共享。\n\n适配器模块有两个主要功能：参数数量较少 和 near-identity 的初始化。\n\n与原始网络的层相比，适配器模块很小。 这意味着当添加更多任务时，总模型大小增长相对缓慢。\n\nAdapter模型的稳定训练需要 near-identity 的初始化；下图实验证明初始化很重要\n\n![](https://i.loli.net/2021/10/24/cifQn7Gb2vejPyM.png)\n\n横坐标为初始化分布的标准差\n\n### Instantiation for Transformer Networks\n\n![](https://i.loli.net/2021/10/24/R7zDTM84XbqAfuI.png)\n\n为了限制参数的数量，提出了一种瓶颈结构。\n\nAdapter首先将原始的d维特征投影到较小的维度 m，然后应用非线性再投影回d维。\n\n每层添加的参数总数(包括偏置)为 $2md+d+m$ 。\n\n通过设置 $m≪d$ ，限制每个任务添加的参数数量，使用的参数大约是原始模型参数的0.5−8%。\n\n瓶颈维度 m 提供了一种在性能和参数效率之间进行权衡的简单方法。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"二分查找基本思想：减而治之","url":"/2021/10/23/二分查找基本思想：减而治之/","content":"\n# 二分查找基本思想：减而治之\n\n![](https://i.loli.net/2021/10/23/rHePAwZuEWGXUgh.jpg)\n\n这里「减」是「减少问题」规模的意思，治是「解决」的意思。「减治思想」从另一个角度说，是「排除法」，意即：每一轮排除掉一定不存在目标元素的区间，在剩下 可能 存在目标元素的区间里继续查找。每一次我们通过一些判断和操作，使得问题的规模逐渐减少。又由于问题的规模是有限的，我们通过有限次的操作，一定可以解决这个问题。\n\n可能有的朋友听说过「分治思想」，「分治思想」与「减治思想」的差别就在于，我们把一个问题拆分成若干个子问题以后，应用「减治思想」解决的问题就只在其中一个子问题里寻找答案。\n\n## 二分查找算法的应用范围\n\n### 在有序数组中进行查找一个数（二分下标）\n\n这里「数组」和「有序」是很重要的，我们知道：数组具有 随机访问 的特性，由于数组在内存中 连续存放，因此我们可以通过数组的下标快速地访问到这个元素。如果数据存放在链表中，访问一个元素我们都得通过遍历，有遍历的功夫我们早就找到了这个元素，因此，在链表中不适合使用二分查找。\n\n### 在整数范围内查找一个整数（二分答案）\n\n如果我们要找的是一个整数，并且我们知道这个整数的范围，那么我们就可以使用二分查找算法，逐渐缩小整数的范围。这一点其实也不难理解，假设我们要找的数最小值为 0，最大值为 N，我们就可以把这个整数想象成数组 [0, 1, 2,..., N] 里的一个值，这个数组的下标和值是一样的，找数组的下标就等于找数组的值。这种二分法用于查找一个有范围的数，也被称为「二分答案」，或者「二分结果」，也就是在「答案区间」里或者是「结果区间」里逐渐缩小目标元素的范围；\n\n 在我们做完一些问题以后，我们就会发现，其实二分查找不一定要求目标元素所在的区间是有序数组，也就是说「有序」这个条件可以放宽，半有序数组或者是山脉数组里都可以应用二分查找算法。\n\n旋转数组和山脉数组有什么样的特点呢？可以通过当前元素附近的值推测出当前元素一侧的所有元素的性质，也就是说，旋转和山脉数组的值都有规律可循，元素的值不是随机出现的，在这个特点下，「减治思想」就可以应用在旋转数组和山脉数组里的一些问题上。我们可以把这两类数组统一归纳为部分有序数组。\n\n## 二分查找算法的两种思路\n\n思路 1：在循环体中查找元素 （先介绍）；\n思路 2：在循环体中排除目标元素一定不存在的区间。\n\n- 如果这个二分查找的问题比较简单，在输入数组里不同元素的个数只有 1 个，使用思路 1 ，在循环体内查找这个元素；\n- 如果这个二分查找的问题比较复杂，要你找一个可能在数组里不存在，或者是找边界这样的问题，使用思路 2 ，在循环体内排除一定不存在目标元素的区间会更简单一些。\n\n\n\n```java\npublic class Solution {\n  \n    // 「力扣」第 704 题：二分查找\n  \t// 循环体中\n    public int search(int[] nums, int target) {\n        int len = nums.length;\n\n        int left = 0;\n        int right = len - 1;\n        // 目标元素可能存在在区间 [left, right]\n        while (left <= right) {\n            // 推荐的写法是 int mid = left + (right - left) / 2;\n            int mid = (left + right) / 2;\n            if (nums[mid] == target) {\n                return mid;\n            } else if (nums[mid] < target) {\n                // 目标元素可能存在在区间 [mid + 1, right]\n                left = mid + 1;\n            } else {\n                // 目标元素可能存在在区间 [left, mid - 1]\n                right = mid - 1;\n            }\n        }\n        return -1;\n    }\n  \t// 排除\n  \tpublic int search1(int[] nums, int target){\n      \tint len = nums.length;\n      \tint left = 0;\n      \tint right = len - 1;\n      \t// 目标元素可能存在在区间 [left, right]\n      \twhile(left < right){\n          \tint mid = left + (right - left) / 2;\n          \tif (nums[mid] < target){\n              \t// 下一轮搜索区间是 [mid+1, right]\n              \tleft = mid + 1;\n            } else{\n              \t// 下一轮搜索区间是 [left, mid]\n              \tright = mid;\n            }\n        }\n      \tif (nums[left] == target){\n          \treturn left;\n        }\n      \treturn -1;\n    }\n}\n```\n\n## 二分查找的细节（重点）\n\n\n\n### 细节 1：循环可以继续的条件\n\nwhile (left <= right) 表示在区间里只剩下一个元素的时候，我们还需要继续查找，因此循环可以继续的条件是 left <= right，这一行代码对应了二分查找算法的思路 1：在循环体中查找元素。\n\n### 细节 2：取中间数的代码\n\n取中间数的代码 int mid = (left + right) / 2; ，严格意义上是有 bug 的，这是因为在 left 和 right 很大的时候，left + right 有可能会发生整型溢出，这个时候推荐的写法是：\n\n```java\nint mid = left + (right - left) / 2;\n```\n\n这里要向大家说明的是 /2 这个写法表示 下取整。这里可能有的朋友有疑问：这里取中间位置元素的时候，为什么是取中间靠左的这个位置，能不能取中间靠右那个位置呢？答案是完全可以的。先请大家自己思考一下这个问题，我们放在细节 3 说。\n\n有些朋友可能会看到 int mid = (left + right) >> 1; 这样的写法，这是因为整数右移 1 位和除以 2（向下取整）是等价的，这样写的原因是因为位运算比整除运算要快一点。但事实上，高级的编程语言，对于 / 2 和除以 2 的方幂的时候，在底层都会转化成为位运算，我们作为程序员在编码的时候没有必要这么做，就写我们这个逻辑本来要表达的意思即可，这种位运算的写法，在 C++ 代码里可能还需要注意优先级的问题。\n\n在 Java 和 JavaScript 里有一种很酷的写法：\n\n```java\nint mid = (left + right) >>> 1;\n```\n\n这种写法也是完全可以的，这是因为 >>> 是无符号右移，在 left + right 发生整型溢出的时候，右移一位由于高位补 0 ，依然能够保证结果正确。如果是写 Java 和 JavaScript 的朋友，可以这样写。在 Python 语言里，在 32 位整型溢出的时候，会自动转成长整形，这些很细枝末节的地方，其实不是我们学习算法要关注的重点。\n\n我个人认为这几种种写法差别不大，因为绝大多数的算法面试和在线测评系统给出的测试数据，数组的长度都不会很长，遇到 left + right 整型溢出的概率是很低的，我们推荐大家写 int mid = left + (right - left) / 2;，让面试官知道你注意了整型溢出这个知识点即可。\n\n### 细节 3：取中间数可不可以上取整\n\n我们在「细节 2」里介绍了 int mid = (left + right) / 2; 这个表达示里 / 2 这个除号表示的含义是下取整。很显然，在区间里有偶数个元素的时候位于中间的数有 22 个，这个表达式只能取到位于左边的那个数。一个很自然的想法是，可不可以取右边呢？遇到类似的问题，首先推荐的做法是：试一试就知道了，刚刚我们说了实证的精神，就把\n\n```java\nint mid = (left + right + 1) / 2;\n// 或\nint mid = left + (right - left + 1) / 2;\n```\n\n因为我们的思路是根据中间那个位置的数值决定下一轮搜索在哪个区间，每一轮要看的那个数当然可以不必是位于中间的那个元素，靠左和靠右都是没有问题的。\n\n甚至取到每个区间的三分之一、四分之一、五分之四，都是没有问题的。\n\n```java\nint mid = left + (right - left) / 3;\n// 或\nint mid = left + 4 * (right - left) / 5;\n```\n\n一般而言，取位于区间起点二分之一处，首先是因为这样写简单，还有一个更重要的原因是：取中间位置的那个元素在平均意义下效果最好。这一点怎么理解呢？\n\n在没有任何「**先验知识**」的情况下，在搜索区间里猜中间位置是最好的。\n\n\n\n# 例题\n\n## [35. 搜索插入位置](https://leetcode-cn.com/problems/search-insert-position/)\n\n```java\nclass Solution {\n    public int searchInsert(int[] nums, int target) {\n        int len = nums.length;\n        int left = 0;\n        int right = len-1;\n        int mid=0;\n        // 目标元素可能存在在区间 [left, right]\n        // 区间里只剩下一个元素的时候，我们还需要继续查找\n        while(left <= right){\n            mid = left + (right - left) / 2;\n            if (nums[mid] == target){\n                return mid;\n            }else if(nums[mid] < target){\n                left = mid + 1;\n            }else{\n                right = mid -1;\n            }\n        }\n        // 分别处理如下四种情况\n        // 目标值在数组所有元素之前  [0, -1]\n        // 目标值等于数组中某一个元素  return mid;\n        // 目标值插入数组中的位置 [left, right]，return  right + 1\n        // 目标值在数组所有元素之后的情况 [left, right]， return right + 1\n        return right + 1;\n    }\n  \t\n  \tpublic int searchInsert1(int[] nums, int target) {\n        int length = nums.length;\n\n        if (nums[length - 1] < target) {\n            return length;\n        }\n        int left = 0;\n        int right = length;\n        while(left < right){\n            int mid = left + (right - left)/ 2;\n            if(nums[mid] < target){\n                left = mid + 1;\n            }else{\n                right = mid;\n            }\n        }\n        return left;\n    }\n}\n```\n\n## [34. 在排序数组中查找元素的第一个和最后一个位置](https://leetcode-cn.com/problems/find-first-and-last-position-of-element-in-sorted-array/)\n\n- 不可以找到target后向两边扩散（线性查找），这样的话时间复杂度为 $O(N)$\n- 应该使用两次二分查找，先找target第一次出现的位置，再找target最后一次出现的位置，注意分类讨论，并且把分类讨论的结果合并。\n\n```java\nclass Solution {\n    public int[] searchRange(int[] nums, int target) {\n        int len = nums.length;\n        if(len==0){\n            return new int[]{-1,-1};\n        }\n        int left = 0;\n        int right = len-1;\n        if(target > nums[right] || target<nums[left]){\n            return new int[]{-1,-1};\n        }\n\n        int firstPosition = findFirstPosition(nums, target);\n        if(firstPosition == -1){\n            return new int[]{-1,-1};\n        }\n        int lastPosition = findLastPosition(nums, target);\n        return new int[]{firstPosition, lastPosition};\n    }\n\n    private int findFirstPosition(int[] nums, int target){\n        int left = 0;\n        int right = nums.length-1;\n        while(left < right){\n            int mid = left + (right - left) / 2;\n            // 小于一定不是解\n            if(nums[mid] < target){\n                // 下一轮搜索区间是 [mid+1, right]\n                left = mid + 1;\n            } else{\n                // nums[mid] > target, 下一轮搜索区间是[left ,mid]\n                right = mid;\n            }\n        }\n        if(nums[left] == target){\n            return left;\n        }\n        return -1;\n    }\n\n    private int findLastPosition(int[] nums, int target){\n        int left = 0;\n        int right = nums.length - 1;\n        while(left < right){\n            int mid = left + (right-left+1)/2;\n            if(nums[mid] > target){\n                // 下一轮搜索区间是[left, mid-1]\n                right = mid -1;\n            } else{\n                // 下一轮搜索区间是[mid, right]\n                left = mid;\n            }\n        }\n        return left;\n    }\n}\n```\n\nfindFirstPosition() \n\n情况 ① ：当 nums[mid] < target 时\n\n- mid 一定不是 target 第一次出现的位置；\n- 由于数组有序，mid 的左边一定比 nums[mid] 还小，因此 mid 的左边一定不是 target 第一次出现的位置；\n- mid 的右边比 nums[mid] 还大，因此 mid 的右边有可能存在 target 第一次出现的位置。\n  因此下一轮搜索区间是 [mid + 1..right]，此时设置 left = mid + 1；\n\n情况 ② ：当 nums[mid] == target 时\n\n- mid 有可能是 target 第一次出现的位置；\n- mid 的左边也有可能是 target 第一次出现的位置；\n- mid 的右边一定不是 target 第一次出现的位置。\n  因此下一轮搜索区间在 [left..mid]，此时设置 right = mid。\n\n情况 ③ ：当 nums[mid] > target 时\n\n- mid 一定不是 target 第一次出现的位置；\n- mid 的右边也一定不是 target 第一次出现的位置；\n- mid 的左边有可能是 target 第一次出现的位置，因此下一轮搜索区间在 [left..mid - 1]，此时设置 right = mid - 1。\n\n重点在这里：把情况 ② 和情况 ③ 合并，即当 nums[mid] >= target 的时候，下一轮搜索区间是 [left..mid]，此时设置 right = mid - 1。这样做是因为：只有当区间分割是 [left..mid] 和 [mid + 1..right] 的时候，while(left < right) 退出循环以后才有 left == right 成立。\n\nfindLastPosition() 也可以类似分析，这里省略。\n\n在本题解中，while(left < right) 只表示退出循环以后有 left == right 成立，不表示搜索区间为左闭右开区间，本题解以及我的其它题解中，对循环不变量的定义均为：在 nums[left..right] 中查找目标元素。\n\n## [153. 寻找旋转排序数组中的最小值](https://leetcode-cn.com/problems/find-minimum-in-rotated-sorted-array/)\n\n### 二分法\n\n旋转排序数组，几乎就是有序的数组，可以通过比较特定位置的元素的值判断达到减治的效果（逐渐缩小搜索区间）\n\n很自然的，会看 **中间数** （位于待搜索区间中间位置的元素），由于不是有序数组，因此不能称之为中位数。\n\n另外，待搜索区间头和尾的元素位置特殊的元素，有两个比较自然的思路是：\n\n- 思路1：看看当前搜索区间的 **左边界** 和 **中间数**，是不是可以缩小搜索区间的范围\n- 思路2：看看当前搜索区间的 **右边界** 和 **中间数**，是不是可以缩小搜索区间的范围\n\n要想清楚不妨举几个例子：\n\n例1：$[1,2,3,4,5]$\n\n例2：$[2,3,4,5,1]$\n\n这两个例子的 **中间数** 都比左边界大，但 旋转排序数组 的**最小值**  一个在中间数的左边，一个在右边，因此思路1不合适。\n\n针对思路2，依然写两个例子，这两个例子分别是 **中间数比右边界大** 和 **中间数比右边界小**，看看能不能推导出一般化的结论。\n\n例3：$[7,8,9,10,11,12,1,2,3]$\n\n中间数 11 比右边界 3 大，因此中间数左边的数（包括中间数）都不是 旋转排序数组的最小值，因此下一轮搜索的区间是 $[mid+1, right]$ ，将下一轮搜索的左边界设置成中间数位置 +1，即 $left = mid+1$\n\n例4：$[7,8,1,2,3]$\n\n中间数 1 比右边界3小，说明中间数到右边界是递增的，那么中间数右边的（不包括中间数）一定不是 旋转数组的最小值，可以排除，但中间数有可能是整个数组中的最小值，就如本例，因此， 在下一轮搜索区间是 $[left,mid]$，于是把右边界设置为 $right=mid$\n\n从例 3 和例 4 可以看出，不论中间数比右边界大，还是中间数比右边界小，我们都可以排除掉将近一半的元素，把原始问题转换成一个规模更小的子问题，这正是「减而治之」思想的体现，因此思路 2 可行。\n\n```java\nclass Solution {\n    public int findMin(int[] nums) {\n        int len = nums.length;\n        int left = 0;\n        int right = len-1;\n\n        while(left < right){\n            int mid = left + (right - left) / 2;\n            if(nums[mid] > nums[right]){\n                left = mid +1;\n            }else{\n                // 因为题目说 可以假设数组中不存在重复元素\n                // 此时一定有 nums[mid] < nums[right]\n                right = mid;\n            }\n        }\n        return nums[left];\n    }\n}\n```\n\n### 分治法\n\n分治法是将原问题划分成若干与原问题同结构且规模更小的子问题，等到这些子问题解决了以后，原问题也得到了\n\n```java\nclass Solution {\n    public int findMin(int[] nums) {\n        int len = nums.length;\n        return findMin(nums, 0, len-1);\n    }\n\n    private int findMin(int[] nums, int left, int right){\n        if(left == right){\n            return nums[left];\n        }\n\n        if(left+1 == right){\n            return Math.min(nums[left], nums[right]);\n        }\n        int mid = left + (right - left) / 2;\n\n        // 这一步是关键\n        if(nums[left] < nums[right]){\n            return nums[left];\n        }\n\n        if(nums[mid] < nums[right]){\n            // 右边是顺序数组， [mid +1, right] 这个区间里的元素可以不看\n            return findMin(nums, left, mid);\n        } else{\n            return findMin(nums, mid+1, right);\n        }\n\n    }\n}\n```\n\n## [154. 寻找旋转排序数组中的最小值 II](https://leetcode-cn.com/problems/find-minimum-in-rotated-sorted-array-ii/)\n\n有序数组可能存在重复值\n\n### 二分法\n\n- 当中间数比右边界表示的数大的时候，中间数一定不是目标数\n- 当中间数比右边界表示的数小的时候，中间数就可能是目标数\n- 当中间数比有边界表示的数相等时：此时只把右边界排除掉就好\n\n```java\nclass Solution {\n    public int findMin(int[] nums) {\n        int len = nums.length;\n        int left = 0;\n        int right = len - 1;\n\n        while(left < right){\n            int mid = left + (right - left) /2;\n            if(nums[mid] > nums[right]){\n                left = mid + 1;\n            } else if(nums[mid] < nums[right]){\n                right = mid;\n            } else{\n                assert nums[mid] == nums[right];\n                right -- ;\n            }\n        }\n        return nums[left];\n    }\n}\n```\n\n### 分治法\n\n分治法将原问题划分成若干与原问题同结构且规模更小的子问题，等到这些子问题解决了以后，原问题也得到了解决。\n\n```java\nclass Solution {\n    public int findMin(int[] nums) {\n        int len = nums.length;\n        return findMin(nums, 0, len-1);\n    }\n\n    private int findMin(int[] nums, int left, int right){\n        if(left==right){\n            return nums[right];\n        }\n        if(left+1 == right){\n            return Math.min(nums[left] , nums[right]);\n        }\n        \n        if(nums[left] < nums[right]){\n            return nums[left];\n        }\n\n        // 分治边界\n        int mid = left+(right-left)/2;\n        if(nums[mid] == nums[right]){\n            return findMin(nums, left, right-1);\n        } else if(nums[mid] < nums[right]){\n            return findMin(nums, left, mid);\n        } else{\n            return findMin(nums, mid+1, right);\n        }\n    }\n}\n```\n\n## [33. 搜索旋转排序数组](https://leetcode-cn.com/problems/search-in-rotated-sorted-array/)\n\n题中数组不存在重复元素。\n\n根据示例 [4, 5, 6, 7, 0, 1, 2] ，自己手写几个旋转数组。不难发现：将待搜索区间从中间一分为二，位于中间的元素 nums[mid] 一定会落在其中一个有序区间里。需要分类讨论。\n\n![](https://i.loli.net/2021/10/29/6HSdnsJcp7wYvtb.png)\n\n**中间元素和右边界的关系** 为例，其它情况类似。由于不存在重复元素，**所以它们的关系不是大于就是小于**。\n\n**关键**：把比较好些的判断（`target` 落在有序的那部分）放在 `if` 的开头考虑，把剩下的情况放在 `else` 里面。\n\n```java\nclass Solution {\n    public int search(int[] nums, int target) {\n        int len = nums.length;\n        if (len == 0){\n            return -1;\n        }\n        int left = 0;\n        int right = len - 1;\n        while(left < right){\n            // 根据分支的逻辑将中间数改成上取整\n            int mid = left + (right - left + 1) / 2;\n            if(nums[mid] < nums[right]){\n                // 此时 [mid..right] 有序\n                if(nums[mid] <= target && target <= nums[right]) {\n                    // 如果 target 的值落在这个区间里, 下一轮搜索区间是[mid..right],此时设置left = mid\n                    left = mid;\n                } else{\n                    // 否则，下一轮搜索区间是 [left..mid-1] 此时设置 right = mid - 1\n                    right = mid - 1;\n                }\n            } else{\n                // 此时 nums[mid] >= nums[right] 注意此时 mid 可能与right重合\n                // 数组前半个部分有序，即[left..mid] 有序， 为了与上一个分支的逻辑一致，认为[left .. mid-1]\n                if(nums[left] <= target && target <= nums[mid-1]){\n                    // 如果target的值落在区间 [left..mid-1] 里，设置right = mid -1\n                    right = mid - 1;\n                } else{\n                    // 否则，下一轮搜索区间是 [mid..right] 此时设置 left = mid\n                    left = mid;\n                }\n            }\n        }\n        if (nums[left] == target){\n            return left;\n        }\n        return -1;\n    }\n}\n```\n\n## [4. 寻找两个正序数组的中位数](https://leetcode-cn.com/problems/median-of-two-sorted-arrays/)\n\n解题核心思想：\n\n- 使用二分查找确定两个有序数组的 【分割线】，中位数就由分割线左右两侧的元素决定；\n- 分割线满足这样的性质：左右两边元素个数相等（这里忽略两个数组长度之和奇偶性的差异）\n- 分割线左边所有元素 小于等于 分割线右边所有算是\n- 由于分割线两边元素个数相等，移动分割线就会有【此消彼长】的现象，所以使用二分法去定位\n\n这条分割线的特点是：\n\n- 当数组的总长度为偶数的时候，分割线左右的数字个数总和相等；当是奇数时，分割线左边数字比右边仅仅多1\n- 分割线左边的所有元素都小于等于分割线右边的所有元素\n\n如果找到这条分割线，那么中位数可以确定下来，同样得分奇偶性：\n\n- 当数组总长度为偶数时，中位数就是分割线左边最大值与分割线右边最小值的平均数\n- 当是奇数时，中位数就是分割线左边的最大值，因此在数组长度为奇数时，中位数就是分割线左边的最大值。\n\n因为两个数组本别是有序数组，因此，我们只需要判定交叉的关系中，是否满足左边依然小于等于右边即可，即\n\n- 第1个数组分割线左边的第1个数小于等于第2个数组分割线右边的第一个数\n- 第2个数组分割线左边的第1个数小于等于第1个数组右边的第1个数\n\n通过不断缩减搜索区间确定分割线的位置\n\n- 当数组总长度为偶数时，左边一共有 $\\frac{len(nums1) + len(nums2)}{2}$ 个元素\n- 当数组总长度为奇数时，左边一共有 $\\frac{len(nums1) + len(nums2)}{2}+1$个元素\n\n奇数的时候是除以2向下取整，所以计算左边元素总数的时候就得 +1。也可以向上取整\n$$\n\\frac{len(nums1) + len(nums2) +1}{2}\n$$\n这里用到了一个小技巧，把下取整，修改为上取整的时候，只需要在被除数的部分，加上除数减 1 即可\n\n这样问题就转化为，我们在其中一个数组找到 $i$ 个元素，则另一个数组的元素个数就一定是 $\\frac{len(nums1) + len(nums2) +1}{2} - i$\n\n于是怎么找到 $i$ 是要解决的问题。\n\n找 i 个元素，我们通常的做法是找索引为 i的元素，因为下标是从 0 开始编号，因此编号为 i 的元素，就刚刚好前面有 i 个元素。因此，i 就是第 1 个数组分割线的右边的第 1 个元素。\n\n下面我们来看怎么找 i，需要分类讨论。\n\n情况1：如下图，此时分割线左边元素比右边多1，但是第一个数组分割线比右边第一个数6小于第二个数组分割线左边第一个数8，说明第一个数组左边的数少了，分割线要右移。\n\n![](https://i.loli.net/2021/11/01/mtGepBKROfFNsxH.png)\n\n情况 2：如下图所示，此时分割线左边的元素总数比右边多 1，但是第 一 个数组分割线左边第 1 个数 8 大于第 二 个数组分割线左边第 1 个数 7。说明，第 1 个数组左边的数多了，分割线要左移。\n\n![](https://i.loli.net/2021/11/01/TpRnHvheiKLZW1o.png)\n\n就是在这种不断缩小搜索范围的方法中，定位我们要找的 `i` 是多少。\n\n极端情况\n\n这里要注意一个问题，那就是我们要在一个短的数组上搜索 i 。在搜索的过程中，我们会比较分割线左边和右边的数，即 nums[i]、 nums[i - 1]、 nums[j]、 nums[j - 1]，因此 这几个数的下标不能越界。\n\n![](https://i.loli.net/2021/11/01/fAvgj7uar2Ksq8X.png)\n\n此时，分割线在第 2 个数组的左边没有值，会导致 nums2[j - 1] 的访问越界。因此我们必须在短的数组上搜索 i 。i 的定义是分割线的右边，而它的左边一定有值。这样就能保证，分割线在第 2 个数组的左右两边一定有元素，即分割线一定可以在第 2 个数组的中间切一刀。\n\n即使我在短数组上搜索边界 `i` ，还真就可能遇到 `i` 或者 `j` 的左边或者右边取不到元素的情况，它们一定出现在退出循环的时候。\n\n![](https://i.loli.net/2021/11/01/fMULmj9KbnyeWG2.png)\n\n最后，我们把关心的「边界线」两旁的 44 个数的极端情况都考虑一下：\n\n- 考虑nums1:\n- - 当 i=0 时，对应上图右边，此时数组 nums1 在红线左边为空，可以设置 num1_left_max = 负无穷，这样在最终比较的时候，因为左边粉红色部分要选择出最大值，它一定不会被选中\n  - 当 i=m 时，对应上图左边，此时数组 nums1 在红线右边为空，可设置 num1_right_min = 正无穷，这样在最终比较的时候，因为右边蓝色部分要选择出最小值，它一定不会被选中，于是能兼容其它情况。\n- 数组nums2 同理\n\n```java\nclass Solution {\n    public double findMedianSortedArrays(int[] nums1, int[] nums2) {\n        if(nums1.length > nums2.length){\n            int[] temp = nums1;\n            nums1 = nums2;\n            nums2 = temp;\n        }\n\n        int m = nums1.length;\n        int n = nums2.length;\n\n        // 分割线左边的所有元素需要满足的个数 m + (n-m+1)/2\n        int totalLeft = (m+n+1)/2;\n\n        // 在nums1的区间[0,m]里查找恰当的分割线\n        // 使得nums1[i-1] <= nums2[j] && nums2[j-1] <= nums1[j]\n        int left = 0;\n        int right = m;\n        while(left < right){\n            int i = left + (right - left +1) / 2;\n            int j = totalLeft - i;\n            if(nums1[i-1] > nums2[j]){\n                // 下一轮搜索 [left, i-1]\n                right = i - 1;\n            }else{\n                // 下一轮搜索 [i, right]\n                left = i;\n            }\n        }\n\n        int i = left;\n        int j = totalLeft - i;\n\n        int nums1LeftMax = i==0? Integer.MIN_VALUE:nums1[i-1];\n        int nums1RightMin = i==m? Integer.MAX_VALUE:nums1[i];\n        int nums2LeftMax = j == 0 ? Integer.MIN_VALUE : nums2[j - 1];\n        int nums2RightMin = j == n ? Integer.MAX_VALUE : nums2[j];\n\n        if( ((m+n) % 2) == 1 ){\n            return Math.max(nums1LeftMax, nums2LeftMax);\n        }else{\n            return (double)((Math.max(nums1LeftMax, nums2LeftMax) + Math.min(nums1RightMin, nums2RightMin))) /2;\n        }\n\n    }\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["LeetCode"]},{"title":"Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks","url":"/2021/10/18/Adapting-BERT-for-Continual-Learning-of-a-Sequence-of-Aspect-Sentiment-Classification-Tasks/","content":"\n# Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks\n\n增量学习ASC任务序列的CL系统应解决以下两个问题：\n\n- 将从以前的任务中学到的知识转移到新的任务中，帮助它学习更好的模型\n- 维护以前任务的模型性能，以便不会忘记它们\n\n针对这些问题，本文提出了一种新的基于胶囊网络的模型B-CL  (*BERT-based Continual Learning*) ，受《Parameter-efficient transfer learning for NLP》的Adapter Bert启发。\n\nB-CL通过前向和后向知识转移显著提高了ASC在新任务和旧任务上的效果。\n\nASC任务定义如下：给定一个方面(例如，相机评论中的图像质量)和在特定领域(例如，相机)中包含该方面的意义，分类句子对该方面表示正面、负面还是中性(无意见)。\n\n利用胶囊和动态路由 来识别与新任务相似的先前任务，并利用它们的共享知识来帮助新任务学习，并使用任务掩码来保护任务特定的知识，以避免遗忘(CF)。\n\n## Adapter-BERT\n\n一个 adapter 是具有残差连接的2层全连接网络。只有adapter(黄框)和layer norm(绿色框)层是可训练的。其他模块(灰色框)被冻结。提出的的B-CL，用CLA代替适配器。CLA有两个子模块：知识共享模块(KSM)和任务特定模块(TSM)\n\n![](https://i.loli.net/2021/10/18/7tVlqd2pLezUja4.png)\n\n在结束任务的训练期间，只训练适配器和规格化层，不改变任何其他BERT参数，这对CL是好的，因为微调BERT本身会导致严重的遗忘。\n\n## Capsule Network\n\n与CNN不同的是，CapsNet用矢量胶囊取代了标量特征检测器，可以保留图像中的位置和厚度等额外信息。典型的CapsNet有两层胶囊层。\n\n初级图层存储低级特征映射，类别层生成分类概率，每个胶囊对应一个类。它使用动态路由算法使每个较低级别的封装能够将其输出发送到类似的(或“agreed”，由点积计算的)较高级别封装。这是用来识别和分组相似任务及其共享功能或知识的关键属性。\n\n值得注意的是，所提出的B-CL不采用整个胶囊网络，因为只对胶囊层和动态路由感兴趣，而对最大边际损失和分类器不感兴趣。\n\n## Continual Learning Adapter (CLA)\n\nB-CL的目标是：\n\n- 通过知识共享实现相关旧任务与新任务之间的知识转移；\n\n- 通过防止新任务学习覆盖先前任务的特定任务知识来获得回避。\n\nCLA的体系结构如图所示\n\n- 知识共享模块(KSM)，用于从相似的先前任务和新任务中识别和利用可共享的知识\n- 任务特定模块(TSM)，用于学习任务特定神经元并保护它们不被新任务更新。\n\n![](https://z3.ax1x.com/2021/10/18/5UAeUS.png)\n\nCLA接受两个input：\n\n- 来自transformer层内部前馈层的隐藏状态 $h^{(t)}$\n- task ID $t$\n\noutput是隐藏状态，具有适合第t个任务的特征。\n\nKSM 利用胶囊层（见下文）和动态路由对相似的任务和可共享的知识进行分组，\n\n而 TSM 利用任务掩码 (TM) 来保护特定任务的神经元并让其他神经元自由。 这些自由的神经元稍后被TSM用于一项新的任务。由于TM是可微的，所以整个系统B-CL可以被端到端地训练。下面将详细介绍每个模块。\n\n### Knowledge Sharing Module (KSM)\n\nKSM 将相似的任务和共享的知识（特征）分组到它们之间，以实现相似任务之间的知识转移。 这是通过两个胶囊层（任务胶囊层和知识共享胶囊层）和胶囊网络的动态路由算法实现的。\n\n#### Task Capsule Layer (TCL)\n\nTCL中的每个胶囊代表一个任务，TCL准备从每个任务派生低级特征。因此，对于每个新任务，TCL都会添加一个胶囊。\n\n这种增量生长是有效且容易的，因为这些胶囊是离散的并且不共享参数。而且，每个胶囊只是一个具有少量参数的2层完全连接的网络。\n\n$h^{(t)} \\in R^{d_t\\times d_e}$ 为CLA的输入，$d_t$ 是tokens数量，$d_e$是维度。\n\n令到目前为止学习的任务集为 $T_{prev}$（在学习新任务 t 之前）和 $|T_{prev}|= n$\n\n在 TCL 中，我们有 n+1 个不同的胶囊代表所有过去的 n 个学习任务以及新任务 t。\n\n第 $i(i≤n+1)$ 个任务的封装为:\n$$\np_i^{(t)} = f_i (h^{(t)})\n$$\n其中 $f_i(\\cdot) = MLP_i(\\cdot)$ 代表为2层全连接层。\n\n#### Knowledge Sharing Capsule Layer (KCL)\n\nKCL 中的每个知识共享胶囊都捕获那些具有相似特征或共享知识的任务（即它们的任务胶囊 $\\{p_i^{(t)}\\}_1^{n+1}$ ）。\n\n这是通过动态路由算法自动实现的。 召回动态路由鼓励每个较低级别的胶囊（在案例中为任务胶囊）将其输出发送到类似（或“agreed”）的更高级别的胶囊（在我们的案例中为知识共享胶囊）。\n\n本质上，相似的任务胶囊（具有许多共享特征）通过较高的系数（决定任务胶囊可以进入下一层的程度）“聚集”在一起，而不同的任务（具有很少的共享特征）则通过低系数。\n\n这种聚类识别来自多个任务封装的共享特征或知识，并且有助于在相似任务之间向后转移。\n\nKCL首先将每个任务胶囊 $p_i^{(t)}$ 变成临时特征 $u_{j|i}^{(t)}$：\n$$\nu_{j|i}^{(t)} = W_{ij} p_i^{(t)}\n$$\n临时特征与权重 $c^{(t)}_{ij}$ 相加以获得知识共享胶囊 $s^{(t)}_j$ 中的初始值：\n$$\ns_j^{(t)} = \\sum_i c_{ij}^{(t)} u_{j|i}^{(t)}\n$$\n其中 $c_{ij}^{(t)}$ 是耦合系数加和为1。请注意，方程 1 中每个任务的任务胶囊映射到方程 3 中的知识共享胶囊，$c_{ij}^{(t)}$ 表示第 i 个任务的表示对第 j 个知识共享胶囊的信息量。\n\n因此，知识共享胶囊可以表示不同的可共享知识。这确保仅使用与新任务显着或相似的任务胶囊，而忽略（并因此保护）其他任务胶囊以学习更一般的可共享知识。\n\n在反向传播时，用较小梯度更新具有低 $c^{(t)}_{ij }$ 的相异任务，然而相似的任务有较高的$c^{(t)}_{ij }$被更新较大的梯度。这鼓励在类似任务之间向后转移。\n\nDynamic Routing\n\n$c_{ij}^{(t )}$ 是由“Routing Softmax”计算的：\n$$\nc_{ij}^{(t)} = \\frac{exp(b_{ij}^{(t)})}{ \\sum_o exp(b_{io}^{(t)})}\n$$\n其中每个 $b_{ij}$ 是对数先验概率，显示任务胶囊 $i$ 与知识共享胶囊 $j$ 的显著性或相似性。\n\n它被初始化为0，表示它们之间在开始时没有显著联系。应用动态路由算法来更新 $b_{ij}$：\n$$\nb_{ij}^{(t)} \\leftarrow b_{ij}^{(t)} + a_{ij}^{(t)}\n$$\n其中 $a_{ij}$ 是协议系数，直观上，这一步倾向于聚合知识共享胶囊上的相似（或“agreed”）任务，具有更高的一致性系数 $a_{ij}^{(t)}$，因此具有更高的 logit $b^{(t)}_{ij}$ 或耦合系数 $c_{ij}^{(t )}$ 。协议系数的计算公式为 :\n$$\na_{ij}^{(t)} = u_{j|i} ^{(t)} \\cdot v_{j}^{(t)}\n$$\n其中 $v_{j}^{(t)}$ 是规范化的表示形式，通过非线性 squash\n$$\nv_{j}^{(t)} = \\frac{ ||s_j^{(t)}||^2 }{1+||s^{(t)}_j||} \\frac{s_j^{(t)}}{||s_j^{(t)}||}\n$$\n对于第一个任务 $s_j^{(t)} =  u_{j|i}^{(t)} $ , 其中 $v^{(t)}_{j}$ 归一化为[0，1]到表示知识共享胶囊 j 的激活概率。\n\n### Task Specific Module (TSM)\n\n虽然知识共享对ASC很重要，但为以前的任务保存特定于任务的知识以防止遗忘(CF)也同样重要。\n\n为此，使用任务掩码。具体地说，首先检测每个旧任务使用的神经元，然后在学习新任务时关闭或屏蔽所有使用过的神经元。\n\n![](https://z3.ax1x.com/2021/10/18/5UY0HO.png)\n\n每项任务的两行对应于TSM中的 $k^{(t)}_0$ 和 $k^{(t)}_1$。在训练前的细胞中，0的细胞是需要保护(掩蔽)的神经元，那些没有编号的细胞是游离的神经元(未使用)。\n\n在学习第一个任务（任务 0）后，获得了用橙色标记的有用神经元，每个神经元中都标有 1，作为学习未来任务的掩码。 在学习任务 1 中，那些对任务 0 有用的神经元被屏蔽（左侧的橙色神经元或细胞中的 0 为 0）。 该过程还学习了任务 1 的有用神经元，用 1 标记为绿色。 当任务 2 到达时，任务 0 和 1 的所有重要神经元都被屏蔽，即其掩码条目设置为 0（训练前的橙色和绿色）。 在训练任务 2 之后，我们看到任务 2 和任务 1 有一个对它们都很重要的共享神经元。 共享神经元以红色和绿色标记。\n\n在训练后的细胞中，那些带有1的细胞显示了对当前任务重要的神经元，这些神经元被用作未来的mask。具有一种以上颜色的单元格表示它们由多个任务共享。这0个没有颜色的单元格不会被任何任务使用。\n\n### Task Masks\n\n给定知识共享胶囊 $s^{(t)}_j$，TSM 通过全连接的网络将它们映射到输入 $k_l^{(t)} $，其中 $l$ 是 TSM 中的第 $l$ 层。\n\n在训练任务的分类器期间，为TSM中每个层 $l$ 的每个任务 $t$ 训练任务掩码（“软”二元掩码）$m_l^{(t)}$，指示该层中对任务重要的神经元。\n\n在这里借用了hard attention想法，并利用任务ID embedding来训练任务掩码。\n\n对于task ID t，其嵌入 $e^{(t)}_l$ 由网络的其他部分一起学习的可微确定性参数组成。\n$$\nm_l^{(t)} = \\sigma(se_l^{(t)})\n$$\n给定 TSM 中每一层的输出 $k^{(t)}_l$，按元素相乘 $k^{(t)}_l ⊗ m_l^{(t)}$ 。最后一层 $k^{(t)}$ 的屏蔽输出通过跳跃连接馈送到下一层BERT。\n\n在学习任务 $t$ 之后，保存最终 $m^{(t)}_l$ 并将其添加到集合 $\\{m^{(t)}_l\\}$。\n\n### Training\n\n对于每个过去的任务 $i_{prev} \\in T_{prev}$，其掩码 $m^{(i_{prev})}_l$ 指示该任务使用的需要保护的神经元。在学习任务 t 中，$m_l^{(i_{prev})}$ 用于将 TSM 中第 $l$ 层所有使用的神经元上的梯度 $g_l^{(t)}$ 设置为 0。\n\n在修改梯度之前，我们首先通过所有先前任务的掩码，累积所有使用的神经元。 由于 $m_l^{(iprev)}$ 是二进制的，我们使用最大池化来实现累计：\n$$\nm_l^{(t_{ac})} = MaxPool(\\{m_l^{i_{prev}}\\})\n$$\n$m_l^{(t_{ac})}$ 被应用于梯度：\n$$\ng_l^{(t)} = g_l^{(t)} \\otimes (1- m_l^{(t_{ac})})\n$$\n对应于 $m^{(t_{ac})}_l$ 中的 1 个条目的那些梯度设置为 0，而其他保持不变。 通过这种方式，旧任务中的神经元受到保护。 请注意，我们扩展（复制）向量  $m^{(t_{ac})}_l$  以匹配 $g^{(t)}_l$ 的维度。\n\n虽然这个想法是直观的，但 $e^{(t)}_l $ 并不容易训练。为了使 $e^{(t)}_l $  的学习更容易和更稳定，应用了退火策略。也就是说，s 在训练期间退火，引入梯度流，并在测试期间设置 $s=s_{max}$。\n\n往上三个等式 将单位阶跃函数近似为掩码，当 s → ∞ 时，$m^{(t)}_l \\to \\{0,1\\}$。一个训练 epoch 开始时，所有神经元都是同等活跃的，在这个 epoch 内逐渐极化。\n\n具体地说，s 按如下方式退火：\n$$\ns = \\frac{1}{s_{max}} + (s_{max} - \\frac{1}{s_{max}}) \\frac{b-1}{B-1}\n$$\n其中 b 是批次索引，B 是epoch中的批次总数。\n\n## 实验\n\n\n\n### 数据集\n\n![](https://z3.ax1x.com/2021/10/18/5UOc7R.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"只用一行代码可以提高模型表现吗？","url":"/2021/10/10/只用一行代码可以提高模型表现吗？/","content":"\n# 只用一行代码能提高模型表现吗?\n\n一行代码能做什么，有的人能发顶会，而有的人...\n\n相信大家在训练模型的时候都会遇到一个现象，训练集损失降到一定的值之后，验证集的损失就开始上升了，在实验中一般奇怪的是准确率还跟着上升。这是为什么？如下图所示：\n\n![](https://z3.ax1x.com/2021/10/09/5kiiNV.png)\n\n先看图(a)，是一个正常的训练过程，对于阶段A，随着training loss的降低，test loss也会 跟着降低;\n\n但是到阶段B后，我们继续在训练集上训练，会让test loss上升。我们通常认为这是过拟合了，因为泛化误差变大了。\n\n图 (b) 是ICML2020上《Do We Need Zero Training Loss After Achieving Zero Training Error》提出的flooding方法。这是一种使训练损失在一个小常量附近浮动的方法，以防止训练损失趋近于零 (这也是flooding的约束假设)。\n\n为什么要防止训练损失趋近于0呢？\n\n如果我们在模型已经记住了训练数据，完全没有错误的情况下仍继续训练，训练损失可以很容易地变得(接近)零，特别是对于过度参数化的模型。我们的模型其实就是个函数拟合器，在训练集上拟合的太好就容易发生过拟合。\n\n经过推导（下文），flooding其实也和正则化的一些方法一样，通过各种方式避免训练过多。正则化方法可以被认为是间接控制训练损失的方法，通过引入额外的约束假设。\n\n这里科普一下花书对于正则化的官方定义：\n\n> 凡是可以减少泛化误差(过拟合) 而不是减少训练误差的方法——正则化方法。\n\n其实对抗训练从理论上也是一种正则化方法，而正则化其实也可以理解成我们在求解最优化问题中的约束条件。我们通常希望将模型约束到一个较为”平坦“的损失，能够使得模型鲁棒性、泛化性更好。\n\n从svm的角度来思考这个问题。对于一个线性可分的二分类问题，有无数条分类面能将其分开，而svm是去挑选能满足“最大间隔”的分类器。从另一个角度来理解是，越平坦的损失，是不是能越尽可能的将不同类给分开，因为样本进行些许扰动，损失的变化不会太大，相当于进行细微扰动后的样本也不会被分类到另一类去。\n\n## flooding 方法分析\n\n论文其实就一行代码：\n\n```python\nlogits = model(x)\nloss = criterion(logits, y)\nloss = (loss - b).abs() + b # This is it!\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n```\n\n泛洪是直接解决训练损失变为(接近)零的问题。当训练损失达到合理的小值时，泛洪故意阻止训练损失的进一步减少。\n\n设原来的损失函数为 $\\mathcal{L}(\\theta)$ ，改为 $\\tilde{\\mathcal{L}}(\\theta)$:\n$$\n\\tilde{\\mathcal{L}}(\\theta) =  |\\mathcal{L}(\\theta) - b| + b\n$$\n其中 b 是超参数阈值\n\n当 $\\mathcal{L}(\\theta) > b$ 时， $ \\tilde{\\mathcal{L}}(\\theta) =\\mathcal{L}(\\theta)$, 这个时候和正常他梯度下降无异；\n\n当$\\mathcal{L}(\\theta) <b$ 时， $ \\tilde{\\mathcal{L}}= 2b - \\mathcal{L}(\\theta)$ 变成了梯度上升了。\n\n> 当training loss大于一个阈值（flood level）时，进行正常的梯度下降；当training loss低于阈值时，会反过来进行梯度上升，让training loss保持在一个阈值附近，让模型持续进行“random walk”，并期望模型能被优化到一个平坦的损失区域，这样发现test loss进行了double decent！一个简单的理解是，这和early stop类的方法类似，防止参数被优化到一个不好的极小值出不来。\n\n这里借用 [我们真的需要把训练集的损失降低到零吗？](https://kexue.fm/archives/7643) 的推导\n\n当损失函数达到 b 之后，训练流程大概就是在交替执行梯度下降和梯度上升。直观想的话，感觉一步上升一步下降，似乎刚好抵消了。事实真的如此吗？我们来算一下看看。假设先下降一步后上升一步，学习率为 $\\epsilon$，那么：\n$$\n\\theta_n = \\theta_{n-1} - \\epsilon g(\\theta_{n-1}) \\ ,\\ \\ \\theta_{n+1} = \\theta_n + \\epsilon g(\\theta_n) \n$$\n其中 $g(\\theta) = \\nabla_{\\theta} \\mathcal{L}(\\theta)$ , 现在有：\n$$\n\\begin{equation}\\begin{aligned}\\theta_{n+1} =&\\, \\theta_{n-1} - \\varepsilon g(\\theta_{n-1}) + \\varepsilon g\\big(\\theta_{n-1} - \\varepsilon g(\\theta_{n-1})\\big)\\\\ \n\\approx&\\,\\theta_{n-1} - \\varepsilon g(\\theta_{n-1}) + \\varepsilon \\big(g(\\theta_{n-1}) - \\varepsilon \\nabla_{\\theta} g(\\theta_{n-1}) g(\\theta_{n-1})\\big)\\\\ \n=&\\,\\theta_{n-1} - \\frac{\\varepsilon^2}{2}\\nabla_{\\theta}\\Vert g(\\theta_{n-1})\\Vert^2 \n\\end{aligned}\\end{equation}\n$$\n近似那一步是使用了泰勒展式对损失函数进行近似展开，最终的结果就是相当于损失函数为梯度惩罚 $\\Vert g(\\theta)\\Vert^2=\\Vert\\nabla_{\\theta}\\mathcal{L}(\\theta)\\Vert^2$、学习率为 $\\frac{\\varepsilon^2}{2}$ 的梯度下降。更妙的是，改为“先上升再下降”，其表达式依然是一样的（这不禁让我想起“先升价10%再降价10%”和“先降价10%再升价10%”的故事）。因此，平均而言，Flooding对损失函数的改动，相当于在保证了损失函数足够小之后去最小化 $\\Vert\\nabla_{\\theta}\\mathcal{L}(\\theta)\\Vert^2$，也就是推动参数往更平稳的区域走，这通常能提供提高泛化性能（更好地抵抗扰动），因此一定程度上就能解释Flooding其作用的原因了。\n\n本质上来讲，这跟往参数里边加入随机扰动、对抗训练等也没什么差别，只不过这里是保证了损失足够小后再加扰动。读者可以参考[《泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练》](https://kexue.fm/archives/7466)了解相关内容，也可以参考“圣经”《深度学习》第二部分第七章的“正则化”一节。\n\n## 关于 b 的选择\n\n b 的选择，原论文说 b 的选择是一个暴力迭代的过程，需要多次尝试\n\n>  The flood level is chosen from $b\\in \\{0, 0.01,0.02,...,0.50\\}$\n\n脑洞：b 无非就是决定什么时候开始交替训练罢了，那如果我们从一开始就用不同的学习率进行交替训练呢？也就是自始自终都执行\n$$\n\\begin{equation}\\begin{aligned}&\\theta_n = \\theta_{n-1} - \\varepsilon_1 g(\\theta_{n-1})\\\\ \n&\\theta_{n+1} = \\theta_n + \\varepsilon_2 g(\\theta_n) \n\\end{aligned}\\end{equation}\n$$\n其中 $\\varepsilon_1> \\varepsilon_2$，这样我们就把 b 去掉了（引入了 $ \\varepsilon_1, \\varepsilon_2$ 的选择，天下没有免费的午餐）。重复上述近似展开，我们就得到\n$$\n\\begin{equation}\\begin{aligned} \n\\theta_{n+1} =& \\, \\theta_{n-1} - \\varepsilon_1g(\\theta_{n-1})+\\varepsilon_2g(\\theta_{n-1} - \\varepsilon_1g(\\theta_{n-1}))\\\\\n\\approx&\\, \\theta_{n-1} - \\varepsilon_1g(\\theta_{n-1}) + \\varepsilon_2(g(\\theta_{n-1}) - \\varepsilon_1\\nabla_\\theta g(\\theta_{n-1})g(\\theta_{n-1}))\\\\\n=&\\, \\theta_{n-1} - (\\varepsilon_1 - \\varepsilon_2) g(\\theta_{n-1}) - \\frac{\\varepsilon_1\\varepsilon_2}{2}\\nabla_{\\theta}\\Vert g(\\theta_{n-1})\\Vert^2\\\\ \n=&\\,\\theta_{n-1} - (\\varepsilon_1 - \\varepsilon_2)\\nabla_{\\theta}\\left[\\mathcal{L}(\\theta_{n-1}) + \\frac{\\varepsilon_1\\varepsilon_2}{2(\\varepsilon_1 - \\varepsilon_2)}\\Vert \\nabla_{\\theta}\\mathcal{L}(\\theta_{n-1})\\Vert^2\\right] \n\\end{aligned}\\end{equation}\n$$\n这就相当于自始自终都在用学习率 $\\varepsilon_1-\\varepsilon_2$ 来优化损失函数 $\\mathcal {L}(\\theta) + \\frac {\\varepsilon_1\\varepsilon_2}{2 (\\varepsilon_1 - \\varepsilon_2)}\\Vert\\nabla_{\\theta}\\mathcal {L}(\\theta)\\Vert^2$ 也就是说一开始就把梯度惩罚给加了进去，这样能提升模型的泛化性能吗？[《Backstitch: Counteracting Finite-sample Bias via Negative Steps》](http://www.danielpovey.com/files/2017_interspeech_backstitch.pdf)里边指出这种做法在语音识别上是有效的，请读者自行测试甄别。\n\n这种做法在这篇博客上做了尝试，可能验证loss会降的更低一点，但具体得分情况还得自己尝试。[我们真的需要把训练集的损失降到零吗？](https://wmathor.com/index.php/archives/1551/)\n\n## 实验测试\n\n在第五届达观杯竞赛中使用的BERT模型，进行了实验。原论文的实验配合Eearly Stop 和 Weight decay 一起使用效果较好。重要的要花时间去调的是b的取值，初始的b值一般设为 验证集loss开始上扬的值的一半。\n\n在我的实验中发现，在预训练后的bert模型加上dice loss之后，验证集loss上扬的情况就不存在了。但是预训练后的bert加上cross entropy还是会上扬。而未经过预训练的bert无论是在dice loss还是cross entropy上都会上扬。分析背后的原因可能有二：\n\n- 预训练后的bert模型表现更加稳定，对数据有一定的认识。\n- cross entropy对每个样本都一视同仁，不管当前样本是简单还是复杂。当简单样本有很多时，模型训练就会被这些简单的样本占据，使得模型难以从复杂样本中学习，而dice loss一旦模型正确分类当前样本（刚刚过0.5），就会使模型更少关注它，而不是像交叉熵那样，鼓励模型迫近0或1这两个点。这就能有效避免模型训练受到简单样本的支配，同时也防止了过拟合。\n\n### 无flooding的情况下\n\n预训练后的bert+dice loss 的情况如下图所示。\n\n![](https://z3.ax1x.com/2021/10/10/5Ah6FP.png)\n\n预训练后的bert + cross entropy，依旧上扬但相比下一个图未经预训练bert的情况要好一些。\n\n![](https://z3.ax1x.com/2021/10/10/5ETvN9.png)\n\n未经预训练的bert+cross entropy\n\n![](https://z3.ax1x.com/2021/10/10/5EIsUA.png)\n\n\n\n\n\n### 经过flooding后\n\nNo_pretrain bert/ cross entropy/  flooding b=0.5/ weight decy=0.01/ early patience=12 相比于上图验证集的loss已经不在无止境的上扬了。在下图的15到20step之间train的loss也不是和上图一样一路走低，而是出现了波动，这和论文的预期一致。\n\n![](https://z3.ax1x.com/2021/10/10/5ECkIP.png)\n\n\n\n未经预训练后的bert b=0.5  dice loss\n\n![](https://z3.ax1x.com/2021/10/10/5EIvb4.png)\n\n未经预训练后的bert，b=1.0 ,cross entropy\n\n![](https://i.loli.net/2021/10/10/DfcyQP3n5oqFLze.png)\n\n\n\n## 结论\n\nflooding确实可以缓解验证集损失上扬的现象，而且本质还是个正则化的功能。至于具体效果有多大，是好是坏还是要根据具体任务去调试b的取值。\n\n不过我的实验可以证明  预训练后的bert 和 dice loss 确实是可以让模型避免出现类似过拟合的现象。\n\n\n\n## References\n\n[我们真的需要把训练集的损失降到零吗？](https://wmathor.com/index.php/archives/1551/)\n\n[【论文】一行代码发一篇ICML？](https://zhuanlan.zhihu.com/p/163676138)\n\n[我们真的需要把训练集的损失降低到零吗？](https://kexue.fm/archives/7643)\n","tags":["ML&DL"]},{"title":"79/130/200/733FloodFill/17/22/784字符串回溯","url":"/2021/10/10/79-130-200-733FloodFill-17-22-784字符串回溯/","content":"\n# 79/130/200/733FloodFill/17/22/784字符串回溯\n\n## Flood Fill\n提示：Flood 是「洪水」的意思，Flood Fill 直译是「泛洪填充」的意思，体现了洪水能够从一点开始，迅速填满当前位置附近的地势低的区域。类似的应用还有：PS 软件中的「点一下把这一片区域的颜色都替换掉」，扫雷游戏「点一下打开一大片没有雷的区域」。\n\n下面这几个问题，思想不难，但是初学的时候代码很不容易写对，并且也很难调试。我们的建议是多写几遍，忘记了就再写一次，参考规范的编写实现（设置 visited 数组，设置方向数组，抽取私有方法），把代码写对。\n\n### [79. 单词搜索](https://leetcode-cn.com/problems/word-search/)\n\n```java\nclass Solution {\n    private static final int[][] DIRECTIONS = {{-1,0}, {0, -1}, {0,1}, {1,0}};\n    private int rows;\n    private int cols;\n    private int len;\n    private boolean[][] visited;\n    private char[] charArray;\n    private char[][] board;\n    public boolean exist(char[][] board, String word) {\n        rows = board.length;\n        if (rows==0){\n            return false;\n        }\n        cols = board[0].length;\n        visited = new boolean[rows][cols];\n\n        this.len = word.length();\n        this.charArray = word.toCharArray();\n        this.board = board;\n\n        for(int i=0;i<rows;i++){\n            for(int j=0;j<cols;j++){\n                if(dfs(i, j, 0)){\n                    return true;\n                }\n            }\n        }\n        return false;\n\n    }\n\n    private boolean inArea(int x,int y){\n        return x>=0 && x<rows && y>=0 && y<cols;\n    }\n\n    private boolean dfs(int x, int y, int begin){\n        if(begin == len-1){\n            return board[x][y] == charArray[begin];\n        }\n        if(board[x][y] == charArray[begin]){\n            visited[x][y] = true;\n            for(int[] direction:DIRECTIONS){\n                int newX = x + direction[0];\n                int newY = y + direction[1];\n                if(inArea(newX,newY) && !visited[newX][newY]){\n                    if(dfs(newX, newY, begin+1)){\n                        return true;\n                    }\n                }\n            }\n            visited[x][y] = false;\n        }\n        return false;\n    }\n}\n```\n\n说明：\n\n偏移量数组在二维平面内是经常使用的，可以把它的设置当做一个技巧，并且在这个问题中，偏移量数组内的 4 个偏移的顺序无关紧要；\n说明：类似使用这个技巧的问题还有：「力扣」第 130 题：被围绕的区域、「力扣」第 200 题：岛屿数量。\n\n对于这种搜索算法，我认为理解 DFS 和状态重置并不难，代码编写也相对固定，难在代码的编写和细节的处理，建议多次编写，自己多总结多思考，把自己遇到的坑记下。\n\n\n\n## [130. 被围绕的区域](https://leetcode-cn.com/problems/surrounded-regions/)\n\n#### 法一：深度优先遍历\n\n关键：与边界相连 $O$ 不能被替换成 $X$\n\n具体步骤：\n\n- 第一步：把四周有 $O$ 的地方都替换成 $-$，在四周进行 floodfill 算法（染色）\n- 第二步：再从头到尾遍历一遍，把$O$ 换成 $X$，把 $-$ 换成 $O$\n\n```java\nclass Solution {\n\n    private static final int[][] DIRECTIONS = {{-1,0}, {0, -1}, {0, 1}, {1,0}};\n    private int rows;\n    private int cols;\n    public void solve(char[][] board) {\n        rows = board.length;\n        if(rows==0) return;\n        cols = board[0].length;\n        if(cols==0) return;\n\n        // 第一步：把四周的 0 以及与 0 连通的 0 都设置成 -\n        // 第一列和最后一列\n        for(int i=0; i<rows; i++){\n            if(board[i][0] == 'O'){\n                dfs(i, 0, board);\n            }\n            if(board[i][cols-1] == 'O'){\n                dfs(i, cols-1, board);\n            }\n        }\n        // 第一行和最后一行\n        for(int i=0;i<cols;i++){\n            if(board[0][i] == 'O'){\n                dfs(0, i, board);\n            }\n            if(board[rows-1][i] == 'O'){\n                dfs(rows-1, i, board);\n            }\n        }\n        // 第二行：遍历一次棋盘\n        // 1. 剩下的O就是被包围的O\n        // 2. - 是原来不能被包围的O，恢复成O\n        for(int i=0; i<rows;i++){\n            for(int j=0;j<cols; j++){\n                if(board[i][j] == 'O'){\n                    board[i][j] = 'X';\n                } else if(board[i][j] == '-'){\n                    board[i][j] = 'O';\n                }\n            }\n        }\n\n    }\n\n    private boolean inArea(int x, int y, int rows, int cols){\n        return x>=0 && x<rows && y>=0 && y<cols;\n    }\n\n    private void dfs(int i, int j, char[][] board){\n        if(inArea(i,j,rows,cols) && board[i][j]=='O'){\n            board[i][j] = '-';\n            for (int k=0; k<4; k++){\n                int newX = i + DIRECTIONS[k][0];\n                int newY = j + DIRECTIONS[k][1];\n                dfs(newX, newY, board);\n            }\n        }\n        \n    }\n}\n```\n\n- 时间复杂度：$O(rows * cols)$ ，其中 rows 和 cols 分别为矩阵的行数和列数， 深度优先遍历过程中，每一个单元格至多只会被标记一次\n- 空间复杂度：$O(rows * cols)$ ，深度优先遍历最多使用的栈的开销为整个棋盘大小\n\n### 法二：广度优先遍历\n\n```java\nclass Solution {\n\n    private static final int[][] DIRECTIONS = {{-1,0}, {0, -1}, {0, 1}, {1,0}};\n    private int rows;\n    private int cols;\n    public void solve(char[][] board) {\n        rows = board.length;\n        if(rows==0) return;\n        cols = board[0].length;\n        if(cols==0) return;\n\n        // 第一步：把四周的‘O’ 全部推入队列，通过广度优先遍历，把’O‘连通的地方全部编辑\n        Queue<int []> queue = new LinkedList<>();\n        for(int i=0; i<rows; i++){\n            if(board[i][0] == 'O'){\n                queue.offer(new int[]{i, 0});\n            }\n            if(board[i][cols-1] == 'O'){\n                queue.offer(new int[]{i, cols-1});\n            }\n        }\n        for(int i=0; i<cols; i++){\n            if(board[0][i] == 'O'){\n                queue.offer(new int[]{0, i});\n            }\n            if(board[rows-1][i] == 'O'){\n                queue.offer(new int[]{rows-1, i});\n            }\n        }\n        while(!queue.isEmpty()){\n            int[] top = queue.poll();\n            int i = top[0];\n            int j = top[1];\n            board[i][j] = '-';\n            for(int[] direction : DIRECTIONS){\n                int newX = i + direction[0];\n                int newY = j + direction[1];\n                if(inArea(newX, newY, rows, cols) && board[newX][newY]=='O'){\n                    queue.offer(new int[]{newX, newY});\n                }\n            }\n        }\n        // 第 2 步：恢复\n        for (int i = 0; i < rows; i++) {\n            for (int j = 0; j < cols; j++) {\n                if (board[i][j] == '-') {\n                    board[i][j] = 'O';\n                } else if (board[i][j] == 'O') {\n                    board[i][j] = 'X';\n                }\n            }\n        }\n    }\n     private boolean inArea(int x, int y, int rows, int cols) {\n        return x >= 0 && x < rows && y >= 0 && y < cols;\n    }\n\n}\n```\n\n### 法三：并查集\n\n- 把四周的 $O$ 都和一个虚拟节点合并起来\n- 在内部，只看两个方向，把 $O$ 都合并起来\n- 最后再扫一次数组，不和 虚拟节点 链接的 $O$都标记成 $X$\n\n并查集的写法容易受到 floorfill的影响， 用并查集的时候，其实只用每一行的右边和下面都看一下，只针对 $O$， 能合并就合并一下。\n\n```java\nclass Solution {\n\n    class UnionFind{\n        private int[] parent;\n\n        public UnionFind(int n){\n            this.parent = new int[n];\n            for(int i=0; i<n; i++){\n                parent[i] = i;\n            }\n        }\n\n        public boolean isConnected(int x, int y){\n            return find(x) == find(y);\n        }\n\n        public int find(int x){\n            while(x != parent[x]){\n                parent[x] = parent[parent[x]];\n                x = parent[x];\n            }\n            return x;\n        }\n\n        public void union(int x,int y){\n            int xRoot = find(x);\n            int yRoot = find(y);\n            if(xRoot == yRoot){\n                return;\n            }\n            parent[xRoot] = yRoot;\n        }\n    }\n\n    private int getIndex(int x, int y, int cols){\n        return x*cols +y;\n    }\n\n    public void solve(char[][] board) {\n        int rows = board.length;\n        if(rows == 0){\n            return;\n        }\n        int cols = board[0].length;\n        if(cols == 0){\n            return;\n        }\n\n        UnionFind unionFind = new UnionFind(rows * cols + 1);\n        int dummyNode = rows*cols;\n        // 填写第一行和最后一行\n        for(int j=0; j< cols; j++){\n            if(board[0][j] == 'O'){\n                unionFind.union(getIndex(0, j, cols), dummyNode);\n            }\n            if(board[rows-1][j] == 'O'){\n                unionFind.union(getIndex(rows-1, j ,cols), dummyNode);\n            }\n        }\n        // 填写第 1 列和最后一列\n        for (int i = 1; i < rows - 1; i++) {\n            if (board[i][0] == 'O') {\n                unionFind.union(getIndex(i, 0, cols), dummyNode);\n            }\n            if (board[i][cols - 1] == 'O') {\n                unionFind.union(getIndex(i, cols - 1, cols), dummyNode);\n            }\n        }\n\n        int[][] directions = new int[][]{{0,1},{1,0}};\n        for(int i=0;i<rows; i++){\n            for(int j=0;j<cols;j++){\n                if(board[i][j] == 'O'){\n                    for(int[] direction : directions){\n                        int newX = i + direction[0];\n                        int newY = j + direction[1];\n                        if(newX < rows && newY<cols && board[newX][newY] =='O'){\n                            unionFind.union(getIndex(i, j, cols), getIndex(newX, newY, cols));\n                        }\n                    }\n                }\n            }\n        }\n\n        for (int i = 1; i < rows - 1; i++) {\n            for (int j = 0; j < cols - 1; j++) {\n                if (board[i][j] == 'O') {\n                    if (!unionFind.isConnected(getIndex(i, j, cols), dummyNode)) {\n                        board[i][j] = 'X';\n                    }\n                }\n            }\n        }\n\n\n    }\n}\n```\n\n\n\n## [200. 岛屿数量](https://leetcode-cn.com/problems/number-of-islands/)\n\nFlood法：从一个区域中提取若干个连通的点与其他相邻区域分开。\n\n从一个点扩散开，找到与其连通的点，其实就是从一个点卡死，进行一次深度优先或广度优先遍历，发现一片连着的区域。把与之相连的所有的格子都标记上，视为发现了一个「岛屿」。\n\n深度优先：\n\n```java\nclass Solution {\n    private boolean[][] visited;\n    private int rows;\n    private int cols;\n    private static final int[][] DIRECTIONS = {{-1,0}, {0,-1}, {1,0}, {0,1}};\n    private char[][] grid;\n    public int numIslands(char[][] grid) {\n        rows = grid.length;\n        if(rows==0) return 0;\n        cols = grid[0].length;\n        if(cols==0) return 0;\n\n        visited = new boolean[rows][cols];\n        this.grid = grid;\n\n        int count = 0;\n        for (int i = 0; i < rows; i++) {\n            for (int j = 0; j < cols; j++) {\n                // 如果是岛屿中的一个点，并且没有被访问过，就进行深度优先遍历\n                if (!visited[i][j] && grid[i][j] == '1') {\n                    dfs(i, j);\n                    count++;\n                }\n            }\n        }\n        return count;\n    }\n\n    private boolean inArea(int x, int y){\n        return x>=0 && x<rows && y>=0 && y<cols;\n    }\n\n    private void dfs(int i, int j){\n        visited[i][j] = true;\n        for(int k=0;k<4;k++){\n            int newX = i + DIRECTIONS[k][0];\n            int newY = j + DIRECTIONS[k][1];\n            // 如果不越界，还是陆地，没有被访问过\n            if (inArea(newX, newY) && grid[newX][newY] == '1' && !visited[newX][newY]) {\n                dfs(newX, newY);\n            }\n\n        }\n    }\n}\n```\n\n广度优先：\n\n```java\nimport java.util.LinkedList;\nimport java.util.Queue;\n\npublic class Solution {\n\n    private final static int[][] DIRECTIONS = {{-1, 0}, {0, -1}, {1, 0}, {0, 1}};\n    private int rows;\n    private int cols;\n    private char[][] grid;\n    private boolean[][] visited;\n\n    public int numIslands(char[][] grid) {\n        rows = grid.length;\n        if (rows == 0) {\n            return 0;\n        }\n        cols = grid[0].length;\n        this.grid = grid;\n        visited = new boolean[rows][cols];\n\n        int count = 0;\n        for (int i = 0; i < rows; i++) {\n            for (int j = 0; j < cols; j++) {\n                if (!visited[i][j] && grid[i][j] == '1') {\n                    bfs(i, j);\n                    count++;\n                }\n            }\n        }\n        return count;\n    }\n\n    private void bfs(int i, int j) {\n        Queue<Integer> queue = new LinkedList<>();\n        queue.offer(i * cols + j);\n        // 注意：这里要标记上已经访问过\n        visited[i][j] = true;\n        while (!queue.isEmpty()) {\n            int cur = queue.poll();\n            int curX = cur / cols;\n            int curY = cur % cols;\n            for (int k = 0; k < 4; k++) {\n                int newX = curX + DIRECTIONS[k][0];\n                int newY = curY + DIRECTIONS[k][1];\n                if (inArea(newX, newY) && grid[newX][newY] == '1' && !visited[newX][newY]) {\n                    queue.offer(newX * cols + newY);\n                    \n                    visited[newX][newY] = true;\n                }\n            }\n        }\n    }\n\n    private boolean inArea(int x, int y) {\n        return x >= 0 && x < rows && y >= 0 && y < cols;\n    }\n}\n```\n\n并查集\n\n关于连通性问题，并查集也是常用的数据结构\n\n思路：并查集中维护连通分量的个数，在遍历的过程中：\n\n- 相邻的陆地（只需要向右看和向下看）合并，只要发生过合并，岛屿数量就减一\n- 在遍历过程中，同时记录空地的数量\n- 并查集中连通分量的个数 - 空地的个数 = 岛屿数\n\n## [733. 图像渲染](https://leetcode-cn.com/problems/flood-fill/)\n\n```java\nclass Solution {\n\n    int m,n;\n    private int[][] directions = {{1,0},{0,1},{-1,0},{0,-1}};\n    public int[][] floodFill(int[][] image, int sr, int sc, int newColor) {\n        m = image.length;\n        n = image[0].length;\n\n        int currColor = image[sr][sc];\n\n        if(currColor!=newColor){\n            dfs(sr, sc,currColor, newColor, image); \n        }\n\n        return image;\n    }\n\n    private boolean inArea(int x, int y){\n        return x>=0 && x<m && y>=0 && y<n;\n    }\n\n    private void dfs(int sr, int sc, int color, int newColor, int[][] image){\n        if(image[sr][sc] == color){\n            image[sr][sc] = newColor;\n            for(int[] direct : directions){\n                int newx = sr + direct[0];\n                int newy = sc + direct[1];\n                if(inArea(newx, newy)){\n                    dfs(newx,newy, color, newColor, image);\n                }\n            }\n        }\n    }\n\n}\n```\n\n## [17. 电话号码的字母组合](https://leetcode-cn.com/problems/letter-combinations-of-a-phone-number/)\n\n- 由于字符追加到后面，是新创建一个对象，因此 **没有显式回溯（状态重置）的过程 **；\n\n```java\nclass Solution {\n\n    public List<String> letterCombinations(String digits) {\n        String[] digitsMap =  {\"abc\",\"def\",\"ghi\",\"jkl\",\"mno\",\"pqrs\",\"tuv\",\"wxyz\"};\n        List<String> res = new ArrayList<>();\n        if(digits.length() == 0){\n            return res;\n        }\n\n        findCombinations(digits, digitsMap, 0, \"\", res);\n        return res;\n    }\n\n    private void findCombinations(String digits, String[] digitsMap, int start, String pre, List<String> res){\n        if(start == digits.length()){\n            res.add(pre);\n            return;\n        }\n\n        String nextStr = digitsMap[digits.charAt(start) - '2'];\n        int len = nextStr.length();\n        for(int i=0; i<len; i++){\n            findCombinations(digits, digitsMap, start+1, pre+nextStr.charAt(i), res);\n        }\n    }\n\n\n}\n```\n\n## [784. 字母大小写全排列](https://leetcode-cn.com/problems/letter-case-permutation/)\n\n![](https://i.loli.net/2021/10/21/bfzBmC8V7c3gFhO.png)\n\n大小写转换问题，使用异或运算转换。\n\nASCII表 A到Z，Z完了之后没有直接到a，中间间隔了6个字符\n\n![](https://i.loli.net/2021/10/21/5IeZNlnCD3J94kY.png)\n\n发现大写字符与其对应的小写字符的ASCII的差为32，32这个值是 $2^5$ 可以表示为 $ 1<<5$\n\n变换大小写这件事等价于：\n\n- 如果字符是小写字符，减去32得到大写字符\n- 如果字符是大写字符，加上32得到小写字符\n\n而这两者合并起来，就是给这个字符做一次不进位的加法，即异或上 $1<<5$​\n\n```java\nclass Solution {\n    public List<String> letterCasePermutation(String s) {\n        int n = s.length();\n        List<String> res = new ArrayList<>();\n        if(n==0){\n            return res;\n        }\n        char[] charArray = s.toCharArray();\n        dfs(charArray, 0, res);\n        return res;\n    }\n\n\n    private void dfs(char[] charArray, int index, List<String> res){\n        if(index == charArray.length){\n            res.add(new String(charArray));\n            return;\n        }\n\n        dfs(charArray, index+1, res);\n        if(Character.isLetter(charArray[index])){\n            charArray[index] ^= 1<<5;\n            dfs(charArray, index+1, res);\n        }\n    }\n\n}\n```\n\n## [22. 括号生成](https://leetcode-cn.com/problems/generate-parentheses/)\n\n### 方法一：深度优先遍历\n\n我们以 `n = 2` 为例，画树形结构图。方法是 「做减法」。\n\n![](https://i.loli.net/2021/10/22/O2Gqza81NJeYDMZ.jpg)\n\n画出图后可分析出的结论：\n\n- 当前左右括号都有大于0个可以使用的时候，才可以产生分支。\n- 产生左分支的时候，只看当前是否还有左括号可以使用\n- 产生右分支的时候，还收到左分支的限制，右边剩余可以使用的括号数量一定得严格大于左边剩余的数量的时候，才可以阐释分支。\n- 在左边和右边剩余的括号数都为0时结算\n\n```java\nclass Solution {\n    public List<String> generateParenthesis(int n) {\n        List<String> res = new ArrayList<>();\n        if(n==0){\n            return res;\n        }\n\n        dfs(\"\",n , n, res);\n        return res;\n    }\n\n    private void dfs(String curStr, int left, int right, List<String> res){\n        // 因为每一次尝试，都是使用新的字符串变量所有无须回溯\n        // 在递归终止的时候，直接把它天道结果集即可\n        if(left==0 && right==0){\n            res.add(curStr);\n            return;\n        }\n\n        // 剪枝 （左括号可以使用的个数严格大于右括号可以使用的个数，才剪枝，注意这个细节）\n        if(left > right){\n            return;\n        }\n\n        if(left>0){\n            dfs(curStr + \"(\" , left-1, right, res);\n        }\n        \n        if(right>0){\n            dfs(curStr + \")\", left , right-1, res);\n        }\n\n    }\n\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["LeetCode"]},{"title":"Meta-Learning Representations for Continual Learning","url":"/2021/10/09/Meta-Learning-Representations-for-Continual-Learning/","content":"\n# Meta-Learning Representations for Continual Learning\n\n持续学习的代理应该能够在现有知识的基础上快速学习新数据，同时最大限度地减少遗忘。\n\n目前基于神经网络函数逼近器的智能系统 很容易遗忘，而且很少经过训练来促进未来的学习。这种糟糕行为的一个原因是，他们从没有为这两个目标明确训练的表征中学习。\n\n本文提出了OML，它的目标是通过学习表征来直接最小化灾难性的干扰，加速未来的学习，并且在连续学习中对在线更新下的遗忘具有健壮性。\n\n证明了学习自然稀疏表示是可能的，这对于在线更新更有效。此外，该算法是对现有的连续学习策略(如MER和GEM)的补充。\n\n> 有经验的程序员学习一门新的编程语言比以前从未编程的人要快得多，而且不需要忘记旧的语言来学习新的语言。\n\n在这项工作中，显式地学习一种持续学习的表示法，以避免干扰并促进未来的学习。设计一个元目标，它使用灾难性干扰作为训练信号，通过在线更新直接优化。目标是学习一种表示，以便模型在 meta-test 时使用的随机在线更新总体上提高其预测的准确性。\n\n## Problem Formulation\n\nContinual Learning Prediction (CLP)问题由无休止的样本流组成\n$$\nT = (X_1,Y_1) ,(X_2,Y_2) ,..., (X_t,Y_t) ...\n$$\n随机向量 $Y_t$ 根据未知分布 $p(Y|X_t)$抽样。我们假设过程 $X_1,X_2,..,X_t,...$有一个边际分布 $\\mu ：X \\to [0,\\infty)$，它反映了每个输入被观察到的频率。这种假设允许各种相关序列。例如，可以从潜在依赖于过去变量 $X_{t-1}$ 和 $X_{t-2}$ 的分布中采样 $X_t$。然而目标 $Y_t$ 仅依赖于 $X_t$ , 而不依赖于过去的 $X_i$。\n\n定义 $S_k = (X_{j+1}, Y_{j+1}),(X_{j+2}, Y_{j+2}) ,...,(X_{j+k}, Y_{j+k}) $ 为从CLP问题 $T$ 中抽样的长度为 $k$ 的随机轨迹。\n\n最后，$p(S_k|T)$ 给出了可以从问题 $T$ 中抽样的所有长度为 $k$ 的轨迹上的分布。\n\n对于给定的CLP问题，我们的目标是学习一个函数 $f_{W,\\theta}$ 它可以预测给定 $X_t$ 的 $Y_t$。更具体地说，设 $l:Y\\times Y \\to R$ 是将预测 $\\hat y \\in Y$ 和目标 $y$ 之间的损失定义为 $l(\\hat y,y)$的函数。\n\n如果假设输入 $X$ 与某个密度 $\\mu$ 成正比： $X \\to [0,\\infty)$, 那么我们希望最小化目标：\n$$\nL_{CLP}(W,\\theta) = E[l(f_{W,\\theta}(X), Y)] = \\int \\left[ \\int l(f_{W,\\theta}(x), y) p(y|x) dy  \\right] \\mu(x) dx\n$$\n其中 $W,\\theta$ 代表一系列参数，是更新和最小化的目标。\n\n为了最小化 $L_{CLP}$，我们将自己限制在通过从 $p(S_k|T)$ 采样的单个 $k$ 长度轨迹上的在线更新来学习。\n\n这改变了标准 iid 设置中的学习问题——模型看到长度为 k 的相关样本的单一轨迹，而不是直接从 $p(x, y) = p(y|x)\\mu(x)$ 中采样。当简单地为IID设置应用标准算法时，此修改可能会引起重大问题。相反，我们需要设计考虑这种相关性的算法。\n\n这个公式可以表示各种连续问题。 一个例子是在线回归问题，例如在给定当前位置的情况下预测机器人的下一个空间位置； 另一个是现有的增量分类基准。 CLP 公式还允许依赖于最近 m 次观测的历史记录的目标 $Y_t$。 这可以通过将每个 $X_t$ 定义为最后 m 个观测值来获得。 $X_t$ 和 $X_{t-1}$ 之间的重叠不违反对相关输入序列的假设。 最后，强化学习中的预测问题——从一个状态预测策略的值——可以通过将输入 $X_t$ 视为状态和要采样的目标返回或引导目标来表示。\n\n\n\n## Method\n\n端到端训练的神经网络在使用从 $p(S_k|T)$ 采样的单个轨迹来最小化CLP loss 方面不是有效的，原因有两个。\n\n首先，它们的样本效率极低，需要多个epoch的训练才能收敛到合理的解决方案。\n\n其次，当在线学习相关数据流时，他们会受到灾难性的干扰。\n\n元学习可以有效地提高神经网络的样本效率。但元学习模型初始化，这种归纳偏差不足以解决灾难性的干扰问题。\n\n经验发现，学习编码器比只学习初始化的性能要好得多，此外，元学习优化问题在学习编码器时表现更好(对超参数不敏感，收敛速度更快)。这种差异的一种解释是，当在高度相关的数据流上学习时，全局且贪婪的更新算法(例如梯度下降)将贪婪地改变神经网络的初始层相对于当前样本的权重。初始层中的这种变化将干扰模型的过去知识。因此，初始化不是增量学习的有效归纳偏差。另一方面，当学习编码器 $\\phi_{\\theta}$ 时，神经网络可以学习使得更新不那么全局的高度稀疏表示(因为连接到零的特征的权重保持不变)。\n\n为了将神经网络应用于问题的求解，作者提出了一种元学习函数 $\\phi_{\\theta} (X)$  ——一种由 θ 参数化的深度表示学习网络(RLN)--从$X \\to R^{d}$中学习。然后学习另一个来自 $R^d\\to Y$的函数 $g_W$，称为预测学习网络(PLN)。\n\n两个函数的组合为 $f_{W,\\theta }(x) = g_{W}(\\phi_{\\theta} (X))$\n\n![](https://z3.ax1x.com/2021/10/10/5APRG8.png)\n\n将 $\\theta$ 视为通过最小化元目标学习的元参数，然后在元测试时固定。在学习 $\\theta $ 之后，我们从 $R^d\\to Y $ 学习 $g_W $，用于从单个轨迹 $S $ 使用单次传递全在线的SGD更新解决CLP问题。\n\n对于元训练，假设由 $p(T)$ 给出的CLP问题上的分布。\n\nOML的目标函数为 ：\n$$\nmin_{W,\\theta} \\sum_{T_i \\sim p(T)} OML(W,\\theta) = \\sum_{T_i\\sim P(T)} \\sum_{S_k^j\\sim p(S_k|T_i)} [ L_{CLP_i} (U(W,\\theta, S_k^j))]\n$$\n其中 $S_k^j = (X_{j+1}^i,Y_{j+1}^i),(X_{j+2}^i, Y_{j+2}^i),...,(X_{j+k}^i,Y_{j+k}^i)$ , \n\n$U(W_t, \\theta, S_k^j) =(W_{t+k},\\theta)$ 表示一个更新 $W_{t+k}$ 是k步SGD后的权重向量\n\nU 中第 $j$ 步更新 在样本 $(X_{t+j}^i, Y_{t+j}^i)$ 使用参数  $W_{t+j-1}, \\theta$ , 得到 $(W_{t+j},\\theta)$ \n\nMAML-Rep和OML目标可以分别实现为算法1和算法2，两者之间的主要区别以蓝色突出显示:\n\n![](https://z3.ax1x.com/2021/10/10/5AkLJP.png)\n\n注意，MAML-Rep使用完整批数据 $S_k$ 进行 $l$ 次内部更新(其中 $l $ 是超参数)，而OML使用 $S_k$ 中的一个数据点进行一次更新。这使得OML可以考虑在线持续学习的影响，例如灾难性的遗忘。\n\nOML 目标的目的是学习适合在线持续学习的表示。 为了说明什么将构成持续学习的有效表示，假设我们有三个输入集群，它们具有显着不同的 $p(Y |x)$，对应于 $p_1$、$p_2$ 和 $p_3$。 对于固定的二维表示 $\\phi_{\\theta} : X \\to R^2$，我们可以考虑由线性模型给出的解 $W\\in R^2$ 的线性模型，该模型为每个 $p_i$ 提供等效准确的解。\n\n这三个过程在图2中的 $W\\in  R^2$参数空间中描述为三条不同颜色的线。\n\n![](https://z3.ax1x.com/2021/10/10/5AZfkd.png)\n\n对于目标是由三个不同的分布 $p_1(Y|x)$ 、$p_2(Y|x)$ 和 $p_3(Y|x)$生成的问题，研究了表示对连续学习的影响。\n\n目的是通过对来自三个分布的样本进行在线学习，找到一个对所有三个分布都有效的参数向量 W。 对于两种不同的表示，这些流形及其交集可能看起来非常不同。 直觉是，当流形平行（允许正泛化）或正交（避免干扰）时，来自 W 的在线更新更有效。 产生这种流形的表示不太可能自然出现。 相反，我们必须明确地找到它。 通过考虑在线持续学习的影响，OML 目标针对这种表示进行了优化。\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"Continual Lifelong Learning in Natural Language Processing: A Survey","url":"/2021/10/09/Continual-Lifelong-Learning-in-Natural-Language-Processing-A-Survey/","content":"\n# Continual Lifelong Learning in Natural Language Processing: A Survey\n\n与计算机视觉或机器人技术相反，NLP领域中的CL仍处于初级阶段\n\n持续学习 (CL) 旨在使信息系统能够跨时间从连续数据流中学习。然而，现有的深度学习体系结构很难在不遗忘以前获得的知识的情况下学习新任务。此外，由于自然语言是模棱两可的，CL对语言学习尤其具有挑战性：它是离散的，组合的，意思是上下文相关的。\n\n## Introduction\n\ncatastrophic forgetting (CF) or catastrophic interference\n\n由于数据分布的变化，模型会随着时间的推移而变得不那么精确--这一现象称为概念漂移。现有的方法不能有效地保留以前学到的知识并同时适应新的信息。\n\n最简单的，从头开始重新训练模型以适应新的任务(或新的数据分布)，是昂贵和耗时的，容量饱和和模型扩展问题也扩大了这个问题。\n\n具体地说，参数模型在学习具有不同分布的数据样本或通过一系列任务进行处理时，最终到达不能存储更多知识的点-即其表示能力接近极限。在这一点上，要么扩展了模型的容量，要么应用了选择性遗忘（可能会导致性能下降）。后一种选择可能导致对新任务(或数据分布)的预测精度恶化或忘记以前获得的知识。\n\n这一约束是由CL的一个定义特性支撑的，即所谓的稳定性-可塑性困境。具体地说，该现象考虑了模型试图在其稳定性(保留先前知识的能力)和可塑性(适应新知识的能力)之间取得平衡。\n\n## Learning Paradigms\n\n目标是从大量任务 $T$ 中顺序学习模型 $f：X\\times T\\to Y$。\n\n该模型在实例 $(x_i，y_i)$ 上训练，$x_i \\in X_{t_i}$ 是输入特征向量，$y_i \\in y_{t_i}$ 是目标向量，$t_i \\in T$表示任务描述符(在最简单的情况下 $t_i=i$)，其中 $i\\in Z$。目标是最大化任务 $T_i$  处的函数 $f$ (由 $θ\\in R$ 参数化)，同时最小化任务 $T_1,...,T_{i-1}$ 的CF。\n\nCL所需的性质 :\n\n- Knowledge retention : 模型不容易发生灾难性的遗忘。\n- Forward transfer : 模型在重用从先前任务中获得的知识的同时学习新的任务。\n- Backward transfer :  模型在学习一项新任务后，在以前的任务上取得了更好的性能。\n- On-line learning : 模型从连续数据流中学习。\n- No task boundaries : 模型无需明确的任务或数据边界即可学习。\n- Fixed model capacity : 无论任务的数量和数据流的长度如何，内存大小都是恒定的。\n\n许多模型仅以有监督的方式使用大的标签数据集进行训练，因此它们不会暴露在涉及很few-shot、无监督或自监督学习的更具挑战性的情况下。\n\n### Related Machine Learning Paradigms\n\n\n\n| *Paradigm*          | *Definition*                                                 | *Properties*                                                 |\n| ------------------- | ------------------------------------------------------------ | :----------------------------------------------------------- |\n| Transfer learning   | 将知识从源任务/域转移到目标任务/域<br/>以提高目标任务的性能。 | \\+ forward transfer<br/> – no backward transfer<br/> – no knowledge retention <br/> – task boundaries<br/> – off-line learning |\n| Multi-task learning | 联合学习多个相关任务，利用参数共享，<br/>提高所有任务的泛化能力。 | \\+ positive transfer <br/>– negative transfer <br/>– task boundaries <br/>– off-line learning |\n| Meta-learning       | Learning to learn学习一般知识，<br/>给出一小串训练样本和许多任务，并快速适应一项新任务。 | \\+ forward transfer<br/> – no backward transfer<br/> – no knowledge retention <br/>– off-line learning |\n| Curriculum learning | 从训练样本中学习按有意义的顺序排列-<br/>任务或数据难度逐渐增加。 | \\+ forward transfer<br/> \\+ backward transfer<br/> \\+ knowledge retention<br/> – task boundaries<br/> – off-line learning |\n| On-line learning    | 通过按顺序提供的训练示例的连续流进行学习。<br/>体验由于非I.I.D.的数据造成的概念漂移。 | \\+ on-line learning<br/> \\+ forward transfer<br/> – no backward transfer<br/> – no knowledge retention<br/>– single task/domain |\n| On-the-job learning | 发现新任务，学习并适应。<br/>On-the-job learning 是在一个开放的环境中进行的，<br/>它涉及到与人和环境的互动。这要归功于CL方法家族。 | \\+ on-line learning<br/> \\+ forward transfer<br/> \\+ backward transfer<br/> \\+ knowledge retention <br/>+ no task boundaries <br/>+ open-world learning<br/> – interactive learning |\n\n### Approaches to Continual Learning\n\nRehearsal 、regularization、architectural methods\n\n纯来自NLP领域的模型数量相当有限。\n\n#### Rehearsal\n\n依靠保留以前任务中的一些训练样本，以便稍后可以将它们显示给手头的任务。\n\niCaRL 由于每个任务都保留训练样本，并在学习模型时定期重放，因此模型的计算和内存需求随着任务数量的增加而成比例增加。为了减少存储，建议使用 latent replay(Pellegrini等人，2019年)或 pseudo-rehearsal(Robins，1995)方法。\n\nPseudo-rehearsal methods 不使用memory中的训练样本，而是通过知道先前任务样本的概率分布来生成样本。方法包括生成性自动编码器和基于生成性对抗网络的模型。\n\n\n\n#### regularization\n\n是依赖于固定模型容量和附加损失项的单模型方法，这有助于在学习后续任务或数据分布的同时巩固知识。例如，Elastic Weight Consolidation(EWC)通过损失正则化来减少遗忘；换句话说，它减缓了对之前任务重要的参数的学习。\n\nMemory method 是正则化方法的特例，可分为两组：synaptic regularization 和 episodic memory (A-GEM、MER、MbPA++) 前一种方法集中于通过以防止改变先前学习的模型参数的方式调整学习率来减少对巩固知识的干扰。而后者存储来自先前看到的数据的训练样本，这些样本稍后被排练以允许学习新的类别。重要的是，梯度情节记忆(GEM)允许正向后转移，并防止过去任务的损失增加。\n\nMBPA++进行稀疏经验回放和局部适应。学者们声称，MBPA++的训练速度比A-GEM快，训练它的时间并不比编解码器模型长。虽然这是可能的，因为稀疏的体验回放，但MBPA++需要额外的内存。\n\nKnowledge distillation methods 与情景记忆方法非常相似，但与GEM不同的是，它们使过去任务中的预测保持不变。具体地说，它是一类通过依赖从大型网络模型(教师)到新的、较小的网络(学生)的知识转移来缓解CF的方法。其基本思想是，学生模型学习生成对教师模型的预测。\n\n\n\n#### architectural\n\n通过将模块化更改应用于网络架构并引入特定于任务的参数来防止遗忘。通常，以前的任务参数保持固定或屏蔽。此外，新的层通常是动态注入的，以使用附加模块来扩充模型，以适应新的任务。\n\nProgressive Networks (PNN）以及它们的改进版本 Dynamically Expandable Network (DEN）Reinforced Continual Learning (RCL）这些都是突出的例子。这些策略的主要缺点是参数数量大幅增加。\n\n与PNN类似，BatchEnsemble(2020)也不受CF的影响，此外，由于只训练快速权重，它支持任务的并行排列，并且比PNN消耗更少的内存。\n\n同样，适配器模块旨在克服大量参数的问题。 它们充当具有少量参数的附加网络层，可即时重新配置原始网络 对于目标任务，同时保持原始网络的参数不变并在不同任务之间共享。\n\n## Continual Learning in NLP Tasks\n\n### Word and Sentence Representations\n\n分布式词向量表示是许多NLP应用程序的基础。\n\n通常，词嵌入是在大型通用语料库上训练的，因为领域内语料库的大小在大多数情况下是不够的。这是有代价的，因为在通用语料库上训练的嵌入通常不适合特定于领域的下游任务，因此，整体性能会受到影响。\n\n在CL设置中，这也意味着词汇可以相对于两个维度改变：时间和领域。一个公认的共识是，由于复杂的语言和社会过程，单词的意义会随着时间的推移而变化。因此，重要的是检测和适应意义和数据分布的变化，同时防止先前从 CF 中学习到的表示。\n\n为了解决这个问题 ， Lifelong domain word embedding via meta-learning提出了一种元学习方法，该方法利用过去多领域语料库中的知识生成改进的新领域嵌入。\n\nContinual Learning for Sentence Representations Using Conceptors 引入了一种随时间更新的句子编码器，使用矩阵概念不断学习依赖于语料库的特征。\n\n重要的是，王等人。(2019b)认为，当一个神经网络模型被训练到一个新的任务上时，嵌入的向量空间会发生不希望的变化，从而导致嵌入对于以前的任务是不可行的。为了缓解嵌入空间失真的问题，他们建议使用锚定对齐句子嵌入。最近出现了一个词嵌入和语言建模交叉点的研究线，称为上下文嵌入，并展示了许多NLP任务的最新成果。\n\n\n\n## Research Gaps and Future Directions\n\n\n\n在现实世界中，我们经常处理部分信息数据。此外，数据来自非 i.i.d。分布，并受代理的干预或环境变化的影响。尽管存在一些尝试，模型从一系列示例中学习而不知道它们来自哪个数据集和分布（例如 d’Autume 等，2019），但这种方法很少见。此外，学习很少的例子（例如通过少样本转移学习）（Liu，2020）是当前模型的主要挑战，尤其是执行分布外泛化（Bengio，2019）。特别是，广泛用于 NLP 的序列到序列模型仍然难以系统泛化（Lake 和 Baroni，2018 年；Bahdanau 等人，2019 年），无法学习高级语言概念的一般规则和推理。例如，Feder 等人最近在反事实语言表示方面的工作。 (2020) 是朝着这个方向迈出的有希望的一步。非平稳学习问题可以通过从数据中理解和推断因果关系来缓解（例如 Osawa 等人，2019 年）——这是一个突出的挑战（Pearl，2009 年）——并提出不太可能的组合存在于训练分布中（Bengio，2019）。也就是说，语言是组合的；因此，该模型可以动态操纵语义概念，这些语义概念可以在新情况下重新组合（Lake 等人，2015 年），后来得到基于语言的溯因推理（例如 Bhagavatula 等人，2020 年）的支持。\n\n\n\n\n\n在模型级别，CL 与贝叶斯原理的结合应该可以更好地识别神经网络每个参数的重要性，并有助于参数修剪和量化（例如 Ebrahimi 等人，2020 年；Golkar 等人，2019 年）。我们认为，不仅参数信息量应该是不确定性引导的，而且先前记忆的周期性重放也应该通过因果关系来告知。此外，重点关注降低模型容量和计算要求也很重要。尽管神经网络的过度参数化很普遍（Neyshabur 等人，2018 年），但许多当前的 CL 方法促进了参数空间的扩展。我们设想进一步的研究工作集中在压缩方法上，例如知识蒸馏、低秩分解和模型修剪。\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"Optimizing Reusable Knowledge for Continual Learning via Metalearning","url":"/2021/10/05/Optimizing-Reusable-Knowledge-for-Continual-Learning-via-Metalearning/","content":"\n# Optimizing Reusable Knowledge for Continual Learning via Metalearning\n\n当网络的权重在新任务的训练过程中被覆盖，从而导致忘记旧信息时，就会发生灾难性遗忘。\n\n为了解决这个问题，作者提出了MetA Reusable Knowledge: MARK, 它提高了权重的可重用性，而不是在学习新任务时被覆盖。\n\nMARK在任务之间保留一组共享权重，将这些共享的权重设想为一个公共知识库(KB)，它不仅用于学习新任务，而且在模型学习新任务时还会丰富新知识。\n\n关键组成部分有两个方面：\n\n- 1.元学习方法提供了用新知识逐步丰富知识库的关键机制，并促进了任务之间的权重可重用性。\n- 2.一组可训练掩码提供了从知识库相关权重中有选择地选择来解决每个任务的关键机制。\n\n以往预防灾难性遗忘（CF）的工作主要遵循两种策略：\n\n- 1.避免修改对解决先前任务至关重要的参数。具体地说，当面对新的任务时，正则化项确保了关键参数的修改尽可能少。一般而言，该方法在任务较少的问题上表现出令人满意的性能，但是当任务数量增加时，诸如权值的累积漂移和它们之间的干扰等问题使得该方法难以扩展。\n- 2.模型的体系结构更改。这包括保留部分网络容量来学习每个任务的方法，以及使用特殊记忆单元来回忆以前任务中的关键训练样本的方法。这些方法的主要问题是额外的模型复杂性并且需要一种有效的方法来回忆以前任务中的关键信息。\n\n与这些先前的策略相反，当学习新任务时，人类会不断地将先前的经验与新情况联系起来，增强先前的记忆，这有助于缓解CF问题。\n\nMASK 一种基于学习策略的新模型，它不是减轻权重覆盖或学习不同任务的独立权重，而是使用元学习方法来促进任务之间的权重可重用性。特别地，将这些共享的权重设想为一个公共知识库(KB)，它不仅用于学习新任务，而且在模型学习新任务时还会丰富新知识。从这个意义上说，MASK背后的KB不是由以向量编码信息的外部存储器给出的，而是由以其权重编码共享信息的可训练模型给出的。作为查询该KB的补充机制，MASK还包括一组可训练掩码，其负责实施选择性寻址方案来查询KB。\n\n因此，为了构建和查询其共享知识库，MARK 使用了两种互补的学习策略。 \n\n- 1、元学习技术提供了实现两个目标的关键机制：\n- - 鼓励对多个任务有用的权重更新\n  - 在模型学习新任务时用新知识丰富知识库。 \n- 2、一组可训练的掩码提供了从知识库相关权重中选择性地选择来解决每个任务的关键机制。\n\nMARK的工作方式是，首先通过检测知识库中学习的每个模式的重要性的函数强制模型重用当前知识，然后如果过去的知识不足以成功执行任务，则扩展其知识。\n\n\n\n## Continual Learning Scenario\n\n每个任务 $t$ 由新的数据分布 $D^t = (X^t, Y^t,T^t)$ 组成，其中 $X^t$  是输入样例，$Y^t$ 是任务标签， $T^t$ 是任务ID。\n\n目标是学习一个分类模型 $f : X\\to Y$ 使用来自T个任务序列的数据：$D = \\{D^1,...,D^T\\}$\n\n## Method\n\n两个主要挑战：\n\n- 如何增量地构建此知识库？（一种称为情景训练的元学习策略）\n- 如何查询此知识库以访问相关的知识片段？（为每个任务训练掩码生成函数）\n\n### Model Architecture\n\n![](https://z3.ax1x.com/2021/10/05/4v9aff.png)\n\n给定来自任务 $T$ 的输入 $X_i$，使用特征提取器 $F^t$ 来获得 $F_i^t$。 \n\n然后将 $F_i^t$ 传递给mask函数 $M^t$ 以生成mask $M_i^t$。\n\n之后，相同的输入 $X_i$ 进入 KB，它具有由 $M_i^t$ 调制的中间激活\n\n最终，调制的特征经过任务相关的分类器 $C^t$，该分类器执行对 $X_i$ 的类别预测。\n\n模型的主要模块：\n\n- Feature Extractor ($F^t$) : 该模块负责为每个输入 $X_i$ 提供初始嵌入，即 $F^t$ 取输入$X_i$ 并输出向量表示$F_i^t$。重要的是要注意，模型 $F^t$ 可以在任务之间共享，也可以特定于每个任务。\n\n- Knowledge Base (KB) : 这是MASK背后的主要模块。当模型面临新的任务时，它负责积累相关知识。实现时使用了带B blocks的卷积架构。模型的这一部分在任务之间共享。\n- Mask-Generating functions ( $M^t$): 将特征向量 $F_i^t$ 作为输入，并为 KB 的每个 block 生成一个实例和依赖于任务的掩码 $M_i^t$。 每个掩码由一组标量组成，每个标量用于KB的卷积块中的每个通道，其乘以每个通道的激活。这些掩码对于选择与每个实例和任务相关的知识至关重要。在实施中，作者使用全连接层。\n- Classifier ( $C^t$) : 这些模块对应于依赖于任务的分类头。 它输入$ F^t_{i,KB}$ , 是通过对 KB 的最后一个 block 的输出进行展平操作而给出的。 给定输入 $X_i$ 的任务 ID，相应的 head 输出模型预测。 实现中使用全连接层。\n\n### MARK Training\n\n首先是通过在第一个任务中端到端地训练知识库来初始化知识库，而不使用元学习和掩码函数。\n\n对于分类任务，首先使用卷积神经网络的规则训练过程来执行知识库初始化。之后，我们交替三个主要步骤对Mark进行每个任务的顺序训练：\n\n- KB Querying : 训练任务相关的掩码生成函数，这些函数用于使用向量 $F_i^t$ 来查询知识库。另外，我们同时训练当前任务的任务分类器。请注意，撇开知识库初始化不谈，在此步骤中，每个新任务仅使用以前任务积累的知识进行训练。\n- KB Update: 使用元学习策略来更新知识库中的权重。该方案允许促进知识库更新，有利于获取可重用的知识以面对新任务。\n- KB Querying : 在使用来自当前任务的知识更新知识库之后，我们重复查询过程，使用这些新知识来优化掩码生成函数和任务分类器。注意，在此步骤中，KB保持固定。\n\n前面三个步骤的应用背后的直觉如下:\n\n> 最初使用从先前任务中积累的知识来查询知识库。 这迫使掩码函数和分类器重用可用知识。 当该知识耗尽时，我们继续将当前任务中的知识添加到知识库中。 最后，我们利用这个新更新的知识库来获得给定任务的最终掩码函数和分类器。\n\n![](https://z3.ax1x.com/2021/10/05/4vkqS0.png)\n\n\n\n### KB Querying\n\n一旦我们通过使用特征提取器 $F^t$ 获得特征向量，模型就可以了解知识库中哪些模块能够最好地解决当前任务。在这个训练阶段，模型训练函数学习如何使用知识库中可用的知识，只关注重用以前任务中的知识，而不修改KB。特别是在这一步中，我们只训练 $M^t$ 和 $C^t$。两者都经过端到端训练，同时保持KB权重冻结。\n\n当我们为模型的每个中间激活生成掩码时，严格地说，我们总共有B个掩码生成函数。然而，为了便于表示，我们将所有此类函数都包含在 $M^t$下，并将其输出视为这些 $B$ 函数结果的拼接。\n\n下式表明函数$M^t$，其中在给定来自任务 $t$ 的输入$X_i$的情况下获得掩码 $M_i^t$:\n$$\nM_i^t = M^t (F_i^t) = \\rho((W^{t,M})^T F_i^t)\n$$\n在此过程中生成的遮罩具有两种效果：\n\n- 给出KB中特定模块对当前输入的重要性的信号\n- 确保梯度更新在真正重要的地方进行。\n\n如果激活映射与某个任务无关，则相应掩码的值将为零，从而使与该激活相关联的梯度更新也为零。\n\n### KB Update\n\n这个训练步骤的目的是将当前任务中的新知识添加到KB。为了实现这一点，使用元学习作为一种方式，迫使模型捕获可以重用的知识，以面对新的任务。下图，训练Mark的元学习过程的示意图。这个过程是 Reptile的改编\n\n![](https://z3.ax1x.com/2021/10/05/4v3dZn.png)\n\n给定任务t，随机生成一组K个小任务，其中每个小任务由来自原始任务的类的子集组成。\n\n对于每个小任务，我们为固定数量的epoch训练当前知识库的独立副本，从而生成K个模型。\n\n然后，使用一组保持训练样本计算每个模型的损失函数的梯度。\n\n最后，使用这些梯度的加权平均值来更新KB。\n\n具体地说，创建了一组K个小任务，其中每个小任务包括从当前任务中随机抽样一组H个类和每个类的h个训练实例。\n\n这允许我们创建一个与主任务不同的小任务，找到不特定于它的权重。使用 $E_{inner}$ epochs的每个小任务训练模型的一个副本。将为e个epochs 训练的副本 k 称为 $KB^{k}_e$。对于每个小任务，使用一个临时分类器 $C^k$，该分类器由 $C^t$参数初始化。在内部循环的最终迭代之后，丢弃这个分类器。\n\n与MAML一样模型训练包括两个嵌套循环，一个内循环一个外循环。内循环负责为当前小任务训练我们的知识库副本，而外循环负责按照梯度方向更新知识库权重，从而快速适应新的小任务。\n\n在每个内循环期间，$KB^k$ 和 $C^k$ 被端到端地训练以用于 $E_{inner}$ epochs。\n\n下式模拟外循环并更新 KB，具体地说，对于每个k，平均 $KB^k_0$之前的 KB 参数的差异:\n$$\nKB = KB - \\alpha \\nabla KB  \\ \\ \\ \\ \\ \\  \\nabla KB = \\frac{1}{E_{inner}}\\sum_{k}^K \\gamma_k (KB^k_{E_{inner}} - KB_0^k)\n$$\n和Reptile 一样梯度更新为每个模型 $KB^k$ 的累积梯度之和的平均值。其中权重 $\\gamma_k$  为：\n$$\n\\gamma_k = \\frac{acc}{\\sum_{j}^K acc_j}\n$$\n其中自同一任务 $t$ 的验证批次上的每个模型的精度作为参考。\n\n\n\n## Results\n\n![](https://z3.ax1x.com/2021/10/05/4vGBgU.png)\n\n- MARK-Task :  为每个任务训练 $F^t$，在它上面添加一个使用 $D^T$训练的分类器。训练完 $ F^t $后，该分类器被丢弃。\n- MARK-Random:  $F^t $由一组随机权重组成。所有任务共享相同的 $F^t$。\n- MARK-Resnet :  所有任务共享在Imagenet上预先训练的Resnet-18作为特征提取器。\n\n\n\n![](https://i.loli.net/2021/10/05/Ez3rPXkuyfm1lTg.png)\n\n随着训练的任务越多，权重更新的数量就会迅速减少，几乎降到零。此外，在这两个基准中，相对于基线的更新次数都有显著减少。这些结果表明，当使用Mark训练模型时，干扰较少，我们将其归因于其存储可重用知识的知识库。\n\n\n\n![](https://i.loli.net/2021/10/05/UXRnuh4dDtSKkeB.png)\n\n- Baseline :简单的顺序学习，没有元学习或掩码生成函数。我们使用与知识库相同的架构。\n- Baseline + ML : 添加元学习，即知识库更新来改进基线。\n- Baseline + Mask :  添加特定于任务的掩码函数来改进基线。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"39/40/46/47/77/78/90/60/93排列、组合、子集相关问题","url":"/2021/10/04/39-40-46-47-77-78-90-60-93排列、组合、子集相关问题/","content":"\n# 39/40/46/47/77/78/90/60/93排列、组合、子集相关问题\n\n## 39 组合总和\n\n#### [39. 组合总和](https://leetcode-cn.com/problems/combination-sum/)\n\n思路分析：根据示例 1：输入: candidates = [2, 3, 6, 7]，target = 7。\n\n候选数组里有 2，如果找到了组合总和为 7 - 2 = 5 的所有组合，再在之前加上 2 ，就是 7 的所有组合；\n同理考虑 3，如果找到了组合总和为 7 - 3 = 4 的所有组合，再在之前加上 3 ，就是 7 的所有组合，依次这样找下去。\n\n![](https://z3.ax1x.com/2021/10/04/4Oev7Q.png)\n\n- 以 target = 7 为根节点，创建一个分支的时候做减法。\n- 每一个箭头表示：从父亲节点的数值减去边上的数值，得到孩子节点的数值。边的值就是题目中给出的 candidate 数组的每个元素的值。\n- 减到0或负数的时候停止，即：节点0和负数节点成为叶子节点。\n- 所有从根节点到节点0的路径（只能从上往下，没有回路）就是题目要找的一个结果\n\n这棵树有 4 个叶子结点的值 0，对应的路径列表是 [[2, 2, 3], [2, 3, 2], [3, 2, 2], [7]]，而示例中给出的输出只有 [[7], [2, 2, 3]]。即：题目中要求每一个符合要求的解是 不计算顺序 的。下面我们分析为什么会产生重复。\n\n### 针对具体例子分析重复路径产生的原因（难点）\n\n产生重复的原因是：在每一个结点，做减法，展开分支的时候，由于题目中说，每一个元素可以重复使用，我们考虑了所有的候选数，因此出现了重复的列表。\n\n一种简单的去重方案是借助哈希表的天然去重功能，但实际操作没那么容易。\n\n另一种可以再搜索的时候去重，遇到这一类相同元素不计算顺序的问题，我们在搜索的时候就需要按某种顺序搜索。具体做法是：每一次搜索的时候设置下一轮搜索的起点 begin ：\n\n![](https://z3.ax1x.com/2021/10/04/4OmrDS.png)\n\n即：从每一层的第2个节点开始，都不能再搜索产生同一层节点使用过的 candidate 里的元素。\n\n- Python3 的 [1, 2] + [3] 语法生成了新的列表，一层一层传到根结点以后，直接 res.append(path) 就可以了；\n- 基本类型变量在传参的时候，是复制，因此变量值的变化在参数里体现就行，所以 Python3 的代码看起来没有「回溯」这个步骤。\n\n```python\nfrom typing import List\nclass Solution:\n  \tdef combinationSum(self, candidates, target):\n      \tsize = len(candidates)\n        if size == 0:\n          \treturn []\n        path = []\n        res = []\n        def dfs(candidates, begin, size, res, target):\n          \tif target < 0:\n              \treturn\n            if target == 0:\n              \tres.append(path)\n                return\n            for  index in range(begin, size):\n              \tdfs(candidates, index, size, path+[candidates[index]], res, target - candidate[index])\n        dfs(candidates, 0, size, path, res, target)\n        return res\n        \n```\n\n```java\npublic class Solution {\n\n    public List<List<Integer>> combinationSum(int[] candidates, int target) {\n        int len = candidates.length;\n        List<List<Integer>> res = new ArrayList<>();\n        if (len == 0) {\n            return res;\n        }\n\n        Deque<Integer> path = new ArrayDeque<>();\n        dfs(candidates, 0, len, target, path, res);\n        return res;\n    }\n\n    private void dfs(int[] candidates, int begin, int len, int target, Deque<Integer> path, List<List<Integer>> res) {\n        // target 为负数和 0 的时候不再产生新的孩子结点\n        if (target < 0) {\n            return;\n        }\n        if (target == 0) {\n            res.add(new ArrayList<>(path));\n            return;\n        }\n\n        // 重点理解这里从 begin 开始搜索的语意\n        for (int i = begin; i < len; i++) {\n            path.addLast(candidates[i]);\n\n            // 注意：由于每一个元素可以重复使用，下一轮搜索的起点依然是 i，这里非常容易弄错\n            dfs(candidates, i, len, target - candidates[i], path, res);\n\n            // 状态重置\n            path.removeLast();\n        }\n    }\n}\n```\n\n### 剪枝提速\n\n- 根据上面画树形图的经验，如果 target 减去一个数得到负数，那么减去一个更大的树依然是负数，同样搜索不到结果。基于这个想法，我们可以对输入数组进行排序，添加相关逻辑达到进一步剪枝的目的；\n- 排序是为了提高搜索速度，对于解决这个问题来说非必要。但是搜索问题一般复杂度较高，能剪枝就尽量剪枝。实际工作中如果遇到两种方案拿捏不准的情况，都试一下。\n\n```java\npublic class Solution {\n\n    public List<List<Integer>> combinationSum(int[] candidates, int target) {\n        int len = candidates.length;\n        List<List<Integer>> res = new ArrayList<>();\n        if (len == 0) {\n            return res;\n        }\n\n        // 排序是剪枝的前提\n        Arrays.sort(candidates);\n        Deque<Integer> path = new ArrayDeque<>();\n        dfs(candidates, 0, len, target, path, res);\n        return res;\n    }\n\n    private void dfs(int[] candidates, int begin, int len, int target, Deque<Integer> path, List<List<Integer>> res) {\n        // 由于进入更深层的时候，小于 0 的部分被剪枝，因此递归终止条件值只判断等于 0 的情况\n        if (target == 0) {\n            res.add(new ArrayList<>(path));\n            return;\n        }\n\n        for (int i = begin; i < len; i++) {\n            // 重点理解这里剪枝，前提是候选数组已经有序，\n            if (target - candidates[i] < 0) {\n                break;\n            }\n            \n            path.addLast(candidates[i]);\n            dfs(candidates, i, len, target - candidates[i], path, res);\n            path.removeLast();\n        }\n    }\n}\n```\n\n### 什么时候使用 `used` 数组，什么时候使用 `begin` 变量\n\n- 排列问题，讲究顺序（即 `[2, 2, 3]` 与 `[2, 3, 2]` 视为不同列表时），需要记录哪些数字已经使用过，此时用 `used` 数组；\n- 组合问题，不讲究顺序（即 `[2, 2, 3]` 与 `[2, 3, 2]` 视为相同列表时），需要按照某种顺序搜索，此时使用 `begin` 变量。\n\n## 40 组合总和 2\n\n#### [40. 组合总和 II](https://leetcode-cn.com/problems/combination-sum-ii/)\n\n`candidates` 中的每个数字在每个组合中只能使用一次。\n\n为了使得解集不包含重复的组合，有两种方案：\n\n- 使用哈希表，编码复杂\n- 需要按顺序搜索，在搜索的过程中检测分支是否会出现重复结果。注意：这里的顺序不仅仅指数组candidate有序，还指按照一定顺序搜索结果。\n\n![](https://z3.ax1x.com/2021/10/04/4OnqeS.png)\n\n```java\nclass Solution {\n    public List<List<Integer>> combinationSum2(int[] candidates, int target) {\n        List<List<Integer>> res = new ArrayList<>();\n        if(candidates.length==0) return res;\n\n        int n  = candidates.length;\n        Deque<Integer> path = new ArrayDeque<>();\n\n        Arrays.sort(candidates);\n\n        dfs(candidates, 0, n, path, target,res);\n        return res;\n    }\n\n\n    private void dfs(int[] candidates, int begin, int n, Deque<Integer> path, int target, List<List<Integer>> res){\n    \n        if(target == 0){\n            res.add(new ArrayList<>(path));\n            return;\n        }\n\n        // 小剪枝：同一层相同数值的节点，从第2个开始，候选数更少，结果一定发生重复，因此跳过\n        \n\n        for(int i=begin;i<n;i++){\n           // 大剪枝：减去 candidates[i] 小于0 ，减去后面的candidates[i+1], candidates[i+2] 肯定也小于0\n           if(target - candidates[i]<0){\n               break;\n           }\n           // 小剪枝：同一层相同数值的节点，从第2个开始，候选数更少，结果一定发生重复，因此跳过\n           if(i > begin && candidates[i] == candidates[i-1]){\n               continue;\n           }\n           path.addLast(candidates[i]);\n           dfs(candidates, i+1, n, path, target - candidates[i], res);\n           path.removeLast();\n        }\n\n    }\n\n}\n```\n\n## 77 组合\n\n#### [77. 组合](https://leetcode-cn.com/problems/combinations/)\n\n法一：根据搜索起点画出二叉树\n\n- 如果组合里有 1， 那么需要再 $[2,3,4]$ 里再找一个数\n- 如果组合里有 2， 那么需要再 $[3,4]$ 里再找一个数，注意：这里不能再考虑 1，因为包含 1 的组合，在第一种情况以及包含。\n\n![](https://z3.ax1x.com/2021/10/05/4jEZSP.png)\n\n```java\n\npublic class Solution {\n\n    public List<List<Integer>> combine(int n, int k) {\n        List<List<Integer>> res = new ArrayList<>();\n        if (k <= 0 || n < k) {\n            return res;\n        }\n        // 从 1 开始是题目的设定\n        Deque<Integer> path = new ArrayDeque<>();\n        dfs(n, k, 1, path, res);\n        return res;\n    }\n\n    private void dfs(int n, int k, int begin, Deque<Integer> path, List<List<Integer>> res) {\n        // 递归终止条件是：path 的长度等于 k\n        if (path.size() == k) {\n            res.add(new ArrayList<>(path));\n            return;\n        }\n\n        // 遍历可能的搜索起点\n        for (int i = begin; i <= n; i++) {\n            // 向路径变量里添加一个数\n            path.addLast(i);\n            // 下一轮搜索，设置的搜索起点要加 1，因为组合数理不允许出现重复的元素\n            dfs(n, k, i + 1, path, res);\n            // 重点理解这里：深度优先遍历有回头的过程，因此递归之前做了什么，递归之后需要做相同操作的逆向操作\n            path.removeLast();\n        }\n    }\n}\n\n```\n\n剪枝\n\n上面的代码，搜索起点遍历到 n，即：递归函数中有下面的代码片段：\n\n```java\n// 从当前搜索起点 begin 遍历到 n\nfor (int i = begin; i <= n; i++) {\n    path.addLast(i);\n    dfs(n, k, i + 1, path, res);\n    path.removeLast();\n}\n```\n\n事实上，如果 n=7, k=4 ，从5开始搜索就已经没有意义了， 这是因为：即使把5选上，后面的数只有，6，7一共三个候选数，凑不出4个数的组合。因此搜索起点有上界，这个上界是多少，可以举例分析。\n\n分析搜索起点的上界，其实是在深度优先遍历的过程中剪枝，剪枝可以避免不必要的遍历，剪枝剪得好，可以大幅度节约算法的执行时间。\n\n下面的图片绿色部分是减掉的枝叶，当n很大的时候，能少遍历很多节点，节约了时间\n\n![](https://z3.ax1x.com/2021/10/05/4jEvkj.png)\n\n容易知道：搜索起点和当前还需要选几个数有关，而当前还需要选几个数与已经选了几个数有关，即与path 的长度相关。举例分析：\n\n例如： n=6, k=4\n\n`path.size() == 1` 的时候，接下来要选择 3 个数，搜索起点最大是 4，最后一个被选的组合是 `[4, 5, 6]`；\n\n`path.size() == 2` 的时候，接下来要选择 2 个数，搜索起点最大是 5，最后一个被选的组合是 `[5, 6]`；\n\n`path.size() == 3` 的时候，接下来要选择 1 个数，搜索起点最大是 6，最后一个被选的组合是 `[6]`；\n\n再如：`n = 15` ，`k = 4`。\n\n`path.size() == 1` 的时候，接下来要选择 3 个数，搜索起点最大是 13，最后一个被选的是 `[13, 14, 15]`；\n\n`path.size() == 2` 的时候，接下来要选择 2 个数，搜索起点最大是 14，最后一个被选的是 `[14, 15]`；\n\n`path.size() == 3` 的时候，接下来要选择 1 个数，搜索起点最大是 15，最后一个被选的是 `[15]`；\n\n可以归纳出：\n\n搜索起点的上界 + 接下来要选择的元素个数 - 1 = n\n\n其中，接下来要选择的元素个数 `= k - path.size()`，整理得到：\n\n搜索起点的上界 = n - (k - path.size()) + 1\n\n所以，我们的剪枝过程就是：把 `i <= n` 改成 `i <= n - (k - path.size()) + 1` ：\n\n```java\n// 只有这里 i <= n - (k - path.size()) + 1 与参考代码 1 不同\n        for (int i = index; i <= n - (k - path.size()) + 1; i++) {\n            path.addLast(i);\n            dfs(n, k, i + 1, path, res);\n            path.removeLast();\n        }\n```\n\n方法二：按照每一个数选与不选画出二叉树\n\n![](https://z3.ax1x.com/2021/10/05/4jV1BD.png)\n\n画一个表格更容易看出边界条件。\n\n![](https://z3.ax1x.com/2021/10/05/4jVJNd.png)\n\n```java\npublic class Solution {\n\n    public List<List<Integer>> combine(int n, int k) {\n        List<List<Integer>> res = new ArrayList<>();\n        if (k <= 0 || n < k) {\n            return res;\n        }\n\n        // 为了防止底层动态数组扩容，初始化的时候传入最大长度\n        Deque<Integer> path = new ArrayDeque<>(k);\n        dfs(1, n, k, path, res);\n        return res;\n    }\n\n    private void dfs(int begin, int n, int k, Deque<Integer> path, List<List<Integer>> res) {\n        if (k == 0) {\n            res.add(new ArrayList<>(path));\n            return;\n        }\n\n        // 基础版本的递归终止条件：if (begin == n + 1) {\n        if (begin > n - k + 1) {\n            return;\n        }\n        // 不选当前考虑的数 begin，直接递归到下一层\n        dfs(begin + 1, n, k, path, res);\n\n        // 不选当前考虑的数 begin，递归到下一层的时候 k - 1，这里 k 表示还需要选多少个数\n        path.addLast(begin);\n        dfs(begin + 1, n, k - 1, path, res);\n        // 深度优先遍历有回头的过程，因此需要撤销选择\n        path.removeLast();\n    }\n}\n\n```\n\n## 78 子集\n\n#### [78. 子集](https://leetcode-cn.com/problems/subsets/)\n\n- 组合问题，需要按顺序读字符，就不需要设置used 数组\n- 在根节点、非叶子节点和叶子节点都需要结算，因此 res.add(path)需要放中间\n\n### 法一： 回溯搜索\n\n执行一次深度优先遍历，一路走到底，走不通的时候，返回回来，继续执行，一直这样下去，知道回到起点。\n\n![](https://z3.ax1x.com/2021/10/06/4xkH81.png)\n\n```java\nclass Solution {\n    public List<List<Integer>> subsets(int[] nums) {\n        List<List<Integer>> res = new ArrayList<>();\n        \n        if(nums.length==0){\n            return res;\n        }\n        int n = nums.length;\n        Deque<Integer> path = new ArrayDeque<>();\n\n        dfs(0, n, nums, path, res);\n\n        return res;\n    }\n\n    private void dfs(int begin, int n, int[] nums, Deque<Integer> path, List<List<Integer>> res){\n        if(begin == n){\n            res.add(new ArrayList<>(path));\n            return;\n        }\n        // 当前数可选 也可以不选\n\n        // 不选 直接进入下一层\n        dfs(begin+1, n, nums, path, res);\n        // 选了，进入下一层\n        path.add(nums[begin]);\n        dfs(begin+1, n, nums, path, res);\n        path.removeLast();\n    }\n\n}\n```\n\n把选这个数作为左分支，把不选这个数作为右分支\n\n![](https://z3.ax1x.com/2021/10/06/4xAyZD.png)\n\n```java\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Stack;\n\npublic class Solution {\n\n    public List<List<Integer>> subsets(int[] nums) {\n        List<List<Integer>> res = new ArrayList<>();\n        int len = nums.length;\n        if (len == 0) {\n            return res;\n        }\n        Stack<Integer> stack = new Stack<>();\n        dfs(nums, 0, len, stack, res);\n        return res;\n    }\n\n    private void dfs(int[] nums, int index, int len,\n                     Stack<Integer> stack, List<List<Integer>> res) {\n        if (index == len) {\n            res.add(new ArrayList<>(stack));\n            return;\n        }\n        // 当前数可选，也可以不选\n        // 选了有，进入下一层\n        stack.add(nums[index]);\n        dfs(nums, index + 1, len, stack, res);\n        stack.pop();\n\n        // 不选，直接进入下一层\n        dfs(nums, index + 1, len, stack, res);\n    }\n\n    public static void main(String[] args) {\n        int[] nums = {1, 2, 3};\n        Solution solution = new Solution();\n        List<List<Integer>> subsets = solution.subsets(nums);\n        System.out.println(subsets);\n    }\n}\n\n```\n\n### 方法二：使用位运算技巧\n\n数组的每个元素，可以有两个状态：\n\n- 不在子数组中（用0表示）\n- 在子数组中（用1表示）\n\n从0到2 的数组个数次幂（不包括）的整数的二进制表示就能表示所有状态的集合\n\n子集数量为 $2^n$  ，也就是1 << n\n用二进制0表示不选，1表示选，遍历从0到 $2^n$ 的所有二进制数\n对每个二进制数也就是每种子集情况都要遍历一边nums数组，\n将二进制数中为1的位置在nums中对应的数加入到当前子集中\n\n![](https://z3.ax1x.com/2021/10/06/4xEaTg.png)\n\n```java\nclass Solution {\n    public List<List<Integer>> subsets(int[] nums) {\n        int size = nums.length;\n        int n = 1 << size;\n        \n        List<List<Integer>> res = new ArrayList<>();\n\n        for(int i =0;i<n;i++){\n            List<Integer> cur = new ArrayList<>();\n            for(int j=0;j<size;j++){\n                if( ((i>>j) & 1) ==1){\n                    cur.add(nums[j]);\n                }\n            }\n            res.add(cur);\n        }\n\n        return res;\n    }\n}\n```\n\n## [90. 子集 II](https://leetcode-cn.com/problems/subsets-ii/)\n\n#### [90. 子集 II](https://leetcode-cn.com/problems/subsets-ii/)\n\n解集 **不能** 包含重复的子集\n\n考虑数组 $[1,2,2]$ ，选择前两个数，或者第一、三个数，都会得到相同的子集。\n\n也就是说，对于当前选择的数 $x$ ，若前面有与其相同的数y，且没有选择 y，此时包含的 x 的子集，必然会出现在包含 y 的所有子集中。\n\n我们可以通过判断这种情况，来避免生成重复的子集。代码实现时，可以先将数组排序；迭代时，若发现没有选择上一个数，且当前数字与上一个数相同，则可以跳过当前生成的子集。\n\n```java\nclass Solution {\n    public List<List<Integer>> subsetsWithDup(int[] nums) {\n        List<List<Integer>> res = new ArrayList<>();\n        if(nums.length==0){\n            return res;\n        }\n        int n = nums.length;\n        Deque<Integer> path = new ArrayDeque<>();\n        Arrays.sort(nums);\n        dfs(0, false, n, path, nums, res);\n        return res;\n    }\n\n\n    private void dfs(int begin,boolean choose, int n, Deque<Integer> path, int[] nums, List<List<Integer>> res){\n        if(begin==n){\n            res.add(new ArrayList<>(path));\n            return;\n        }\n\n        dfs(begin+1, false, n,path, nums, res);\n\n        if(begin>0 && nums[begin]==nums[begin-1] && !choose){\n            return;\n        }\n        path.add(nums[begin]);\n        dfs(begin+1, true, n, path, nums, res);\n        path.removeLast();\n    }\n}\n```\n\n时间复杂度：\n\n$O(n \\times 2^n)$ 其中 n 是数组nums 的长度。排序时间复杂度为 $O(nlogn)$ 。最坏情况下nums中无重复元素，需要枚举其所有 $ 2^n$ 个子集，每个子集加入答案时需要拷贝一份，耗时 $O(n)$, 一共需要 $O(n \\times 2^n) + O(n) = O(n\\times 2^n)$ 的实际来构造子集。\n\n空间： $O(n)$ , 临时数组t的空间代价是O(n)， 递归时栈空间的代价为 $O(n)$\n\n## [60. 排列序列](https://leetcode-cn.com/problems/permutation-sequence/)\n\n一句话题解：以下给出两种方法，思路其实是一样的：通过 计算剩余数字个数的阶乘，一位一位选出第 k 个排列的位数。\n\n基于以下几点考虑，\n\n所求排列一定在叶子节点处得到，可以根据已经选定的数的个数，进而计算还未选定的数的个数，然后计算阶乘，就知道这一个分支的叶子节点的个数。\n\n- 如果 k 大于这一个分支将要产生的叶子节点数，直接跳过这个分支，这个操作叫剪枝。\n- 如果 k 小于等于 这个分支将要产生的叶子节点数，那说明所求的全排列一定在这个分支将要产生的叶子节点里，需要递归求解\n\n![](https://i.loli.net/2021/10/08/GvmfoYzpLq3yanM.png)\n\n![](https://i.loli.net/2021/10/08/6nCmOXqcV7ND8KG.png)\n\n![](https://i.loli.net/2021/10/08/JPbKsoE6WkFvq7H.png)\n\n![](https://i.loli.net/2021/10/08/wGPRyatZ4oMgXA8.png)\n\n![](https://i.loli.net/2021/10/08/pZrdiPentQKCafI.png)\n\n![](https://i.loli.net/2021/10/08/nUOdm7P2skCzGr8.png)\n\n```java\nclass Solution {\n    public String getPermutation(int n, int k) {\n        if(n == 0){\n            return \"\";\n        }\n        boolean[] used = new boolean[n+1];\n        StringBuilder path = new StringBuilder();\n        int[] factorial = new int[n+1]; // 阶乘数组\n        factorial[0] = 1;\n        for(int i=1;i<=n ;i++){\n            factorial[i] = factorial[i-1] * i;\n        }\n\n        dfs(0, n ,k, path, used, factorial);\n        return path.toString();\n    }\n\n    // index 在这一步之前已经选择了几个数字，其值恰好是等于这一步需要确定的下标位置\n    private void dfs(int index,int n,int k, StringBuilder path, boolean[] used, int[] factorial){\n        if(index == n){\n            return;\n        }\n\n        // 计算还未确定的数字的全排列的个数，第一次进入的时候是 n-1\n        int cnt = factorial[n-1-index];\n        for (int i=1; i<=n; i++){\n            if(used[i]) continue;\n            if (cnt < k){\n                k -= cnt;\n                continue;\n            }\n            path.append(i);\n            used[i] = true;\n            dfs(index+1, n, k, path, used, factorial);\n            // 注意：不可以回溯（重置变量），算法设计是【一下子来到叶子结点】，没有回头的过程\n            // 这里加return，后面的数没有必要遍历去尝试了\n            return;\n        }\n\n    }\n\n}\n```\n\n## [93. 复原 IP 地址](https://leetcode-cn.com/problems/restore-ip-addresses/)\n\n![](https://i.loli.net/2021/10/09/gPWyb8XIRqJ3ABr.png)\n\n分析剪枝条件：\n\n- 一开始，字符串的长度小于4 或者 大于 12， 一定不能拼凑出合法的ip\n- 每一个节点可以选择截取的法法只有三种，截1位，截2位，截3位，因此灭一个节点可以生长出的分支最多有3条，根据截取出来的字符串判断是否是合理ip，先截取，再转成int判断。\n- 由于ip段最多就4段，因此这颗三叉树最多4层，这个条件作为递归终止条件之一。\n\n```java\nclass Solution {\n    public List<String> restoreIpAddresses(String s) {\n        int n = s.length();\n        List<String> res =  new ArrayList<>();\n        if (n < 4 || n>12){\n            return res;\n        }\n        Deque<String> path = new ArrayDeque<>(4);\n        dfs(0,0, n, path, s, res);\n        return res;\n    }\n\n    private void dfs(int begin, int splitTimes, int n, Deque<String> path, String s, List<String> res){\n        if(begin == n){\n            if(splitTimes == 4){\n                res.add(String.join(\".\",path));\n            }\n            return;\n        }\n\n        // 看到剩下的不够就退出，n-begin 表示剩余的还未分割的字符串的位数\n        if(n - begin < (4-splitTimes) || n - begin > 3*(4-splitTimes)){\n            return;\n        }\n\n        for(int i=0;i<3;i++){\n            if(begin + i >= n){\n                break;\n            }\n            int ipSegment = judgeIFIpSegemnt(s, begin, begin+i);\n            if(ipSegment != -1){\n                // 在判断是ip段的情况下，采取截取\n                path.addLast(String.valueOf(ipSegment));\n                dfs(begin+i+1, splitTimes+1, n, path, s, res);\n                path.removeLast();\n            }\n        }\n\n    }\n\n\n    // 判断s的子区间 [left, right] 是否能构成一个 ip段\n    private int judgeIFIpSegemnt(String s, int left, int right){\n        int len = right - left + 1;\n        // 大于1 位的时候，不能以0开头\n        if(len>1 && s.charAt(left) == '0'){\n            return -1;\n        }\n\n        // 转成int 类型\n        int res = 0;\n        for(int i=left; i<=right; i++){\n            res = res*10 + s.charAt(i) - '0';\n        }\n\n        if(res > 255){\n            return -1;\n        }\n        return res;\n    }\n\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["LeetCode"]},{"title":"Large-scale Extensible User Intent Classification for Dialogue Systems with Meta Lifelong Learning","url":"/2021/10/02/MeLL-Large-scale-Extensible-User-Intent-Classification-for-Dialogue-Systems-with-Meta-Lifelong-Learning/","content":"\n# MeLL: Large-scale Extensible User Intent Classification for Dialogue Systems with Meta Lifelong Learning\n\n用户意图检测（UIC）对于理解他们在对话系统中的需求至关重要。(文本分类)\n\n这是因为不同域中的用户输入可能具有不同的文本分布和目标意图集。随着底层应用程序的发展，新的UIC任务不断涌现。因此，为大规模可扩展UIC开发一个框架至关重要，该框架能够持续适应新任务，并以可接受的参数增长率避免灾难性遗忘。\n\n作者引入Meta Lifelong Learning (MeLL) framework  解决此问题。\n\n在MELL中，基于BERT的文本编码器被用来学习跨任务的健壮文本表示，其被缓慢更新以用于终身学习。\n\n全局和局部记忆网络用来捕获不同类的跨任务原型表示，将其视为元学习者快速适应不同的任务。\n\n此外，应用最近最少使用的替换策略来管理全局记忆，以使模型大小不会随时间爆炸。\n\n最后，每个UIC任务都有自己的特定于任务的输出层，并仔细总结了各种特性。\n\n## INTRODUCTION\n\n### task-wise UIC models 的缺陷\n\n一个天真的方法是训练task-wise UIC模型。这种方法不适合工业规模的应用，有三个原因:\n\n-  模型参数总数不断增加，与任务数成线性关系。考虑到当前预先训练的语言模型具有数十亿个参数，为UIC训练这样的模型很容易涉及数万亿个参数，从而导致参数爆炸问题。\n- 由于跨域的UIC任务有一些相似之处，单任务方法无法从其他任务中学习可转移的知识，这对提高UIC的性能至关重要。(前向后向迁移)\n- 当需要维护越来越多的模型时，这些方法不可避免地带来了工程负担。\n\n另一种流行的方法是跨任务的多任务训练，其中利用共享编码器来捕获共同知识，并且每个任务具有其自己的预测头。当一个新的UIC任务出现时，我们可能需要为以前的任务重新训练模型，这不仅计算量大，而且很难保持现有UIC任务的性能稳定。\n\n###  Liflong learning & Meta learning\n\n近年来，终身学习受到了研究界的广泛关注。它是一种学习范式，它不断积累过去学到的知识，并用它来帮助未来的任务学习。当终身学习应用于大规模EUIC时，当新任务不断到达时，我们只需要维护相对较少的模型参数，从而缓解了参数爆炸效应。\n\n在终身学习中，解决灾难性的遗忘问题是具有挑战性的，即模型在学习新任务时“忘记”如何解决现有任务。这是特别不可取的，因为我们希望在学习新的UIC任务时保持现有任务的性能稳定。\n\n元学习，旨在获得跨任务的元学习者，这样它就可以用很少的数据样本快速适应新任务。通过获取元学习者，可以获取跨不同UIC任务的可迁移知识，并将其传递给新任务。主要的缺点是元学习者(在本文的情况下是BERT模型) 应该分别适应每项任务，无法避免参数爆炸。\n\n因此，一个自然的问题就产生了：是否有可能为大规模EUIC设计一个持续学习框架，使其既能在模型适用于新的UIC任务时保持现有UIC任务的性能，又能在新的UIC任务数量增加时具有可接受的参数增长率？\n\n### MeLL\n\n![](https://z3.ax1x.com/2021/10/02/4Hyvin.png)\n\n它有一个共享的网络结构，用于学习不断增长的UIC任务，由三个部分组成：文本编码器、全局记忆网络和局部记忆网络。\n\n文本编码器是基于BERT构建的，以生成健壮的文本表示(无论是针对用户的查询还是响应)。对于终身表示学习，这些参数被缓慢更新，这确保由新任务调用的更新操作不会对现有任务产生显著的负面影响。\n\n受基于原型的元学习的启发，全局网络存储不同类别的跨任务原型表示\n\n记忆单元的更新速度很快，捕捉到了可跨任务转移的知识，使我们的模型可以很容易地适应新的任务。随着所有任务的不同类别的数量不断增加，使用LRU(最近最少使用)替换策略来管理全局内存，这样大小就不会随着时间的推移而爆炸。\n\n通过注意力机制将文本编码器产生的特征与全局记忆网络进行融合，并利用它们来学习最终的任务特定的UIC分类器。\n\n学习过程完成后，我们将与任务相关的原型表示复制到任务自己的本地内存网络中，并冻结参数。在推理过程中，我们使用文本编码器和任务自身的局部记忆网络进行特征生成。\n\n编码器被认为是不断消化可传递的表示学习知识并将其传递给特定任务的慢速学习者。\n\n全局记忆网络是快速学习器，能够快速编码给定任务的特定知识。\n\nLRU替换策略和从全局存储网络到局部存储网络的复制机制在不增加过多参数的情况下缓解了灾难性遗忘。\n\n## RELATED WORK\n\n### User Intent Classification\n\nUIC的技术最初应用于搜索引擎中的信息搜索，帮助搜索引擎理解用户发送的搜索查询。 由于对话系统通常通过系统和用户之间的交互提供更好的用户体验，对话系统的 UIC 越来越受欢迎。\n\nUIC 也可以表述为一个排名问题。 Intent-Aware Ranking with Transformers (IART) 模型，基于注意力机制考虑查询意图来选择合适的答案。 MeLL 与这些方法的不同之处在于，它考虑在终身学习环境中解决大量 UIC 任务，这对于工业应用至关重要。\n\n### Lifelong Learning\n\n终身学习或持续学习是一种机器学习范式，它侧重于借助先前学习的任务来解决无限的任务序列。开发终身学习算法的一个关键挑战是提高未来任务的性能，同时避免对现有任务的灾难性遗忘。典型的方法包括经验回放[16，20，40]，知识提炼[6，26]，迁移学习[5，19，39]等。在实际的工业应用中，将经验回放应用于大量的历史任务或存储这些训练好的模型进行知识提取代价很高。\n\n在MEL中，同时使用慢速和快速学习器(即文本编码器和全局记忆网络)来将知识从现有任务转移到新任务。\n\n### Meta-learning\n\n元学习的目标是培养能够在训练数据较少的情况下适应各种任务的元学习者。元学习在计算机视觉中得到了广泛的应用，它被认为是一种K-way N-shot few-shot learning problem。典型的应用包括 few-shot 图像分类[25]、目标检测[12]和许多其他应用。元学习在自然语言处理中的应用研究不多，有文献[31，32，34]。\n\n与前人的工作相比，MELL框架不是一个典型的K-way N-shot算法，而是利用元学习的思想来学习文本编码器，该编码器捕获跨任务可转移的知识。全局存储器中使用的快速更新机制和原型表示类似于几个元学习神经网 络[30，43]。\n\n### Pre-trained Language Models.\n\n尽管它们很有效，但现代语言模型的巨大规模给在工业应用程序中的部署带来了巨大的挑战，因为工业应用程序中有许多任务需要解决。在MEL中，通过使用缓慢和快速更新的元学习器来解决这个问题，这些元学习器能够处理越来越多的任务，而不会在模型中引入太多新参数。\n\n## Method\n\n### Overview\n\n$T_n$ 代表第 $n$ 个UIC 任务， $D_n={(x_{n,i}, y_{n,i})}$ 是 $T_n$ 的训练集，其中 $x_{n,i}$ 是第$D_n$ 中的第 $i$个输入样本（ 即输入文本，根据应用程序是用户查询或响应的场景），而 $y_{n,i}$ 是$x_{n,i}$ 的标签。\n\n在大规模的EUIC设置中，考虑了我们面临无线序列的UIC任务 $T_1,T_2,...$ 的情况。在实际应用程序中，最开始我们通常有少量UIC任务可用。\n\n因此让 $T_1,T_2,...T_N$ 是 $N$ 现有的基本UIC任务， $T_{N+1},T_{N+2},..$ 是新UIC任务的无限序列。我们的目标是构建一个学习系统 $F = \\{f_1,f_2,...,f_N,f_{N+1}, f_{N+2},...\\}$ 不断支持为新的UIC任务获取分类器 $f_{N+1},f_{N+2}$ 同时保持现有分类器 $(f_1,f_2,...,f_N)$ 的性能。\n\n更具体的说，在初始阶段，我们给定了$N$ 训练集 $D_1,D_2,...,D_N$ ，N个基础任务来训练多任务UIC模型。之后模型可以自动扩展到无线数量的新任务 $T_{N+1},T_{N+2},...$ 新任务依次到达。\n\n为了缓解灾难性遗忘和参数爆炸的问题， MeLL 的模型结构在训练和推理过程中具有不同的计算图。 总的来说，它有四个主要组成部分：\n\n- Text Encoder\n- Global Memory Network\n- Local Memory Network\n- Task-specific Network\n\n![](https://z3.ax1x.com/2021/10/02/4bS4BT.png)\n\nUIC的特征是由缓慢更新的 TextEncoder 和快速更新的 Global Memory Network 生成的。\n\n在特定任务的训练过程之后，相应的类表示从全局内存复制到特定任务的 Local Memory Network。在推理过程中，用文本编码器的特定于任务的 Local Memory来生成用于预测的特征。\n\n### Text Encoder\n\n使用 Bert作为 作为模型的主干来学习跨任务的输入文本的通用、深度表示。在这里将 $x_{n,i}$ 表示为 $Q(x_{n,i})$。  随着新的 UIC 任务不断到来，BERT参数会缓慢更新以消化多个任务的可迁移知识。\n\n请注意，在终身学习阶段，应将编码器参数的学习率设置为较小的值，以避免灾难性地忘记先前学习的任务。\n\n### Global and Local Memory Networks\n\n全局记忆网络存储 $K$ 个类别表示的 \" slots \" 槽。令 $y_N$ 为跨 $N$ 任务类的集合。即 $y_N = \\cup_{n=1}^N y_n$ ， $K \\ge |y_N|$\n\n在初始学习阶段，我们设置 global memory G 如下：\n\n对于类别标签 $y^{(m)}\\in y_{N}$， 设 $T^{(m)}$ 是涉及 $y^{(m)}$ 的任务的集合。即 $T^{m} = \\{T_n | n\\in \\{1,...,N\\} \\ ,\\  y^{(m)} \\in y_n \\}$\n\n$D_n^{(m)}$ 是 $D_n$ 的子集，$D^{(m)}_n = \\{(x_{n,i},y_{n,i}) \\in D_n |y_{n,i} = y^{(m)}\\} $\n\n类标签 $y^{(m)}$ 的原型表示向量 $G_N^{(m)}$ 为：\n$$\nG_N^{(m)} = \\frac{1}{|T^{(m)}|} \\sum_{T_n\\in T^{(m)}} \\frac{1}{|D_n^{(m)}|}\\sum_{(x_{n,i},y_{n,i})\\in D_n^{(m)}} Q(x_{n,i})\n$$\n$G_N^{(m)}$ 是所有任务 $T^{(m)}$ 的原型向量的平均池化结果。通过聚合所有 $y_N$ 类表示 $G_N^{(m)}$，完成了 $G$ 的初始计算，将其视为多有$N$ 任务的高级表示。\n\n我们进一步考虑终身学习的情况，一般而言，我们假设模型已经训练了 $j-1$ 个任务 $T_1,...,T_{j-1}$ 其中 $j>N$ ,并且有一个新任务 $T_j$ 到达。\n\n对于类 $y^{(m)} \\in y_j$ , 如果对应的类表示 $G_{j-1}^{(m)}$ 存在于 $G$ 中，更新规则如下：\n$$\nG_j^{(m)} = (1-\\gamma) G^{(m)}_{j-1} + \\frac{\\gamma}{|D_j^{(m)}|} \\sum_{(x_{n,i},y_{n,i})\\in D_j^{(m)}}  Q(x_{n,i})\n$$\n其中 $\\gamma \\in (0,1)$ 是预定义的超参数，平衡已有的任务和新任务的相关重要性。\n\nGlobal memory 的大小有 $K$ 的限制。当它满时，删除 $G$ 中最近最少访问的一项。\n\n新插入$G$ 的类别表示计算为 ：\n$$\nG_j^{(m)} = \\frac{1}{|D_j^{(m)}|} \\sum_{(x_{n,i}, y_{n,i})\\in D_j^{(m)}} Q(x_{n,i})\n$$\n应用LRU替换策略，因为任务的主题趋势可能会随着时间的推移而漂移。最近更新的类表示在不久的将来很有可能再次使用。示例如下图：\n\n![](https://z3.ax1x.com/2021/10/02/4bkzQJ.png)\n\n在任务感知学习过程结束时，对于当前任务 $T_n$，复制 $G$ 对应类的表示  $y_n$ 到它自己的本地内存 $L_n$ 用于推理，所有参数都被冻结。因此当全局记忆网络快速更新时，对 $G$ 的更改不会影响对现有任务的推理。\n\n### Feature Fusion and Model Output\n\n在介绍了 $Q(x_{n,i})$ 和 $G$ 的生成后，现在讨论 MeLL中的前向传播。\n\n假设我们正在学习任务 $T_n$， 当前训练实例为$(x_{n,i}, y_{n,i})\\in D_n$。我们使用类标签集 $y_n$ 查询 $G$, 为每个类 $y^{(m)}\\in y_n$ 生成当前类表示 $G_n^{(m)}$ 。 \n\n注意力分数  $ \\alpha ^{(m)}(x_{n,i}) = softmax(Q(x_{n,i})^T \\cdot G_n^{(m)})$\n\n请注意，注意力分数的计算与标准做法略有不同。 发送到task-specific output layer $Att(x_{n,i})$的最终注意力特征集计算如下：\n$$\nAtt(x_{n,i}) = Q(x_{n,i}) + \\sum_{y^{(m)}\\in y_n} \\alpha^{(m)} (x_{n,i}) \\cdot G_n^{(m)}\n$$\n其中$Q(x_{n,i})$ 被视为残差。预测结果由 $\\hat y_{n,i} = f_n(Att(x_{n,i}))$ 给出。\n\n由于输出层、 text encoder 和 global memory 中参数的梯度完全不同，我们通过反向传播更新这些参数。 local memory networks中的参数在反向传播期间不会更新。\n\n### Algorithmic Analysis\n\n![](https://z3.ax1x.com/2021/10/02/4bZRmj.png)\n\n进一步对 MeLL 进行了更深入的分析，重点关注终身学习如何影响计算复杂性。 \n\n假设我们正在学习一个新任务 $T_j (j>N)$。设 $M$ 是BERT编码器中的参数总数， $d$ 是token嵌入的维数。\n\n推导出全局记忆 、$T_j$ 的全局记忆、注意力融合层和 $T_j$ 的输出赠分别具有 参数 $K\\cdot d$ 、 $|y_j|\\cdot d$ 、0 和 $(d+1)|y_j|$ 参数\n\n与所有第一个 𝑗 任务相关的参数总数为 $M+K\\cdot d + (2d+1)\\cdot |\\cup_{n=1}^j y_j|$ ，其中 $M+(d+1)\\cdot |\\cup_{n=1}^j y_j|$ 个参数在反向传播期间是可训练的。\n\n由于 BERT 编码器具有最多的参数，因此越来越多的任务对模型大小的影响很小。 因此，MeLL 成功解决了参数爆炸问题。此外，与具有重放策略的终身学习算法不同 ，我们的方法不需要任何重放操作。 相反，我们采用复制机制和文本编码器的缓慢更新来避免灾难性的遗忘。\n\n## EXPERIMENTS\n\n![](https://z3.ax1x.com/2021/10/02/4b7QSS.png)\n\n### Baselines\n\n- MTL：对所有任务使用多任务微调方法。在此设置中，我们假设所有任务的数据集都可供我们使用，并且不应用终身学习设置。该模型可以产生我们工作中的上界模型性能。\n- Single：为每个任务训练一个BERT分类器。当任务数较大时，不可避免地会出现参数爆炸问题。\n- Lifelong-freeze：首先在𝑁基本任务上使用多任务微调方法。接下来，它冻结BERT编码器，并且只调优每个新任务的特定于任务的输出层。\n- Lifelong-seq: 与“Lifelong-freeze”类似，不同之处在于当新任务到达时，BERT编码器也将以顺序方式进行调优。因此，它可能会遭受灾难性的遗忘问题。\n- Lifelong-replay: 是“Lifelong-seq”的扩展，它使用从先前任务中随机抽样的数据作为经验回放来重新训练先前任务的模型。\n\n![](https://z3.ax1x.com/2021/10/02/4bHu7R.png)\n\n![](https://z3.ax1x.com/2021/10/02/4bL2HH.png)\n\n![](https://z3.ax1x.com/2021/10/02/4bO1VH.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"第五届达观杯——风险事件标签识别比赛复盘","url":"/2021/10/01/第五届达观杯——风险事件标签识别比赛复盘/","content":"\n# 第五届达观杯Rank4——风险事件标签识别比赛复盘\n\n成绩：A榜第5，B榜第4，最终分数分别为：0.62411600、0.58140504\n\n比赛链接：https://www.datafountain.cn/competitions/512\n\n代码：https://github.com/Coding-Zuo/DaguanFengxian\n\n## 赛题任务\n\n这个比赛的全名是叫：基于大规模预训练模型的风险事件标签识别，在技术层面上可以提取为两个任务，一个是预训练一个是文本分类。\n\n针对预训练赛题方给了一个70g的无标注预训练文本，训练集有14009条，测试集6004条（包含AB榜数据）\n\n赛题全部为脱敏数据（所有文字都转换成了数字表达）脱敏前的数据样例为：\n\n|                           资讯文本                           |   风险事件标签    |\n| :----------------------------------------------------------: | :---------------: |\n| 新冠肺炎疫情给美国劳动力市场造成巨大冲击，首次申请失业救济人数已经连续19周超过100万人，约为疫情暴发前平均水平的6倍 | 宏观_经济数据下滑 |\n| 石化公司双苯厂发生爆炸事故，造成大量苯类污染物进入江河水体，造成重大环境污染 |   事故_生产设施   |\n| 市场监管局执法人员对5家品牌奶茶店进行了检查，发现多家门店存在工作人员健康证不齐全、原材料管理不善等问题 |   卫生_食品安全   |\n|                            脱敏后                            |                   |\n| 210 21048 4210 751252 10 21048 4210 75 125210 21048 4210 75125..... |        1-3        |\n\n在标签列可以看到样本有一级和二级标签之分，共有10个一级，35个二级标签。评价指标为macro F1。\n\n我们尝试过一级标签和二级标签的联合预测，效果不好。\n\n标签类别很多而且不平衡，多的类别上千条，少的类别只有十几个：\n\n![](https://z3.ax1x.com/2021/10/01/47Mv7j.png)\n\n接下来我将分别从预训练模型、模型结构、提分技巧、模型融合复盘整个比赛过程。\n\n## 预训练模型\n\n预训练模型百度网盘地址：链接：https://pan.baidu.com/s/1GCs1m6HiXenurGbjUBetFw 提取码：fxth\n\n对应代码部分：https://github.com/Coding-Zuo/DaguanFengxian/tree/main/bert_model/pretrain\n\n运行过程 处理数据 process_data --> 构建词表 build_vocab --> run pretrain\n\n我们在无标注数据中根据cosine距离选择了四万条和训练集中样本相似的数据进行预训练。\n\n分别预训练了bert-base模型 nezha-base模型，nezha与bert的区别主要是\n\nnezha相比于google开源中文bert使用了更大的预训练语料，还使用了相对位置编码是一种有效的位置编码方案，全字掩蔽策略，混合精度训练和LAMB优化器。\n\nnezha首次将函数型的相对位置编码加入了模型中。好处：主要是因为它可以使模型外推到比训练中遇到的序列长的序列长度。Bert针对每个位置合并了绝对位置编码，该绝对位置编码是嵌入向量，并且直接添加到token embedding。\n\n我们对每种模型保存不同训练步数的checkpoint，可以用于后面的模型融合。\n\n其实预训练策略可以做很多花样的文章，但由于机器有限，我们将主要的精力放在了微调方面。预训练策略只是遵循mlm和nsp。\n\n我们主要使用过的预训练模型有：\n\n- Bert-base-wwm-ext : 哈工大开源版本\n- Nezha-wwm-base: 哪吒官方开源版本\n- Bert120k: 预训练12万step\n- Bert150k: 预训练15万step\n- Bert80k: 预训练8万step\n- Nezha80k：预训练8万step\n- Nezha110k：预训练11万step\n- Nezha150k：预训练15万step\n\n最一开始是使用了word2vec在语料库上进行训练，代码：https://github.com/Coding-Zuo/DaguanFengxian/tree/main/baseline/src/classic_models/word2vec 线上第一次提交是 48点多分 排了七十多名。\n\n然后开始使用bert等开源的权重，那么问题来了脱敏数据里词都是那样的，bert词表用不了怎么办？\n\n- 统计脱敏数据的词频，将对应词频与开源词表上的词频进行对换 (最开始使用的是这种) 线上可达50分左右\n- 将word2vec训练好的embedding替换到bert上\n\n虽然无法还原句子，但频率估计可以还原一部分词，两个频率高的文本，在同一种语境下出现的概率更大，从语义相关性角度来说，可能会有一些语义相关性，改用明文后就可以随便用预训练语言模型了。\n\n\n\n\n\n## 模型结构\n\n我们最终的模型结构大致是：\n\nBert  -->  BiLSTM 1层 --> BiGRU 1层 --> bert_pooler + 胶囊网络 --> Multi-Sample Dropout预测输出\n\n同时加BiLSTM和BiGRU大概有接近一个点的提高。胶囊网络有的预训练模型有一点点提高，但有的有负效果。\n\n还尝试过 用 max_pooling + avg_pooling + 胶囊网络 + bert_pooling等组合，效果均不如直接使用bert_pooler和胶囊网络。\n\n## 提分技巧\n\n### 面对不均衡 dice loss & focal loss & cross entropy loss \n\n代码位置：https://github.com/Coding-Zuo/DaguanFengxian/tree/main/bert_model/training\n\n样本不均衡会带来什么问题呢？\n\n模型训练的本质是最小化损失函数，当某个类别的样本数量非常庞大，损失函数的值大部分被样本数量较大的类别所影响，导致的结果就是模型分类会倾向于样本量较大的类别。\n\n通过类别加权Loss解决, 下图截自香侬科技的论文《Dice Loss for Data-imbalanced NLP Tasks》，分别列举了加权loss，Focal loss（FL）和他们提出的dice loss。我们的实验效果是：FL < Weigth CE < dice loss。所以主要采用了weight ce和dice loss。\n\n![](https://i.loli.net/2021/09/01/YkHOMIlVSPjG5aw.png)\n\nWeight CE通过基于类别的加权的方式可以从不同类别的样本数量角度来控制Loss值，从而一定程度上解决了样本不均衡的问题。\n\n基于类别加权Loss虽然在一定程度上解决了样本不均衡的问题，但是实际的情况是不仅样本不均衡会影响Loss，而且样本的难易区分程度也会影响Loss。\n\n何恺明在论文《Focal Loss for Dense Object Detection》中提出了的Focal Loss，上图第三个公式。对于模型预测为正例的样本也就是p>0.5的样本来说，如果样本越容易区分那么(1-p)的部分就会越小，相当于乘了一个系数很小的值使得Loss被缩小，也就是说对于那些比较容易区分的样本Loss会被抑制，同理对于那些比较难区分的样本Loss会被放大，这就是Focal Loss的核心：**通过一个合适的函数来度量简单样本和困难样本对总的损失函数的贡献。**\n\ndice loss香侬科技的这篇论文可以参考：[Dice Loss for Data-imbalanced NLP Tasks](https://coding-zuo.github.io/2021/09/01/Dice-Loss-for-Data-imbalanced-NLP-Tasks/)\n\n交叉熵“平等”地看待每一个样本，无论正负，都尽力把它们推向1（正例）或0（负例）。但实际上，对分类而言，将一个样本分类为负只需要它的概率＜0.5即可，完全没有必要将它推向0。Dice Loss的自适应损失——DSC，在训练时推动模型更加关注困难的样本，降低简单负例的学习度，从而在整体上提高基于F1值的效果。\n\n### 对比学习\n\n代码位置：https://github.com/Coding-Zuo/DaguanFengxian/blob/main/bert_model/models/modeling_nezha1.py\n\n对比损失可以关注判别更困难的样本。\n\nFeature学习是各类深度学习模型的一个基础、重要的功能。好的feature，将有助于文本任务性能的提升。\n\n表示学习的目标是为输入x 学习一个表示 z，那么如何衡量一个表示z 的好坏可以通过互信息的形式；\n\n互信息：代表我们知道了 z 之后 x的信息量减少了多少，\n\nInfoNCE （又称ntxent loss）\n$$\nL_q = - log\\frac{exp(q\\cdot k_+ /\\tau)}{\\sum_{i=0}^K exp(q\\cdot k_i / \\tau)}\n$$\n实质：核心是通过计算样本表示的距离，拉近正样本，拉远负样本\n\n自监督的时候可以自行构造正负样本，那么有监督的时候就可以根据不同的样本标签来构建正负样本。\n\n![](https://z3.ax1x.com/2021/10/03/4LineI.png)\n\n最大化相同标签的样本相似度，让不同样本标签的相似度比较小。\n\n参考论文 《Supervised Contrastive Learning》、《SUPERVISED CONTRASTIVE LEARNING FOR PRE-TRAINED LANGUAGE MODEL FINE-TUNING》\n\n\n\n### 对抗训练\n\n代码位置：https://github.com/Coding-Zuo/DaguanFengxian/blob/main/bert_model/training/Adversarial.py\n\n很多人反映对抗训练没有效果，我最一开始的结果也是这样的。在开源版的nezha和bert上都会降分。\n\n但随着预训练模型越来越多，模型越来越稳定，对抗训练就可以提分了。在预训练后的nezha上基本上是pgd比较好，但比较耗时，在bert上fgm有时会好一点。每个预训练模型的使用效果都不太一样。\n\n我们还尝试了，不仅在bert的word_embedding上做扰动，还在encoder的第0层做扰动，同时随机在某个batch上不扰动，效果相差不多。\n\n在验证集的效果对比：\n\n- Nezha110k_noAdv: 0.5598\n- Nezha110k_fgm: 0.5639\n- Nezha110k_pgd: 0.5687\n- Bert80k_noAdv: 0.5542\n- Bert80k_fgm:0.5557\n- Bert80k_pgd:0.5650\n- Bert80k_fgm_advEncoder_random:0.5585\n- Bert80k_pgd_advEncoder_random:0.5684\n\n\n\n### Multi-Exit\n\n代码位置：https://github.com/Coding-Zuo/DaguanFengxian/blob/main/bert_model/models/modeling_bert_pabee.py\n\nBert 究竟在哪一层做输出会比较好呢？下图是在nezha80k上进行的实验，普遍发现在第layer9，也就是第10层的输出下普遍较好。其实实验下来发现整体效果不好就放弃了，但后来想想可能是因为12层输出联合训练导致的F1值偏低。其实发现第10层可能比较好，就干脆只用第十层的输出计算loss就好。但后来没有继续尝试。\n\n![](https://z3.ax1x.com/2021/10/03/4qBq9x.png)\n\n\n\n### flooding洪泛法\n\n在最开始使用开源未经预训练的bert进行探索的过程中发现，验证集loss上升，acc也上升。但随着预训练模型的越来越稳定，这种现象就不存在了。\n\n![](https://z3.ax1x.com/2021/10/03/4qwGKe.png)\n\n这种现象很常见，原因是过拟合或者训练验证数据分布不一致导致，即在训练后期，预测的结果趋向于极端，使少数预测错的样本主导了loss，但同时少数样本不影响整体的验证acc情况。ICML2020发表了一篇文章：《[Do We Need Zero Training Loss After Achieving Zero Training Error?](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2002.08709.pdf)》，描述了上述现象出现的原因，同时提出了一种flooding策略，通过超参数b控制训练loss不要过小，阻止进一步过拟合，在此情况下，使model\"random walk\"至一个泛化能力更好的结果，参考 [我们真的需要把训练集的损失降到零吗？](https://wmathor.com/index.php/archives/1551/) 。上图左是加洪泛之前， 上图右是加洪泛之后的，训练集验证集每轮的loss。超参数b的值大概0.2左右小一些。对于模型效果来说，整体影响不大，训练的稍微稳定一点，比赛后期没有再用。\n\n\n\n### Multi-sample Dropout\n\n代码位置：https://github.com/Coding-Zuo/DaguanFengxian/blob/main/bert_model/models/layers.py\n\ndropout目前是NLP任务中很流行的数据扩充手段。Multi-Sample Dropout是对Dropout方法的一种改进，是2019年的一篇工作。Multi-Sample Dropout相比于dropout加快了模型训练过程的收敛速度和提高了泛化能力。\n\n![](https://z3.ax1x.com/2021/10/03/4L7NfP.png)\n\n假设样本经过网络的编码层部分进行编码后得到一个向量表征。这时候，传统的Dropout会对向量表征作用一次，然后输入到分类层进行预测。而Multi-sample Dropout由多个Dropout操作完成。对一个向量表征进行多次dropout后，相当于形成了向量表征的多个版本的。这些不同版本的向量表征通过分类器得到标签的不同概率预测，最终的预测概率分布通过求和或者平均得到。\n\n在比赛的实验中发现，dropout的数量为4，聚合的方式以加和的形式比average效果要好。dropout_rate最开始设为0.4。但后来慢慢发现有时，模型训着训着F1直接变成0了，而且只在bert模型上出现这种问题。找了几天原因发现dropout_rate不能设的太大，改成了0.2。\n\n\n\n### 伪标签\n\n关于伪标签，我个人认为总体指标达不到八十以上的比赛可能不太好管用。尤其这个赛题还是样本极其不均匀的就更不适合。因为第一，模型预测的把握度不大，根据我们线上59分的模型，预测概率为百分之40以上的测试集数据不到1500条，这在伪标签准确度上带来了不确定性。第二样本不均匀，如果直接把这1500条插入到训练集，可能会破坏训练集的一些分布，造成模型不稳定，学跑偏了。\n\n测试结果：线上58.7的模型，在伪标签上重新训练后是58.3分。\n\n\n\n## 模型融合\n\n代码位置： https://github.com/Coding-Zuo/DaguanFengxian/tree/main/bert_model/ensemble\n\n### stacking\n\n![](https://z3.ax1x.com/2021/10/03/4qrZz6.png)\n\n跑了四折的四种预训练模型的stacking。最后的第二层预测使用的是xgboost，整体效果没有达到预期，线上得分仅0.5707\n\n四折的四种模型效果如下：\n\n![](https://z3.ax1x.com/2021/10/03/4qrqOO.png)\n\n效果不佳的原因可能和拆分四折的数据分布有关，导致单模分数不是很高。由于样本不均衡，原先的拆分方法是针对不同类别有放回的随机取样做五折，随机性比较大，不容易过拟合。\n\n为了让模型凑齐所有训练集的预测特征，且不让数据有重复，我使用了无放回的采样，针对不同类别的样本，按顺序分段提取每折样本，并且根据数据id去了一遍重。 在实验的时候发现不同折的数据分布对模型效果影响还蛮大的。\n\n### 投票+rank/概率平均\n\n投票在这次比赛效果非常好。\n\n第一次融七个模型，模型平均分大概五十四五。\n\n- 投票线上结果：0.5809\n- 投票，针对票数相同的结果，选择结果在每个模型的预测rank最靠前的作为结果：0.5852\n- 投票，针对票数相同的结果，选择每个预测结果的概率平均值最大的作为结果：0.5850\n- 根据七个模型的logits选最大的作为预测结果：0.5549\n- 根据预测的概率加和取平均的线上结果：0.5618\n\n模型平均分大概57.5分左右\n\n- 投票+rank ：0.6201\n\n最后将所有线上得分超过60分的测试集结果再放到一起，再进行投票得到最后的最终成绩：0.6241\n\n\n\n## 参考\n\n[短文本匹配Baseline：脱敏数据使用预训练模型的尝试](https://kexue.fm/archives/8213)\n\nhttps://github.com/823316627bandeng/2021-Daguan-Cup\n\n[【2021 第五届“达观杯” 基于大规模预训练模型的风险事件标签识别】1 初赛Rank12的总结与分析](https://zhuanlan.zhihu.com/p/412897603/)\n\n[模型融合—— stacking详细讲解](https://blog.csdn.net/u011630575/article/details/81302994)\n\n[对比学习（Contrastive Learning）](https://zhuanlan.zhihu.com/p/141172794?ivk_sa=1024320u)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["DataGame"]},{"title":"22-括号生成(回溯&深搜)","url":"/2021/09/30/22-括号生成-回溯-深搜/","content":"\n# 22-括号生成(回溯&深搜)\n\n#### [22. 括号生成](https://leetcode-cn.com/problems/generate-parentheses/)\n\n这一类问题是在一棵隐式的树上求解，可以用深度优先遍历，也可以用广度优先遍历。\n一般用深度优先遍历。原因是：\n\n- 代码好写，使用递归的方法，直接借助系统栈完成状态的转移；\n- 广度优先遍历得自己编写结点类和借助队列。\n\n这里的「状态」是指程序执行到 隐式树 的某个结点的语言描述，在程序中用不同的 变量 加以区分。\n\n## 深度优先遍历\n\n减法\n\n![](https://z3.ax1x.com/2021/09/30/4IKLu9.png)\n\n画图以后，可以分析出的结论：\n\n- 当前左右括号都有大于 0 个可以使用的时候，才产生分支；\n- 产生左分支的时候，只看当前是否还有左括号可以使用；\n- 产生右分支的时候，还受到左分支的限制，右边剩余可以使用的括号数量一定得在严格大于左边剩余的数量的时候，才可以产生分支；\n- 在左边和右边剩余的括号数都等于 0 的时候结算。\n\n\n\n```java\npublic class Solution {\n\n    // 做减法\n\n    public List<String> generateParenthesis(int n) {\n        List<String> res = new ArrayList<>();\n        // 特判\n        if (n == 0) {\n            return res;\n        }\n\n        // 执行深度优先遍历，搜索可能的结果\n        dfs(\"\", n, n, res);\n        return res;\n    }\n\n    /**\n     * @param curStr 当前递归得到的结果\n     * @param left   左括号还有几个可以使用\n     * @param right  右括号还有几个可以使用\n     * @param res    结果集\n     */\n    private void dfs(String curStr, int left, int right, List<String> res) {\n        // 因为每一次尝试，都使用新的字符串变量，所以无需回溯\n        // 在递归终止的时候，直接把它添加到结果集即可，注意与「力扣」第 46 题、第 39 题区分\n        if (left == 0 && right == 0) {\n            res.add(curStr);\n            return;\n        }\n\n        // 剪枝（如图，左括号可以使用的个数严格大于右括号可以使用的个数，才剪枝，注意这个细节）\n        if (left > right) {\n            return;\n        }\n\n        if (left > 0) {\n            dfs(curStr + \"(\", left - 1, right, res);\n        }\n\n        if (right > 0) {\n            dfs(curStr + \")\", left, right - 1, res);\n        }\n    }\n}\n```\n\n\n\n如果我们不用减法，使用加法，即 `left` 表示「左括号使用了几个」，`right` 表示「右括号使用了几个」，可以画出另一棵递归树。\n\n![](https://z3.ax1x.com/2021/09/30/4IMFud.png)\n\n```java\npublic class Solution {\n\n    // 做加法\n\n    public List<String> generateParenthesis(int n) {\n        List<String> res = new ArrayList<>();\n        // 特判\n        if (n == 0) {\n            return res;\n        }\n\n        dfs(\"\", 0, 0, n, res);\n        return res;\n    }\n\n    /**\n     * @param curStr 当前递归得到的结果\n     * @param left   左括号已经用了几个\n     * @param right  右括号已经用了几个\n     * @param n      左括号、右括号一共得用几个\n     * @param res    结果集\n     */\n    private void dfs(String curStr, int left, int right, int n, List<String> res) {\n        if (left == n && right == n) {\n            res.add(curStr);\n            return;\n        }\n\n        // 剪枝\n        if (left < right) {\n            return;\n        }\n\n        if (left < n) {\n            dfs(curStr + \"(\", left + 1, right, n, res);\n        }\n        if (right < n) {\n            dfs(curStr + \")\", left, right + 1, n, res);\n        }\n    }\n}\n\n```\n\n\n\n## 回溯算法与深度优先遍历\n\n回溯法 采用试错的思想，它尝试分步去解决一个问题。在分步解决问题的过程中，当它通过尝试发现现有的分步答案不能得到有效的正确的解答的时候，它将取消上一步甚至是上几步的计算，再通过其他的可能的分布解答再次尝试寻找问题的答案。回溯法 通常用递归方法来实现，在反复重复上述的步骤之后可能出现两种情况：\n\n- 找到一个可能存在的答案\n- 在尝试了所有可能的分步方法后宣告该问题没有答案\n\n**深度优先搜索** 算法（Depth-First-Search，DFS）是一种用于遍历或搜索树或图的算法。这个算法会尽可能深的搜索树的分支。当节点v的所在边都已被探寻过，搜索将回溯到发现节点v的那条边的起始节点。这一过程一直进行到已发现从源节点可达的所有节点位置。如果还存咋未发现的节点，则选择其中一个作为源节点并重复以上过程，整个进程反复进行指导所有节点都被访问为止。\n\n「回溯算法」与「深度优先遍历」都有「不撞南墙不回头」的意思。「回溯算法」强调了「深度优先遍历」思想的用途，用一个 **不断变化** 的变量，在尝试各种可能的过程中，搜索需要的结果。强调了 回退操作 对于搜索的合理性。而「深度优先遍历」强调一种遍历的思想，与之对应的遍历思想是「广度优先遍历」。至于广度优先遍历为什么没有成为强大的搜索算法，我们在题解后面会提。\n\n在「力扣」第 51 题的题解《[回溯算法（第 46 题 + 剪枝）](https://leetcode-cn.com/problems/n-queens/solution/gen-ju-di-46-ti-quan-pai-lie-de-hui-su-suan-fa-si-/)》 中，展示了如何使用回溯算法搜索 4 皇后问题的一个解，相信对直观地理解「回溯算法」是有帮助。\n\n## 搜索与遍历\n\n我们每天使用的搜索引擎帮助我们在庞大的互联网上搜索信息。搜索引擎的「搜索」和「回溯搜索」算法里「搜索」的意思是一样的。\n\n搜索问题的解，可以通过 遍历 实现。所以很多教程把「回溯算法」称为爆搜（暴力解法）。因此回溯算法用于 搜索一个问题的所有的解 ，通过深度优先遍历的思想实现。\n\n## 与动态规划的区别\n\n共同点：用于求解多阶段决策问题。多阶段决策问题即：\n\n- 求解一个问题分为很多步骤(阶段)\n- 每个步骤(阶段) 可以有多种选择\n\n不同点：\n\n- 动态规划只需要求我们评估的最优解是多少，最优解对应的具体解是什么并不要求。因此 很适合用于评估一个方案的效果。\n- 回溯算法可以搜索 得到所有的方案(当然也包括最优解)，但是本质上它是一种遍历算法，时间复杂度很高。\n\n## 从全排列问题开始理解回溯算法\n\n我们尝试在纸上写 3 个数字、4 个数字、5 个数字的全排列，相信不难找到这样的方法。以数组 `[1, 2, 3]` 的全排列为例。\n\n- 先写以1开头的全排列，他们是: $[1,2,3],[1,3,2]$，即1 + $[2,3]$的全排列 （注意：递归结构体现在这里）\n- 再写以2开头的全排列，他们是: $[2,1,3],[2,3,1]$  ,即 `2` + `[1, 3]` 的全排列；\n- 最后写以 3 开头的全排列，它们是：`[3, 1, 2], [3, 2, 1]`，即 `3` + `[1, 2]` 的全排列。\n\n总结搜索的方法： 按顺手枚举每一位可能出现的情况，已经选择的数字在当前要选择的数字中不能出现。按照这种策略就能做到不重不漏。这样的思路可以用一个树形结构表示。\n\n![](https://z3.ax1x.com/2021/10/01/4Ts5cD.png)\n\n说明：\n\n- 每一个节点表示了求解全排列问题的不同阶段 ，这些阶段通过变量的 「不同的值」体现，这些变量的不同的值，称之为「状态」；\n\n- 使用深度优先遍历有「回头」的过程，在「回头」以后， 状态变量需要设置成为和先前一样 ，因此在回到上一层结点的过程中，需要撤销上一次的选择，这个操作称之为「状态重置」；\n\n- 深度优先遍历，借助系统栈空间，保存所需要的状态变量，在编码中只需要注意遍历到相应的节点的时候，状态变量的值是正确的，具体做法是：往下走一层的时候，path遍历在尾部追加，而往回走的时候，需要撤销上一次的选择，也是在尾部操作， 因此path变量是一个栈\n\n- 深度优先遍历通过回溯操作，实现了全局使用一份状态变量的效果\n\n  \n\n使用编程的方法得到全排列，就是在这样的一个树形结构中完成 **遍历**，从树的根结点到叶子结点形成的路径就是其中一个全排列。\n\n## 设计状态变量\n\n- 首先这棵树除了根节点和叶子结点以外，每一个结点做的事情其实是一样的，即：在已经选择了一些数的前提下，在剩下的还没有选择的数中，依次选择一个数，这样显然是一个递归的结构。\n- 递归的终止条件是：一个排列的数字已经选够了，因此外卖需要一个变量来表示当前程序递归到第几层，外卖把这个变量叫做depath，或者index，表示当前要确定的是某个全排列中下标为index 的那个数是多少。\n- 布尔数组 used，初始化的时候都为false 表示这些数还没有被选择，当我们选定一个数的时候，就将这个数组的对应位置设为 true，这样在考虑下一个位置的时候，就能以 $O(1)$ 的时间复杂度判断这个数是否被选择过，这是一种以空间换时间的思想。\n\n这些变量称为「状态变量」，它们表示了在求解一个问题的时候所处的阶段。需要根据问题的场景设计合适的状态变量。\n\n## 代码实现\n\n#### [46. 全排列](https://leetcode-cn.com/problems/permutations/)\n\n```java\npublic class Solution{\n  \tpublic List<List<Integer>> permute(int[] nums){\n      \tint len = nums.length;\n        // 使用一个动态数组保存所有可能的全排列\n        List<List<Integer>> res = new ArrayList<>();\n        if (len == 0){\n            return res;\n        }\n        boolean[] used = new boolean[len];\n        List<Integer> path = new ArrayList<>();\n        dfs(nums, len, 0, path, used, res);\n        return res;\n    }\n  \tprivate void dfs(int[] nums, int len, int depth, List<Integer> path, boolean[] used,\n                    List<List<Integer>> res){\n      \tif(depth == len){\n          \tres.add(path);\n        }\n      \t// 在非叶子节点处，产生不同的分支，这一操作的语义是：在还未选择的数中依次选择一个元素作为下一个位置的元素，这显然是通过一个循环实现。\n      \tfor(int i=0;i<len;i++){\n          \tif(!used[i]){\n              \tpath.add(nums[i]);\n              \tused[i]=true;\n              \tdfs(nums,len,depth+1,path,used,res);\n              \t//注意：下面这两行代码发生回溯，回溯发生从在深层节点回到浅层节点的过程，代码在形式上和递归前是对称的\n              \tused[i] = false;\n              \tpath.remove(path.size() - 1);\n            }\n        }\n    }\n  \n  \tpublic static void main(String[] args){\n      \tint[] nums={1,2,3};\n      \tSolution solution = new Solution();\n      \tList<List<Integer>> lists = solution.permute(nums);\n      \tSystem.out.println(lists);\n    }\n}\n\n```\n\n```python\nclass Solution:\n    def permute(self, nums):\n        size = len(nums)\n\n        if len(nums) == 0:\n            return []\n        used = [False for _ in range(size)]\n        res = []\n\n        def dfs(nums, size, depth, path, used, res):\n            if depth == size:\n                res.append(path)\n                return\n\n            for i in range(size):\n                if not used[i]:\n                    used[i] = True\n                    path.append(nums[i])\n                    dfs(nums, size, depth + 1, path, used, res);\n                    used[i] = False\n                    path.pop()\n\n        dfs(nums, size, 0, [], used, res)\n        return res\n\nif __name__ == '__main__':\n    nums = [1, 2, 3]\n    solution = Solution()\n    res = solution.permute(nums)\n    print(res)\n```\n\n执行 `main` 方法以后输出如下：\n\n```\n[[], [], [], [], [], []]\n```\n\n原因出现在递归终止条件这里：\n\n```java\nif (depth == len) {\n    res.add(path);\n    return;\n}\n```\n\n```python\nif depth == size:\n\t\tres.append(path)\n    return \n```\n\n变量path 所指向的列表在深度优先遍历过程中只有一份，深度优先遍历完成以后，回到了根节点，成为空列表\n\n在java中，参数传递是值传递，对象类型变量在传参的过程中，复制的是变量的地址。这些地址被添加到res变量，但实际上指向的是同一块内存地址，因此我们会看到6个空的列表对象。解决办法很简单，在 res.add(path)这里做一次拷贝即可。\n\n```java\nif(depth == len){\n  \tres.add(new ArrayList<>(path));\n  \treturn;\n}\n```\n\n```python\nif depth == size:\n  \tres.append(path[:])\n    return\n```\n\n复杂度分析：\n\n回溯算法由于其遍历的特点，时间复杂度一般都比较高，有些问题分析起来很复杂。一些问题剪枝剪得好的话，复杂度会降的很低，因此分析最坏时间复杂度的意义不是很大。但视情况而定\n\n时间复杂度 $O(N\\times N!)$\n\n非叶子结点的个数，一次为(按层数来)：\n$$\n1+A_N^1 + A_N^2 +...+A_N^{N-1} = 1 + \\frac{N!}{(N-1)!} + \\frac{N!}{(N-2)!} +...+N!\n$$\n1是根节点，在第一层，节点个数为 N个选一个的排列 故为$A_N^1$\n\n空间复杂度： $O(N\\times N!)$\n\n递归树深度 $logN$ ，全排列个数 $N!$ , 每个全排列占空间$N$ 。取较大者\n\n\n\n### 为什么不是广度优先遍历\n\n- 首先是正确性，只有遍历状态空间，才能得到所有符合条件的解，这一点 BFS 和 DFS 其实都可以；\n- 在深度优先遍历的时候，不同状态之间的切换很容易 ，可以再看一下上面有很多箭头的那张图，每两个状态之间的差别只有 11 处，因此回退非常方便，这样全局才能使用一份状态变量完成搜索；\n- 如果使用广度优先遍历，从浅层转到深层，状态的变化就很大，此时我们不得不在每一个状态都新建变量去保存它，从性能来说是不划算的；\n- 如果使用广度优先遍历就得使用队列，然后编写结点类。队列中需要存储每一步的状态信息，需要存储的数据很大，真正能用到的很少 。\n- 使用深度优先遍历，直接使用了系统栈，系统栈帮助我们保存了每一个结点的状态信息。我们不用编写结点类，不必手动编写栈完成深度优先遍历。\n\n\n\n\n\n做题的时候，建议 先画树形图 ，画图能帮助我们想清楚递归结构，想清楚如何剪枝。拿题目中的示例，想一想人是怎么做的，一般这样下来，这棵递归树都不难画出。\n\n在画图的过程中思考清楚：\n\n- 分支如何产生；\n- 题目需要的解在哪里？是在叶子结点、还是在非叶子结点、还是在从跟结点到叶子结点的路径？\n- 哪些搜索会产生不需要的解的？例如：产生重复是什么原因，如果在浅层就知道这个分支不能产生需要的结果，应该提前剪枝，剪枝的条件是什么，代码怎么写？\n\n\n\n## 练习\n\n### 题型一：排列、组合、子集相关问题\n\n提示：这部分练习可以帮助我们熟悉「回溯算法」的一些概念和通用的解题思路。解题的步骤是：先画图，再编码。去思考可以剪枝的条件， 为什么有的时候用 used 数组，有的时候设置搜索起点 begin 变量，理解状态变量设计的想法。\n\n- [46. 全排列（中等）](https://leetcode-cn.com/problems/permutations/)\n\n- [47. 全排列 II（中等）](https://leetcode-cn.com/problems/permutations-ii/)：思考为什么造成了重复，如何在搜索之前就判断这一支会产生重复；\n\n- [39. 组合总和（中等）](https://leetcode-cn.com/problems/combination-sum/)\n\n- [40. 组合总和 II（中等）](https://leetcode-cn.com/problems/combination-sum-ii/)\n\n- [77. 组合（中等）](https://leetcode-cn.com/problems/combinations/)\n\n- [78. 子集（中等）](https://leetcode-cn.com/problems/subsets/)\n\n- [90. 子集 II（中等）](https://leetcode-cn.com/problems/subsets-ii/)：剪枝技巧同 47 题、39 题、40 题；\n\n- [60. 第 k 个排列（中等）](https://leetcode-cn.com/problems/permutation-sequence/)：利用了剪枝的思想，减去了大量枝叶，直接来到需要的叶子结点；\n\n- [93. 复原 IP 地址（中等）](https://leetcode-cn.com/problems/restore-ip-addresses/)\n\n\n\n### 题型二：Flood Fill\n\n提示：Flood 是「洪水」的意思，Flood Fill 直译是「泛洪填充」的意思，体现了洪水能够从一点开始，迅速填满当前位置附近的地势低的区域。类似的应用还有：PS 软件中的「点一下把这一片区域的颜色都替换掉」，扫雷游戏「点一下打开一大片没有雷的区域」。\n\n下面这几个问题，思想不难，但是初学的时候代码很不容易写对，并且也很难调试。我们的建议是多写几遍，忘记了就再写一次，参考规范的编写实现（设置 visited 数组，设置方向数组，抽取私有方法），把代码写对。\n\n- [733. 图像渲染（Flood Fill，中等）](https://leetcode-cn.com/problems/flood-fill/)\n\n- [200. 岛屿数量（中等）](https://leetcode-cn.com/problems/number-of-islands/)\n\n- [130. 被围绕的区域（中等）](https://leetcode-cn.com/problems/surrounded-regions/)\n\n- [79. 单词搜索（中等）](https://leetcode-cn.com/problems/word-search/)\n\n说明：以上问题都不建议修改输入数据，设置 `visited` 数组是标准的做法。可能会遇到参数很多，是不是都可以写成成员变量的问题，面试中拿不准的记得问一下面试官\n\n### 题型三：字符串中的回溯问题\n\n提示：字符串的问题的特殊之处在于，字符串的拼接生成新对象，因此在这一类问题上没有显示「回溯」的过程，但是如果使用 StringBuilder 拼接字符串就另当别论。\n在这里把它们单独作为一个题型，是希望朋友们能够注意到这个非常细节的地方。\n\n- [17. 电话号码的字母组合（中等）](https://leetcode-cn.com/problems/letter-combinations-of-a-phone-number/)，[题解](https://leetcode-cn.com/problems/letter-combinations-of-a-phone-number/solution/hui-su-sou-suo-wu-xian-shi-hui-su-yan-du-you-xian-/)；\n\n- [784. 字母大小写全排列（中等）](https://leetcode-cn.com/problems/letter-case-permutation/)；\n\n- [22. 括号生成（中等）](https://leetcode-cn.com/problems/generate-parentheses/) ：这道题广度优先遍历也很好写，可以通过这个问题理解一下为什么回溯算法都是深度优先遍历，并且都用递归来写。\n\n### 题型四：游戏问题\n\n回溯算法是早期简单的人工智能，有些教程把回溯叫做暴力搜索，但回溯没有那么暴力，回溯是有方向地搜索。「力扣」上有一些简单的游戏类问题，解决它们有一定的难度，大家可以尝试一下。\n\n- [51. N 皇后（困难）](https://leetcode-cn.com/problems/n-queens/)：其实就是全排列问题，注意设计清楚状态变量，在遍历的时候需要记住一些信息，空间换时间；\n\n- [37. 解数独（困难）](https://leetcode-cn.com/problems/sudoku-solver/)：思路同「N 皇后问题」；\n\n- [488. 祖玛游戏（困难）](https://leetcode-cn.com/problems/zuma-game/)\n\n- [529. 扫雷游戏（困难）](https://leetcode-cn.com/problems/minesweeper/)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["LeetCode"]},{"title":"Efficient lifelong learning with A-GEM","url":"/2021/09/28/EFFICIENT-LIFELONG-LEARNING-WITH-A-GEM/","content":"\n# Efficient lifelong learning with A-GEM\n\n从样本复杂度、计算和内存成本方面研究了当前终身学习方法的效率。\n\n首先引入了一个新的、更现实的评估协议，学习者只观察每个例子一次，超参数选择是在一个小的、不相交的任务集上完成的，不用于实际的学习体验和评估 .\n\n其次，引入了一个新的度量标准来衡量学习者获得一项新技能的速度。\n\n第三，提出了 GEM 的改进版本，称为平均 GEM (A-GEM)，它具有与 GEM 相同甚至更好的性能，同时在计算和内存效率方面几乎与 EWC 和其他基于正则化的方法一样。\n\n最后，包括A-GEM在内的所有算法，如果提供指定所考虑的分类任务的任务描述符，则可以更快地学习。\n\n\n\n## Learning protocol\n\n一种新的学习范式，即学习者对一组与实际用于评估的任务集不相交的任务进行交叉验证。在这种情况下，学习者将必须学习并将在一个全新的任务序列上进行测试，并且它将仅在该数据流上执行一次。\n\n以前关于终身学习的工作采用直接从监督学习中借用的学习范式。 有 T 个任务，每个任务由训练集、验证集和测试集组成。 在训练期间，学习者根据需要对每个任务的数据进行尽可能多的传递。\n\n此外，通过根据交叉验证网格搜索所需的次数扫描整个任务序列来调整验证集上的超参数。最后，使用由先前交叉验证过程选择的模型在每个任务的测试集上报告度量。\n\n由于当前范式违反了我们对 LLL 的更严格定义，即学习者只能对数据进行一次传递，因为我们想强调从数据中快速学习的重要性，因此现在引入了一种新的学习范式。\n\n我们考虑由以下有序数据集序列描述的两个任务流：\n\n$D^{CV}=\\{D_1,...,D_{T^{CV}}\\}$  和 $D^{EV} = \\{D_{T^{CV}+1},...,D_T\\}$\n\n其中 $D_k = \\{(x_i^k,t_i^k,y_i^k)_{i=1}^{n_k}\\}$  是第k个任务的数据集。 $T^{CV}<T$ 在实验中 $T^{CV}=3,T=20$\n\n我们假设所有数据集都来自相同的任务分布。 为了避免符号混乱，让上下文指定 $D_k$ 是指第 k 个数据集的训练集或测试集。\n\n$D^{CV}$是交叉验证期间将使用的数据集的流。$D^{CV}$允许学习者出于模型超参数选择的目的多次重放所有样本。\n\n相反，$D^{EV}$是用于测试集的最终训练和评估的实际数据集。学习者将观察 $D^{EV}$ 中的训练样本一次且仅一次，并且所有指标都将在 $D^{EV}$的测试集上报告。\n\n<img src=\"https://z3.ax1x.com/2021/09/28/4WN3Nt.png\" style=\"zoom:150%;\" />\n\n## Metrics\n\n引入了一个新的度量标准来衡量学习速度，它有助于量化一个学习算法学习一个新任务的能力—— Learning Curve Area (LCA)\n\n每个任务的训练数据集 $D_k$ 由总共 $B_k$个小批次组成。在每次呈现一小批任务 $k$ 之后，使用相应的测试集来评估学习者在所有任务上的表现。\n\n设 $\\alpha_{k,i,j}\\in [0,1]$是用任务 $k$ 的第 $i$ 个小批次训练模型后，在任务 $j$ 的测试集上评估的准确度。\n\n假设连续体中的第一个学习任务由1索引 ( $T^{CV}+1$ 对应于 $D^{EV}$ ) ,  $T$ 的最后一个(对于 $D^{CV}$ ，将是 $T^{CV}$)，我们定义了以下指标：\n\n### Average Accuracy\n\n$A\\in [0,1]$ 用所有小批量连续训练模型后的平均精度，直到任务k被定义为：\n$$\nA_k = \\frac{1}{k} \\sum_{j=1}^k \\alpha_{k, B_{k},j}\n$$\n$A_T$ 是所有任务的平均准确率，是最后一个任务学习后得到的。这是LLL中最常用的度量。\n\n### Forgetting Measure\n\n$F \\in [-1,1]$  模型经过所有小批量连续训练后的平均遗忘，直到任务 k 被定义为：\n$$\nF_k = \\frac{1}{k-1} \\sum_{j=1}^{k-1} f_j^k\n$$\n其中 $f_j^k$ 是在使用所有小批量训练模型直到任务 k 并计算为：\n$$\nf_j^k = max_{l\\in \\{1,...,k-1\\}} \\alpha_{l,B_l,j} - \\alpha_{k, B_k, j}\n$$\n在学习完所有任务后测量遗忘很重要，原因有两个。它量化了过去任务的准确率下降，并给出了模型学习新任务的速度的间接概念，因为健忘的模型几乎没有剩余的知识可以转移，特别是如果新任务与其中一个任务更密切相关 。\n\n### Learning Curve Area\n\n$LCA \\in [0,1]$ 让我们首先定义模型在所有 T 任务训练后的平均 b-shot 性能（其中 b 是小批量数）：\n$$\nZ_b = \\frac{1}{T} \\sum_{k=1}^T \\alpha_{k,b,k}\n$$\n$\\beta$ 处的 LCA 是作为  $b \\in [0, \\beta]$ 函数的收敛曲线 $Z_b$ 的面积：\n$$\nLCA_{beta} = \\frac{1}{\\beta+1} \\int_0^{\\beta} Z_b db = \\frac{1}{\\beta+1} \\sum_{b=0}^{\\beta} Z_b\n$$\nLCA 有一个直观的解释。 $LCA_0$ 是平均 0-shot 表现，与 GEM 中的前向转移相同。 $LCA_{\\beta}$  是  $Z_b$ 曲线下的区域，如果 0-shot 性能好并且学习器学习速度快，则该区域很高。 特别是，可能有两个模型具有相同的 $Z_b$ 或 $A_T$ ，但 $LCA_{\\beta}$ 非常不同，因为一个模型比另一个学习得快得多，而它们最终都获得了相同的最终精度。 该度量旨在区分这两种情况，并且对于相对较小的 $\\beta$ 值是有意义的，因为我们对从少数示例中学习的模型感兴趣。\n\n\n\n\n\n## Averaged gradient episodic memory (A-GEM)\n\nA-GEM 建立在 GEM  的基础上，该算法利用小的情节记忆在单遍设置中表现良好，并对损失函数提出了一个小的改变，使 GEM 在 训练时间，同时保持相似的表现； \n\n### 回顾GEM\n\nGEM在计算和内存成本方面很高。GEM通过为每个任务 $k$ 存储情节记忆 $M_k$ 来避免灾难性遗忘。\n\n在最小化当前任务 t 的损失的同时，GEM 将任务 k<t 的情景记忆的损失视为不等式约束，避免其增加但允许其减少。\n$$\nl (f_{\\theta}, M_k) = \\frac{1}{|M_k|} \\sum_{(x_i,k,y_i)\\in M_k} l(f_{\\theta}(x_i,k),y_i) \n$$\n这有效地允许 GEM 进行其他 LLL 方法不支持的 积极反向迁移。 形式上，在任务 t，GEM 解决以下目标：\n$$\nminimize_{\\theta} \\ \\ l (f_{\\theta}, D_t)  \\ \\ s.t. \\ \\ l(f_{\\theta}^{t-1}, M_k) \\ \\ \\ \\ \\forall k<t\n$$\n其中 $f_{\\theta}^{t-1}$ 是训练到任务 $t−1$ 的网络，为了检查损失的增加，GEM 计算先前任务的损失梯度 $g_k$ 向量与当前任务 $g$ 的梯度更新之间的角度。只要与任何 $g_k$ 的角度大于 90°，它就会将建议的梯度投影到 L2 范数梯度 $\\hat g$ 中最接近的，从而使角度保持在边界内。 形式上，GEM 解决的优化问题由下式给出：\n$$\nminimize_{\\hat g} \\ \\ \\frac{1}{2}||g-\\hat g||_2^2  \\ \\ \\ s.t. \\ \\ <\\hat g,g_k> \\ge0  \\ \\ \\ \\forall k<t\n$$\n这是一个 凸优化中二次规划的问题， P 变量（网络中的参数数量）中的二次规划 (QP)，对于神经网络而言，可能是数百万。为了有效地解决这个问题，GEM 在对偶空间中工作，这导致只有 t − 1 个变量的更小的 QP：\n$$\nminimize_v \\ \\  \\frac{1}{2}v^TGG^Tv + g^TG^Tv \\ \\ s.t. \\ \\ v\\ge0\n$$\n其中 $G = -(g_1,...,g_{t-1}) \\in R^{(t-1)\\times P}$ 是在训练的每个梯度步骤计算的。一旦上式找到最优解 $v^{*}$， 投影梯度可以计算为：$\\hat g =G^Tv^{*} + g$\n\n虽然 GEM 已被证明在单个 epoch 设置中非常有效，但性能提升的前提是训练时的计算负担很大。在每个训练步骤中，GEM使用情景记忆中的所有样本来计算矩阵 $G$ ，并且它还需要求解QP。当M的大小和任务数量很大时，此内循环优化变得令人望而却步。\n\n![](https://z3.ax1x.com/2021/09/28/4fSh5T.png)\n\n### A-GEM\n\nGEM 确保在每个训练步骤中，每个单独的先前任务的损失（由情景记忆中的样本近似）不会增加\n\n而 A-GEM 试图确保在每个训练步骤中，先前任务的平均情景记忆损失不会增加。 形式上，在学习任务 t 时，A-GEM 的目标是：\n$$\nminimize_{\\theta} \\ \\ \\ l(f_{\\theta}, D_t) \\ \\ \\ s.t. \\ \\ l(f_{\\theta}, M) \\le l (f_{\\theta}^{t-1}, M) \\ \\ where \\ M =\\cup_{k<t} M_{k}\n$$\n相应的优化问题简化为：\n$$\nminimize_{\\hat g} \\ \\ \\frac{1}{2}||g-\\hat g||_2^2 \\ \\ s.t. \\ \\ \\hat g^Tg_{ref} \\ge 0\n$$\n 其中 $g_{ref}$ 是使用从情节记忆 $(x_{ref},y_{ref}) \\sim M$ 中随机采样的批次计算得出的梯度。\n\n换句话说，A-GEM 用单个约束替换了 GEM 的 t-1 约束，其中 $g_{ref}$ 是从情景记忆的随机子集计算出的先前任务的梯度的平均值。\n\n现在可以非常快速地解决上面方程的约束优化问题； 当梯度 g 违反约束时，它通过以下方式进行投影：\n$$\n\\hat g = g- \\frac{g^Tg_{ref}}{g^{T}_{ref}g_{ref}} g_{ref}\n$$\n这使得 A-GEM 不仅内存高效，因为它不需要存储矩阵 G，而且比 GEM 快几个数量级，因为 \n\n- 1）不需要计算矩阵 G 而只需要计算内存样本的随机子集的梯度 \n\n- 2）它不需要解决任何 QP，只需要解决一个内积\n\n- 3）它会产生更少的违背约束，特别是当任务数量很大时。 \n\n  所有这些因素一起使 A-GEM 更快，同时不会妨碍其在单程设置中的良好性能。\n\n### 证明推导 A-GEM update rule\n\n给出了A-GEM更新规则 $\\hat g = g- \\frac{g^Tg_{ref}}{g^{T}_{ref}g_{ref}} g_{ref}$ 的证明\n\nA-GEM的优化目标:\n$$\nminimize_{\\hat g} \\ \\ \\frac{1}{2}||g-\\hat g||_2^2 \\ \\ s.t. \\ \\ \\hat g^Tg_{ref} \\ge 0\n$$\n将 $\\hat g$ 替换为 $z$，并重写：\n$$\nminimize_{\\hat g} \\ \\ \\frac{1}{2} z^z -g^z \\ \\ s.t. \\ -z^Tg_{ref}\\le 0\n$$\n请注意，目标中丢弃了项 $g^Tg$，并更改了不等式约束的符号。 上面定义的约束优化问题的拉格朗日可以写成：\n$$\nL(z,\\alpha) = \\frac{1}{2} z^Tz - g^z - \\alpha z^Tg_{ref}\n$$\n方程的对偶：\n$$\n\\theta_D(\\alpha) = min_z L(z,\\alpha)\n$$\n通过将 $L(z, \\alpha)$ 相对于 z 的导数设置为零来找到最小化 $L(z, \\alpha)$ 的值 $z^∗$：\n$$\n\\nabla_z L(z,\\alpha) = 0 \\ ,\\ z^* = g+\\alpha g_{ref}\n$$\n代入 $z^∗$ 值后的简化对偶:\n$$\n\\theta_D(\\alpha) = \\frac{1}{2} (g^Tg +2\\alpha g^Tg_{ref} +\\alpha^2 g^Tg_{ref} ) - g^Tg-2\\alpha g^Tg_{ref} - \\alpha^2g_{ref}^Tg_{ref} \\\\\n= -\\frac{1}{2}g^Tg - \\alpha g^Tg_{ref} - \\frac{1}{2}^2g_{ref}^Tg_{ref}\n$$\n对偶的解 $\\alpha^* = max_{\\alpha;\\alpha>0} \\theta_{D}(\\alpha)$ ：\n$$\n\\nabla_{\\alpha} \\theta_D(\\alpha)  = 0 \\ ,\\ \\alpha^* = -\\frac{g^Tg_{ref}}{g^T_{ref}g_{ref}}\n$$\n通过将$\\alpha^*$放入上式中，A-GEM更新规则：\n$$\nz^{*} = g- \\frac{g^Tg_{ref}}{g^T_{ref} g_{ref}} = \\hat g\n$$\n\n\n\n\n\n\n## Joint embedding model using compositional task descriptors\n\n在这一部分中，将讨论如何改进包括A-GEM在内的所有LLL方法的前向迁移。\n\n为了加速新任务的学习，我们考虑使用组合任务描述符，其中组件在任务之间共享，从而允许迁移。\n\n例如，组合任务描述符的示例是所考虑任务的自然语言描述或指定要在任务中识别的对象的属性值的矩阵。\n\n如果模型已经学习并记住了两个独立的属性(例如，羽毛的颜色和喙的形状)，则它可以在提供指定其属性(黄色羽毛和红色喙)的值的描述符的情况下快速识别新的类，尽管这是完全不可见的组合。\n\n借鉴小样本学习文献中的思想，我们学习了图像特征和属性嵌入之间的联合嵌入空间。\n\n形式上，让 $x^k \\in X$ 是输入（例如，图像），$t^k$ 是大小为 $C_k × A$ 的矩阵形式的任务描述符，其中 $C_k$ 是第 k 个任务中的类数，A 是 数据集中每个类的属性总数。 联合嵌入模型由特征提取模块 $\\phi_{\\theta}: x^k \\to \\phi_{\\theta}(x^k)$，其中 $\\phi_{\\theta}(x^k)\\in R^D$ 和任务嵌入模块 $\\psi_w :t^k \\to \\psi_w(t^k)$，其中 $\\psi_w(t^k)\\in R^{C_k\\times D}$ 组成 。\n\n在这项工作中，$\\phi_{\\theta}(.)$ 被实现为标准的多层前馈网络，而 $\\psi_w(.)$ 被实现为维度 A × D 的参数矩阵。这个矩阵可以解释为一个属性查找表，因为每个属性都与一个 D 维向量相关联，通过类中存在的属性的线性组合，从中构建类嵌入向量；\n\n\n\n任务描述符嵌入然后是任务中存在的类的嵌入向量的串联。 在训练期间，通过最小化交叉熵损失来学习参数 θ 和 ω：\n$$\nl_k(\\theta, w) = \\frac{1}{N} \\sum_{i=1}^N - log(p(y_i^k| x_i^k,t^k;\\theta,w))\n$$\n其中 $(x_i^k, t^k, y_i^k)$ 是第刻个任务的第i个样本， 如果 $y_i^k=c$ 概率分布为：\n$$\np(c|x_i^k ,t^k;\\theta, w) = \\frac{exp([\\phi_{\\theta}(x_i^k) \\psi_w(t^k)^T]_c) }{\\sum_j exp([\\phi_{\\theta}(x_i^k) \\psi_w(t^k)^T]_j)}\n$$\n其中 $[a]_i$ 表示向量 a 的第 i 个元素。 请注意，架构和损失函数是通用的，不仅适用于 A-GEM，还适用于任何其他 LLL 模型（例如，基于正则化的方法）。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"14-最长公共前缀(二分、分治)","url":"/2021/09/25/14-最长公共前缀-二分、分治/","content":"\n# 14-最长公共前缀(二分、分治)\n\n#### [14. 最长公共前缀](https://leetcode-cn.com/problems/longest-common-prefix/)\n\n## 法一：横向扫描\n\n用 $LCP(S_1,...,S_n)$ 表示字符串 $S_1,...,S_n$的最长公共前缀，可得\n$$\nLCP(S_1,...,S_n) = LCP(LCP (LCP(S_1,S_2), S_3),...,S_n)\n$$\n基于该结论可得到一种查找字符串数组中的最长前缀的简单方法。依次遍历字符串中的每个字符串。对于每个遍历到的字符串，更新最长公共前缀，当遍历完所有的字符串以后，即可得到字符串数组中的最长公共前缀。\n\n![](https://i.loli.net/2021/09/25/nxdtFq4bUIczGSC.png)\n\n如果在尚未遍历完所有的字符串时，最长公共前缀已经是空串，则最长公共前缀一定是空串，因此不需要继续遍历\n\n```java\nclass Solution {\n    public String longestCommonPrefix(String[] strs) {\n        if(strs == null || strs.length==0){\n            return \"\";\n        }\n\n        String prefix = strs[0];\n        int count = strs.length;\n\n        \n        for(int i = 1;i < count;i++){\n            prefix = longestCommonPrefix(prefix, strs[i]); \n            if(prefix.length() == 0){\n                break;\n            }\n        }\n        return prefix;\n    }\n\n    public String longestCommonPrefix(String str1, String str2){\n        int length = Math.min(str1.length(), str2.length());\n        int index = 0;\n        while(index<length && str1.charAt(index) == str2.charAt(index)){\n            index++;\n        }\n        return str1.substring(0,index);\n    }\n\n}\n```\n\n```python\nclass Solution:\n    def longestCommonPrefix(self, strs: List[str]) -> str:\n        if not strs:\n            return \"\"\n\n        prifix , count = strs[0], len(strs)\n\n        for i in range(1,count):\n            prifix = self.lcp(prifix, strs[i]);\n            if not prifix:\n                break\n        return prifix\n\n    def lcp(self, str1,str2):\n        length, index = min(len(str1), len(str2)), 0\n        while index < length and str1[index] == str2[index]:\n            index+=1\n        return str1[:index]\n```\n\n复杂度：\n\n- 时间$O(mn)$ m是字符串数组中的字符串的平均长度，n是字符串的数量。最坏情况下，字符串数组中的每个字符串的每个字符都会被比较一次。\n- 空间 $O(1)$\n\n## 法二：纵向扫描\n\n方法一是横向扫描，依次遍历每个字符串，更新最长公共前缀。另一种方法是纵向扫描。\n\n纵向扫描时，从前往后遍历所有字符串的每一列，比较相同列上的字符是否相同。\n\n如果相同则继续对下一列进行比较，如不相同则当前列不再属于公共前缀，当前列之前的部分为最长。\n\n```java\nclass Solution {\n    public String longestCommonPrefix(String[] strs) {\n        if(strs==null || strs.length==0){\n            return \"\";\n        }\n        int length = strs[0].length();\n        int count = strs.length;\n\n        for(int i=0;i<length;i++){\n            char c = strs[0].charAt(i);\n            for(int j=1;j<count;j++){\n                if(i==strs[j].length() || strs[j].charAt(i)!=c){\n                    return strs[0].substring(0,i);\n                }\n            }\n        }\n        return strs[0];\n    }\n}\n```\n\n复杂度分析\n\n- 时间复杂度 $O(mn)$ 其中 m 是字符串数组中的字符串的平均长度，n是字符串的数量\n- 空间 $O(1)$\n\n## 方法三：分治\n\n注意到 LCP 的计算满足结合律，有以下结论：\n$$\nLCP(S_1, ..., S_n) = LCP(LCP(S_1,...,S_k), LCP(S_{k+1},...,S_n))\n$$\n其中$LCP(S_1,...,S_n)$ 是字符串 $S_1,...,S_n$ 的最长公共前缀，$1<k<n$\n\n基于上述结论，可以使用分治法得到字符串数组中的最长公共前缀。对于问题 $LCP(S_i,...,S_j)$，可以分解成两个子问题 $LCP(S_i,...,S_{mid})$ 与$LCP(S_{mid+1},...,S_j)$\n\n其中 $mid = \\frac{i+j}{2}$ 对两个子问题分别求解，然后对两个子问题的解计算最长公共前缀，即原问题的解。\n\n![](https://i.loli.net/2021/09/26/QpVbrwdZINPjcty.png)\n\n```java\nclass Solution {\n    public String longestCommonPrefix(String[] strs) {\n        if(strs==null || strs.length==0){\n            return \"\";\n        }\n\n        return longestCommonPrefix(strs, 0, strs.length-1);\n    }\n\n\n    public String longestCommonPrefix(String[] strs, int start, int end){\n        if(start == end){\n            return strs[start];\n        }\n        int mid = (end - start) / 2 + start;\n        String lcpLeft = longestCommonPrefix(strs, start, mid);\n        String lcpRight = longestCommonPrefix(strs, mid+1, end);\n        \n        int minLength = Math.min(lcpLeft.length(), lcpRight.length());\n        for(int i=0;i<minLength;i++){\n            if(lcpLeft.charAt(i) != lcpRight.charAt(i)){\n                return lcpLeft.substring(0,i);\n            }\n        }\n        return lcpLeft.substring(0,minLength);\n    }\n\n\n}\n```\n\n时间复杂度：\n\n- 时间复杂度：$O(mn)$ 其中m 是字符串数组中的字符串的平均长度，n是字符串的数量。\n\n  时间复杂度的递推式是 $T(n) = 2\\cdot T(n/2) + O(m)$， 通过计算可得 $T(n) = O(mn)$\n\n- 空间复杂度 $O(mlogn)$ ，n为字符串数量。空间复杂度取决于递归调用的层数，层数最大为 logn，每层需要 m 的空间存储返回结果。\n\n##  方法四：二分查找\n\n虽然，最长公共前缀的长度不会超过字符串数组中的最短字符串的长度。\n\n用 minLength 表示字符串数组中的最短字符串长度，则可以再 $[0,minLength]$ 的范围内通过二分查找找到最长公共前缀的长度。每次取查找范围的中间值mid， 判断每个字符串的长度为mid 的前缀是否相同，如果相同则最长公共前缀的长度一定大于或等于 mid， 如果不相同则最长公共前缀的长度一定小于mid， 通过上述方式将查找范围缩小一半，直到得到最长公共前缀的长度。\n\n![](https://i.loli.net/2021/09/26/32HvnS78gisxEcI.png)\n\n```java\nclass Solution {\n    public String longestCommonPrefix(String[] strs) {\n        if(strs==null || strs.length==0){\n            return \"\";\n        }\n\n        int minLength = Integer.MAX_VALUE\n        for(String str:strs){\n            minLength = Math.min(minLength, str.length());\n        }\n        int low = 0, high=minLength;\n        while(low < high){\n            int mid = (high - low + 1) /2 + low;\n            if(isCommonPrefix(strs, mid)){\n                low = mid;\n            }else{\n                high = mid-1;\n            }\n        }\n        return strs[0].substring(0,low);\n    }\n\n    public boolean isCommonPrefix(String[] strs, int length){\n        String str0 = strs[0].substring(0, length);\n        int count = strs.length;\n        for(int i=1;i<count;i++){\n            String str = strs[i];\n            for(int j=0;j<length;j++){\n                if(str0.charAt(j) != str.charAt(j)){\n                    return false;\n                }\n            }\n        }\n        return true;\n    }\n\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["LeetCode"]},{"title":"凸优化一","url":"/2021/09/24/凸优化一/","content":"\n# 凸优化一\n\n从一个可行解的集合中，寻找出最优的元素\n\n任何一个优化问题都可以写成这个形式：\n$$\nminimize \n$$\n","tags":["Convex optimization"]},{"title":"Gradient Episodic Memory for Continual Learning","url":"/2021/09/24/Gradient-Episodic-Memory-for-Continual-Learning/","content":"\n# Gradient Episodic Memory for Continual Learning\n\n人工智能的一个主要障碍是模型在不忘记先前获得的知识的情况下，更快地解决新问题的能力很差。\n\n首先，提出了一套度量标准来评估在数据连续体上学习的模型。这些度量不仅通过它们的测试准确性来表征模型，而且还根据它们在任务之间传输知识的能力来表征模型。\n\n其次，提出了一个持续学习的模型，称为梯度情节记忆(GEM)，它可以减轻遗忘，同时允许知识有益地转移到以前的任务中。\n\n## Intro\n\n有监督学习的设置 $D_{tr} = \\{(x_i,y_i)\\}_{i=1}^n$ , 其中每个示例 $(x_i，y_i)$ 由特征向量 $x_i\\in X$ 和目标向量 $y_i\\in Y$组成。\n\n大多数监督学习方法假设每个示例 $(x_i，y_i)$ 是来自描述单个学习任务的固定概率分布 $P$ 的独立同分布(IID)样本。目标是构造一个模型 $f:X \\rightarrow Y$ ，用于预测目标向量 $y$ 与未见的特征向量 $x$ ,其中$(x,y) \\sim P$\n\n为了实现这一点，监督学习方法通常采用 经验风险最小化 (ERM) 原则 [Vapnik, 1998]，其中 f 是通过最小化这个式子：\n$$\n\\frac{1}{|D_u|} \\sum_{x_i,y_i \\in D_u} l(f(x_i), y_i)\n$$\n在实践中，ERM通常需要多次遍历训练集。[经验风险最小化(Empirical Risk Minimization)](https://zhuanlan.zhihu.com/p/103786559)\n\n将逐个样本地观察数据的连续体：\n$$\n(x_1,t_1,y_1), ...,(x_i,t_i,y_i),...,(x_n,t_n,y_n)\n$$\n$t_i \\in T$ 该任务描述符标识与 $(x_i，y_i)\\sim P_{t_i}$ 相关联的任务。\n\n重要的是，样本不是从三元组 $(x，t，y)$上的固定概率分布中提取的，因为在切换到下一任务之前可以观察到来自当前任务的整个示例序列。\n\n连续学习的目标是构造一个能够预测与测试对 $(x，t)$ 相关的目标 $y$ 的模型 $f: X\\times T$，其中 $(x,y)\\sim P_t$。\n\n在这种情况下，ERM面临着未知的挑战：\n\n- Non-iid input data : 数据连续体相对于任何固定概率分布 $P(X,T,Y)$ 不是独立同分布的，因为一旦任务切换，就可以观察到来自新任务的整个样本序列。\n- Catastrophic forgetting : 学习新任务可能会损害学习者在以前解决的任务中的表现。 \n- Transfer learning :  当连续体中的任务相关时，就存在迁移学习的机会。这将转化为更快地学习新任务，以及提高旧任务的性能。\n\n## A Framework for Continual Learning\n\n连续体的数据三元组 $(x_i,t_i,y_i)$ 由特征向量 $x_i\\in X_{t_i}$ , 任务描述符 $t_i\\in T$ , 目标向量 $y_i\\in Y_{t_i}$\n\n为简单起见，我们假设连续体是局部 iid，即每个三元组 $(x_i，t_i，y_i)$ 都满足 $(x_i,y_i) \\sim^{iid} P_{t_i}(X,Y)$\n\n目标是学习一个预测器 $f:X\\times T \\to Y$，它可以在任何时候被查询以预测与测试对 $(x，t)$ 相关联的目标向量 $y$，其中$(x,y)\\sim P_t$。这样的测试对可以属于我们在过去观察到的任务，可以属于当前的任务，也可以属于我们将在未来体验的任务。\n\n### Task descriptors\n\n框架中的一个重要组成部分是任务描述符  $t_1, . . . , t_n \\in T$ 。在最简单的情况下，任务描述符是整数 $t_i = i \\in Z$，枚举出现在数据连续集中的不同任务。更一般地说，任务描述符 $t_i$ 可以是结构化对象，例如一段自然语言，解释如何解决第 $i$ 个任务。丰富的任务描述符为  zero-shot learning 提供了机会，因为可以单独使用新的任务描述符来推断任务之间的关系。此外，任务描述符消除了类似学习任务的歧义。特别是，相同的输入 $x_i$ 可能出现在两个不同的任务中，但需要不同的目标。任务描述符可以引用多个学习环境的存在，或提供有关每个示例的附加（可能是分层的）上下文信息。然而，在本文中，作者专注于减轻从连续数据中学习时的灾难性遗忘，并将 zero-shot learning  留给未来的研究。\n\n### Training Protocol and Evaluation Metrics\n\n大多数关于学习一系列任务的文献描述了一种设置\n\n- i) 任务数量 很小\n- ii) 每个任务的示例数量很大\n- iii) 学习者对每个任务的样本执行多次传递\n- iv) 报告的唯一指标是所有任务的平均性能。 \n\n相比之下，本文:\n\n- i) 任务数量很大，\n- ii) 每个任务的训练样本数量很少\n- iii) 学习者只观察与每个任务相关的样本一次\n- iv) 报告了衡量转移和遗忘的指标。\n\n除了监控其跨任务的表现外，评估模型传递知识的能力也很重要。更具体地说：\n\n- Backward transfer (BWT) ：学习任务 t 对前一任务 k ≺ t 的性能的影响。当学习任务 t 时，提高了先前任务 k 的性能(存在正面的反向迁移)。当学习任务 t 会降低先前任务 k 的性能时(存在负面的反向迁移)。 越大意味着灾难性遗忘越严重。\n  $$\n  \\frac{1}{T} \\sum_{i=1}^T R_{T,i}\\frac{1}{T-1} \\sum_{i=1}^{T-1} R_{T,i} - R_{i,i}\n  $$\n  \n\n- Forwardtransfer(FWT) ：学习任务t 对未来任务 k>t 的性能的影响。 $\\hat b$是每个任务在随机初始化时的测试精度向量。\n  $$\n  \\frac{1}{T-1} \\sum_{i=2}^{T-1} R_{i-1,i} - \\hat b_i\n  $$\n  \n\n- Retained Accuracy(RA) 是模型在训练结束时跨任务的平均准确率。\n  $$\n   \\frac{1}{T} \\sum_{i=1}^T R_{T,i}\n  $$\n  \n\n讨论第一个任务的向后转移或最后一个任务的正向转移是没有意义的。\n\n考虑为每一个 $T$ 任务访问测试集。在模型学习完任务 $t_i$之后，我们评估了它在所有 $T$ 个任务上的测试性能。通过这样做，我们构造了矩阵 $R \\in R^{T\\times T}$，其中 $R_{i,j}$ 是在观察到来自任务 $t_i$ 的最后一个样本之后，模型对任务 $t_j$ 的测试分类精度。\n\n### Gradient of Episodic Memory (GEM)\n\n梯度情景记忆（GEM），一种持续学习的模型。 GEM 的主要特征是情景记忆 $M_t$，它存储来自任务 t 的观察示例的子集。 为简单起见，我们假设整数任务描述符，并使用它们来索引情节记忆。 当使用整数任务描述符时，不能期望显着的正向转移（zero-shot learning）。 相反，我们专注于通过有效使用情景记忆来最小化负向后迁移（灾难性遗忘）。\n\n实际上，学习者总共有 $M$ 个存储单元的预算。如果总任务数 $T$ 已知，我们可以为每个任务分配 $m=M/T$个存储器。如果总任务数 $T$ 未知，我们可以在观察新任务时逐渐减小 $m$ 值\n\n为简单起见，假设内存中填充了来自每个任务的最后 $m$ 个示例，尽管可以采用更好的内存更新策略(例如为每个任务构建核心重置)。在下文中，我们考虑由 $\\theta \\in R^P$参数化的预测因子 $f_\\theta$，并将第k个任务的记忆损失定义为:\n$$\nl(f_{\\theta} ,M_k) = \\frac{1}{|M_k|} \\sum_{(x_i,k,y_i)\\in M_k} l(f_{\\theta}(x_i,k),y_i)\n$$\n显然，将当前示例中的损失与上式一起最小化会导致过度拟合存储在 $M_k$中的示例。作为另一种选择，我们可以通过蒸馏的方法来保持过去任务的预测不变 ---- iCaRL: Incremental classifier and representation\n\n然而，这将认为正向后向转移是不可能的。 相反，我们将使用 上面的损失 作为不等式约束，避免它们的增加但允许它们的减少。 与最先进的 [Kirkpatrick et al., 2017, Rebuffi et al., 2017] 相比，我们的模型允许正向后向转移。\n\n更具体地说，在观察三元组 $(x,t,y)$时，我们解决了以下问题：\n$$\nminimize_{\\theta}  \\ \\ \\ l(f_{\\theta}(x,t) ,y)\n$$\n\n$$\n\\text{subject to (受制于)}  \\ \\ \\ l(f_{\\theta}, M_k) \\le l(f_{\\theta}^{t-1},M_k) \\ \\  for \\ all \\ k\\lt t\n$$\n\n其中 $f_{\\theta}^{t-1}$ 是任务 t−1学习结束时的预测器状态。\n\n首先，在每次参数更新g之后，只要我们保证以前任务的损失不增加，就没有必要存储旧的预测值 $f_{\\theta}^{t-1}$。\n\n其次，假设函数是局部线性的（因为它发生在小优化步骤周围）并且memory代表过去任务的样本，我们可以通过计算它们的损失梯度向量之间的角度来 ，判断先前任务损失的增加 和建议的更新。在数学上，我们将上面式子的约束重新表述为：\n$$\n<g, g_k> := <\\frac{\\partial l(f_{\\theta}(x,t),y)}{\\partial \\theta} , \\frac{\\partial l(f_{\\theta},M_k)}{ \\partial\\theta}> \\ge 0 , \\ for \\ all \\ k <t\n$$\n如果满足上面的不等式，则参数更新 $g$ 不太可能增加先前任务的损失。\n\n另一方面，如果违反了一个或多个不等式约束，那么至少有一个先前的任务在参数更新后损失会增加。 如果违反不等式，我们建议将梯度 g 投影到满足所有上式约束 的最近梯度  $\\hat g$（以平方 $l2$ 范数表示）。 因此：\n$$\nminimize_{\\hat g}  \\ \\ \\ \\frac{1}{2} ||g-\\hat g||^2_2\n$$\n\n$$\n\\text{subject to (受制于)}  \\ \\ \\ <\\hat g,g_k> \\ge 0 \\ for \\ all \\ k <t\n$$\n\n为了有效地解决上式，回想一下具有不等式约束的二次规划Quadratic Program （QP）的原始值：\n\n[凸优化笔记(3)Quadratic Programming简介](https://zhuanlan.zhihu.com/p/36081404)\n$$\nminimize_{z} \\ \\ \\frac{1}{2} z^TCz + p^Tz\n$$\n\n$$\n\\text{subject to (受制于)}  \\ \\ \\ Az\\ge b\n$$\n\n其中$C\\in R^{p\\times p}, p\\in R^p,A\\in R^{(t-1)\\times p} ,b\\in R^{t-1}$ , 上式的对偶问题是：\n$$\nminimize_{u,v} \\ \\ \\frac{1}{2} u^TCu -b^Tv\n$$\n\n$$\n\\text{subject to (受制于)}  \\ \\ \\ A^Tv -Cu=p \\ ,\\ v\\ge 0\n$$\n\n 如果 $(u^⋆,v^⋆)$是上个式子的解，则存在解 $z^⋆$ 满足 $Cz^⋆=Cu^⋆$\n\n有了这些符号，我们将原始GEM QP 写成：\n$$\nminimize_{z} \\ \\ \\frac{1}{2} z^Tz -g^Tz + \\frac{1}{2}g^Tg\n$$\n\n$$\n\\text{subject to (受制于)}  \\ \\ \\ Gz \\ge 0\n$$\n\n其中 $G = -(g_1,...,g_{t-1})$ 并且去掉常数项 $g^Tg$ 。这是 p 个变量（神经网络的参数数量）上的 QP，可以以数百万计。 但是，我们可以将 GEM QP 的对偶假设为：\n$$\nminimize_{v} \\ \\ \\frac{1}{2} v^TGG^Tv + g^TG^Tv\n$$\n\n$$\n\\text{subject to (受制于)}  \\ \\ \\ v \\ge 0\n$$\n\n由于 $u  = G^Tv + g$ 并且 $g^T g$ 是常数，这是在 $t − 1 ≪ p$ 个变量上的 QP，即目前观察到的任务数量。一旦我们解决了 $v^*$ 的对偶问题(上式) ，我们就可以将投影梯度更新恢复为 $\\hat g = G^Tv^* + g$。 在实践中，我们发现添加一个小的常数 $\\gamma \\ge 0 $ 到 $v^⋆$ 会使梯度投影偏向于有利于有益向后转移的更新。\n\n![](https://i.loli.net/2021/09/24/bDAQzaEYd6gPOIy.png)\n\n\n\n## 实验\n\n![](https://i.loli.net/2021/09/24/gDpRaMANdbhT9kc.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"12,13-整数互换罗马数字","url":"/2021/09/24/12-13-整数互换罗马数字/","content":"\n# 12,13-整数互换罗马数字\n\n#### [12. 整数转罗马数字](https://leetcode-cn.com/problems/integer-to-roman/)\n\n```java\nclass Solution {\n    public String intToRoman(int num) {\n        int[] values={1000,900,500,400,100,90,50,40,10,9,5,4,1};\n        String[] rom={\"M\",\"CM\",\"D\",\"CD\",\"C\",\"XC\",\"L\",\"XL\",\"X\",\"IX\",\"V\",\"IV\",\"I\"};\n        StringBuilder sb=new StringBuilder();\n        \n        for(int i=0;i<values.length;i++){\n            while(num>=values[i]){\n                sb.append(rom[i]);\n                num-=values[i];\n            }\n        }\n\n        return sb.toString();\n    }\n  \n  \n}\n```\n\n\n\n#### [13. 罗马数字转整数](https://leetcode-cn.com/problems/roman-to-integer/)\n\n通常情况下，罗马数字中小的数字在大的数字的右边。若输入的字符串满足该情况，那么可以将每个字符视作一个单独的值，累加每个字符对应的数值即可。\n\n例如 $\\texttt{XXVII}$ 可视作 X+X+V+I+I=10+10+5+1+1=27。\n\n若存在小的数字在大的数字的左边的情况，根据规则需要减去小的数字。对于这种情况，我们也可以将每个字符视作一个单独的值，若一个数字右侧的数字比它大，则将该数字的符号取反。\n\n例如 XIV 可视作 X−I+V=10−1+5=14。\n\n\n\n```java\nclass Solution {\n    Map<Character, Integer> symbolValues = new HashMap<Character, Integer>() {{\n        put('I', 1);\n        put('V', 5);\n        put('X', 10);\n        put('L', 50);\n        put('C', 100);\n        put('D', 500);\n        put('M', 1000);\n    }};\n\n    public int romanToInt(String s) {\n        int ans = 0;\n        int n = s.length();\n        for (int i = 0; i < n; ++i) {\n            int value = symbolValues.get(s.charAt(i));\n            if (i < n - 1 && value < symbolValues.get(s.charAt(i + 1))) {\n                ans -= value;\n            } else {\n                ans += value;\n            }\n        }\n        return ans;\n    }\n}\n\n```\n\n","tags":["LeetCode"]},{"title":"10-正则表达式","url":"/2021/09/23/10-正则表达式/","content":"\n# 10-正则表达式\n\n#### [10. 正则表达式匹配](https://leetcode-cn.com/problems/regular-expression-matching/)\n\n## 定义状态\n\n- 定义动态数组$dp[m][n]$ （n为字符串p的长度+1，m为字符串s的长度+1）\n- - 为什么要加1\n  - 因为我们还要处理空字符串的情况，比如p为空，s为空，或者p为空，s不为空\n- $dp[m][n]$ 的含义：p的前$n-1$ 个字符能否匹配s的前$m-1$个字符\n- - 为什么是n-1和m-1?\n  - 因为动态数组里面加了一列和一行空字符的匹配情况，故需要-1才能对应相应字符串\n\n因此创建好的dp数组如下图：\n\n![](https://i.loli.net/2021/09/23/UDWLfZIH8nmwYCX.jpg)\n\n## 确定动态转移方程\n\n说明：为了区别dp数组与字符串索引的区别(因为相差1)，我们设 $i=r-1,j=c-1$ （r为dp里面的行索引，c为dp里面的列索引）\n\n有以下几种情况是需要我们处理的：\n\n- 当 $s[i]=p[j] \\ || \\  p[j]=='.'$ （即正好能够匹配或者相对应的是一个 $.$）\n\n那么我们只需要看一下前面 $dp[r-1][c-1]$的状态，$dp[r][c]$继续延续即可\n\n即状态转移方程为 $dp[r][c]=dp[r-1][c-1]$\n\n![](https://i.loli.net/2021/09/23/wzgDScn6VBvR93J.jpg)\n\n- 当 $p[j] =='*'$ （即匹配到了万能字符 $*$）\n\n  两种情况分别对应的处理方式为：\n\n  如果 $*$ 的前一个字符正好对应了$s$，状态转移过程为：$dp[r][c] = dp[r-1][c]$\n\n  如果是$*$的前一个字符为 $.$ 那么只需看$.$的前面字符匹配情况，状态转移过程为: $dp[r][c]=dp[r][c-2]$\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  边界：\n\n  - 首先我们要确定 $dp[0][0]$，当p为空，s为空时，肯定匹配成功。那么$dp[0][0]=true$\n\n  - 当 p 为空字符串，而s不为空时，dp数组必定为False，正好初始化dp数组的时候设置的是False；即dp数组的第一列为False可以确定\n\n  - 当s为空字符串，而p不为空时，我们无需判断p里面的第一个值是否为\"\"，如果为\"\",那肯定匹配不到为Fasle,原数组正好是Fasle，所以直接从2开始判断即可。如果遇到了*,只要判断其对应的前面两个元素的dp值\n\n    \n\n  \n\n\n\n```java\nclass Solution {\n    public boolean isMatch(String s, String p) {\n        int n = s.length();\n        int m = p.length();\n        boolean[][] dp=new boolean[n+1][m+1];\n        s = ' '+s;\n        p = ' '+p;\n        dp[0][0] = true;\n\n        for(int i=0;i<=n;i++){\n            for(int j=0;j<=m;j++){\n                if(i==0 && j==0) continue;\n                if(p.charAt(j) != '*'){\n                    if(p.charAt(j)=='.' || s.charAt(i)==p.charAt(j)){\n                        if(i>0 && j>0){\n                            dp[i][j] = dp[i-1][j-1];\n                        }\n                    }\n                } else{\n                    if(j>=2) dp[i][j] = dp[i][j-2];\n                    if(i>0 && j>0){\n                        if(p.charAt(j-1) == '.' || s.charAt(i)==p.charAt(j-1)){\n                            if(dp[i-1][j]) dp[i][j]=true;\n                        }\n                    }\n                }\n            }\n        }\n        return dp[n][m];\n    }\n    \n  }\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["LeetCode"]},{"title":"Learning to learn without forgetting by maximizing transfer and minimizing interference","url":"/2021/09/22/LEARNING-TO-LEARN-WITHOUT-FORGETTING-BY-MAXIMIZING-TRANSFER-AND-MINIMIZING-INTERFERENCE/","content":"\n# Learning to learn without forgetting by maximizing transfer and minimizing interference\n\n在对非平稳数据分布进行持续学习方面，仍然是将神经网络扩展到人类现实环境的主要挑战。\n\n在这项工作中，我们提出了一种关于连续学习的新概念，即迁移和干扰之间的对称权衡，可以通过实施跨样本的梯度对齐来优化该权重。\n\nMeta-Experience Replay (MER) 通过将经验回放与基于优化的元学习相结合。\n\n该方法学习使基于未来梯度的干扰可能性较小，而基于未来梯度的迁移可能性更大的参数。\n\n作者在连续终身监督学习基准和非静态强化学习环境中进行了实验，实验表明，MER算法和基线算法之间的性能差距随着环境变得更加不平稳和存储的经验在总经验中所占的比例变小而增大。\n\n##  Continual learning problem\n\n人工智能的一个长期目标是建立能够长期自主操作的代理。这样的代理必须渐进地学习并适应不断变化的环境，同时保持对以前所学知识的记忆，这种设置称为终身学习。本文是持续学习的一个变体。\n\n在持续学习中，假设学习者接触到一系列任务，其中每个任务都是来自相同分布的一系列经验。\n\n作者希望在这种情况下开发一种解决方案，在无监督的情况下发现任务的概念，同时在每次体验后逐步学习。\n\n这很有挑战性，因为在标准的离线单任务和多任务学习中，隐含地假设数据是独立同分布的平稳分布。不幸的是，每当情况不是这样的时候，神经网络往往都会举步维艰。\n\n持续学习面临的最大问题是灾难性的遗忘(干扰)，其中最主要的担忧是神经网络缺乏稳定性，而主要的解决方案是通过专注于保留过去的知识来限制经验之间的权重共享程度。\n\n另一个问题是稳定性-可塑性两难问题，在这种观点下，首要关注的是：网络稳定性(保存过去的知识)和可塑性(快速学习当前经验)之间的平衡。\n\n以前的持续学习技术侧重于平衡有限的权重共享和某种机制以确保快速学。在本文中，作者扩展了这一观点，指出——对于在无限数量的分布上的连续学习，需要及时考虑前向和后向的权重共享和稳定性-塑性权衡，如下图：\n\n![](https://i.loli.net/2021/09/22/iCN3JmMckXxLfHy.png)\n\n稳定性-可塑性困境考虑了当前学习的可塑性以及它如何降低旧学习。 迁移-干扰权衡考虑了稳定性-可塑性困境及其对前向和后向权重共享的依赖。这种对称的观点是至关重要的，因为单纯专注于降低权重共享程度的解决方案不太可能在未来产生迁移。\n\n本文提出的迁移-干扰权衡为持续学习问题的梯度对齐目标提供了一个新的视角。这是问题的核心，因为这些梯度是学习期间基于 SGD 的优化器的更新步骤，并且梯度的角度和管理权重共享的程度之间存在明显的联系。\n\n与过去对持续学习的概念观点的关键区别在于，我们不仅关注相对于过去例子的当前迁移和干扰，而且还关注随着我们学习而向前发展的迁移和干扰的动态。\n\n然而，在过去的工作中，基于当前的学习和过去的学习，对权重共享的动态进行了临时更改，而没有制定关于最佳权重共享动态的一致理论。就我们对未来的元学习进行推广而言，这应该使模型更容易在非平稳环境中执行持续学习。\n\n作者通过在过去关于经验回放的工作的基础上实现这一点，经验重播一直是用神经网络解决非平稳问题的中流砥柱。\n\n作者提出了一种新的meta-experience replay(MER)算法，它结合了经验回放和基于优化的元学习。MER在各种有监督的持续学习和持续强化学习环境中显示出巨大的潜力。\n\n## The transfer-interference trade-off for continual learning\n\n在参数 $\\theta$ 和损失 $L$ 的瞬间，我们可以在使用 SGD 训练时定义两个任意不同样本 $(x_i,y_i)$ 和 $(x_j,y_j)$ 之间的迁移和干扰的操作度量。 迁移发生在：\n$$\n\\frac{\\partial L(x_i,y_i)}{\\partial \\theta} \\cdot \\frac{\\partial(x_j,y_j)}{\\partial\\theta} \\gt 0\n$$\n这意味着学习样本 $ i$ 将在不重复的情况下提高示例 $j$ 的表现，反之亦然。干扰发生在：\n$$\n\\frac{\\partial L(x_i,y_i)}{\\partial \\theta} \\cdot \\frac{\\partial(x_j,y_j)}{\\partial\\theta} \\lt 0\n$$\n当 $i$ 和 $j$ 使用一组重叠参数进行学习时，它们之间存在权重共享。因此，当权重共享最大化时，迁移潜力最大化，而当权重共享最小化时，干扰潜力最小化。\n\n持续学习中稳定性-可塑性困境的过去解决方案在简化的时间环境中运行，其中学习分为两个阶段：\n\n- 所有过去的经历都被归结为**旧记忆**\n- 而目前正在学习的数据则是**新学习**\n\n在此设置中，目标是简单地最小化时间上向后投影的干扰，这通常是通过显式或隐式地降低权重共享的程度来实现的。\n\n然而，这种观点的重要问题是，这个系统仍然需要学习，未来会带来什么在很大程度上是未知的。这使得我们有责任不采取任何措施来潜在地破坏网络在不确定的未来有效学习的能力。这种考虑使我们将稳定性-可塑性问题的时间范围向前扩展，更一般地说，将其转变为一个持续学习问题，我们将其标记为解决迁移-干扰权衡问题。如上图A。\n\n具体来说，重要的是不仅要减少来自我们当前时间点的反向干扰，而且我们必须以不限制我们未来学习能力的方式这样做。这种更普遍的观点承认问题中的一个微妙之处：梯度对齐的问题以及因此跨样本的权重共享在时间上向后和向前出现。\n\n在这里，作者提出了一个潜在的解决方案，我们学习以一种在每个时间点促进梯度对齐的方式进行学习。 跨样本的权重共享可以通过迁移来提高对未来的性能，但不破坏之前的性能。 因此，我们的工作对持续学习问题采用了元学习的观点。 希望学习以一种从整体分布中推广到其他样本的方式来学习每个样本。\n\n## A system for learning to learn without forgetting\n\n在典型的离线监督学习中，我们可以在数据集 $D$ 内$x, y$ 的平稳分布上表达我们的优化目标：\n$$\n\\theta = \\text{argmin}_{\\theta} E_{(x,y)\\sim D}[L(x,y)]\n$$\n如果我们想要最大限度地迁移和最小化干扰，我们可以想象在目标上增加辅助损失以使学习过程偏向那个方向是有用的。\n\n考虑公式1和2，一个明显有益的选择将是还直接考虑相对于在随机选择的数据点评估的损失函数的梯度。\n\n如果我们可以最大化这些不同点的梯度之间的点积，它将直接鼓励网络在梯度方向对齐的地方共享参数，并在相反方向的梯度引起干扰的地方保持参数分开。\n\n因此，理想情况下，针对以下目标进行优化:\n$$\n\\theta  = argmin_{\\theta} E_{[(x_i,y_i),(x_j,y_j)] \\sim D} [L(x_i,y_i)+L(x_j,y_j)] - \\alpha\\frac{\\partial L(x_i,y_i)}{\\partial \\theta}\\cdot \\frac{\\partial L(x_j,y_j)}{\\partial \\theta}\n$$\n其中 $(x_i，y_i)$和 $(x_j，y_j)$是随机抽样的唯一数据点。我们将尝试设计一个针对这一目标进行优化的持续学习系统。然而，要在实践中实施这种学习过程，还必须解决多方面的问题。\n\n第一个问题是，持续学习处理的是对非平稳数据流的学习。我们通过实现一个经验回放模块来解决这个问题，该模块增强了在线学习，这样我们就可以对到目前为止看到的所有样本的固定分布进行近似优化。\n\n另一个实际问题是，这种损失的梯度取决于损失函数的二阶导数，这一点计算效率不高。通过使用具有最小计算开销的元学习算法间接地将目标近似为一阶Tayor展开来解决这一问题。\n\n### Experience replay\n\n**Learning objective:** 持续的终身学习环境对神经网络的优化提出了挑战，非平稳流中的样本层出不穷。反而，我们希望我们的网络在目前为止看到的所有样本平稳分布上进行优化。经验回放 (1992) 是一种古老的技术，它仍然是尝试在非平稳环境中学习的深度学习系统的核心组成部分，我们将在这里采用最近工作中的约定 (A deeper look at experience replay2017) ; Scalable recollections for continual lifelong learning2017) 利用这种方法。\n\n经验回放的中心特征是保持对所见样本的记忆 $M$，该记忆与当前样本的训练交织在一起，目的是使训练更稳定。因此，经验回放在 $M$ 逼近 $D$ 的程度上逼近等式 3 中的目标：\n$$\n\\theta = argmin_{\\theta} E_{(x,y)\\sim M} [L(x,y)]\n$$\n$M$ 具有当前大小 $M_{size}$和最大大小$M_{max}$。 使用reservoir sampling来更新缓冲区。这确保在每个时间步，看到的 N 个示例中的任何一个在缓冲区中的概率都等于 $M_{size}/N$。\n\n缓冲区的内容类似于所有样本的平稳分布，以至于存储的项目捕获了过去样本的变化。 遵循离线学习的标准做法，我们通过从 M 捕获的分布中随机抽样一批 B 来进行训练。\n\n**Prioritizing the current example:** 我们探索的经验回放变体与离线学习的不同之处在于，当前样本具有特殊作用，可确保它始终与从回放缓冲区采样的样本交错。这是因为在我们继续下一个样本之前，我们希望确保我们的算法能够针对当前样本进行优化(特别是如果它没有添加到记忆中)。在看到的N个样本上，这仍然意味着我们已经将每个样本作为当前样本进行了训练，每步的概率为1/N。我们提供了进一步详细说明在这项工作中如何使用经验回放的算法在附录G中。\n\n![](https://i.loli.net/2021/09/23/d5yaHjVocX6Qki1.png)\n\n![](https://i.loli.net/2021/09/23/RnH3td1zVFSY4yW.png)\n\n![](https://i.loli.net/2021/09/23/8UvcJVqlsBICPD3.png)\n\n**Concerns about storing examples: **显然，将所有经验都存储在内存中是不可行的。因此，在这项工作中，我们重点展示当每种方法只提供很小的内存缓冲区时，我们可以获得比基线技术更高的性能。\n\n\n\n\n\n\n\n### Combining experience replay with optimization based meta-learning\n\n**First order meta-learning:** \n\nFOMAML  和 Reptile\n\nReptile通过泰勒展开指出，这两个算法对于相同的损失函数是近似优化的。Reptile可以有效地针对大致相同的目标进行优化，同时不需要像 MAML 那样针对每个学习的任务将数据拆分为训练和测试拆分。Reptile 是通过使用基于 SGD 的优化器和学习率 $\\alpha$ 顺序优化 s batch数据来实现的。在对这些批进行训练之后，我们在训练 $\\theta_0$ 之前获取初始参数，并将它们更新为 $\\theta_0 \\leftarrow \\theta_0 + \\beta *(\\theta_k-\\theta_0)$，其中 $\\beta$ 是元学习更新的学习率。\n\n该过程对每个系列的s批进行重复（算法2）。Reptile在一组s批次中大致优化了以下目标：\n$$\n\\theta = argmin_{\\theta} E_{B_1,...,B_s \\sim  D} [2\\sum_{i=1}^s [L(B_i) - \\sum_{j-1}^{i-1}\\alpha\\frac{\\partial L(B_i)}{\\partial\\theta} \\cdot \\frac{\\partial L(B_j)}{\\partial\\theta}]]\n$$\n\n\n![](https://i.loli.net/2021/09/23/EOur37U1qD8GiWw.png)\n\n**The MER learning objective:** 在这项工作中，我们修改了Reptile算法，将其与经验回放模块适当地集成在一起，在最大化迁移和最小化干扰的同时，促进了持续学习。\n\n正如我们在附录I中的推导过程中更详细地描述的那样，在按顺序提供样本的在线设置中实现 Reptile 目标并非易事，并且只能部分实现，因为我们对缓冲区和批次的采样策略。根据上一节关于体验回放的评论，这允许我们使用MER算法在持续学习环境中针对以下目标进行优化：\n$$\n\\theta = argmin_{\\theta} E_{[(x_{11},y_{11}),..., (x_{sk},y_{sk})] \\sim M}[2\\sum_{i=1}^s\\sum_{j-1}^k[L(x_{ij},y_{ij}) - \\sum_{q=1}^{i-1} \\sum_{r=1}^{j-1} \\alpha\\frac{\\partial L(x_{ij},y_{ij})}{\\partial \\theta} \\cdot \\frac{\\partial  L(x_{qr},y_{qr})}{\\partial\\theta}] ]\n$$\n**The MER algorithm:** MER 使用储层采样维护经验回放式记忆 M，并在每个时间步从缓冲区中抽取 s 个批次，包括 k-1 个随机样本，以与当前示例一起训练。每个批次内的k个样本中的每一个都被视为其自己的大小为1的Reptile批次，在该批次被处理之后具有内循环爬行动物元更新。然后，我们在外部循环中跨s个批次再次应用Reptile元更新。我们在算法1中提供了关于MER的更多细节。当β=1时，该过程近似于上面的目标。采样函数生成 s 个更新批次。 通过首先添加当前示例然后从 M 中交错 k - 1 个随机样本来创建每个批次。\n\n![](https://i.loli.net/2021/09/23/v6gkTQhElaUqI91.png)\n\n**Prioritizing current learning:** 为了确保强正则化，我们希望在 Reptile 更新中处理的批次数量足够大 - 足以让经验重播开始过拟合 M。因此，我们还需要确保我们提供足够的优先级来学习当前样本，特别是因为我们可能不会将其存储在 M 中。为了在算法 1 中实现这一点，我们从 M 中采样 s 个单独的批次，这些批次按顺序处理并且每个批次都与当样本交错。\n\n\n\n## 实验\n\nHow does MER perform on supervised continual learning benchmarks?\n\n![](https://i.loli.net/2021/09/23/y5uKOL7wV9R6JFT.png)\n\nHow do the performance gains from MER vary as a function of the buffer size?\n\n![](https://i.loli.net/2021/09/23/oGej8vdghWEXN3i.png)\n\nHow effective is MER at dealing with increasingly non-stationary settings?\n\n![](https://i.loli.net/2021/09/23/XGjxz7R5edQa3vL.png)\n\nDoes MER lead to a shift in the distribution of gradient dot products?\n\n![](https://i.loli.net/2021/09/23/EuLksbjeQTSCYRh.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"5-最长回文子串","url":"/2021/09/19/5-最长回文子串/","content":"\n# 5-最长回文子串\n\n#### [5. 最长回文子串](https://leetcode-cn.com/problems/longest-palindromic-substring/)\n\n## 动态规划DP\n\n- 思想：对于一个子串而言，如果它是回文串，并且长度大于2，那么它首尾的两个字母去掉之后，它仍然是个回文串\n- 状态转移方程： $(s[i]==s[j])\\ \\&\\& \\ dp[i+1][j-1]$ ，$dp[i][j]$ 表示子串$s[i..j]$是否是回文\n- 边界条件,即子串的长度为 1 或 2。： $dp(i,i)=true$ 单个字符串， $dp(i,i+1) = (Si==Si+1)$两个字符\n\n```java\nclass Solution {\n    public String longestPalindrome(String s) {\n        int n = s.length();\n        boolean[][] dp = new boolean[n][n];\n        String ans=\"\";\n        for(int k=0;k<n;k++){\n            for(int i=0;i+k<n;i++){\n                int j = i+k;\n                // 先处理两种临界情况\n                if(k==0) { //k=0时，j=i, dp[i][j]相当于一个字符串，一定是回文串\n                    dp[i][j] = true;\n                } else if(k==1){ // k=1时，j=i+1 dp[i][j]相当于连续两个字符，相同时，一定是回文串\n                    dp[i][j] = (s.charAt(i)==s.charAt(j));\n                } else{ // k>1 时，需满足状态转移方程，dp[i,j] = dp[i+1][j-1] && (si==sj)\n                    dp[i][j] = (s.charAt(i)==s.charAt(j) && dp[i+1][j-1]);\n                }\n                if(dp[i][j] && k+1>ans.length()){ //更新回文串长度\n                    ans = s.substring(i, i+k+1); // 注意子串方法的具体用法\n                }\n            }\n        }\n        return ans;\n    }\n}\n```\n\n时间复杂度：$O(n^2)$ 其中n是字符串长度，动态规划的状态总数为$O(n^2)$，对于每个状态，我们需要转移的实际为$O(1)$\n\n空间复杂度: $O(n^2)$ 即存储动态规划需要的空间\n\n## 中心扩展\n\n思想\n\n- 前提条件:\n- - 根据状态转移方程，可以发现所有的状态，在转移的时候可能性都是唯一的。也就是说，我们可以从每一种边界情况开始扩展，也可以得出所有状态对应的答案。\n  - 边界情况，对应的子串实际上就是我们扩展出的回文串的回文中心\n- 本质：\n- - 枚举所有的回文中心，并尝试扩展，直到无法扩展为止，此时的回文串长度即为回文中心下的最长回文串长度。\n\n```java\n//中心扩展\n    public String longestPalindrome(String s) {\n        if(s==null || s.length()==0) return \"\";\n        int start = 0, end=0; // 初始化最大回文串的起点和终点\n        // 遍历每个位置，当做中心\n        for(int i=0;i<s.length();i++){\n            // 分别拿到奇数偶数的回文串长度\n            int len_odd = expandCenter(s,i,i);\n            int len_even = expandCenter(s,i,i+1);\n            int len = Math.max(len_odd,len_even);\n            // 计算对应最大回文子串的起点和终点\n            if(len > end-start){\n                /**\n                    这里为什么要len-1？ ，因为for循环是从0开始的，\n                    如果是奇数回文，假设有个回文是3个，那么len=3，此时中心i是下标1（从0开始），那么(len-1)/2和len/2的结果都是1，因为整数会向下取整\n                    但是如果是偶数回文，假设和有个回文是4个，那么len=4，此时的中心是一条虚线，但是i的位置在1，因为s是从左向右遍历的，\n                    如果从左向右i的位置就会在2，这个时候 (len-1)/2 =1 ,len/2=2 ,很明显为了保证下标正确，我们需要的是(len-1)/2，原因是i在中心线的左边一位。\n                    所以要少减一个1\n                 */\n                start = i-(len-1)/2;\n                end = i+len/2;\n            }\n        }\n        return s.substring(start, end+1);//注意这里end+1是因为java自带左闭右开的原因\n    }\n\n    private int expandCenter(String s,int left, int right){//起始的左右边界\n        // left = right的时候，此时回文中心是一个字符，回文串的长度是奇数\n        // right = left+1的时候，此时回文中心是一个空隙，回文串的长度是偶数\n        // 跳出循环的时候恰好满足 s.charAt(left)!=s.charAt(right)\n        while(left >=0 && right<s.length() && s.charAt(left)==s.charAt(right)){\n            left--;\n            right++;\n        }\n        return right-left-1; //回文串的长度是right-left+1-2 = right -left -1\n    }\n```\n\n时间复杂度：$O(n^2)$ 其中n是字符串长度，长度为1和2的回文中心分别有n和n-1个，每个回文中心最多会向外扩展$O(n)$次\n\n空间复杂度：$O(1)$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["LeetCode"]},{"title":"7,8-整数反转,字符串转换整数(atoi)","url":"/2021/09/18/7,8-整数反转,字符串转换整数(atoi)/","content":"\n# 7,8-整数反转,字符串转换整数(atoi)\n\n#### [7. 整数反转](https://leetcode-cn.com/problems/reverse-integer/)\n\n## 法一：按位转换\n\n- 弹出 和 推入数字\n- - 弹出：num = x%10;   x/=10\n  - 推入：result = result x 10 + num  (这里有可能溢出)\n- 模式识别：整数运算注意溢出\n- - 转换为 INT_MAX / INT_MIN的逆运算\n\n判断某数乘十是否会溢出，就把该数 和  INT_MAX 除10 进行比较\n\n```java\nclass Solution{\n\t\tpublic int reverse(int x){\n\t\t\t\tint rev = 0;\n      \twhile(x!=0){\n          \tif(rev < Integer.MIN_VALUE /10 || rev > Integer.MAX_VALUE/10){\n              \treturn 0;\n            }\n          \tint digit = x % 10;\n          \tx /= 10;\n          \trev = rev * 10 + digit;\n        }\n      \treturn rev;\n\t\t}\n}\n```\n\n```python\nclass Solution:\n  \tdef reverse(self, x):\n      \tINT_MIN, INT_MAX = -2**31 , 2**31-1\n        rev = 0\n        while x!=0:\n          \t# INT_MIN也是一个负数，不能写成 rev < INT_MIN //10\n            if rev < INT_MIN//10 +1 or rev > INT_MAX//10:\n              \treturn 0\n            digit =x %10\n            # python3 的 取模运算在x为负时也会返回[0,9) 以内的数，因此需要进行特殊判断\n            if x<0 and digit >0:\n              \tdigit -= 10\n            # 同理python3的整数除法在x为负数时会向下(更小的负数)取整，因此不能写成 x//=10\n            x = (x - digit) // 10\n            rev = rev * 10 + digit\n     return rev\n       \n```\n\n#### [8. 字符串转换整数 (atoi)](https://leetcode-cn.com/problems/string-to-integer-atoi/)\n\n这道题的难点在于要考虑到各种边界问题，一不留神少了一步判断可能执行就报错了。\n根据题目描述，可能会出现各种输入条件，比如：\n\n\" 123\"\n\" -345 \"\n\" -+7890\"\n\"11223344556677889900\"\n\" -112233.44.55aabb\"\n等等...\n我们总结一下，字符串可能包含下面三种类型:\n\n![](https://i.loli.net/2021/09/20/ScF8Ka7fytjEh9g.jpg)\n\n紫色的第一部分是空格，在转换的时候需要过滤掉\n黄色的部分是正负号，如果是正号则忽略，是负号则需要记录这个正负号状态\n蓝色是第三部分，这部分字符串中会包含任意字符，但我们只需要\"0\"到\"9\"这几个字符\n\n\n\n此外，对于11223344556677889900这样的字符串，明显是超长了，所以当字符串大于最大的32位整数，或者小于最小的32位整数，后面就不用判断了。\n题目要求是只能存储32位大小的有符号整数，所以不能用long做存储，而且需要提前判断整数的大小。\n\n![](https://i.loli.net/2021/09/20/Na5lE2B9p8mO3DG.jpg)\n\n上图绿色的是最大的32位整数，三个蓝色的数组代表三种不同的输入。\n如果是第一种2147483650，这个值本身就比最大32位整数要大了，存到int里面就溢出了，所以提前一位判断，也就是到黄色格子那一位的时候就要判断了。\n第一种情况当前的值大于214748364直接返回最大值即可。\n对于第二种、第三种情况，如果当的值等于214748364，即前面若干位都一样，再单端判断最后一位，也就是橙色格子那一位。如果最后一位大于等于7，同样也是直接返回最大值。\n\n对于负数也是类似的判断方式:\n\n![](https://i.loli.net/2021/09/20/aT2Ueq6OHWIJX43.jpg)\n\n如果当前值小于-214748364，直接返回最小值即可。\n如果当前值等于-214748364，再判断最后一位，如果大于等于8，返回最小值。\n\n总结一下整个执行流程:\n\n过滤掉前面若干个空格(如果有的话)\n判断正号、负号位，如果是负号则记录下状态，表示输入的是负数。\n循环判断后面的字符串是否是0到9，如果是则累加这个值\n当前的值跟最大、最小32位整数比较看是否溢出\n如果是正数，且大于214748364，直接返回最大值\n如果是正数，且等于214748364，再判断最后一位是否大于7\n如果是负数，且小于-214748364，直接返回最小值\n如果是负数，且等于-214748364，再判断最后一位是否大于8\n循环结束后，根据负号的标志位返回对应的正数或负数\n\n```java\nclass Solution {\n\tpublic int myAtoi(String str) {\n\t\tif(str==null) {\n\t\t\treturn 0;\n\t\t}\n\t\tint n = str.length();\n\t\tint i = 0;\n\t\tint res = 0;\n\t\tboolean is_negative = false;\n\t\t//第一步，跳过前面若干个空格\n\t\twhile(i<n && str.charAt(i)==' ') {\n\t\t\t++i;\n\t\t}\n\t\t//如果字符串全是空格直接返回\n\t\tif(i==n) {\n\t\t\treturn 0;\n\t\t}\n\t\t//第二步，判断正负号\n\t\tif(str.charAt(i)=='-') {\n\t\t\tis_negative = true;\n\t\t}\n\t\t//如果是正负号，还需要将指针i，跳过一位\n\t\tif(str.charAt(i)=='-' || str.charAt(i)=='+') {\n\t\t\t++i;\n\t\t}\n\t\t//第三步，循环判断字符是否在 0~9之间\n\t\twhile(i<n && str.charAt(i)>='0' && str.charAt(i)<='9') {\n\t\t\t//'0'的ASCII码是48，'1'的是49，这么一减就从就可以得到真正的整数值\n\t\t\tint tmp = str.charAt(i)-48;\n\t\t\t//判断是否大于 最大32位整数\n\t\t\tif(!is_negative &&(res>214748364 ||(res==214748364 && tmp>=7))) {\n\t\t\t\treturn 2147483647;\n\t\t\t}\n\t\t\t//判断是否小于 最小32位整数\n\t\t\tif(is_negative &&(-res<-214748364 || (-res==-214748364 && tmp>=8))) {\n\t\t\t\treturn -2147483648;\n\t\t\t}\n\t\t\tres = res*10 + tmp;\n\t\t\t++i;\n\t\t}\n\t\t//如果有负号标记则返回负数\n\t\tif(is_negative) {\n\t\t\treturn -res;\n\t\t}\n\t\treturn res;\n\t}\n}\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["LeetCode"]},{"title":"La-MAML: Look-ahead Meta Learning for Continual Learning","url":"/2021/09/17/La-MAML-Look-ahead-Meta-Learning-for-Continual-Learning/","content":"\n# La-MAML: Look-ahead Meta Learning for Continual Learning\n\n持续学习问题涉及能力有限的训练模型，这些模型在一组未知数量的顺序到达的任务上表现良好。\n\n虽然元学习在减少新旧任务之间的干扰方面显展示出潜力，但目前的训练过程往往要么很慢，要么离线，而且对许多超参数很敏感。\n\n作者提出*Look-ahead MAML (La-MAML)* 一种optimisation-based的快速元学习算法，用于在线持续学习，并辅之以小情节记忆。\n\n作者在元学习更新中提出的对每个参数学习率的调制，并将其与先前关于超梯度 *hypergradients* 和元下降 *meta-descent* 的工作联系起来。\n\n与传统的基于先验的方法相比，这提供了一种更灵活和更有效的方式来减轻灾难性遗忘。\n\n作者开发了一种基于梯度的元学习算法，以实现高效的在线持续学习。先提出了一种连续元学习的基本算法，称为连续MAML(C-MAML)，它利用replay-buffer并优化了一个减轻遗忘的元目标。随后提出了一种对C-MAML的改进，称为La-MAML，它包括对每参数学习率(LRs)的调制，以跨任务和时间调整模型的学习速度。\n\n## 连续学习与元学习\n\n### 涉及论文\n\n- GEM：Gradient episodic memory for continual learning.\n- MER：Learning to learn without forgetting by maximizing transfer and minimizing interference (2019)\n- Reptile：On first-order meta-learning algorithms  (2017)\n- 12：Meta-learning representations for continual learning.  (2019)\n- Generative-replay：Continual learning with deep generative replay. (2019)\n- A-GEM：Efficient lifelong learning with a-GEM  (2019)\n- Online-aware Meta Learning (OML) : Meta-learning representations for continual learning (2019)\n- 2 : Continuous adaptation via meta-learning in nonstationary and competitive environments (2018)\n- 10 : Online meta-learning (2019)\n- 19:Continual adaptation for model-based RL (2019)\n- BGD：Task Agnostic Continual Learning Using Online Variational Bayes\n- UCB：Uncertainty-guided continual learning with bayesian neural networks\n- AlphaMAML : Adaptive Model-Agnostic Meta-Learning.\n\n灾难性的遗忘是Continual Learning的最大的挑战之一，当随机梯度下降（SGD）所需的i.i.d.采样条件被违反时，可能会发生这种情况，因为属于要学习的不同任务的数据按顺序到达。\n\n连续学习(CL)算法还必须有效地利用其有限的模型容量，因为未来任务的数量是未知的。因此，确保各任务之间的梯度对齐至关重要，以便在实现其目标方面取得共同进展。\n\n梯度情节记忆(GEM)研究了CL中权重分担和遗忘之间的关系，并开发了一种显式尝试最小化梯度干扰的算法。\n\nMeta Experience Replay(MER)形式化了迁移-干扰权衡，并表明GEM的梯度排列目标与一阶元学习算法Reptile优化的目标一致。\n\n除了对齐梯度外，元学习算法对 CL 也很有前景，因为它们可以直接使用元目标来影响模型优化并改进泛化或迁移等辅助目标。这避免了为了更好的CL而定义诸如稀疏性这样的启发式激励。缺点是它们通常很慢，很难调整，使它们更适合离线继续学习[12]。\n\n### 持续学习\n\n方法大致三种类型 replay-based, regularisation (or prior-based) 和  meta-learning-based .\n\n- replay-based：为了避免灾难性遗忘的问题，replay-based的方法在内存中维护以前任务的样本集合。利用情节缓冲器（*episodic-buffer*）统一采样旧数据以模拟独立同分布的方法。*Generative-replay*训练生成模型能够重放过去的样本，但由于复杂的非平稳分布建模的困难而引起的可扩展性问题。GEM和A-GEM将存储器样本考虑在内，来确定改变的低干扰梯度以更新参数。\n- Regularisation-based：是一种启发式的方法，确保保留先前任务的性能的来约束网络权重，从而完全避免使用重放。这包括惩罚被认为对旧任务很重要的权重的改变，或者强制实施权重或表征稀疏性，以确保在任何时间点只有一部分神经元保持活跃。\n- Meta-Learning-based：这些方法是最近才出现的。MER 受 GEM 的启发，利用重放来激励新旧任务之间的梯度对齐。 OML引入了用于预训练算法的元目标，以离线学习最优表示，该最优表示随后被冻结并用于CL。[2，10，19]研究正交设置，其中学习代理使用所有先前看到的数据来快速适应传入的数据流，从而忽略灾难性遗忘的问题。\n\n## 预备知识\n\n首先通过顺序地观察 $T$ 个任务的训练数据$[D_1,D_2,...,D_T]$ 来学习 $T$ 个任务的序列 $[\\tau_1,\\tau_2 ,...,\\tau_T]$  \n\n定义$X^i,Y^i = \\{(x_n^i,y_n^i)\\}_{n=0}^{N_i}$ 为$N_i$个输入标签集合从数据 $D_i$ 中随机抽取。\n\n在在线学习过程中的任意时间步长 $j$，我们的目标是最小化模型在迄今看到的所有 $t$ 个任务上的经验风险 $(τ_{1:t})$，给定对来自先前任务 $τ_i(i<t)$ 的数据 $(X_i，Y_i)$ 限制访问。我们将这一目标称为累积风险，具体如下：\n$$\n\\sum_{i=1}^t \\mathbf{E}_{(X^i,Y^i)}[l_i(f_i(X^i;\\theta) ,Y^i)] = \\mathbf{E}_{(X^{1:t},Y^{1:t})}[L_t(f(X^{1:t};\\theta), Y^{1:t})]\n$$\n其中 $l_i$ 是在任务$\\tau_i$上的loss， $f_i$ 是学习器，参数 $θ_0^j$ 是从输入到输出的特定任务映射参数。\n\n$L_t = \\sum_{i=1}^t l_i$  是任务 $τ_{1:t}$ 的所有任务损失之和，其中 $t$ 从 1到 $T$ 。设 $l$ 表示要最小化的某些损失目标。\n\n作用于参数$θ_0^j$ (由 $U(θ_0^j)$ ) 表示的SGD运算定义为：\n$$\nU(\\theta^j_0)=\\theta_1^j = \\theta_0^j-\\alpha \\nabla_{\\theta_0^j} l(\\theta_0^j) = \\theta_0^j - \\alpha g_0^j\n$$\n$U$ 可以为 $U_k(\\theta_0^j) = U...\\circ U\\circ U(\\theta_0^j) =\\theta_k^j$\n\n### Model-Agnostic Meta-Learning (MAML):\n\nmeta learning 成为一种流行的训练模型的方法，能够在有限的数据上进行快速调整。MAML建议优化模型参数，以学习一组任务，同时改进辅助目标，如任务分布中的few-shot 少样本泛化。\n\n基于梯度的元学习中使用的一些常用术语：\n\n- 初始化：在训练期间的给定时间步长 $j$ 处，模型参数 $θ_0^j$ (或为简单起见，$θ_0$)通常被称为初始化，因为其目的是找到对不可见数据进行 few-shot 基于梯度的适配的理想起点。\n\n- inner-updates 快速或内部更新：是对 $θ_0$ 的副本进行基于梯度的更新，以优化某些内部目标(在本例中，对于某些$τ_i$，为$l_i$)。\n\n- meta-update：元更新涉及从 $θ_0$ 到 $θ_k$ 的快速更新的轨迹，然后进行到 $θ_0$的永久梯度更新(或缓慢更新)。该缓慢更新是通过评估 $θ_k$ 上的辅助目标(或元损失meta loss $L_{meta}$)并通过轨迹微分以获得$\\nabla_{\\theta_k}L_{meta}(\\theta_k)$来计算的。因此，MAML在时间步 $j$ 优化 $θ_0^j$，以便在对它们的样本进行几次梯度更新之后，对 $\\{\\tau_{1:t}\\}$中的任务执行最佳性能。它在每一次元更新中都进行了优化，目标是：\n  $$\n  min_{\\theta_0^j}\\mathbf{E}_{\\tau_{1:t}}[L_{meta}(U_k(\\theta_0^j))] = min_{\\theta_0^j}\\mathbf{E}_{\\tau_{1:t}}[L_{meta}(\\theta_k^j)]\n  $$\n\n### 元学习与持续学习目标的等价性:\n\nReptile证明了Reptile算法和MAML算法等一阶和二阶元学习算法的近似等价性\n\nMER随后表明，他们的CL目标是在一组任务$\\tau_{1:t}$之间最小化损失并调整梯度，直到任何时间 $j$（在左边），可以通过Reptile目标（在右边）进行优化，即：\n$$\nmin_{\\theta_0^j} (\\sum_{i=1}^t(l_i(\\theta_0^j)) - \\alpha\\sum_{p,q\\le t}(\\frac{\\partial l_p(\\theta_0^j)} {\\partial\\theta_0^j} \\cdot \\frac{\\partial l_q(\\theta_0^j)}{\\partial\\theta_0^j})) =min_{\\theta_{0}^j }\\mathbf{E_{\\tau_{1:t}}}[L_t(U_k(\\theta_0^j))]\n$$\n其中 meta-loss $L_t = \\sum_{i=1}^t l_i$  根据Tasks $\\tau_{1:t}$中的样本进行评估。这意味着元学习初始化的过程与学习CL的最优参数一致。\n\n### 在线感知元学习(OML)：\n\n[12]提出了元学习 *Representation-Learning Network*  网络(RLN)的概念，为*Task-Learning Network* (TLN)提供适合协作学习的表示。\n\nRLN的表示是在离线阶段学习的，在该阶段使用灾难性遗忘作为学习信号进行训练。当TLN经历时间相关更新时，来自固定任务集（$\\tau_{val}$）的数据被反复用于评估RLN和TLN。\n\n在每个元更新的内循环中，TLN使用冻结的RLN对流式任务数据进行快速更新。然后，通过根据来自$\\tau_{val}$的数据以及当前任务计算的 meta loss来评估RLN和更新的TLN。\n\n这将测试在尝试学习流任务的过程中，模型在 $\\tau_{val}$ 上的性能发生了怎样的变化。然后对meta loss进行微分，以获得针对TLN和RLN的缓慢更新的梯度。\n\n这两个loss的组合被称为OML目标，以模拟内环中的CL和测试外环中的遗忘。RLN学习最终为CL的TLN提供更好的表示，该表示被证明具有紧急稀疏性。\n\n\n\n## Method\n\n在上一节中，我们看到OML目标可以直接规范CL行为，并且MER利用了元学习和CL目标的近似等价性。我们注意到，OML离线训练静态表示和MER算法慢得令人望而却步。\n\n作者表明，通过多步MAML过程在线优化OML目标等同于更有效的样本效率CL目标。\n\n###  Continual-MAML (C-MAML)\n\nC-MAML旨在在线优化OML目标，这样学习当前任务就不会导致忘记以前见过的任务。我们定义了这个目标，适用于优化模型的参数 θ 而不是时间步 j 的表示，如下所示：\n$$\nmin_{\\theta_0^j} OML(\\theta_0^j,t) = min_{\\theta_0^j} \\sum_{S_k^j\\sim D_t}[L_t(U_k(\\theta_0^j,S_k^j))]\n$$\n其中 $S_k^j$ 是来自前任务$\\tau_{t}$ 中的k个数据元组的流 $(X_{j+l}^t,Y_{j+l}^t)^k$ ，这是模型在时间 $j$ 处看到的。\n\nMeta loss $L_t = \\sum_{i=1}^l l_i$ 在 $\\theta_{k}^j = U_k(\\theta_{0}^j ,S_k^j)$ 上评估。它评估 $\\theta_k^j$ 对于上面第一个公式中定义的持续学习预测任务的适合性，直到$\\tau_t$.\n\n省略了隐含的数据参数 $(x^i,y^i) \\sim (X^i,Y^i)$ 这是任何任务$\\tau_i$的 $L_t$ 中每个损失 $l_i$ 的输入。附录B\n$$\nmin_{\\theta_0^j}\\mathbf{E}_{\\tau_{1:t}} [L_t(U_k(\\theta_0^j))] = min_{\\theta_0^j} \\sum_{i=1}^t(l_i(\\theta_0^j) - \\alpha \\frac{\\partial l_i(\\theta_0^j)} {\\partial\\theta_0^j} \\cdot \\frac{\\partial l_t(\\theta_0^j)}{\\partial\\theta_0^j})\n$$\n与上文元学习与合作学习目标的等价性那个公式不同它是不对称的，它集中于对其$\\tau_t$的梯度和$\\tau_{1:t}$的平均梯度，而不是在任务$\\tau_{1:t}$之间对齐所有成对梯度。附录D\n\n作者的经验表明，旧任务之间的梯度对齐不会退化，而学习了新任务，避免了重复优化它们之间的任务间对齐的需要。\n\n这导致MER目标的显著加速，该目标试图将所有$\\tau_{1:t}$ 上相乘均匀分布的批次。由于每个s 在梯度更新中有 $1/t-th$ 次贡献，因此MER有必要对包括s在内的许多此类均匀批次进行多次传递。\n\n在训练期间，如MER中所示，通过对输入数据流的存储采样来填充重放缓冲器R。\n\n在每次元更新开始时，从当前任务中采batch b。b还与从R采样的批次组合以形成元批次meta-batch $b_m$，其表示来自旧任务和新任务的样本。$\\theta_0^j$通过k个基于sgd的内部更新进行更新，每次从b查看一个当前任务的样本。外部损失或元损失$L_t(\\theta_k^j)$是在$b_m$上评估的。它指示参数 $\\theta_k^j$ 在时间 $j$ 之前看到的所有任务 $\\tau_{1:t}$上的性能。 附录C\n\n### Lookahead-MAML (La-MAML)\n\n尽管元学习激励了任务内和任务间组的梯度对齐，但在新旧任务的梯度之间仍然可能有一些干扰，$\\tau_{1:t−1}$和 $\\tau_{t}$。\n\n这将导致忘记 $\\tau_{1:t−1}$，因为它的数据不再对我们完全可用。在训练新任务的开始阶段尤其如此，因为新任务的梯度不一定与旧任务一致。因此，需要一种机制来确保元更新相对于$\\tau_{1:t-1}$是保守的、避免负迁移。元更新的幅度和方向需要根据更新对 $\\tau_{1:t-1}$ 损失的影响程度进行调整。La-MAML包括一组可学习的pre-parameter学习率(LR)，用于内部更新，如图1所示。\n\n![](https://i.loli.net/2021/09/18/LhcnFOblKsCEPAZ.png)\n\n对于每批数据，初始权重经历一系列 $k$ 次快速更新以获得 $\\theta_k^j$ (这里 $j=0$)，其针对元损失进行评估以相对于权重 $\\theta_0$ 和LRs $\\alpha_0$ 反向传播梯度。首先，$\\alpha^0$更新为$\\alpha^1$，然后用于将 $\\theta_0^0$更新为 $\\theta_0^1$，蓝色框表示快速权重，绿色框表示慢速更新的梯度。LRs和权重以异步方式更新。\n\n这是因为我们观察到上面OML的方程的梯度相对于内循环的 LR 的表达式直接反映了旧任务和新任务之间的对齐情况。扩充的学习目标被定义为:\n$$\nmin_{\\theta_0^j,\\alpha^j} \\sum_{S_k^j\\sim D_t}[L_t(U_k(\\alpha^j,\\theta_0^j,S_k^j))]\n$$\n以及该目标在时间 $j$ 的梯度，相对于 LR 向量 $\\alpha^j$ （定义为$g_{MAML}(\\alpha^j)$）:\n$$\ng_{MAML}(\\alpha^j) = \\frac{\\partial}{\\partial\\alpha^j} L_t(\\theta^j_k) = \\frac{\\partial}{\\partial\\theta^j_k}L_t(\\theta^j_k) \\cdot (-\\sum_{k'=0}^{k-1}\\frac{\\partial}{\\partial\\theta_k^j} l_t(\\theta_{k'}^j))\n$$\n附录A推导\n\n$g_{MAML}(\\alpha)$ 中的第一项对应于元损失的梯度在batch上 $b_m:g_{meta}$。第二项表示来自内部更新的累积梯度：$g_{traj}$。\n\n该表达式表明，当$g_{meta}$和 $g_{traj}$之间的内积较高时，LRs的梯度将为负，即两者对齐；当两者正交（不干扰）时为零，当两者之间存在干扰时为正。\n\n负的(正的)LR梯度会拉高(降低)LR的大小。如下图：\n\n![](https://i.loli.net/2021/09/18/jQSTzt4dbLsVyqa.png)\n\n$g_{traj}$ (蓝色虚线)和 $g_{meta}$对齐的不同场景，从扰动(左)到对齐(右)。黄色箭头表示内部更新。当梯度对齐(扰动)时，LR $\\alpha$ \n\n增加(减少)。\n\n我们建议在元更新中异步更新网络权重和LRs。设$\\alpha^{j+1}$ 为更新的LR 向量，该向量通过在时间 $j$ 处采用上一个等式中的LR梯度进行SGD而获得。然后，我们将权重更新为：\n$$\n\\theta_0^{j+1} \\leftarrow \\theta_0^j - max(0,\\alpha^{j+1}) \\cdot \\nabla_{\\theta_0^j} L_t(\\theta_k^j)\n$$\n其中k是在内循环中采取的步数。 将 LRs $\\alpha^{j+1}$ 修剪为正值，以避免上升梯度，并且也避免进行扰动的参数更新，从而减轻灾难性遗忘。因此，元目标保守地调节学习的速度和方向，以便在新任务上取得更快的学习进度，同时促进旧任务的迁移。\n\n![](https://i.loli.net/2021/09/18/oB2XVDnMRjteq3b.png)\n\nLine(a)，(b)是C-MAML和La-MAML之间的唯一区别，C-MAML使用固定标量LR $\\alpha$ 进行元更新到 $\\theta_0^j$，而不是 $\\alpha^{j+1}$。\n\n\n\n作者的基于元学习的算法结合了基于先验和基于回放的方法的概念。LR在重放样本上的梯度和流任务之间的相互作用的指导下，以数据驱动的方式调制参数更新。然而，由于LR随着每次元更新而演变，它们的衰变是暂时的。这与许多基于先验的方法不同，在这些方法中，对参数更改的惩罚逐渐变得非常高，以至于网络容量饱和。\n\n随着任务的到来，可学习的LR可以调整为高值和低值，因此是一种更简单、灵活和优雅的约束权重的方法。这种异步更新类似于信任区域优化或前瞻搜索，因为每个参数的步长是根据对它们应用假设更新后产生的损失进行调整的。\n\n\n### 与其他工作的联系\n\n#### Stochastic Meta-Descent (SMD)\n\n当学习非平稳数据分布时，使用衰减的LR策略并不常见。严格递减 LR 策略旨在更接近收敛于固定分布的固定 mimima，这与在线学习的目标不一致。由于数据分布的范围未知，因此也不可能手动调整这些计划。\n\n然而，LRS的适应性仍然是非常需要的，以适应优化的场景，加速学习，调节适应的程度，以减少灾难性遗忘。我们的自适应LRS可以连接到离线监督学习(OSL)中的元下降。虽然存在几种不同的变种，但它们背后的核心思想和我们的方法是获得适应。当我们根据新旧任务梯度之间的相关性调整增益以在所有任务上共享进展时，[4，25]利用两个连续随机梯度之间的相关性来更快地收敛。我们利用元目标关于LRS的可微性，自动获得LR超梯度。\n\n#### Learning LRs in meta-learning\n\nMeta-SGD 建议学习MAML中的LRS以进行few-shot学习。他们的更新和我们的更新有一些显著的不同。它们同步更新权重和LR，而我们对LRS的异步更新用于执行更保守的权重更新。\n\n我们更新的直觉来自于需要减轻梯度干扰及其与持续学习中普遍存在的转移-干扰权衡的联系。α-MAML解析地更新了MAML更新中的两个标量LR，以实现更自适应的few-shot学习。我们的每个参数的LR通过反向传播被隐式地调制，以基于它们在任务之间的排列来调节参数的变化，为我们的模型在CL领域提供了更强大的适应性。\n\n\n\n\n\n## 实验\n\n指标\n\n使用保留精度retained accuracy（RA）度量来比较各种方法。RA 是模型在训练结束时跨任务的平均准确率。\n\n反向迁移和干扰 *backward-transfer and interference*(BTI)值，它衡量每个任务从学习到最后一个任务结束的准确性的平均变化。 较小的 BTI 意味着训练期间遗忘较少。\n\n*Efficient Lifelong Learning (ELL)*:  高效终身学习 (LLL)：在A-GAM中形式化，高效终身学习的设置假设每个任务的传入数据必须仅通过一次处理：一旦处理，数据样本将不再可访问，除非它们被添加到 回放记忆。\n\n![](https://i.loli.net/2021/09/18/DrAbfMtzHEpoaBP.png)\n\n\n\n随着训练的进行，模型会演变成对遗忘的抵抗力。这意味着超过一个点，它可以在传入样本的一个小窗口上持续进行梯度更新，而不需要进行元更新。\n\n![](https://i.loli.net/2021/09/18/mdJEbSojDI7aBvf.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"4-三种方法彻底解决中位数问题","url":"/2021/09/15/4-三种方法彻底解决中位数问题/","content":"\n# 4-三种方法彻底解决中位数问题\n\n#### 4. 寻找两个正序数组的中位数\n\n## 两种思想\n\n- 真合并：使用归并的方式，合并两个有序数组，得到一个大的有序数组，大的有序数组中的中间位置的元素即为中位数。$O(n+m),O(n+m)$\n- 假合并：不需要合并两个有序数组，只要找到中位数的位置即可。由于两个数组的长度已知，因此中位数对应的两个数组下标之和也是已知的。维护两个指针，初始时分别指向两个数组的下标为0的位置，每次将指向较小的指针后移一位（如果一个指针已经到达数组末尾，则只需要移动另一个数组的指针），直到达到中位数的位置。$O(n+m),O(1)$\n\n## 常见的思想改进：假合并、奇偶合并\n\n通过假合并的思想可以将空间复杂度优化到$O(1)$但对于时间复杂度并没有什么优化，此方法代码复杂，不仅要考虑奇偶问题，更需要高了一个数组遍历后的各种边界问题。\n\n假合并的一个优化点是 将奇偶两种情况合并到了一起：\n\n- 如果是奇数，我们需要知道第(len+1)/2 个数就可以了，如果遍历的话需要遍历int(len/2)+1次\n- 如果是偶数，需要知道第(len/2) 和 len/2 + 1个数，也是需要遍历len/2 + 1次\n- 返回中位数，奇数需要最后一次遍历结果就可以，偶数需要最后一次和上一次的结果。所以用两个变量left和right。right保存当前循环的结果，在每次循环前将right赋值给left。这样在最后一次循环的时候，left将得到right的值，也就是上一次的循环结果，加下来right更新为最后一次的结果len/2+1次\n\n另一种合并的思想是: 我们可以在奇数的时候, 在**末尾等处添加一个占位符**#等, 这样也是可以将奇数合并成偶数的情况的.此方法的另一个优化点就是 通过在if条件中**加入大量的限制条件**, 从而实现了对于各种边界问题的处理, 这也是一种很重要的思想.\n\n![](https://i.loli.net/2021/09/15/rwI2miAu4SPgOX7.jpg)\n\n![](https://i.loli.net/2021/09/15/G1OiltFhg24JmY8.jpg)\n\n此方法的时间复杂度相对于下面两种思想还是太高了, 大家不用特意掌握此方法, 但是这两个优化的思想还是很重要的, 要好好的理解一下.\n\n接下来我们就来详细讲解两个时间复杂度超低的算法代码思想.\n\n## 寻找第k小数(记住这个)\n\n 主要就是根据两个数的三种比较结果, 不断地去除不满足的元素的过程.\n\n![](https://i.loli.net/2021/09/15/hKBdPugr5fRxXeC.jpg)\n\n这个思想最难的点在于 **三种特殊情况的处理**, 我们能否想到这三种情况, 并将他们**完美的融入到代码之中**, 我感觉这才是真正的难点所在.\n\n![](https://i.loli.net/2021/09/15/AdgYMm3xTtQSup8.jpg)\n\n最开始对于奇数和偶数的两种情况进行了判断, 其实是可以将两种情况合并的, 只需要在奇数时求两次同样的k就可以了.\n\n![](https://i.loli.net/2021/09/15/rmU7B5WJRGELvjd.jpg)\n\n接下来处理了三种特殊情况中的两种特殊情况: 一个数组为空 和 k=1.\n\n![](https://i.loli.net/2021/09/15/XGbolki98Dv6BCK.jpg)\n\n下面的**几个定义**就非常重要了, 一定要弄清这些定义的含义, 才能更轻松的理解代码.\n\n![](https://i.loli.net/2021/09/15/vkMmQjqNbPZGIwn.jpg)\n\nindex1, index2作为数组的起始点的下标, 初值都是0, 但是随着两个数组不断被删除元素, 这两个起始点也是在不断的进行变化, 具体变化方式就是 index1 = newIndex1 + 1, 因为在删除元素的时候 连同比较位置也一同删去了, 所以新的开始是 比较位置 的后一位.\n\nnewindex1, newindex2作为比较点就是图中被框中的两个数的下标, 它的赋值过程就涉及到了 最后一个边界情况. 因为当一个数组较短时, 其中一个比较点可能已经到达了数组的最后, 所以它的值是 两种情况下较小的那个数.\n\n![](https://i.loli.net/2021/09/15/mAW7YsNX5GBcKgH.jpg)\n\n接下来就是根据两个比较点的大小来进行不同的操作过程了, 这里最难理解的点就是 k -= (newIndex1 - index1 + 1), 也就是减去元素的个数问题了. 我们根据上面的图来举例, 图中index1的值为0, newindex1的值经过计算为1, 通过比较后, 可以看到 红色的数 就是被删除的数, 也就是两个, 所以我们需要在最后+1才是真实被删去的个数. 对于此类问题在确定最终个数的时候, 我们都可以通过这样的特例来决定代码的书写, 至此代码就全部讲解完成了.\n\n![](https://i.loli.net/2021/09/15/1hmHAU4Ets7iFJ9.jpg)\n\n## 理解中位数作用进行划分数组 \n\n最后这种思想时间复杂度比上面的还低，上面的思想每一轮循环可以将查找范围减少一半，因此时间复杂度是$O(log(m+n))$ ，但这种思想可以对确定的较短的数组进行二分查找，所以它的时间复杂度是$O(logmin(m,n))$\n\n划分数组正好和上面算法完全相反，它的思想特别复杂，但思想理解了，代码写起来倒是没太大难度。\n\n首先要明白中位数的作用：将一个集合划分为两个长度相等的子集，其中一个子集中的元素总是大于另一个子集中的元素。这种思想无论是在机构数组中都是适用的，这就衍生出了下面的思想。\n\n首先讨论奇偶两种不同情况的不同划分方式，\n\n![](https://i.loli.net/2021/09/16/8n9sSjVk7C3GKZM.jpg)\n\n然后在写代码时，由于计算机的取整操作，我们是可以将这两种情况合并成一种代码书写方式，其中的$i$和$j$分别是两个数组的划分位置。\n\n![](https://i.loli.net/2021/09/16/LzOv1Zxn8pBUVQo.jpg)\n\n同样我们也会遇到复杂的边界问题, 但下面这种处理方式是真的非常优秀.\n\n![](https://i.loli.net/2021/09/16/bBhSzkpAOunIXNj.jpg)\n\n上面问题都考虑完了, 其实就可以写代码了, 但是我们需要进行两个条件的判断: B[j−1]≤A[i] 以及A[i−1]≤B[j], 为了优化代码, 经过分析后, 我们发现这两种情况是可以等价转换的. 也就是只需要进行一个条件的判断即可.\n\n![](https://i.loli.net/2021/09/16/YZ2lJiL9IomnaKH.jpg)\n\n代码中有个注意点就是java中的**三目运算符? :** 在Python中是没有引入这个符号的, 但是Python利用了已有的关键字if...else实现了这个功能.\n\n![](https://i.loli.net/2021/09/16/gRSNcoWaidyOnhz.jpg)\n\n\n\n```java\nclass Solution {\n\n    //常规思想 假合并\n    public double findMedianSortedArrays(int[] nums1, int[] nums2) {\n        int m = nums1.length;\n        int n = nums2.length;\n        int len = m+n;\n        int left = -1, right= -1; // 记录前后两个数\n        int aStart=0, bStart=0;  // 记录两个数组的移动\n        for(int i=0;i<=len/2;i++){\n            left = right; // 每次循环前将right的值赋给left\n            // A移动的条件：B遍历到最后 或当前 A<B 满足一个即可\n            if(aStart < m && (bStart>=n || nums1[aStart] < nums2[bStart])){\n                right = nums1[aStart++];\n            }else{\n                right= nums2[bStart++];\n            }\n        }\n        if((len & 1) == 0) // 与1交，判断奇偶数，更快速\n            return (left+right)/2.0;\n        else\n            return right;\n    }\n\n\n\t\t// 第k小数\n    public double findMedianSortedArrays(int[] nums1, int[] nums2) {\n        int length1 = nums1.length;\n        int length2 = nums2.length;\n        int totallength = length1+ length2;\n        if(totallength % 2 == 1){ // 可以将两种情况合并，奇数会求两次同样的k\n            int midIndex = totallength/2;\n            double median = getKthElement(nums1, nums2, midIndex+1);\n            return median;\n        } else{\n            int midIndex1 = totallength/2 -1 , midIndex2 = totallength/2;\n            double median = (getKthElement(nums1, nums2, midIndex1 + 1) + getKthElement(nums1, nums2, midIndex2 + 1)) / 2.0;\n            return median;\n        }\n    }\n\n    public int getKthElement(int[] nums1, int[] nums2, int k){\n        /**\n        主要思路：要找到第k(k>1)小的元素，那么就取pivot1 = nums1[k/2-1] 和 pivot2 = nums2[k/2-1] 进行比较\n        这里的“/” 表示整除\n        nums1 中小于等于pivot1的元素有 nums1[0..k/2-2] 共计k/2-1个\n        nums2 中小于等于pivot2的元素有 nums2[0..k/2-2] 共计k/2-1个\n        取 pivot = min(pivot1 , pivot2) 两个数组中小于等于 pivot的元素共计不会超过(k/2-1)+(k/2-1) <=k-2个\n        这样pivot本身最大也只能是第k-1小的元素\n        * 如果 pivot = pivot1,那么 nums1[0 .. k/2-1] 都不可能是第 k 小的元素。把这些元素全部 \"删除\",剩下的作为新的 nums1 数组\n         * 如果 pivot = pivot2,那么 nums2[0 .. k/2-1] 都不可能是第 k 小的元素。把这些元素全部 \"删除\",剩下的作为新的 nums2 数组\n        由于我们 \"删除\" 了一些元素（这些元素都比第 k 小的元素要小）,因此需要修改 k 的值,减去删除的数的个数\n         */\n        int length1 = nums1.length, length2=nums2.length;\n        int index1 = 0, index2=0;\n        int kthElemnt=0;\n        while(true){\n            //特殊情况\n            if(index1==length1){//第二种特殊情况，一个数组为空\n                return nums2[index2+k-1];\n            }\n            if(index2==length2){//一个数组为空\n                return nums1[index1+k-1];\n            }\n            if(k==1){ // 第三种情况，k=1\n                return Math.min(nums1[index1], nums2[index2]);\n            }\n            // 正常情况， index1，index2作为起始点，newindex1,newindex2作为比较点 在不停的更新\n            int half = k/2;\n            int newIndex1 = Math.min(index1 + half, length1) -1;// 第一种情况，发生越界，需要记录比较的位置\n            int newIndex2 = Math.min(index2 + half, length2) - 1;\n            int pivot1 = nums1[newIndex1], pivot2=nums2[newIndex2]; //获取两个需要比较的数\n            if(pivot1<=pivot2){\n                // 将两种情况合并\n                k -= (newIndex1 -index1 +1); //两者相减后+1，这才是真正减去的长度\n                index1 = newIndex1+1; //连同比较位置也一同删去了，所以新的开始是 比较位置的后一位\n            } else{\n                k -= (newIndex2-index2+1);\n                index2 = newIndex2+1;\n            }\n        }\n    }\n  \n  \tpublic double findMedianSortedArrays(int[] nums1, int[] nums2) {\n        if (nums1.length > nums2.length) {\n            return findMedianSortedArrays(nums2, nums1);\n        }\n\n        int m = nums1.length;\n        int n = nums2.length;\n        int left = 0, right = m;\n        // median1：前一部分的最大值\n        // median2：后一部分的最小值\n        int median1 = 0, median2 = 0;\n\n        while (left <= right) { // 一直循环找到一个最大的i满足A[i-1]≤B[j]\n            // 前一部分包含 nums1[0 .. i-1] 和 nums2[0 .. j-1]\n            // 后一部分包含 nums1[i .. m-1] 和 nums2[j .. n-1]\n            int i = (left + right) / 2; //二分法,i从区间中间开始\n            int j = (m + n + 1) / 2 - i;//+1的操作将总数为奇数和偶数合并为一种情况\n\n            //nums_im1, nums_i, nums_jm1, nums_j 分别表示 nums1[i-1], nums1[i], nums2[j-1], nums2[j]\n            //当一个数组不出现在前一部分时,对应的值为负无穷,就不会对前一部分的最大值产生影响\n            int nums_im1 = (i == 0 ? Integer.MIN_VALUE : nums1[i - 1]);\n            //当一个数组不出现在后一部分时,对应的值为正无穷,就不会对后一部分的最小值产生影响\n            int nums_i = (i == m ? Integer.MAX_VALUE : nums1[i]);\n            int nums_jm1 = (j == 0 ? Integer.MIN_VALUE : nums2[j - 1]);\n            int nums_j = (j == n ? Integer.MAX_VALUE : nums2[j]);\n\n            if (nums_im1 <= nums_j) {\n                median1 = Math.max(nums_im1, nums_jm1);\n                median2 = Math.min(nums_i, nums_j);\n                left = i + 1;\n            }\n            else {\n                right = i - 1;\n            }\n        }\n        return (m + n) % 2 == 0 ? (median1 + median2) / 2.0 : median1;\n    }\n\n}\n```\n\n```python\nclass Solution(object):\n    # 常规思想\n    def findMedianSortedArrays1(self, nums1, nums2):\n        \"\"\"\n        :type nums1: List[int]\n        :type nums2: List[int]\n        :rtype: float\n        \"\"\"\n        m = len(nums1)\n        n = len(nums2)\n        lens = m+n\n        left, right = -1, -1\n        aStart,bStart = 0,0\n        for i in range(lens//2+1):\n            left = right  # 每次循环将right的值赋值给left\n            # A移动的条件，B遍历到最后 或A<B满足一个\n            if aStart<m and (bStart >= n or nums1[aStart] < nums2[bStart]):\n                right = nums1[aStart]\n                aStart+=1\n            else:\n                right = nums2[bStart]\n                bStart+=1\n        if (lens & 1)  == 0:\n            return (left+right)/2.0\n        else:\n            return right\n          \n          \n    # 第k小数\n    def findMedianSortedArrays(self, nums1, nums2):\n        def getKthElemnt(k):\n            index1, index2 = 0, 0\n            while True:\n                # 特殊情况\n                if index1 == m:\n                    return nums2[index2+k-1]\n                if index2 == n:\n                    return nums1[index1+k-1]\n                if k==1:\n                    return min(nums1[index1], nums2[index2])\n\n                newIndex1 = min(index1 + k//2 -1, m-1)\n                newIndex2 = min(index2 + k//2 -1, n-1)\n                privot1,privot2 =nums1[newIndex1], nums2[newIndex2]\n                if privot1<=privot2:\n                    k-=newIndex2-index1 +1\n                    index1 = newIndex2 + 1\n                else:\n                    k -= newIndex2 - index2 +1\n                    index2 = newIndex2 +1\n\n        m,n = len(nums1), len(nums2)\n        totalLength = m+n\n        if totalLength % 2==1:\n            return getKthElemnt((totalLength+1)//2)\n        else:\n            return (getKthElemnt(totalLength//2)+ getKthElemnt(totalLength//2+1)) /2 \n          \n    # 划分数组\n    def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -> float:\n        if len(nums1) > len(nums2):\n            return self.findMedianSortedArrays(nums2, nums1)\n\n\n        infinty = 2**40  # 代表正无穷\n        m, n = len(nums1), len(nums2)\n        left, right = 0, m\n        # median1：前一部分的最大值\n        # median2：后一部分的最小值\n        median1, median2 = 0, 0\n\n\n        while left <= right: # 一直循环找到一个最大的i满足A[i−1]≤B[j]\n            # 前一部分包含 nums1[0 .. i-1] 和 nums2[0 .. j-1]\n            # // 后一部分包含 nums1[i .. m-1] 和 nums2[j .. n-1]\n            i = (left + right) // 2\n            j = (m + n + 1) // 2 - i\n\n\n            # nums_im1, nums_i, nums_jm1, nums_j 分别表示 nums1[i-1], nums1[i], nums2[j-1], nums2[j]\n            # 当一个数组不出现在前一部分时,对应的值为负无穷,就不会对前一部分的最大值产生影响\n            nums_im1 = (-infinty if i == 0 else nums1[i - 1]) # 注意写法与java不同\n            # 当一个数组不出现在后一部分时,对应的值为正无穷,就不会对后一部分的最小值产生影响\n            nums_i = (infinty if i == m else nums1[i])\n            nums_jm1 = (-infinty if j == 0 else nums2[j - 1])\n            nums_j = (infinty if j == n else nums2[j])\n\n\n            if nums_im1 <= nums_j:\n                median1, median2 = max(nums_im1, nums_jm1), min(nums_i, nums_j)\n                left = i + 1\n            else:\n                right = i - 1\n\n\n        return (median1 + median2) / 2 if (m + n) % 2 == 0 else median1\n\n\n```\n\n\n\n#### [](https://leetcode-cn.com/problems/median-of-two-sorted-arrays/)\n\n![](https://i.loli.net/2021/09/15/roqxyuNvdGnAlX7.jpg)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["LeetCode"]},{"title":"Multimodal Emergent Fake News Detection via Meta Neural Process Networks","url":"/2021/09/11/Multimodal-Emergent-Fake-News-Detection-via-Meta-Neural-Process-Networks/","content":"\n# Multimodal Emergent Fake News Detection via Meta Neural Process Networks\n\n基于深度学习的模型在对感兴趣事件的大量标注数据进行训练时表现出较好的性能，而在其他事件上由于领域漂移的影响，模型的性能往往会下降。此外，添加来自新出现的事件的知识需要从头开始构建新的模型或继续微调模型，这对于现实世界的设置来说是不切实际的。（需新注入知识）\n\n假新闻通常出现在新到的活动上，我们很难及时获得足够的帖子。在突发事件的早期阶段，我们通常只有少数相关的验证帖子。 如何利用一小部分经过验证的帖子来使模型快速学习。(few-shot挑战)\n\n本文作者提出 MetaFEND 将元学习(meta learning)和神经过程(np)方法集成在一起解决此类问题。 还提出了标签嵌入模块和硬注意机制，通过处理分类信息和修剪无关帖子来提高效率。\n\nMetaFEND的目标是：调整参数以更好地利用给定的支持数据点作为条件。\n\n##  Limitations of Current Techniques\n\nfew-shot learning 是一种为了克服利用一小部分数据实例进行快速学习。\n\n元学习是一种促进few-shot learning的研究路线，其基本思想是利用以前任务中的全局知识来促进新任务的学习。然而，现有元学习方法的成功与一个重要的假设高度相关：任务来自相似的分布，共享的全局知识适用于不同的任务。这种假设在假新闻识别问题中通常不成立，因为不同事件新闻的写作风格、内容、词汇量甚至类别分布往往不尽相同。\n\n如下图假新闻在事件中的比例明显不同。\n\n![](https://i.loli.net/2021/09/18/iFMmdJVBwKY8tNA.png)\n\n不同事件之间的显著差异对事件异构性提出了严峻的挑战，这不能简单地通过全局共享知识来解决\n\n另一个少机会学习的研究方向是 neural processes [Attentive neural processes]，它使用一小部分数据实例作为条件进行推理。尽管神经过程表现出更好的泛化能力，但它们基于一组固定的参数，并且通常受到欠拟合等限制，从而导致性能不令人满意。\n\n这两种模型的研究思路是相辅相成的：\n\n- 元学习中的参数自适应机制可以提供更多的参数灵活性，以缓解神经过程的不匹配问题。\n- neural processes 可以使用一小部分数据实例作为条件，而不是将所有信息编码到参数集中，从而帮助应对MAML的异构性挑战。\n\n尽管将这两种流行的小范围方法集成在一起是有希望的，但在给定的小数据实例集上的不兼容操作是基于这两种方法开发模型的主要障碍。\n\n## BACKGROUND\n\n假新闻定义为故意编造的、可以查证为虚假的新闻。\n\n作者的目标是利用从过去事件中学到的知识，通过几个例子对新发生的事件进行有效的假新闻检测。更正式地，我们将假新闻检测定义为紧随少镜头问题。\n\n设 $E$ 表示一组新闻事件。在每个新闻事件$e∼E$中，有一些关于这事件$e$的带标签的贴子。\n\n在培训阶段的每一集中，标记的帖子被划分为两个独立的集合：support set和query set 。 利用support set对模型进行训练，学习如何对query set 进行假新闻检测\n\nsupprt set : $\\{X_e^s, Y_e^s\\} = \\{x_{e,i}^s, y^s_{e,i}\\}_{i=1}^K$\n\nquery set  : $\\{X_e^q,Y_e^q\\} = \\{x_{e,i}^q, y_{e,i}^q\\}_{i=K+1}^N$ \n\n在推理阶段，为每个事件提供带有 $K$ 个标签的帖子。该模型利用其对应的K个标签帖子作为支持集，对给定事件e进行假新闻检测。\n\n### MAML\n\n元学习过程分为两个阶段：元训练和元测试。\n\n在元训练阶段，基线模型 $f_{\\theta}$ 借助于支持集，根据具体事件 e 进行调整。例如一个事件的具体模型$f_{\\theta_e}$ 在对应的query set上被评估，loss $L(f_{\\theta_e}, \\{X_e^q,Y_e^q\\})$  在 $\\{X_e^q,Y_e^q\\}$ 被用于更新基线模型 $f_{\\theta}$。 \n\n在元测试阶段，基线模型 $f_\\theta$ 根据事件$e'$进行调整，使用元训练阶段过程中获取的特定于事件的参数$\\theta_e'$，用于对事件$e'$的查询集 ${X_{e'}^q,Y_{e'}^q}$ 进行预测。\n\nMAML更新参数向量 $\\theta$ 使用事件 $e$ 上的一个或多个梯度下降更新。例如，使用一个梯度更新时：\n$$\n\\theta_e = M(f_{\\theta}, \\{X_e^s,Y_e^s\\}) = \\theta - \\alpha \\nabla_{\\theta} L(f_{\\theta}, \\{X_e^s,Y_e^s\\})\n$$\n通过优化模型的性能来训练模型参数关于 $\\theta$ 跨从中采样的事件$p(E)$。更具体地说，元目标如下：\n$$\nmin_{\\theta} \\sum_{e \\sim E} L(f_{\\theta_i}) = \\sum_{e \\sim E} L(f_{\\theta-\\alpha L(f_{\\theta,\\{X_e^s,Y_e^s\\}})}, {X_e^q,Y_e^q})\n$$\n\n### Limitations of MAML\n\nMAML可以通过一个和几个梯度更新捕捉任务不确定性。然而，在虚假新闻检测中问题是，当事件是异构的时，事件不确定性很难通过一个或多个梯度步骤编码到参数中。\n\n此外，即使给定的支持数据和感兴趣的查询数据来自同一事件，也不能保证它们都高度相关。在这种情况下，支持集上假新闻检测损失的参数自适应可能会对某些帖子产生误导。\n\n### Conditional Neural Process (CNP)\n\nCNP包括四个主要部分：编码器、特征提取器、聚合器和解码器。这个条件神经过程的基本思想借助 $\\{X_e^s,Y_e^s\\} = \\{x_{e,i}^s,y_{e,i}^s\\}^K_{i=1}$ 作为上下文进行预测。\n$$\np(y_{e,i}^q| \\{X_e^s,Y_e^s\\}, x_{e,i}^q) = f(agg(g(\\{X_e^s,Y_e^s\\}))  \\oplus h(x_{e,i}^q))\n$$\n\n### Limitations of CNP\n\nCNP的一个广泛认可的限制是欠拟合。对于不同的上下文数据点，它们在预测中的重要性通常是不同的。然而，CNP的聚合器对所有支持数据一视同仁，无法获得依赖于查询的上下文信息。此外，CNP只是将帖子的输入特征和数字标签值连接在一起作为输入，而忽略标签的分类特征。\n\n\n\n## Method\n\n提出了一种 meta neural process框架，它可以通过模拟任务将元学习方法和神经过程方法融合在一起。\n\n为了应对异构新闻事件带来的挑战，我们进一步提出了一个标签嵌入组件来处理分类标签，以及一个hard attention件，该组件可以从具有不平衡类分布的支持集中选择信息量最大的信息。\n\n![](https://i.loli.net/2021/09/20/HOkN9KbidLxR37y.png)\n\n### Meta-learning Neural Process Design\n\n提出的框架包括两个阶段：\n\n- 事件适应：事件适应阶段是在支持集的帮助下，使模型参数适应特定事件。\n- 事件监测：检测阶段是在支持度和自适应参数集的帮助下，检测给定事件的假新闻。\n\n#### 事件适应\n\n第i个support data $\\{x_{e,i}^s,y_{e,i}^s\\}$ 是一个例子，在事件适应阶段，$\\{x_{e,i}^s,y_{e,i}^s\\}$ 被用作目标数据，剩余的support set $\\{X_e^s,Y_e^s\\} \\setminus {x_{e,i}^s, y_{e,i}^s}$  ，被用于上下文集，$\\setminus$是差集的意思。\n\n在support set上事件适应目标函数：\n$$\nL_e^s = \\sum_i log p_{\\theta}(y_{e,i}^s| \\{X_e^s,Y_e^s\\} \\setminus \\{x_{e,i}^s,y_{e,i}^s\\}, x_{e,i}^s)\n$$\n然后，我们在 $L_e^s$ 上为事件 e 更新参数 $\\theta$ 。一个或多个梯度下降更新。例如，使用一个梯度更新时：\n$$\n\\theta_e = \\theta - \\alpha \\nabla_{\\theta} L_{e}^s\n$$\n\n#### 识别阶段\n\n提出的模型具有特定事件参数集 $\\theta_e$ , 获取query set $X_e^q$ 和整个support set $\\{X_e^s,Y_e^s\\}$ 作为输入和输出预测 $\\hat Y_e^q $对于 query set $X_e^q$ 。 对应识别阶段的loss：\n$$\nL_e^q = log p_{\\theta_e} (Y_e^q | X_e^s,Y_e^s,X_e^q)\n$$\n通过这个元神经过程，我们可以学习一个初始化参数集，该参数集可以快速地学习使用给定的上下文输入输出作为条件条件来检测关于新到达事件的假新闻。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"11-盛最多的水的容器","url":"/2021/09/11/11-盛最多的水的容器/","content":"\n# 11-盛最多的水的容器\n\n#### [11. 盛最多水的容器](https://leetcode-cn.com/problems/container-with-most-water/)\n\n## 双指针\n\n### 双指针的思想来源\n\n双指针大多都是对两层循环的优化，所以当暴力法设计到两层循环遍历的时候，我们就应该有这种思想：能不能用双指针。\n\n其次，就是要有限制的满足条件，对于本题而言，限制的满足条件就是 每次移动数字较小的那个指针，我们必须根据题目找到某个条件，而这个条件就是双指针的移动条件，也就是双指针思想的基础。\n\n### 双指针移动的条件\n\n要找到这个条件，我们就需要从题目的定义出发，也就是容纳的水量的含义，当左右指针分别指向数组的左右两端，那么， 容纳的水量 = 两个指针指向的数字中较小的值 x 指针之间的距离。\n\n接下来就只要考虑移动双指针后两者的变化情况即可。如果我们移动数字较大的那个指针，那么前者 【两个指针指向的数字中较小值】不会增加，后者 【指针之间的距离】会减小，那么这个成绩会减小。因此，我们移动数字较小的那个指针，这是从公式变化角度解释如何移动指针的\n\n### 双指针合理性的证明\n\n双指针的含义：\n\n- 双指针代表的是，可以作为容器边界的所有位置的范围\n- 在一开始，双指针指向数组的左右边界，表示数组中所有的位置都可以作为容器的边界。\n- 在这之后，我们每次将，对应的数字较小的那个指针往另一个指针的方向移动一个位置，就表示我吗任务 这个指针不再能作为容器的边界了。\n\n### 两个优化点\n\n- 当任意一边移动后，如果高度低于之前的高度，那么继续移动就行，不需要计算面积和比对两步多余操作。\n- 两边相等时，可以同时移动左右边界，相等的时候，两边界都是瓶颈，就算只移动一侧，移动后得到的容器也受之前瓶颈的影响而只会减小，所以如果想得到更大的容积，同时移动两侧就可以了（同时跳过瓶颈）\n\n```python\nclass Solution:\n    def maxArea(self, height: List[int]) -> int:\n        l, r = 0, len(height) - 1\n        ans = 0\n        while l < r:\n            area = min(height[l], height[r]) * (r - l)\n            ans = max(ans, area)\n            if height[l] <= height[r]:  # 移动较小的那一端\n                l += 1\n            else:\n                r -= 1\n        return ans\n\n```\n\n\n\n```java\npublic class Solution {\n    public int maxArea(int[] height) {\n        int l = 0, r = height.length - 1;\n        int ans = 0;\n        while (l < r) {\n            int area = Math.min(height[l], height[r]) * (r - l);\n            ans = Math.max(ans, area);\n            if (height[l] <= height[r]) {  // 移动较小的那一端\n                ++l;\n            }\n            else {\n                --r;\n            }\n        }\n        return ans;\n    }\n}\n\n\n```\n\n![](https://i.loli.net/2021/09/22/FvEY1UzIbAVuZan.jpg)","tags":["LeetCode"]},{"title":"1-两数之和到四数之和","url":"/2021/09/11/1-两数之和到四数之和/","content":"\n# 1-两数之和到四数之和\n\n给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target  的那 两个 整数，并返回它们的数组下标。\n\n你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。\n\n你可以按任意顺序返回答案。\n\n \n\n示例 1：\n\n输入：nums = [2,7,11,15], target = 9\n输出：[0,1]\n解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。\n示例 2：\n\n输入：nums = [3,2,4], target = 6\n输出：[1,2]\n示例 3：\n\n输入：nums = [3,3], target = 6\n输出：[0,1]\n\n\n\n## 两数之和\n\n### 哈希表法\n\n哈希表的方法首先用在了两数之和(无序数组)上, 哈希表的使用最主要的目的就是为了降低时间复杂度, 缩减寻找第二个元素使用的时间(将时间复杂度由O(n)降为O(1)), 其中无序数组是哈希表使用的重要条件, 因为当数组有序后, 我们完全可以直接使用 双指针 的方法来降低时间复杂度, 它的使用比 哈希表 更加方便快捷, 空间复杂度也更低, 所以数组有序之后, 我们应该首选 双指针 的方法.\n\n在使用哈希表的时候, 也有一个很重要的优化点, 就是 遍历两遍哈希表 和 遍历一遍哈希表 的区别. 简单来说就是, 如果我们先将第一个元素放入哈希表中, 然后再寻找第二个元素, 那么我们就需要 遍历两遍哈希表, 如果我们先寻找第二个元素, 之后再将元素放入到哈希表中, 那么就只需要 遍历一遍哈希表.\n\n![](https://i.loli.net/2021/09/11/vUaHFiVBCbOrKlx.jpg)\n\n上面是我们第一次使用哈希表的情况, 第二次使用哈希表就到了 《四数之和II四数组相加》, 首先由于它具有四个独立的数组, 相当于四维空间, 所以我们很难在这么高的空间维度上直接使用 双指针 的方法, 其次它并没有要求 不重复元组 的情况, 这就给了我们使用 哈希表 的可能性, 因为不用担心复杂的去重操作, 但是使用哈希表一般也是两维的空间, 所以我们必须先进行降维操作, 也就是将四个数组进行分组, 由三种结果的时间复杂度来判断, 我们很容易就选择了 两两分组 的情况.\n\n![](https://i.loli.net/2021/09/11/Gi9Cge7pl1Uuhm5.jpg)\n\n之后对于哈希表的使用, 就是两种不同情况的使用了。如果需要直接返回相应数组的下标值, 那是很简单的, 我们只需要将 下标值 当做 哈希表的值 即可。(两数之和中的使用)\n\n```java\npublic int[] twoSum(int[] nums, int target){\n  \tMap<Integer,Integer> map = HashMap<Integer,Integer>();\n  \tfor(int i=0;i < nums.length;i++){\n      \tif(map.containsKey(target - nums[i])){\n          \treturn new int[]{map.get(target-nums[i]), i};\n        }\n      \tmap.put(nums[i], i);\n    }\n  \treturn new int[0];\n}\n```\n\n```python\nclass Solution(object):\n    def twoSum(self, nums, target):\n        \"\"\"\n        :type nums: List[int]\n        :type target: int\n        :rtype: List[int]\n        \"\"\"\n        hashtable = dict()\n        for i,num in enumerate(nums):\n          \tif target - num in hashtable:\n              \treturn [hashtable[target-nums], i]\n            hashtable[num] = i\n        return []\n```\n\n\n\n### 双指针法\n\n对于n数之和, 除了哈希表的方法, 最常用的就是 双指针 的方法了, 上文也提到了, 使用双指针最重要的条件就是数组是有序的, 当然这只是针对n数之和的题型, 对于其他题型, 并不需要要求数组是有序\n\n在n数之和中使用双指针必要条件就是数组是有序的, 这就需要我们根据实际情况来判断 数组是否需要进行排序. 比如在 两数之和 中, 就算使用暴力法也才$O(n^2)$, 但进行排序最快也需要$O(nlogn)$的时间复杂度, 所以对于两数之和来说, 是真的没必要.\n\n但是对于 三数之和 和 四数之和 就很有必要了, 因为它们时间复杂度实在太高了, 最关键的是它们元组的重复情况也比较多, 想利用哈希表进行去重是非常困难的, 最终只能选择将数组排序后使用 双指针 的方法.\n\n#### [167. 两数之和 II - 输入有序数组](https://leetcode-cn.com/problems/two-sum-ii-input-array-is-sorted/)\n\n![](https://z3.ax1x.com/2021/09/11/hxu1ER.png)\n\n#### 两数之和中有序和无序的区别\n\n在无序数组中寻找第二个数就没有多少捷径, 毕竟数组无序, 很多经典的方法都用不上, 最后只能牺牲空间来换取时间, 利用哈希表将空间复杂度增加到了$O(n)$, 从而降低了寻找第二个数的时间复杂度.\n\n但是当数组有序之后, 就能使用一些经典的算法同时仍然保证空间复杂度为O(1), 不需要牺牲空间来换取时间, 比如下面马上介绍的 二分法 和 双指针 方法.\n\n这里给我们提供了一种思维, 那我们是不是也可以将无序数组先进行排序后, 再使用这些经典算法呢? 当然可以这么做, 但对于两数之和来说, 必要性不是太大. 因为最快的排序算法也需要O(nlogn)的时间复杂度, 对于两数之和确实提升也不是太大, 但是对于 三数之和/四数之和 还是挺实用的, \n\n#### 二分法和寻找插入位置的区别\n\n数组有序了，使用二分法寻找第二个数可以将时间复杂度降到$O(nlogn)$ 。\n\n寻找插入位置无论，最终无论是否找到目标值，返回的位置结果都是相同的，而且题中说明数组中无重元素，保证了返回位置的唯一性，所以最终 left == right == mid，返回哪个都无所谓，也并不是需要特殊的将等于目标值这种情况单独写出来。所以代码只讨论了两种情况，最终返回一个值，非常简洁。\n\n本题使用的二分法，首先并没有要求数组无重复元素，其次我们要的是具体的等于目标值的位置，并不是寻找插入位置，所以在找不到的情况下，只能返回[-1,-1]。首先的返回结果就有了两种情况.\n\n其次由于有重复元素的存在, 若直接使用之前的只讨论两种情况的二分法是会出错的, 这里必须要讨论三种情况, 且在相等的情况下直接返回正确的结果, 在找不到的情况下返回 [-1, -1].\n\n本题另外的一个小的注意点是: 返回的下标从1开始, 只要在原本的返回结果上+1就可以了.\n\n还有一个注意点是, 为了避免重复寻找,在寻找第二个数时,只在第一个数的右侧寻找, 也就是left = i+1.\n\n```python\nclass Solution(object):\n    # 二分法\n    def twoSum(self, numbers, target):\n        \"\"\"\n        :type numbers: List[int]\n        :type target: int\n        :rtype: List[int]\n        \"\"\"\n        n = len(numbers)\n        for i in range(n):\n            left, right = i+1, n  # 采用左闭右开区间[left, right), left+1避免重复\n            while left<right:\n                mid = (right - left) //2 + left; # 防止溢出\n                if numbers[mid] == target - numbers[i]: # 数组中存在重复元素，必须判断相等\n                    return [i+1, mid+1]\n                elif numbers[mid] > target - numbers[i]:\n                    right = mid # 右开，真正右端点为mid-1\n                else:\n                    left = mid + 1 # 左闭，所以小+1\n        return [-1, -1]\n    # 双指针\n    def twoSum1(self, numbers, target):\n        low, high =0, len(numbers-1)\n        while low<high:\n            total = numbers[low] + numbers[high]\n            if total == target:\n                return [low+1, high+1] # 返回下标从1开始\n            elif total<target:\n                low+=1\n            else:\n                high-=1\n```\n\n```java\nclass Solution {\n    // 二分法\n    public int[] twoSum(int[] numbers, int target) {\n        for(int i=0;i<numbers.length;i++){\n            int left = i+1, rigth = numbers.length;  // 采用左闭右开区间[left,right),left+1避免重复\n            while(left<right){\n                mid = left + right /2 + left; // 防止溢出\n                if(numbers[mid] == target - numbers[i]){ // 数组中存在重复元素,必须判断相等\n                    return new int[]{i+1, mid+1} ; // 返回的下标从1开始,都+1\n                }else if(numbers[mid] > target-numbers[i]){ //中点大于目标值,在左侧\n                    right = mid;//右开，真正右端点为mid-1\n                }else{\n                    left = mid+1; // 左闭 +1\n                }\n            }\n        }\n        return new int[]{-1,-1};\n    }\n    // 双指针法\n    public int[] twoSum2(int[] numbers, int target){\n        int low = 0, high = numbers.length-1;\n        while (low < high) { // 指针移动条件\n            int sum = numbers[low] + numbers[high];\n            if (sum == target) {\n                return new int[]{low + 1, high + 1}; // 返回下标从1开始\n            } else if (sum < target) {\n                ++low;\n            } else {\n                --high;\n            }\n        }\n        return new int[]{-1, -1};\n    }\n}\n```\n\n\n\n## 三数之和\n\n#### [15. 三数之和](https://leetcode-cn.com/problems/3sum/)\n\n![](https://i.loli.net/2021/09/12/6x3jq9QeCmGfa2B.jpg)\n\n难点在于题目要求的不重复的三元组，它的可重复情况是非常多的，无法像两数之和那样，只要将第一个元素放入哈希表中，就可以轻松解决元素重复的问题了。\n\n对于三数之和，即使使用哈希表去重，它的操作也是比较困难的，所以不能简单的使用三重循环枚举所有的三元组，然后用哈希表去重，这样工作量比较大。\n\n因此必须换一种做法，从源头上解决元素重复问题，如果给定数组是有序的，那么其中可重复的情况就是可控的了，处理起来也简单，所以先把数组从小到大排序，随后用普通的三重循环就可。\n\n然后就会发现，重复的元素都是相邻元素，只要保证每一重循环时，相邻两次枚举的元素不是相同的元素，这样就可以避免元组重复的情况了。\n\n### 双指针\n\n使用普通的三层循环确实也能解决问题, 但是 $O(n^3)$ 的时间复杂度也确实太高了, 这时我们想到了在 有序数组的两数之和 中使用的双指针的方式, 虽然现在是三数之和, 但当我们正常遍历了第一层循环之后, 剩下的两个数不就形成了 有序数组的两数之和 了吗? 所以我们只要 保持第二重循环不变, 而将第三重循环变成一个从数组最右端开始向左移动的指针, 同时加上上述讨论的避免重复的条件, 最终代码就完成了.\n\n```java\nclass Solution {\n    public List<List<Integer>> threeSum(int[] nums) {\n        List<List<Integer>> res = new ArrayList<>();\n        if(nums.length==0 ||  (nums[0]==0 && nums.length==1))  return res;\n\n        Arrays.sort(nums);\n\n       \n        // 枚举a\n        for(int first=0;first<nums.length;first++){\n            //需要和上一次枚举的数不同\n            if(first>0 && nums[first] == nums[first-1]){\n                continue;\n            }\n            // c对应的指针初始指向数组的最右排\n            int third = nums.length-1;\n            int target = -nums[first];\n            //枚举b\n            for(int second= first+1 ;second<nums.length;second++){\n                // 需要和上一次枚举不同\n                if(second > first+1 && nums[second] == nums[second-1]) {\n                    continue;\n                }\n                // 需要保证b指针在c指针的左侧\n                while(second < third && nums[second]+nums[third] > target){\n                    --third;\n                }\n                //如果指针重合，后续也不会满足条件，可以退出循环\n                if(second==third) break;\n                if (nums[second] + nums[third] == target){\n                    res.add(Arrays.asList(nums[first],nums[second],nums[third]));\n                }\n            }\n\n        }\n        return res;\n\n    }\n}\n```\n\n\n\n```python\nclass Solution(object):\n    def threeSum(self, nums):\n        \"\"\"\n        :type nums: List[int]\n        :rtype: List[List[int]]\n        \"\"\"\n        res = list()\n        if len(nums) == 0: return res\n        nums.sort()\n\n        # 枚举a\n        for first in range(len(nums)):\n            if first>0 and nums[first] == nums[first-1]:\n                continue\n            thrid = len(nums) -1\n            target = -nums[first]\n            for second in range(first+1, len(nums)):\n                if second> first+1 and nums[second] == nums[second-1]:\n                    continue\n                while second<thrid and nums[second] + nums[thrid] > target:\n                    thrid-=1\n                if second==thrid:\n                    break\n                if nums[second]+nums[thrid] == target:\n                    res.append([nums[first], nums[second], nums[thrid]])\n        return res\n        \n```\n\n## 三数之和变体\n\n#### [16. 最接近的三数之和](https://leetcode-cn.com/problems/3sum-closest/)\n\n还是使用双指针解决\n\n排序+双指针\n\n```java\nclass Solution {\n    public int threeSumClosest(int[] nums, int target) {\n        Arrays.sort(nums);\n        int n = nums.length;\n        int best = 10000000;\n\n        // 枚举 a\n        for (int i = 0; i < n; ++i) {\n            // 保证和上一次枚举的元素不相等\n            if (i > 0 && nums[i] == nums[i - 1]) {\n                continue;\n            }\n            // 使用双指针枚举 b 和 c\n            int j = i + 1, k = n - 1;\n            while (j < k) {\n                int sum = nums[i] + nums[j] + nums[k];\n                // 如果和为 target 直接返回答案\n                if (sum == target) {\n                    return target;\n                }\n                // 根据差值的绝对值来更新答案\n                if (Math.abs(sum - target) < Math.abs(best - target)) {\n                    best = sum;\n                }\n                if (sum > target) {\n                    // 如果和大于 target，移动 c 对应的指针\n                    int k0 = k - 1;\n                    // 移动到下一个不相等的元素\n                    while (j < k0 && nums[k0] == nums[k]) {\n                        --k0;\n                    }\n                    k = k0;\n                } else {\n                    // 如果和小于 target，移动 b 对应的指针\n                    int j0 = j + 1;\n                    // 移动到下一个不相等的元素\n                    while (j0 < k && nums[j0] == nums[j]) {\n                        ++j0;\n                    }\n                    j = j0;\n                }\n            }\n        }\n        return best;\n    }\n}\n\n```\n\n时间复杂度：$O(N^2)$ 其中 N 是数组 $\\textit{nums}$ 的长度。我们首先需要 $O(N \\log N)$ 的时间对数组进行排序，随后在枚举的过程中，使用一重循环 O(N) 枚举 a，双指针 O(N) 枚举 b 和 c，故一共是 O(N^2)\n\n空间复杂度：$O(\\log N)$。排序需要使用 $O(\\log N)$ 的空间。然而我们修改了输入的数组 $\\textit{nums}$，在实际情况下不一定允许，因此也可以看成使用了一个额外的数组存储了 $\\textit{nums}$ 的副本并进行排序，此时空间复杂度为 O(N)。\n\n\n\n\n\n## 四数之和\n\n#### [18. 四数之和](https://leetcode-cn.com/problems/4sum/)\n\n![](https://i.loli.net/2021/09/13/mxBF4fzwX9Crky5.jpg)\n\n### 思想同三数之和：排序+双指针\n\n四数之和 本质上 和三数之和一样的，由于都有大量的重复元素在，都不能使用哈希表进行简单的去重，都需要先进行排序后才方便遍历处理，同时为了优化时间复杂度，再加上双指针方法的使用，如果只是想简单实现的话，那么在三数之和上直接多加一重循环。但在代码上不同点在于：并非只是简单的家一重循环而已，而是进行了优化处理。\n\n因为四数之和相比较于三数之和来说，情况更加复杂，时间复杂度也比较高，而且这个时间复杂度通过算法降下来很难。只能通过对代码的优化，直接减少大量不必要的遍历情况，从而来缩短代码的运行时间。\n\n对于代码的优化主要分为两大块：一部分是为了避免出现重复的四元组，在遍历上的优化，这部分和三数之和相似，不过更复杂。\n\n首先是对前两重循环进行的去重操作, 当 i 或者 j 的值与前面的值相等时忽略, 之后又对 双指针 进行了去重操作, 这里有个重要的注意点: 一定注意代码中是 先进行了指针的移动还是先进行了去重的比较, 对于不同的顺序, 比较的元素是完全不同的. 如果先进行了指针的移动, 对于左指针来说, 需要比较的元素就是 当前元素和前面的一个元素, 如果是先进行去重的比较, 那比较的元素就是 当前元素和后面的一个元素, 再进行指针的移动. 对于右指针的情况正好是完全相反的.\n\n第二部分就是在循环遍历中先通过计算特定的四个数之和, 以此来判断接下来的循环操作情况.\n\n比如 在确定第一个数 nums[i] 之后, 如果nums[i]+nums[i+1]+nums[i+2]+nums[i+3]>target, 也就是此时的最小的4个数之和都大于target, 说明此时剩下的三个数无论取什么值, 四数之和一定大于 target, 因此直接退出第一重循环就可以了\n\n在确定第一个数 nums[i] 之后,如果nums[i]+nums[n−3]+nums[n−2]+nums[n−1]<target, 也就是此时的最大的4个数之和都小于target, 说明此时剩下的三个数无论取什么值, 四数之和一定小于 target,因此第一重循环直接进入下一轮, 枚举nums[i+1], 使用 continue 关键字.\n\n对于第二层循环也是同样的判断方法, 通过这两层循环的判断优化, 能直接删去大量的不满足情况, 减少代码运行的时间. 这也能给我们带来启发, 在算法层面不能进行优化的时候, 可以选择对代码的细节进行优化, 同样可以起到节省时间的效果.\n\n```python\nclass Solution(object):\n    def fourSum(self, nums, target):\n        \"\"\"\n        :type nums: List[int]\n        :type target: int\n        :rtype: List[List[int]]\n        \"\"\"\n        res = list()\n        length = len(nums)\n        if not nums or length<4:\n            return res\n\n        nums.sort()\n        # 定义4个指针i,j,left,right  i从0开始遍历,j从i+1开始遍历,留下left和right作为双指针\n        for i in range(length - 3):\n            if i > 0 and nums[i] == nums[i - 1]: # 当i的值与前面的值相等时忽略\n                continue\n            # 获取当前最小值,如果最小值比目标值大,说明后面越来越大的值根本没戏\n            if nums[i] + nums[i + 1] + nums[i + 2] + nums[i + 3] > target:\n                break # 这里使用的break,直接退出此次循环,因为后面的数只会更大\n            # 获取当前最大值,如果最大值比目标值小,说明后面越来越小的值根本没戏,忽略\n            if nums[i] + nums[length - 3] + nums[length - 2] + nums[length - 1] < target:\n                continue # 这里使用continue,继续下一次循环,因为下一次循环有更大的数\n            # 第二层循环j,初始值指向i+1\n            for j in range(i + 1, length - 2):\n                if j > i + 1 and nums[j] == nums[j - 1]: # 当j的值与前面的值相等时忽略\n                    continue\n                if nums[i] + nums[j] + nums[j + 1] + nums[j + 2] > target:\n                    break\n                if nums[i] + nums[j] + nums[length - 2] + nums[length - 1] < target:\n                    continue\n                left, right = j + 1, length - 1\n                # 双指针遍历,如果等于目标值,left++并去重,right--并去重,当当前和大于目标值时right--,当当前和小于目标值时left++\n                while left < right:\n                    total = nums[i] + nums[j] + nums[left] + nums[right]\n                    if total == target:\n                        res.append([nums[i], nums[j], nums[left], nums[right]])\n                        left += 1 # left先+1之后,和它前面的left-1进行比较,若后+1,则和它后面的left+1进行比较\n                        while left < right and nums[left] == nums[left - 1]:\n                            left += 1\n                        right -= 1\n                        while left < right and nums[right] == nums[right + 1]:\n                            right -= 1   \n                    elif total < target:\n                        left += 1\n                    else:\n                        right -= 1\n        return res\n```\n\n\n\n```java\nclass Solution {\n    public List<List<Integer>> fourSum(int[] nums, int target) {\n        List<List<Integer>> res = new ArrayList<>();\n        if(nums==null || nums.length==0) return res;\n        int length=nums.length;\n\n        Arrays.sort(nums);\n\n        // 定义4个指针， i,j,left,right  i从0开始遍历，j从i+1开始遍历，留下left和right作为双指针\n        for(int i=0;i<length - 3;i++){\n            if(i > 0 && nums[i] == nums[i-1]) continue;\n\n            // 获取当前最小值，如果最小值比目标值大，说明后面越来越大的值根本没戏\n            if(nums[i] + nums[i+1] + nums[i+2] +nums[i+3] > target){\n                break; // 这里使用的break，直接退出此次循环，因为后面的数只会更大\n            }\n\n            // 获取当前最大值，如果最大值比目标值小，说明后面的值越来越小\n            if(nums[i] + nums[length-3] + nums[length-2] + nums[length-1] < target){\n                continue; // 继续下一次循环，因为下一次循环有更大的数\n            } \n\n            // 第二层循环j，初始值指向i+1\n            for(int j=i+1; j<length-2;j++){\n                if(j>i+1 && nums[j] == nums[j-1]){  // 当j的值与前面的值相等时忽略\n                    continue;\n                }\n\n                if(nums[i] + nums[j] + nums[j+1] + nums[j+1] > target) {\n                    break;\n                }\n\n                if(nums[i] + nums[j] + nums[length-2] + nums[length-1] < target){\n                    continue;\n                }\n\n                int left = j+1, right=length-1;\n                //双指针遍历，如果等于目标值，left++并去重，right--并去重，当当前和大于目标值时right-- 否则left++\n                while(left < right){\n                    int sum = nums[i] + nums[j] + nums[left] + nums[right];\n                    if(sum==target){\n                        res.add(Arrays.asList(nums[i], nums[j], nums[left], nums[right]));\n                        left++; // left先+1之后，和它前面的left-1进行比较，若后+1，则和它后面的left+1进行比较\n                        while(left < right && nums[left]==nums[left-1]){\n                            left++;\n                        }\n                        right--;\n                        while(left<right && nums[right]== nums[right+1]){\n                            right--;\n                        }\n                    }else if(sum<target){\n                        left++;\n                    }else{\n                        right--;\n                    }\n\n                }\n\n            }\n            \n        }\n        return res;\n    }\n}\n```\n\n## 四数之和二\n\n#### [454. 四数相加 II](https://leetcode-cn.com/problems/4sum-ii/)\n\n![](https://i.loli.net/2021/09/14/met83ga7UN9wGL2.jpg)\n\n### 维数太高，分治处理\n\n此题一看似乎和四数之和差不多，但本质上却有很大的区别，首先无论是三数之和还是四数之和，它们都是在一个数组上的操作，本质上都是一维的，同时它们都要求找到不重复的元组，这就限制了我们不能简单的使用哈希表进行去重操作。最终只能将数组排序后使用双指针。\n\n但本题是四个独立的数组，相当于是四个维度，想在四个维度上使用双指针的方法显然不现实。同时此题只要求我们找到所有4个元素的和为0的元组个数即可，并没有要求是不重复的元组，这样就简单了很多，也是可以使用哈希表法。\n\n此题是在使用哈希表的时候，会遇到如下三种情况：\n\n- hashmap存一个数组，如A。然后计算三个数组之和，如BCD。时间复杂度为：$O(n)+O(n^3)=O(n^3)$\n- hashmap存三个数组之和，如ABC。然后计算一个数组，如D。时间复杂度为：$O(n^3)+O(n) = O(n^3)$\n- hashmap存两个数组之和，如AB，然后计算两个数组之和，如CD，时间复杂度为：$O(n^2)+O(n^2) = O(n^2)$\n\n根据事件复杂度来看，选第三种情况\n\n确定了使用的方法(哈希表)以及分类方法(两两分组)。此题和两数之和中使用的哈希表有很大的区别，在两数之和中，我们需要的是满足条件的下标值，所以在哈希表中存的是元组的下标值。\n\n但在本题中，我们需要的是元组个数，所以哈希表中的值应存取出现的次数，这就有点难度。使用java map中的merge或getOrDefault\n\n```java\nclass Solution {\n    public int fourSumCount(int[] A, int[] B, int[] C, int[] D) {\n        int res = 0;\n        if(A.length==0) return res;\n\n        Map<Integer,Integer> countAB = new HashMap<>();\n        for(int u: A){\n            for(int v:B){\n                // 存储u+v的结果，不存在赋值为1，存在在原来基础上+1\n                // 另一种表达countAB.merge(u+v, 1, (old,new_)-> old+1);\n                countAB.put(u+v, countAB.getOrDefault(u+v, 0) + 1);\n            }\n        }\n\n        for(int u: C){\n            for(int v:D){\n                if(countAB.containsKey(-u-v)){\n                    res+=countAB.get(-u-v);\n                }\n            }\n        }\n        return res;\n\n    }\n}\n```\n\n```python\nclass Solution(object):\n    def fourSumCount(self, A, B, C, D):\n        \"\"\"\n        :type nums1: List[int]\n        :type nums2: List[int]\n        :type nums3: List[int]\n        :type nums4: List[int]\n        :rtype: int\n        \"\"\"\n        # Counter类是dict()子类，用于计数可哈希对象\n        # 它是一个集合，元素字段键(key) 一样存储，他们的技术存储为值\n        countAB = collections.Counter(u+v for u in A for v in B)\n        ans = 0 \n        for u in C:\n            for v in D:\n                if -u-v in countAB:\n                    ans+=countAB[-u-v]\n        return ans\n```\n\n\n\n##  n数之和方法总结\n\n对于两数之和，看它是否有序的，如果是无序的就用哈希表法，如果是有序的可以使用双指针。\n\n对于一个数组上的三数之和、四数之和等，无论数组是否有序，都排序后使用双指针法\n\n对于多个数组之和的情况，首先对它们进行分组来实现降维操作，一般来说分为两个相等的小组，之后再使用哈希表法。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["LeetCode"]},{"title":"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks","url":"/2021/09/10/Model-Agnostic-Meta-Learning-for-Fast-Adaptation-of-Deep-Networks/","content":"\n# Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\n\nMAML一种模型无关的元学习算法，即它可以与任何经过梯度下降训练的模型兼容，并适用于各种不同的学习问题，包括分类、回归和强化学习。\n\n元学习的目标是在各种学习任务上训练一个模型，它只需要少量的训练样本就可以解决新的学习任务。\n\nMAML模型的参数被显式地训练，使得少量的梯度步骤和来自新任务的少量训练数据将在该任务上产生良好的泛化性能，易于微调。\n\n核心思想是训练模型的初始参数，以便在参数通过一个或多个梯度步骤更新后，模型在新任务上具有最大性能，该梯度步骤使用来自该新任务的少量数据计算。\n\n这种快速灵活的学习是一种挑战，因为模型必须将其先前的经验与少量新信息相结合，同时避免过拟合新数据。此外，先验经验和新数据的形式将取决于任务。因此，为了获得最大的适用性，learn to learn (或元学习)的机制应该适用于任务和完成任务所需的计算形式。\n\n## Model-Agnostic Meta-Learning\n\n目标是训练能够实现快速适应的模型，这是一种通常被形式化为few-shot learning的问题设置。\n\n### Meta-Learning Problem Set-Up\n\nfew-shot learning元学习的目标是训练一个只使用几个数据点和训练迭代就能快速适应新任务的模型。\n\n实际上，元学习问题将整个任务视为训练示例。\n\n定义一个模型为f，它将观测值 $x$ 映射到输出 $a$ 。在元学习期间，模型被训练成能够适应大量或无限数量的任务。\n\n定义每个任务 $T =  \\{L(x_1,a_1,...,x_H,a_H) , q(x_1), q(x_{t+1}|x_t,a_t ), H\\}$\n\n$L$ 是loss function，初始观测 $q(x_1)$ 的分布，通过$q(x_{t+1}|x_t,a_t)$ 转换分布，事件长度$H$\n\n在独立同分布的有监督学习问题中，长度H=1。该模型可以通过在每个时间步 $t$ 选择一个输出来产生长度为 $H$ 的样本。Loss函数提供任务特殊的反馈。\n\n在元学习场景中，我们考虑希望模型能够适应的任务$p(T)$上的分布。在 K-shot 学习设置中，该模型被训练成仅从$q_i$ 中提取的K个样本和由任务 $T_i$ 生成的反馈 $L_{T_i}$ 中学习 从$p(T)$中提取的新任务$T_i$。\n\n在元训练过程中，从 $p(T)$ 中抽取一个任务 $T_i$，用K个样本训练模型，并从$T_i$中相应的$L_{T_i}$ 损失反馈，然后在$T_i$的新样品上进行测试。然后，通过考虑来自 $q_i$ 的新数据上的测试误差如何相对于参数变化来改进模型 $f$。\n\n实际上，采样任务 $T_i$ 上的测试误差充当元学习过程的训练误差。在元训练 (meta-training)结束时，从$p(T)$采样新任务，从K个样本中学习后，通过模型的性能来衡量元性能(meta-performance)。一般来说，元测试 (meta-testing) 任务是在元训练 (meta-training) 期间执行的。\n\n### A Model-Agnostic Meta-Learning Algorithm\n\n我们怎样才能鼓励这种通用型代表的出现呢？例如，神经网络可能学习广泛适用于 $p(T)$ 中所有任务的内部特征，而不是单个任务。\n\nMAML对这个问题采取了明确的方法：由于模型将在新任务上使用基于梯度的学习规则进行微调，因此我们的目标是学习一个模型，使此基于梯度的学习规则能够在从$p(T)$提取的新任务上取得快速进展，而不会过拟合。\n\n实际上，我们的目标是找到对任务变化敏感的模型参数，以便参数的微小变化将在沿损失梯度方向改变时，对从 $p(T)$ 得出的任何任务的损失函数产生较大的改进。如下图：\n\n![](https://i.loli.net/2021/09/11/UE2pXjfwd81z4hb.png)\n\n我们对模型的形式不做任何假设，只是假设它由一些参数向量 $θ$ 参数化，并且损失函数在 $θ$ 中足够平滑，因此我们可以使用基于梯度的学习技术。\n\n定义这个参数化的模型为$f_{\\theta}$ , 参数为$\\theta$。 当适应一个新的任务$T_i$时，模型的参数 $θ$ 变为 $θ_i'$。\n\n使用任务 $T_i$ 上的一个或多个梯度下降更新来计算更新后的参数向量 $θ_i'$。例如，当使用一个梯度更新时:\n$$\n\\theta_i' = \\theta - \\alpha \\nabla_{\\theta}L_{T_i}(f_{\\theta})\n$$\n步长 $α$ 可以固定为超参数。\n\n通过优化 $f_{θ_i'}$相对于从 $p(T)$ 采样的任务的 $θ$ 的性能，来训练模型参数。更具体地说，元目标如下：\n$$\nmin_{\\theta} \\sum_{T_i\\sim p(T)} L_{T_i}(f_{\\theta_i'}) = \\sum_{T_i\\sim p(T)} L_{T_i}(f_{\\theta - \\alpha \\nabla_{\\theta}L_{T_i}(f_{\\theta})})\n$$\n注意，元优化 （meta-optimization）是在模型参数θ上执行的，而目标是使用更新后的模型参数 $θ'$来计算的。实际上，MAML提出的方法旨在优化模型参数，使得新任务上的一个或少量梯度步骤将在该任务上产生最有效的行为。\n\n跨任务的元优化通过随机梯度下降（SGD），模型参数θ更新如下：\n$$\n\\theta \\leftarrow \\theta - \\beta \\nabla_{\\theta} \\sum_{T_i \\sim p(T)} L_{T_i}(f_{\\theta_i’})\n$$\n$β$ 是元步长。下图概述了一般情况下完整的算法：\n\n![](https://i.loli.net/2021/09/11/WxXC1fqEFamYi6n.png)\n\n输入任务不同分布的任务$p(T)$ 和两个学习步长超参数 $\\alpha, \\beta$ ,并且随机初始化模型参数。\n\n第一层循环，遍历每个任务 $T_i \\sim p(T)$ 中采样的batch\n\n第二层循环，在每个任务 $T_i$  中 评估关于 K 个样本的 $∇_θ L_{T_i}(f_θ)$  使用梯度下降计算自适应参数：$θ_i' = θ − α\\nabla_θL_{T_i}(f_θ)$\n\n然后根据所有任务上的损失梯度更新模型的参数  $\\theta \\leftarrow \\theta - \\beta \\nabla_{\\theta} \\sum_{T_i \\sim p(T)} L_{T_i}(f_{\\theta_i’})$\n\nMAML元梯度更新涉及通过梯度的梯度, 在计算上，这需要额外的反向传播 $f$ 来计算Hessian-vector乘积。\n\n### Species of MAML\n\n监督学习和强化学习的元学习算法的具体实例。他们在损失函数的形式以及数据由任务生成并呈现给模型的方式上不同。\n\n#### Supervised Regression and Classification\n\n![](https://i.loli.net/2021/09/11/w39XVGQOYhamNUk.png)\n\nFew-shot 学习在监督任务领域得到了很好的研究，其目标是仅从该任务的几个输入/输出对中学习新函数，使用类似任务的先前数据进行元学习。\n\n我们可以定义horizon H=1并将timestep下标放到 $x_t$上，因为模型接受单个输入并生成单个输出，而不是一系列输入和输出。\n\n任务 $T_i$ 从 $q_i$生成K个观测值 $x$ ，并且任务损失由模型的输出 $x$ 与该观测值和任务对应的目标值 $y$ 之间的误差表示。\n\n用于监督分类和回归的两种常见损失函数是交叉熵和均方误差(MSE)，对于使用均方误差的回归任务，损失形式为：\n$$\nL_{T_i}f(\\phi) = \\sum_{x^{(j)},y^{(y)} \\sim T_i} ||f_{\\phi}(x^{(j)}) - y^{(j)}||_2^2\n$$\n其中 $x^{(j)}, y^{(j)}$ 是任务$T_i$的一对输入输出，类似地，对于具有交叉熵损失的离散分类任务，损失采取以下形式：\n$$\nL_{T_i}f(\\phi)= \\sum_{x^{(j)},y^{(y)} \\sim T_i} y^{(j)} log f_{\\phi}(x^{(j)}) + (1-y^{(j)})log(1-f_{\\phi}(x^{(j)}))\n$$\n\n#### Reinforcement Learning\n\n![](https://i.loli.net/2021/09/11/Jx6hAWtNTHg9B2O.png)\n\n在强化学习（RL）中，Few-shot 元学习的目标是使模型能够仅使用测试设置中的少量经验快速获取新测试任务的策略。\n\n例如，一个模型可能会学习如何快速找到如何导航迷宫，以便在面对新的迷宫时，只需几个样本就可以确定如何可靠地到达出口。\n\n每个RL 任务$T_i$ 包含一个初始化状态分布$q_i(x_1)$ 和一个转换分布 $q_i(x_{t+1}| x_i,a_t)$ , 损失函数为 $L_{T_i}$\n\n因此，整个任务是一个具有horizon H的马尔可夫决策过程（MDP），在该过程中，学习者可以查询有限数量的样本轨迹以进行少量镜头学习。\n\nMDP的任何方面都可以在 $p(T)$ 中的任务之间改变。正在学习的模型 $f_θ$ 是一个策略，它从状态 $x_t$映射到每个操作上的分布时间步长 $t∈\\{1，.，H\\}$。任务$T_i$ 的模型的损失:\n$$\nL_{T_i}(f_\\phi) = -\\mathbf{E} _{x_t,a_t\\sim f_{\\phi},q_{T_i}} [\\sum_{t=1}^H R_i(x_t, a_t)]\n$$\n\n## 实验\n\nMAML能实现新任务的快速学习吗？\n\nMAML是否可以用于多个不同领域的元学习，包括有监督的回归、分类和强化学习？\n\n用MAML学习的模型可以通过额外的渐变更新和/或示例继续改进吗？\n\n\n\n![](https://i.loli.net/2021/09/11/NRBL1JGZyIUeY9C.png)\n\n![](https://i.loli.net/2021/09/11/myPCEawdMIN1Yt2.png)\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"Dynamically Addressing Unseen Rumor via Continual Learning","url":"/2021/09/07/Dynamically-Addressing-Unseen-Rumor-via-Continual-Learning/","content":"\n# Dynamically Addressing Unseen Rumor via Continual Learning\n\n谣言往往与新出现的事件联系在一起，因此，处理没见过的谣言的能力对于谣言真实性分类模型至关重要。\n\n以前的工作通过改进模型的泛化能力来解决这个问题，假设即使在新的事件爆发之后，模型也会保持不变。\n\n在这项工作中，提出了一种解决方案，以根据谣言域创建的动态不断更新模型。\n\n与这种新方法相关的最大技术挑战是由于新的学习而灾难性地忘记了以前的学习。\n\n作者采用持续学习策略来控制新的学习，以避免灾难性的遗忘，并提出了一种可以联合使用的额外策略来加强遗忘的缓解。\n\n谣言检测任务两个重要难点：\n\n- 处理没见过的谣言的能力 (处理训练阶段未见的新谣言)\n\n- 谣言早发现\n\n此前的方法试图通过关注静态设置中的模型泛化通用性来解决这一挑战，如下图a。目标是提高模型$M_{\\theta_{t=1}}$ 在不更新模型的情况下，在看不见的话题领域(“古尔利特”和“普京失踪”)上表现好。然而，增强模型的泛化能力是一个困难的问题，特别是对于总是引入新主题和词汇的任务。\n\n![](https://i.loli.net/2021/09/07/yUqli7RJVG489HX.png)\n\n因此，作为另一种解决方案，通过训练一个能够不断适应新出现的谣言的分类器，在动态设置中对未见过谣言进行分类。如上图b。\n\n通过这种方式，可以及时发现虚假谣言，而不必考虑可见谣言和不可见谣言之间的巨大分布差距。\n\n持续学习想解决的主要挑战是在学习新的Domain时灾难性地忘记以前学习的Domain。\n\n这篇文章作者采用了基于排练的(rehearsal- based)持续学习(CL)策略，利用以前遇到的领域的情景记忆来约束未来的学习，并提出了一种简单的技术TTOKENS，可以联合使用来进一步减少灾难性遗忘。\n\n## 方法\n\n### Task Definition\n\n谣言真实性分类是识别给定谣言文本 $X$ 是真、假还是无法核实的任务。\n\n谣言数据集及其对应的标签为集合 $D = \\{(X_i,y_i,Rm)\\}_i^N$ 其中 $y\\in\\{True, False,Unverifiabel\\}$\n\nRM是谣言域标签，N是数据集的大小。\n\n主要目标是训练一个谣言真实性分类模型M，该模型可以从传言领域流中学习，通过时间t而不会发生灾难性的遗忘。 \n\n将谣言域流定义为 $S=\\{D_1,···,D_T\\}$，其中 $D_t$ 表示流中第 $t$ 个谣言域的数据集，$T$ 是流的长度。\n\nT也等于时间戳的长度和谣言域的数量。\n\n在每个时间戳，使用一个新的谣言域数据集 $D_t$ 来顺序训练模型 $M$，并用时间 $k$ 处的谣言域表示训练后的模型参数，$θ_{t=k}$。\n\n### Base Model\n\nBERT-BASE 编码器和一个分类层。给定输入谣言 $X=x_1,···,x_m$，该模型计算：\n$$\nH= BERT([CLS] +X)\n$$\n\n$$\nP(y|X) = Softmax(WH_{[CLS]}+b)\n$$\n\n$H_{[cls]}$是 $[cls]$ 标记的嵌入,可训练参数为 $θ=[W，b]$ \n\n在训练期间，冻结编码层，并且仅使用交叉熵损失来训练分类器参数θ：\n$$\nL_{\\theta_t}(D_t) = -\\sum_j^{D_t}log P(y|X)\n$$\n\n### Rehearsal-based CL Strategies\n\n基于排练的CL策略依赖于“episodic memory” $M$ 来存储先前遇到的样本。$M$ 被定期重播，以避免灾难性遗忘，并加强过去知识和新知识之间的联系。\n\n#### REPLAY (Robins, 1995)\n\nCL的Memory M的一个简单利用是扩展当前任务数据 $D_t$，并使用 $L_{θ_t}(D_t+M)$来优化模型的参数。基本上，它可以被视为一个数据效率高的多任务框架，它只利用受Memory M大小限制的数据集的一小部分。\n\n#### Gradient Episodic Memory (GEM) (Lopez- Paz and Ranzato, 2017)\n\n另一种利用方法是使用当前域样本来约束梯度更新，使得Memory M中的样本的损失永远不会增加：\n$$\nL_{\\theta_t} s.t. L_{\\theta_t}(M) \\le L_{\\theta_{t-1}} (M)\n$$\nGEM通过随模型参数数量变化的二次规划求解器计算梯度约束。\n\n### Task-Specific Tokens (TTOKENS)\n\n各种工作表明，对大型预训练语言模型的输入上下文对模型的结果有巨大影响。换句话说，可以利用LM/MLM的这种上下文相关特性来有意地控制/区分不同域的表示。为了应用该策略，对输入文本 $X$ 进行预处理以从其对应的谣言域标签 Rm开始。形式上，给定第 $t$ 传言域Rm：\n$$\nH = BERT([CLS] + Rm + X)\n$$\n该策略可以很容易地与其他CL策略一起使用，因为它是在数据处理步骤中完成的。\n\n## 实验\n\n数据集PHEME的一个显著特点是根据谣言事件进行分类。总共有9个不同的事件/域，更多详细信息如表1所示。\n\n![](https://i.loli.net/2021/09/07/eyQsigEztvZm8Bf.png)\n\n以前的工作在静态设置中利用了这个数据集，其中8个域被组合成一个训练集，剩下的1个域用作单个不可见的测试域。在这项工作中，本文的任务是在动态设置中进行的。为了与动态设置结合，将PHEME的每个域视为单独的特定于域的数据集$D_T$，并以0.4/0.1/0.5的比率将它们拆分为Train/dev/test。\n\n### Evaluation Method\n\n在完成对第k个领域的学习后，对其在所有T领域测试集上的测试性能进行了评估。\n\n这一步的结果是矩阵 $R \\in \\mathbf{R}^{T×T}$，其中 $R_{i,j}$ 是在观察到来自第 $i$ 域的最后一个样本之后，模型在第 $j$ 域上的测试分类精度。\n\n基于此矩阵，计算了两个特定于CL的度量：\n\n- Avg. Accuracy (ACC) 对于了解模型在学习新领域时性能如何变化很有用。此指标的计算方法如下：\n\n$$\nACC = \\frac {1}{T}\\sum_{i=1}^T R_{T,i}\n$$\n\n请注意，在流的末尾，t=9，即Avg. Accuracy (ACC) 正好是所有任务的平均精确度。\n\n- Backward Transfer (BWT) 是一种用于测量新学习任务对先前学习任务影响的CL方法。此指标的计算公式为：\n\n$$\nBWT = \\frac{1}{T-1} \\sum_{i=1}^{T-1} R_{T,i} - R_{i,i}\n$$\n\n负的BWT表明模型灾难性地忘记了以前的任务。\n\n### model\n\n两个没有CL策略的基线模型，并在我们的动态设置中进行了评估。BERT-BL是指在PHEME数据集上微调的BERT基础模型。M2-BL是指对统一错误信息表示进行微调的另一个基线，该基线被证明能有效提高未发现领域的泛化性。\n\n![](https://i.loli.net/2021/09/07/Xcvu63ni5SpVJoQ.png)\n\n对于作者提出的模型，作者用上文提到的CL策略的各种组合来训练基于BERT的分类器，以评估所采用的策略在对不可见领域的鲁棒性方面的有效性。\n\n\n\n\n\n\n\n![](https://i.loli.net/2021/09/07/PUig5Oe9qcIFbjR.png)\n\n模型性能随时间变化的可视化分析。用于Replay的ACC热力图(A)没有TTOKENS和(B)有TTOKENS。比较(A)和(B)之间的每一列，使用TTO-KENS的(B)列通常显示较深的阴影，这代表更好的性能。\n\n对于TTOKENS成功背后的推理，作者有两个假设：1）TTOKENS隐含地充当了领域差异的信号，并鼓励模型为每个领域学习单独的知识。或者，2）TTOKENS作为一个良好的开端“上下文”，帮助基于LM的编码器在必要时将输入编码为更可分离的——这意味着，来自相同域的输入在向量空间中比来自不同域的输入更接近。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"Rumor Detection on Social Media with Bi-Directional Graph Convolutional Networks","url":"/2021/09/04/Rumor-Detection-on-Social-Media-with-Bi-Directional-Graph-Convolutional-Networks/","content":"\n# Rumor Detection on Social Media with Bi-Directional Graph Convolutional Networks\n\nhttps://github.com/TianBian95/BiGCN\n\n从社交媒体上如此海量的信息中识别谣言正成为一项艰巨的挑战。\n\n一些深度学习方法被应用于通过谣言传播的方式来发现谣言，如递归神经网络(RvNN)等。然而，这些深度学习方法只考虑了深度传播的特点，而忽略了谣言检测中广泛分散的结构。\n\n实际上，propagation(传播)和dispersion(扩散)是谣言的两个关键特征。\n\n作者提出一种新的双向图模型，称为双向图卷积网络(Bi-GCN)，通过对谣言的自上而下和自下而上的传播进行操作，来探索这两个特性。\n\n- 利用具有自上而下谣言传播有向图的GCN来学习谣言传播模式\n- 具有相反方向的谣言扩散图，以捕获谣言扩散的结构\n\n此外，来自消息来源的信息涉及到GCN的每一层，以增强谣言根源的影响力。\n\n## 相关工作\n\n### 传统方法\n\n传统的检测方法主要采用用户特征、文本内容和传播模式等手工制作的特征来训练监督分类器，例如：Decision Tree、 Random Forest、Support Vector Machine (SVM)。\n\n一些研究应用了有效的特征，如用户评论、时间结构特征，以及帖子的情感态度。\n\n然而，这些方法主要依赖于特征工程，非常耗时费力。此外，这些手工制作的特征通常缺乏从谣言的传播和扩散中提取的高层次表示。\n\n### 深度学习方法\n\n最近的研究已经利用从传播路径/树或网络中挖掘高层表示的深度学习方法来识别谣言。也就是深度学习方法：\n\n#### RNN\n\n长短期记忆(LSTM)、门控递归单元(GRU)和递归神经网络(RvNN)，因为他们能够从随着时间的谣言传播中学习序列特征。\n\n然而，这些方法在效率上有很大的局限性，因为时间结构特征只关注谣言的顺序传播，而忽略了谣言散布的影响。\n\n#### CNN\n\n谣言传播的结构也表明了谣言的某些传播行为。因此，一些研究试图通过引用基于卷积神经网络(CNN)的方法来涉及谣言传播结构中的信息。基于CNN的方法可以获得局部邻域内的相关特征，但不能处理图或树中全局结构关系。因此，这些方法忽略了谣言传播的全局结构特征。实际上，CNN不是为从结构化数据中学习高级表示而设计的，但图形卷积网络(GCN)是。\n\n### GNN\n\nGCN，或称为无向GCN(UD-GCN)，只聚合依赖于相关帖子之间的关系的信息，而丢失了以下内容的顺序。\n\n虽然UD-GCN具有处理谣言传播的全局性结构特征的能力(其实传统基于消息传递的GNN只是局部特征)，但它没有考虑谣言传播的方向，但这已被证明是谣言检测的重要线索。\n\n具体而言，沿着关系链的深度传播和在社区的广泛扩散是谣言的两个主要特征。\n\n为了同时处理谣言的传播和扩散，本文提出了一种新的双向GCN(Bi-GCN)，它同时适用于谣言的自上而下和自下而上的传播。\n\n![](https://i.loli.net/2021/09/05/cs9LYDfpmCzixIV.png)\n\nTop-Down graph convolutional Networks (TD-GCN)         /     Bottom-Up graph convolutional Networks (BU-GCN)\n\nTD-GCN从谣言树中节点的父节点获取信息来制定谣言传播，而BU-GCN从树中节点的子节点收集信息来表示谣言的散布。\n\nDetect rumors on twitter by promoting information campaigns with generative adversarial learning.采用对抗性学习方法来提高谣言分类器的性能，其中鉴别器用作分类器，相应的生成器通过产生冲突噪声来改进鉴别器。\n\n## 方法\n\n### 符号\n\n$C = \\{c_1,c_2,...,c_m\\}$ 为数据集，$c_i$ 是第 $i$ 个事件，$m$是事件的数量。\n\n$c_i = \\{r_i,w_1^i,...,w_{n_i-1}^i,G_i\\}$ , $n_i$ 是事件$c_i$中帖子的数量，$r_i$是源帖子，每个$w_j^i$代表第$j$个相关回应帖子, $G_i$ 指的是传播结构。$ G_i$定义为 $⟨V_i，E_i⟩$其中 $r_i$是根节点\n\n类别标签${N,F,T,U}$ (Non-rumor, False Rumor, True Rumor, and Unverified Rumor)\n\n### DropEdge\n\nDropEdge是一种减少基于GCN模型的过拟合的新方法。在每个训练周期内，随机地从输入图中剔除边，以一定的速率生成不同的变形副本。\n\n因此，这种方法增加了输入数据的随机性和多样性，就像随机旋转或摆动图像一样。形式上，假设图 $A$ 中的边总数为$N_e$，dropping率为$p$，则DropEdge之后的邻接矩阵$A‘$计算如下：\n\n$$\nA' = A - A_{drop}\n$$\n其中$A_{drop}$ 是$N_e\\times p$ 随机采样原始边集合得到\n\n### Bi-GCN Rumor Detection Model\n\n![](https://i.loli.net/2021/09/06/YR7cEdVmlXPBwtb.png)\n\nBi-GCN的核心思想是从谣言传播和谣言扩散两个方面学习合适的高层表示。\n\n在Bi-GCN模型中，采用两层1stChebNet作为基本的GCN组件。如图2所示，分4个步骤详细阐述了使用Bi-GCN进行谣言检测的过程。\n\n首先讨论如何将Bi-GCN模型应用于一个事件，即第i个事件的$c_i→y_i$。其他事件的计算方式相同。为了更好地展示我们的方法，我们在下面的内容中省略了下标i。\n\n#### 1 Construct Propagation and Dispersion Graphs\n\n基于转发和回复关系，我们构造了传播结构 $⟨V, E⟩$ 对于谣言事件。\n\n然后，设 $A\\in R^{n_i×n_i}$和 $X$ 分别为其对应的基于谣言传播树$c_i$的邻接矩阵和特征矩阵。\n\n$A$ 经过drop变成$A'$ ,基于$A'$ 和 $X$ ，我们可以建立我们的Bi-GCN模型。我们的Bi-GCN由两个组件组成：\n\n Top-Down Graph Convolutional Network (TD- GCN)\n\nBottom-Up Graph Convolutional Network (BU- GCN)\n\n两个分量的邻接矩阵是不同的\n\nTD-GCN中，$A^{TD}=A'$  , 对于BU-GCN，邻接矩阵为 $A^{BU}=A'^⊤$。TD-GCN和BU-GCN采用相同的特征矩阵X。\n\n#### 2 Calculate the High-level Node Representations\n\nTD-GCN经过两层图卷积:\n$$\nH_1^{TD} = \\sigma(\\hat A^{TD}XW_0^{TD})\n$$\n\n$$\nH_2^{TD} = \\sigma(\\hat A^{TD}H_1^{TD}W_1^{TD})\n$$\n\nBU-GCN相同\n\n#### 3 Root Feature Enhancement\n\n众所周知，谣言事件的来源帖子总是信息丰富，影响广泛。要更好地利用源帖信息，从节点与源帖的关系中学习更准确的节点表示。\n\n对于第k个GCL处的TD-GCN，我们将每个节点的隐藏特征向量与来自第(k−1)个GCL的根节点的隐藏特征向量连接起来，以构造新的特征矩阵\n$$\n\\hat H_k^{TD} = concat(H_k^{TD},(H_{k-1}^{TD})^{root})\n$$\n其中$H_0^{TD}=X$\n\n#### 4 Representations of Propagation and Dispersion for Rumor Classification\n\n传播表示和扩散表示分别由TD-GCN和BU-GCN的节点表示聚合而成。\n\n在这里，我们使用均值合并算子来聚集来自这两组节点表示的信息。它的公式是\n$$\nS^{TD} = MEAN(\\hat H_2^{TD})\n$$\n\n$$\nS^{BU} = MEAN(\\hat H_2^{BU})\n$$\n\n然后，我们将传播的表示和扩散的表示连接起来，将信息合并为\n$$\nS=concat(S^{TD},S^{BU})\n$$\n最后，事件ˆy的标签通过几个全连接层和SoftMax层：\n$$\n\\hat y = Softmax(FC(S))\n$$\n$\\hat y \\in R^{1\\times C}$\n\n## 实验\n\n![](https://z3.ax1x.com/2021/09/06/hhosuq.png)\n\n![](https://z3.ax1x.com/2021/09/06/hhTFIS.png)\n\n\n\n![](https://z3.ax1x.com/2021/09/06/hhTQaT.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"Interpretable Rumor Detection in Microblogs by Attending to User Interactions","url":"/2021/09/04/Interpretable-Rumor-Detection-in-Microblogs-by-Attending-to-User-Interactions/","content":"\n# Interpretable Rumor Detection in Microblogs by Attending to User Interactions\n\nhttps://github.com/serenaklm/rumor_detection\n\n通过学习区分社区对微博中真假claim的响应来解决谣言检测问题。\n\n现有最先进的模型是基于对会话树建模的树模型。然而，在社交媒体中，发布回复的用户可能是对整个thread的回复，而不是对特定用户的回复。\n\n提出Multi-head post-level attention模型(PLAN)来构建推文之间的远距离交互。并提出几个变体：\n\n- 结构感知自注意模型(StA-PLAN)，将树形结构信息合并到Transformer中\n- 分层token和post-leve attention(StA-HiTPLAN), 通过token-level 自注意力学习句子表征\n\n这篇工作重点是利用社区对虚假claim的响应来检测虚假索赔。这一研究领域通过将自然语言处理应用于针对claim的评论来利用社区的集体智慧。这些工作背后的关键原则是，社交媒体上的用户会分享对不准确信息的看法、猜测和证据。\n\n## 样本\n\n![](https://i.loli.net/2021/09/04/FwWU2TKkf1Ji5B3.png)\n\n源贴：“沃尔玛捐赠1万美元支持达伦·威尔逊和正在进行的种族主义警察谋杀案#弗格森#抵制沃尔玛URL”。\n\n推特R_1及其回复推文R_1_1对消息来源的真实性表示怀疑。\n\n推特R_2_1和R_3_1提供了确凿的证据，揭穿了消息来源的说法是假的。\n\n虽然R_2_1和R_3_1分别是R_2和R_3的子节点，但它们可以为树上的所有其他节点(如R_1_1和R_1)提供重要信息。\n\n因此，应该考虑所有tweet之间的互动，而不仅仅是父节点和他们的孩子节点之间的互动。\n\n## 相关工作\n\n两篇使用树形结构进行建模的sota对社交媒体中谣言检测有限制。\n\nRumor detection on twitter with tree-structured recursive neural networks(2018) ，将来源claim及其回复推文组织成树形结构，使用递归神经网络对传播树中的信息传播进行建模。来自不同节点的信号以自下而上或自上而下的方式进行粗略地重新聚合。在自下而上的模型中，信息从子节点传播到父节点，在自上而下的模型中，信息从父节点传播到子节点，反之亦然。\n\nTree lstms with convolution units to predict stance and rumor veracity in social media conver- sations.(2019) ，组织了树状结构的对话线程，并探索了用于谣言检测的branch和tree LSTM的几种变体。\n\n这两篇论文都使用了树模型，目的是对会话线索中存在的结构信息进行建模。在树模型中，信息从父级传播到子级，反之亦然。然而，社交媒体对话中的线索结构有所不同，每个用户通常能够观察到对话的不同分支中的所有回复。揭穿假新闻的用户不能只针对他回复的人创建的内容也可能适用于该帖子中的其他推文。树模型不会对来自其他分支的节点之间的交互进行显式建模，这是对社交媒体会话建模时的一个关键限制。\n\n自动区分真假Claims的现有方法利用了各种特征：\n\n- Claims的内容\n- Claims来源的重点考虑和社交网络\n- 使用可信来源(例如，维基百科)进行事实核查\n- 社区对Claims的反应。\n\n这篇重点在社区响应，接下来展开介绍介绍。\n\n### Content Information\n\n早期关于欺骗性内容检测的工作研究了语言线索的使用，例如代词的百分比、词长、动词数量和词类。也有工作对关于虚假的评论，目击者的陈述，和讽刺。利用语言特征对假新闻的检测也进行了研究。这种对概念内容的分析依赖于可能是领域或主题所独有的语言特征。\n\n### Source and Social Network\n\n研究假新闻的来源及其社交网络，在内容中加入来源信息提高了假新闻分类准确率。为传播假新闻而创建的账户往往具有不同的社交网络特征。\n\n### Fact Checking\n\n事实核查网站，如PolitiFact。com和snopes.com依靠人工验证来揭穿假新闻，但无法匹配假新闻的生成速度(Funke 2019)。自动事实核查旨在对照诸如维基百科(Ciampaglia et al.2015年)。\n\n最近，索恩等人(2018)提出了FEVER共享任务，针对包含500万维基百科文档的数据库验证输入Claim，并将每个Claim分为三类：支持、驳斥或信息不足。\n\n事实核查是一种更有原则的假新闻检测方法。然而，它也需要建立一个经过核实的事实语料库，而且可能不适用于证据很少的新Claim。\n\n### Community Response\n\n研究人员致力于通过构建分类器来自动预测claim的有效性，分类器利用对社交媒体帖子的评论和回复，以及传播模式。\n\nMa(2018a)采用多任务学习方法构建了一个学习立场感知特征的分类器用于谣言检测。\n\nLi (2019)对他的模型采用了多任务学习方法，并在他的模型中包括了用户信息。\n\nChen(2017)提出汇集截然不同的特征，以捕捉帖子随时间的上下文变化。\n\n除了语言特征，其他研究人员也关注了用户的人口统计或交互预测来确定用户的可信度。\n\nYang(2012)收集了从事传播假新闻的用户特征，通过对传播路径进行分类，仅利用用户特征构建了假新闻检测器\n\nLi(2019)使用用户信息和内容特征相结合的方式训练具有多任务学习目标的LSTM。\n\n在本文中，仅从帖子和评论两个方面来检测谣言和假新闻。提出了一种用于谣言检测的Transformer，而不是递归树模型。\n\n## 任务定义\n\n问题陈述，将每个线程thread定义为：\n$$\nX = \\{x_1,x_2,...,x_n\\}\n$$\n其中$x_1$是源tweet，$x_i$是按时间顺序排列的第 $i$ 条tweet，$n$是线程中的tweet数量。\n\n会话树中除了文本信息外，还有可以利用的结构信息。在树形结构模型中，如果$x_i$对$x_j$进行应答，反之亦然，则只对Twitter $x_i$和$x_j$进行关联。\n\n本文模型允许任何帖子关注同一主题中的任何其他帖子。\n\n在提出的结构感知模型中，用关系标签来标记任何一对推文 $x_i$ 和 $x_j$之间的关系 $R(i,j)\\in \\{\\text{parent, child, before, after, self}\\}$ 。$R(i，j)$ 的值是通过依次应用以下规则集来获得的：\n\n- parent：如果 $x_i$ 直接回复 $x_j$\n- child：如果 $x_j$ 直接回复 $x_i$\n- before：如果 $x_i$ 在 $x_j$之前到来\n- after：如果 $x_i$ 在 $x_j$之后\n- self：如果i=j\n\n谣言检测任务简化为学习预测每个$(X，R)$到其谣言类别 $y$。\n\n在两个谣言检测数据集上进行了实验，即Twitter15和Twitter16数据，以及PHEME 5数据。\n\n对于我们正在处理的数据集，分类标签是不同的：\n\n- Twitter15 and Twitter16：$y\\in \\{\\text{non-rumor, false-rumor, true-rumor, unverified}\\}$\n- PHEME: $y\\in \\{\\text{false-rumor, true-rumor, unverified}\\}$\n\n## 方法\n\n然而，正如我们将在下表中的数据统计中看到的那样，数据集中的树非常浅，大部分评论直接回复源tweet，而不是回复其他tweet。\n\n![](https://i.loli.net/2021/09/05/2RAGI9fpbJ5D387.png)\n\n我们发现，在社交媒体中，由于整个帖子通常都是可见的，回复根帖子的用户可能会继续与更活跃的用户进行对话，而不是专门为根帖子撰写回复。因此，在对社交媒体对话进行建模时，没有对推文之间的每一种可能的成对交互进行显式建模的树模型是次优的，所以用Transformer-based模型。\n\n### Post-Level Attention Network (PLAN)\n\n![](https://i.loli.net/2021/09/05/aq5MoQL1iACXujd.png)\n\n首先将对话树的结构展平，并将推文按时间顺序排列成直线结构，源推文作为第一条推文。对于我们的计划模型，我们在线性结构中对每个推文 $x_i$ 应用最大池化来获得它的句子表示 $x_i$。\n\n然后传递一个句子嵌入序列 $X' = (x_1',x_2',...,x_n')$  通过s个数的多头注意力(MHA)层来模拟推文之间的交互。\n\n我们将这些MHA层称为post-level attention层。因此，这将改变 $X' =(x_1',x_2',...,x_n')$ 为 $U=(u_1,u_2,...,u_n)$\n\n最后，使用注意力机制对推文进行插值，然后通过一个全连接层进行预测。\n\n\n\n### Structure Aware Post-Level Attention Network (StA-PLAN)\n\n模型的一个可能的局限性是，我们通过以线性结构组织推文来丢失结构信息。转换树中固有存在的结构化信息对于假新闻检测可能仍然有用。\n\n树模型在这方面更优越，因为结构信息是显式建模的。为了将树模型的优点和自我注意机制结合起来，对计划模型进行了扩展，使其显式地包含了结构信息。\n$$\n\\alpha_{ij} = softmax(\\frac{q_ik_j^T+a_{ij}^K}{\\sqrt{d_k}})\n$$\n\n$$\nz_i = \\sum_{j=1}^n\\alpha_{ij}(v_j+a_{ij}^V)\n$$\n\n$a^V_{ij}$ 和 $a^K_{ij}$ 都是代表tweet对之间五种可能的结构关系(即parent, child, before, after, self)之一的向量\n\n### Structure Aware Hierarchical Token and Post-Level Attention Network (StA-HiTPLAN)\n\n![](https://i.loli.net/2021/09/05/Cx2RNBdOwAUcMI1.png)\n\nPLAN模型执行最大池化，以获得每条推文的句子表示。\n\n然而，让模型学习单词向量的重要性可能会更理想。因此，提出了一种分层的注意模型--token-level的注意和post-level的注意力。分层模型的概述如图所示。\n\n在使用注意机制插入输出之前，我们执行token-level自注意力，而不是使用最大池化来获得句子表示。\n\n每条推文可以表示为一系列单词记号$x_i=(x_{i,1}，x_{i,2}，...，x_{i,|xi|})$。我们在一条推文中通过MHA层传递了单词token的序列。这允许tweet中的token之间进行交互，将这些层称为token-level关注层。\n\n### Time Delay Embedding\n\n在不同的时间间隔创建的推文可以有不同的解释。首次创建源claim时表示不相信的推文可能很常见，因为claim可能尚未经过验证。然而，在传播的后期阶段，可疑的推文可能表明消息来源的说法是假的倾向很高。\n\n因此，提出的三个模型PLAN、STA-PLAN和STA-HiTPLAN研究了带有时延信息的Tweet编码的实用性。\n\n为了包括每个tweet的时间延迟信息，根据从源tweet创建时起的延迟将tweet绑定。\n\n将时间箱的总数设置为100，每个箱代表10分钟的间隔。延迟超过1000分钟的推文将落入最后一个时间段。\n$$\nTDE_{pos,2i} = sin\\frac{pos}{10000^{2i/d_model}}\n$$\n\n$$\nTDE_{pos,2i+1} = cos\\frac{pos}{10000^{2i/d_model}}\n$$\n\n其中pos表达为时间bin，$pos\\in[0,100)$\n\n## 实验\n\n### 数据集\n\n![](https://i.loli.net/2021/09/05/2RAGI9fpbJ5D387.png)\n\n![](https://i.loli.net/2021/09/05/MUWvDm5j36NYksw.png)\n\n- Twitter15 and Twitter16 ：对于Twitter15和Twitter16的数据集，每个声明中都有很大比例的转发：Twit-15和Twitter16分别为89%和90%。因为作者假设转发不会给模型带来新信息，所以删除了Twitter15和Twitter16的所有转发。在删除转发后，观察到少数索赔将只剩下来源Claim。既然作者的方法背后的原则是，我们可以利用人群的信号来侦测谣言，那么没有任何回复的说法就应该是“未经核实的(unverified)”。因此，在训练数据中修改了这类说法的标签为“未经证实”\n\n![](https://i.loli.net/2021/09/05/vaNkVL2UIAhD4n1.png)\n\n![](https://i.loli.net/2021/09/05/7riuNwZBWUvxjE6.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["context detection"]},{"title":"Dice Loss for Data-imbalanced NLP Tasks","url":"/2021/09/01/Dice-Loss-for-Data-imbalanced-NLP-Tasks/","content":"\n# Dice Loss for Data-imbalanced NLP Tasks\n\n许多自然语言处理任务，如序列标注和机器阅读理解(MRC)，都面临着严重的数据失衡问题：\n\n- 负样本明显多于正样本，占据绝大多数的负例会支配模型的训练过程，导致模型倾向于负例，而测试时使用的F1指标需要每个类都能准确预测；\n- 大量简单负例（easy-negative）使训练不堪重负。负例占绝大多数也意味着其中包含了很多简单样本，这些简单样本对于模型学习困难样本几乎没有帮助，反而会在交叉熵的作用下推动模型遗忘对困难样本的知识。\n\nloss中最常用的交叉熵实际上是以精度为导向的，这造成了训练和测试之间的差异。在训练时，每个训练样本对目标函数的贡献相等，而在测试时，F1 score更关注正例。\n\n本文认为这种问题是交叉熵本身的特点带来的：交叉熵“平等”地看待每一个样本，无论正负，都尽力把它们推向1（正例）或0（负例）。但实际上，对分类而言，将一个样本分类为负只需要它的概率＜0.5即可，完全没有必要将它推向0。\n\n基于这个观察，作者使用现有的Dice Loss，并提出一个基于Dice Loss的自适应损失——DSC，在训练时推动模型更加关注困难的样本，降低简单负例的学习度，从而在整体上提高基于F1值的效果。\n\n## 从Cross Entropy 到 Dice Losses\n\n### 交叉熵损失(CE)\n\n以二分类作为说明，记输入为 $x$, 输出为一个二值概率 $p = [p_0,p_1]$, 并且有一个二元真值 $y = [y_0,y_1]$\n\n首先交叉熵损失是：\n$$\nCE =  -(y_0log\\ p_0 + y_1log \\ p_1)\n$$\n显然，对每个样本，CE都对它们一视同仁，不管当前样本是简单还是复杂。当简单样本有很多时，模型训练就会被这些简单的样本占据，使得模型难以从复杂样本中学习。于是，一种简单的改进方法是，降低模型在简单样本上的学习速率，从而得到下述加权交叉损失：\n$$\nWeighted \\ CE = -\\alpha(y_0log \\ p_0+y_1log \\ p_1)\n$$\n对不同样本，我们可以设置不同的权重，从而控制模型在该样本上学习的程度。但是此时，权重的选择又变得比较困难。因为我们的目标是缓解数据集的不平衡问题，从而提高基于F1评测标准的效果，我们希望有一种损失函数能够直接作用于F1。\n\n### Sørensen–Dice系数（DSC）\n\n一种现有的方法——Sørensen–Dice系数（简称DSC）——去衡量F1。\n\nDSC是一种用于衡量两个集合之间相似度的指标：\n$$\nDSC(A,B) = \\frac{2|A\\cap B|}{|A|+|B|} \n$$\n\n$$\nF1 = \\frac{2(precision*recall)}{precision+recall}\n$$\n\n$$\nA = precision = \\frac{TP}{TP+FP} ,  B = recall =\\frac{TP}{TP+FN}\n$$\n\n如果我们令A是所有模型预测为正的样本的集合，令B是所有实际上为正的样本集合，那么DSC就可以重写为：\n$$\nDSC(D,f) = \\frac{2TP}{2TP+FN+FP}=F1\n$$\n其中D数据集，f是一个分类模型。于是在这个意义上DSC与F1是等价的。\n\n既然如此，就直接优化DSC，然而上述表达式是离散的，为此，需要把上述DSC表达式转化为连续的版本，从而可以视作一种soft F1。\n\n对于单个样本x，直接定义它的DSC：\n$$\nDSC(x,f) = \\frac{2p_1y_1}{p_1+y_1}\n$$\n可以看到如果x是父类，那么它的DSC就为0，从而不会对训练有贡献。为了让父类也能有所贡献，所以增加一个平滑项：\n$$\nDSC_s(x,f) = \\frac{2p_1y_1 + \\epsilon}{p_1+y_1+\\epsilon}\n$$\n但这样一来，又需要我们根据不同的数据集手动地调整平滑项。而且当easy-negative样本很多的时候，即便使用上述平滑项，整个模型训练过程仍然会被它们主导。基于此，我们使用一种“自调节”的DSC（这里就和focal loss很像）：\n$$\nDSC(x,f) = \\frac{2(1-p_1)p_1\\cdot y_1 + \\epsilon}{(1-p_1)p_1 + y_1 + \\epsilon}\n$$\n比较上面两个DSC，可以发现，$1-p_1$ 实际上充当了缩放系数，对于简单样本($p_1$ 趋向于1或0)，$(1-p_1)p_1$ 使得模型更少地去关注他们。\n\n从导数上看，一旦模型正确分类当前样本（刚刚经过0.5），DSC就会使模型更少关注它，而不是像交叉熵那样，鼓励模型迫近0或1这两个点。这就能有效避免因简单样本过多导致模型训练受到简单样本的支配。\n\n事实上，这比较类似Focal Loss(FL)，降低已分好类的样本的学习权重：\n$$\nFL = -(y_0(1-p_0)^\\gamma log p_0 + y_1(1-p_1)^\\gamma log p_1)\n$$\n不过，FL即使能对简单样本降低学习权重，但它本质上仍然是在鼓励简单样本趋向于0或1，这就和DSC有了本质上的区别。因此，说DSC通过“平衡”简单样本和困难样本的学习过程，从而提高了最终的F1值（因为F1要求各类都有比较好的结果）\n\n### Dice Loss(DL)与Tversky Loss(TL)\n\n除了上述DSC外，还比较了两种$DSC_s(x,f)$的变体，分别是Dice Loss（DL）和Tversky Loss（TL）：\n$$\nDL = 1 - \\frac{2p_1y_1+\\epsilon}{p_1^2+y_1^2+\\epsilon}\n$$\n\n$$\nTL = 1-\\frac{p_1y_1 + \\epsilon}{p_1y_1+\\alpha p_1y_0+\\beta p_0y_1 + \\epsilon}\n$$\n\n在$\\alpha=\\beta=0.5$时，TL就退化为DSC。           \n\n### 损失总结\n\n![](https://i.loli.net/2021/09/01/YkHOMIlVSPjG5aw.png)\n\n后三个统称为Dice loss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["nlp"]},{"title":"Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs","url":"/2021/08/29/Edge-augmented-Graph-Transformers-Global-Self-attention-is-Enough-for-Graphs/","content":"\n# Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs\n\n--do_train --do_eval --train_batch_size 64 --num_train_epochs 50 --embeddings_learning_rate 0.7e-4 --encoder_learning_rate 0.7e-4 --classifier_learning_rate 7e-4 --warmup_steps 200 --max_seq_len 132 --dropout_rate 0.15 --metric_key_for_early_stop \"macro avg__f1-score__level_2\" --logging_steps 200 --patience 6 --label2freq_level_1_dir /data2/code/DaguanFengxian/bert_model/data/label2freq_level_1.json --label2freq_level_2_dir /data2/code/DaguanFengxian/bert_model/data/label2freq_level_2.json --processor_sep \"\\t\" --loss_fct_name dice ","tags":["GNN"]},{"title":"Breadth First Reasoning Graph for Multi-hop Question Answering","url":"/2021/08/17/Breadth-First-Reasoning-Graph-for-Multi-hop-Question-Answering/","content":"\n# Breadth First Reasoning Graph for Multi-hop Question Answering\n\n为了解决GNNs不必要的更新和简单的边结构阻碍直接提取准确的答案跨度，和更可解释性。\n\n作者提出了一种新的广度优先推理图(BFR-Graph)模型，它提供了一种新的更符合推理过程的消息传递方式。\n\n在BFR-Graph中，推理信息要求从问题结点开始，逐跳传递到下一个句子结点，直到所有的边都经过，可以有效地防止每个结点的过度平滑或不必要的多次更新。\n\n为了引入更多的语义，我们还将推理图定义为考虑了共现关系数和句子间距离的加权图。\n\n然后基于GNN提出了一种更直接、更易解释的方法来聚合不同粒度级别的分数。\n\n## 现有GNN方法的几个问题\n\n- 首先，当前的方法将所有节点（包括一些不必要的节点）一起更新到每一层中，这可能导致节点收敛到相似的值，并失去对具有更多层的GNN的识别能力。\n- 第二，虽然GNN设计了不同类型的边，但是在没有考虑句子之间的其他关系信息的情况下，相同类型的边之间没有更细粒度的区别。\n- 第三，现有的方法只是潜在地融合了GNN和上下文编码器的隐藏表示，而没有以直接和可解释的方式进行答案广度提取。\n\n## 方法\n\n![](https://i.loli.net/2021/08/19/NlBA3uLTydzhVwi.png)\n\n与现有的基于GNN的方法不同，BFR-Graph对消息传递引入了新的限制：消息只从问题开始，然后逐跳传递到后面的句子节点。此外，考虑到共现实体和句子之间的距离，图被构造成一个加权图。此外，利用BFR图的推理结果，设计了多分数答案预测。\n\n简言之，我们在加权图上提出了广度优先推理，然后在多任务联合训练的框架下，结合多级分数进行回答预测。\n\n### Paragraph Selection\n\n训练一个二分类bert，对每个段落进行打分，选择得分在前N位的段落作为有用的段落，然后将这些段落连接在一起作为上下文C。\n\n### Context Encoding\n\nbert输出+bi-attention后获得问题和上下文的编码表达\n$$\nH = \\{h_0, ..., h_{L-1}\\}\n$$\n其中L是输入序列的长度(连接问题和上下文)，d是双关注层的输出维度(也是BERT的维度)。\n\n为了实现句子级表示，首先获得每个句子的标记级表示：\n$$\nS_i^{seq} = H[s_i^{start}:s_i^{end}]\\in \\mathbb{R}^{L_{s_i}\\times d}\n$$\n获得每个句子的表示是用了Bi-LSTM的方法\n$$\ns_i = \\sum_{k=0}^{L_s}\\alpha_k^iS_i^{seq}[k, :]\\in \\mathbb{R}^d\n$$\n$\\alpha_k^i$ 是第i个句子中第k个token的权重，通过两层MLP output size=1获得\n\n### Weighted Graph Construction\n\n为了更好地挖掘句子之间复杂的关系信息，定义了正相关和负相关两种类型的相关性：\n\n- 正相关：如果表示句子 $i$ 和 $j$ 的节点具有 n(n≥1) 个相同命名实体，则添加一条边，该边的权重为：\n\n$$\nw_{ij} = \\frac{1}{1+e^{-n+K_1}}\n$$\n\n- 负相关：否则，如果两个节点最初来自同一段落，则添加一条边，该边的权重为：\n\n$$\nw_{ij} = \\frac{1}{1+e^{d+K_2}}\n$$\n\n其中d是两个句子的距离(例如，如果该句子紧跟在段落中的另一个句子之后，则d=1，如果它们之间有句子，则d=2，依此类推)。K1和K2是超参数。\n\n是同质图，它包含单一类型的节点和边。\n\n### Breadth First Reasoning\n\n下图直观地显示了BFR-Graph和典型GNN之间的区别。\n\n![](https://i.loli.net/2021/08/19/oCzcsTeIHXZGW3f.png)\n\n当我们在段落上推理来回答一个问题时，我们从问题开始，一跳一跳地找到下一个句子。\n\n对于节点表示句子的GNN，以下消息传递是不必要的，可能会抑制无用节点的干扰：\n\n- 从后一个节点到前一个节点\n- 某个节点尚未收到来自问题的消息，但它会更新其他节点。\n\n具体地说，当同时满足以下条件时，节点i由节点j更新：\n\n- 节点 $i$ 和节点 $j$ 是邻居\n- 节点 $j$ 是Active的\n- 节点 $i$ 和节点 $j$ 之间的边以前没有经过\n\nBFR-Graph的整个消息传递过程:\n\n![](https://z3.ax1x.com/2021/08/19/fqlv7j.png)\n\n消息更新传递的函数还是GAT\n\n### Multi-score Answer Prediction\n\nHotpotQA数据集中的答案是上下文的span。现有工作仅计算编码器输出（如BERT）上的跨度概率，或额外连接GNN的隐藏输出。不同的是，我们通过计算从GNN获得的句子分数和段落分数来使用更易于解释的方法。如下图：\n\n![](https://z3.ax1x.com/2021/08/19/fq3Cad.png)\n\n通常，作为答案跨度的开始/结束的上下文中的第y个单词的分数通过以下方式计算：\n$$\n\\phi_{start}(y) = MLP_1(H[y,:])\n$$\n\n$$\n\\phi_{end}(y) = MLP_2(H[y,:])\n$$\n\n然后，计算GNN中每个节点对应的句子得分：\n$$\n\\phi_{sent}(s_i) =MLP_3(s_i)\n$$\n计算段落分数, 通过全局最大池：\n$$\n\\phi_{para}(p_j) = MLP_4(Max(\\{s_0^{p_j},...,s_{L_{p_j}-1}^{P_j}\\}))\n$$\n$s_i^{P_j}$是第$i$句话在第Pj段中的表达。这也可以通过在所有语句节点上取每个维度上的最大隐藏值来实现。\n\n最后，上下文中第y个单词作为答案范围开始的概率由以下公式确定：\n$$\np_{start}(y) = softmax(\\phi'_{start}(y))\n$$\n\n$$\n\\phi'_{start}(y) = \\phi_{start}(y) + \\phi_{sent}(s_i) + \\phi_{para}(p_j)\n$$\n\n并且可以类似地计算上下文中的第y个单词作为答案跨度结束的概率。\n\n如果一个句子或段落的得分较高，则位于其中的单词更有可能是答案。\n\n最后是一个多任务预测\n\n## 消融实验\n\n![](https://z3.ax1x.com/2021/08/19/fqGFjf.png)\n\n\n\n\n\n\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"RealFormer: Transformer Likes Residual Attention","url":"/2021/08/15/RealFormer-Transformer-Likes-Residual-Attention/","content":"\n# RealFormer: Transformer Likes Residual Attention\n\n提出了一个简单的基于Transformer的体系结构，创建一条“直接”路径在整个网络中传播原始注意力分数\n\n如下图(c), 每个RealFormer层都获取前一层中所有注意力头部的原始注意力分数，并在顶部添加“残差分数”(计算方式与常规Transformers中的注意力分数相同)。\n\n换句话说，RealFormer可以被视为向Post-LN Transformer添加一个简单的跳跃连接。不会向计算图中添加任何乘法运算，因此预期性能与之相当。\n\nRealFormer中的AT往往更稀疏，跨层相关性更强，我们认为这可能具有一些正则化效应，可以稳定训练，有利于微调。\n\n![](https://i.loli.net/2021/08/15/jZrDYmkL5EzFqi6.png)t\n\n- (a) 传统transformer的PostLN\n- (b) PreLN 论文：ON LAYER NORMALIZATION IN THE TRANSFORMER ARCHITECTURE，这种设计为每个子层增加了LN作为“预处理”步骤。\n\n## 方法\n\n标准Transformer Encoder\n$$\n\\text{MultiHead}(Q,K,V) = Concat(head_1,...,head_h)W^O\n$$\n\n$$\nhead_i = Attention(QW_i^Q, KW^K_i,VW^V_i)\n$$\n\n$$\nAttention(Q',K',V') = Softmax(\\frac{Q'K'^T}{\\sqrt{d_k}})V'\n$$\n\n$$\nFFN(x) = \\sigma(xW_1+b_1)W_2+b_2\n$$\n\nPost-LN是Vaswani等人提出的原创体系结构。对每个子层末尾的输出进行标准化。\n\n相反，Pre-LN规格化子层输入，并创建直接路径(没有LN)来传播序列中的令牌嵌入。\n\n### Residual Attention Layer Transformer\n\nRealFormer紧跟Post-LN设计，简单地增加了一个skip edge来连接相邻层中的多头注意力，如上图c所示。\n\n形式上添加一个$Prev$，是上一个softmax的注意力分数也就是pre-softmax，形状为$(heads,\\text{from_seq_len},\\text{to_seq_len})^2$\n$$\n\\text{ResidualMultiHead}(Q,K,V,Prev) = Concat(head1,...,head_h)W^O\n$$\n\n$$\nhead_i = \\text{ResidualAttention}(QW_i^Q,KW_i^K,VW_i^V,Prev_i)\n$$\n\n$Prev_i$ 的形状为$(\\text{from_seq_len,to_seq_len})$ 对应于每个$head_i$\n$$\n\\text{ResidualAttention}(Q',K',V',Prev') = \\text{Softmax}(\\frac{Q'K'^T}{\\sqrt{d_k}}+Prev')V'\n$$\n新的注意力分数$\\frac{Q'K'^T}{\\sqrt{d_k}}+Prev'$\n\n## 实验\n\n![](https://i.loli.net/2021/08/15/K86nvAXLs954SMf.png)\n\n值得特别指出的是第一张图和第四张图。从第一张图我们可以看到，对于RealFormer结构，加大模型规模（large到xlarge）可以带来性能的明显提升，而ALBERT论文曾经提到加大BERT的模型规模并不能带来明显受益，结合两者说明这可能是PostLN的毛病而不是BERT的固有毛病，换成RealFormer可以改善这一点。从第四张图我们可以看到，RealFormer结构训练50万步，效果就相当于PostLN训练100万步，这表明RealFormer有着很高的训练效率。\n\n除了上述实验外，论文还对比了不同学习率、不同Dropout比例的效果，表明RealFormer确实对这些参数是比较鲁棒的。原论文还分析了RealFormer的Attention值分布，表明RealFormer的Attention结果更加合理。\n\n### 分析\n\nRealFormer对梯度下降更加友好，这不难理解，因为$A_n = \\frac{Q_nK_n^T}{\\sqrt{d_k}} + A_{n-1}$的设计确实提供了一条直通路，使得第一层的Attention能够直通最后一层，自然就没有什么梯度消失的风险了。相比之下，PostLN是 $LayerNorm(x+f(x))$ 的结构，看上去$x+f(x)$防止了梯度消失，但是LayerNorm这一步会重新增加了梯度消失的风险，造成的后果是初始阶段前面的层梯度很小，后面的层梯度很大，如果用大学习率，后面的层容易崩，如果用小学习率，前面的层学不好，因此PostLN更难训练，需要小的学习率加warmup慢慢训。\n\n还有一个就是叠加的问题PreLN每一步都是$x+f(x)$的形式，到了最后一层变成了$x+f_1(x)+f_2(x)+...++f_n(x)$的形式，一层层累加，可能导致数值和方差都很大，最后迫不得已强制加一层Layer Norm让输出稳定下来。这样，尽管PreLN改善了梯度状况，但它本身设计上就存在一些不稳定因素。\n\nRealformer的$A_n = \\frac{Q_nK_n^T}{\\sqrt{d_k}} + A_{n-1}$存在叠加问题吗？如果只看A，那么确实有这样的问题，但A后面还要做个softmax归一化后才参与运行，也就是说，模型对矩阵A是自带归一化功能的，所以它不会有数值发散的风险。而且刚刚相反，随着层数的增加，A的叠加会使得A的元素绝对值可能越来越大，Attention趋近于onehot形式，造成后面的层梯度消失，但是别忘了，我们刚才说PostLN前面的层梯度小后面的大，而现在也进一步缩小了后面层的梯度，反而使得两者更同步，从而更好优化了；\n\n另一方面Attention的概率值可能会有趋同的趋势，也就是说Attention的模式可能越来越稳定了。带来类似ALBERT参数共享的正则化效应，这对模型效果来说可能是有利的。同时，直觉上来想，用RealFormer结构去做FastBert之类的自适应层数的改进，效果会更好，因为RealFormer的Attention本身会有趋同趋势，更加符合FastBert设计的出发点。\n\n此外，我们也可以将RealFormer理解为还是使用了常规的残差结构，但是残差结构只用在**Q**,**K**而没有用在**V**上。\n\n为啥**V**“不值得”一个残差呢？从近来的一些相对位置编码的改进中，笔者发现似乎有一个共同的趋势，那就是去掉了**V**的偏置，比如像NEZHA的相对位置编码，是同时在Attention矩阵（即**Q**,**K**）和**V**上施加的，而较新的XLNET和T5的相对位置编码则只施加在Attention矩阵上，所以，似乎去掉**V**的不必要的偏置是一个比较好的选择，而RealFormer再次体现了这一点。\n\n\n\n### RealFormer与Baseline Transformers在本质上有什么不同？\n\ndev set中随机抽样了8,192个示例，并可视化了这些示例中每个token(不包括padding)在表2中的三个预先训练的BERT-Base模型中的所有层和所有头部的注意概率分布。\n\n特别地，对于每个(token、layer、head)三元组，我们计算关注权重(概率)的熵作为关注度的“稀疏度量”。直观地说，熵越低，注意力权重分布就越偏斜，因此注意力就越稀疏。\n\n![](https://i.loli.net/2021/08/15/av6cdqPg5HxWShY.png)\n\n用RealFormer训练好的BERT-BASE对8192个突出例子的标记注意概率的熵分布\n\n为了更好地辨认，每一层中的注意力都是按照熵的中位数排序的。根据熵的中位数对分布重新进行颜色编码：红色(中位数>4.5)、黄色(1.5≤中位数≤4.5)、蓝色(中位数<1.5)。也就是说，颜色越冷意味着注意力越稀疏。有一个明显的趋势是，较高的层往往具有较稀疏的注意力。\n\n\n\n下面的是post-LN和pre-LN的熵分布\n\n![](https://i.loli.net/2021/08/15/MIfAtTSK6QV1a4h.png)\n\n![](https://i.loli.net/2021/08/15/yIdzx7tDBTnNuUM.png)\n\n\n\n\n\n\n\n\n\n\n\n","tags":["NLP"]},{"title":"Centos 6无法使用yum解决办法","url":"/2021/08/12/Centos-6无法使用yum解决办法/","content":"\n# Centos 6无法使用yum解决办法\n\n12月后Centos 6 系统无法使用yum出现错误(文章底部看)\n\n相信已经有一部分朋友今天连接到CentOS 6的服务器后执行yum后发现报错，那么发生了什么？\n\nCentOS 6已经随着2020年11月的结束进入了EOL（Reaches End of Life），不过有一些老设备依然需要支持，CentOS官方也给这些还不想把CentOS 6扔进垃圾堆的用户保留了最后一个版本的镜像，只是这个镜像不会再有更新了\n\n官方便在12月2日正式将CentOS 6相关的软件源移出了官方源，随之而来逐级镜像也会陆续将其删除。\n\n不过有一些老设备依然需要维持在当前系统，CentOS官方也给这些还不想把CentOS 6扔进垃圾堆的用户保留了各个版本软件源的镜像，只是这个软件源不会再有更新了。\n\n## 错误详情\n\n```bash\n[root@c8-20 ~]# yum makecache\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\nYumRepo Error: All mirror URLs are not using ftp, http[s] or file.\n Eg. Invalid release/repo/arch combination/\nremoving mirrorlist with no valid mirrors: /var/cache/yum/x86_64/6/base/mirrorlist.txt\nError: Cannot find a valid baseurl for repo: base\n```\n\n或\n\n```bash\n[root@li496-237 ~]# yum -y install unzip zip\nLoaded plugins: fastestmirror\nSetting up Install Process\nDetermining fastest mirrors\nYumRepo Error: All mirror URLs are not using ftp, http[s] or file.\nEg. Invalid release/repo/arch combination/\nYumRepo Error: All mirror URLs are not using ftp, http[s] or file.\nEg. Invalid release/repo/arch combination/\nYumRepo Error: All mirror URLs are not using ftp, http[s] or file.\nEg. Invalid release/repo/arch combination/\nhttp://mirrors.linode.com/centos/6/os/x86_64/repodata/repomd.xml: [Errno 14] PYCURL ERROR 22 - \"The requested URL returned error: 404 Not Found\"\nTrying other mirror.\nTo address this issue please refer to the below knowledge base article \n\nhttps://access.redhat.com/articles/1320623\n\nIf above article doesn't help to resolve this issue please open a ticket with Red Hat Support.\n\nError: Cannot retrieve repository metadata (repomd.xml) for repository: base. Please verify its path and try again\n```\n\n一键修复\n\n```\nsed -i \"s|enabled=1|enabled=0|g\" /etc/yum/pluginconf.d/fastestmirror.conf\nmv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup\ncurl -o /etc/yum.repos.d/CentOS-Base.repo https://www.xmpan.com/Centos-6-Vault-Aliyun.repo \nyum clean all\nyum makecache\n```\n\n手动修复教程:\n\n首先把fastestmirrors关了\n\n```\n#编辑\nvi /etc/yum/pluginconf.d/fastestmirror.conf\n#修改\nenable=0\n#或者执行以下命令\nsed -i \"s|enabled=1|enabled=0|g\" /etc/yum/pluginconf.d/fastestmirror.conf\n```\n\n先把之前的repo挪到备份，然后下面两个二选一\n\n```\nmv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak\n```\n\n替换为官方Vault源(海外服务器用)\n\n```\ncurl -o /etc/yum.repos.d/CentOS-Base.repo https://www.xmpan.com/Centos-6-Vault-Official.repo\n```\n\n或者替换为阿里云Vault镜像(国内服务器用)\n\n```\ncurl -o /etc/yum.repos.d/CentOS-Base.repo https://www.xmpan.com/Centos-6-Vault-Aliyun.repo\n```\n\n\n\n\n\n\n\n\n\n\n\n","tags":["配置记录"]},{"title":"从 SGD 到 AdamW 原理和代码解读","url":"/2021/08/12/从-SGD-到-AdamW-原理和代码解读/","content":"\n# 从 SGD 到 AdamW 原理和代码解读\n\n深度学习优化算法经历了 SGD -> SGDM -> NAG ->AdaGrad -> AdaDelta -> Adam -> Nadam -> AdamW 这样的发展历程。\n\n接下来用一个框架来梳理所有的优化算法。\n\n首先定义：待优化参数：$w$ , 目标函数：$f(x)$ , 初始学习率 $\\alpha$\n\n然后，开始进行迭代优化。在每个epoch $t$:\n\n1. 计算目标函数关于当前参数的梯度: $g_t = ∇f(w_t)$\n2. 根据历史梯度计算一阶动量和二阶动量: $m_t = \\phi(g_1,g_2,...,g_t); V_t =\\psi(g_1,g_2,...,g_t)$\n3. 计算当前时刻的下降梯度: $\\eta = \\alpha \\cdot m_t/\\sqrt V_t$\n4. 根据下降梯度进行更新: $w_{t+1} = w_t -\\eta_t$\n\n步骤3、4对于各个算法几乎都是一致的，主要的差别就体现在1和2上。\n\n也就是计算一阶动量$m_t$ 和 二阶动量$V_t$时采用不同的套路。\n\n此外在所有优化器代码里有一些函数作用是相通的：\n\n> 共性的方法有：\n\n- add_param_group(param_group) : 把参数放进优化器中，这在Fine-tune预训练时可以使冻结层可训练，并随着训练的进行添加到优化器中。\n- load_state_dict(state_dict): 把优化器的状态加载进去。\n- state_dict():返回优化器状态，以dict形式\n- step(closure=None):优化一步参数\n- zero_grad(set_to_none=False):把所有的梯度值设为0\n\n> 使用方法：\n\n```python\nfor input, target in dataset:\n  def closure():\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    return loss\n  optimizer.step(closure)\n```\n\n## SGD\n\nSGD没有动量的概念，也就是说：\n$$\nm_t = g_t ; V_t=I^2\n$$\n代入步骤3，可以看到下降梯度就是最简单的\n$$\n\\eta_t = \\alpha \\cdot g_t\n$$\nSGD最大的缺点就是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点。\n\n## SGD with Momentum\n\n为了抑制震荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一点。\n\n在SGD的基础上引入了一阶动量：\n$$\nm_t = \\beta_1\\cdot m_{t-1} +(1-\\beta_1)\\cdot g_t\n$$\n一阶动量就是各个时刻梯度方向的指数移动平均，约等于最近$1/(1-\\beta_1)$个时刻的梯度向量和的平均值。\n\n也就是说，t 时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。\n\n$\\beta_1$的经验值为0.9，这意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。想象高速公路上汽车转弯，在高速向前的同时略微偏向，急转弯可是要出事的。\n\n## SGD with Nesterov Acceleration\n\nSGD还有一个问题是困在局部最优的沟壑里震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能呆在这里了。可是你如果爬上高地，就会方向外卖的世界还很广阔。\n\n因此外卖不能停留在当前位置去观察未来的方向，而是要向前一步，多看一步，看远一些。\n\nNAG全称Nesterov Accelerated Gradient，是在SGD、SGDM的基础上的进一步改进，改进点在于步骤1。\n\n我们知道在时刻 t 的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算。那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。\n\n因此NAG在步骤1，不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向：\n$$\ng_t =∇ f(w_t-\\beta_1\\cdot m_{t-1}/\\sqrt{V_{t-1}})\n$$\n然后用下一个点的梯度方向，与历史累积动量结合，计算步骤2中当前时刻的累加动量。\n\n> 定义优化器：\n\n```\nCLASS torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n```\n\n> 参数：\n\n- **params** (iterable) – 优化器作用的模型参数。\n- **lr** (float) – learning rate，相当于是统一框架中的 $\\alpha$\n- **momentum** (float, optional) – 动量参数。(默认值：0)\n- **weight_decay** (float, optional) – 权重衰减系数 weight decay (L2 penalty) (默认值：0)\n- **dampening** (float, optional) – dampening for momentum (默认值：0)\n- **nesterov** (bool, optional) – 允许 Nesterov momentum (默认值：False)\n\n> 源码解读：\n\n```python\nimport torch\nfrom .optimizer import Optimizer, required\n\n\n[docs]class SGD(Optimizer):\n    r\"\"\"Implements stochastic gradient descent (optionally with momentum).\n\n    Nesterov momentum is based on the formula from\n    `On the importance of initialization and momentum in deep learning`__.\n\n    Args:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float): learning rate\n        momentum (float, optional): momentum factor (default: 0)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        dampening (float, optional): dampening for momentum (default: 0)\n        nesterov (bool, optional): enables Nesterov momentum (default: False)\n\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n\n    .. note::\n        The implementation of SGD with Momentum/Nesterov subtly differs from\n        Sutskever et. al. and implementations in some other frameworks.\n\n        Considering the specific case of Momentum, the update can be written as\n\n        .. math::\n            \\begin{aligned}\n                v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n                p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n            \\end{aligned}\n\n        where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the \n        parameters, gradient, velocity, and momentum respectively.\n\n        This is in contrast to Sutskever et. al. and\n        other frameworks which employ an update of the form\n\n        .. math::\n            \\begin{aligned}\n                v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n                p_{t+1} & = p_{t} - v_{t+1}.\n            \\end{aligned}\n\n        The Nesterov version is analogously modified.\n    \"\"\"\n\n    def __init__(self, params, lr=required, momentum=0, dampening=0,\n                 weight_decay=0, nesterov=False):\n        if lr is not required and lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if momentum < 0.0:\n            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n        if weight_decay < 0.0:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n\n        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n                        weight_decay=weight_decay, nesterov=nesterov)\n        if nesterov and (momentum <= 0 or dampening != 0):\n            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n        super(SGD, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(SGD, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('nesterov', False)\n\n[docs]    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group['weight_decay']\n            momentum = group['momentum']\n            dampening = group['dampening']\n            nesterov = group['nesterov']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad  # 得到每个参数的梯度，也就是g_t\n                if weight_decay != 0: # 如果使用weight_decay的话，相当于目标函数上加上 L2正则\n                    d_p = d_p.add(p, alpha=weight_decay)\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if 'momentum_buffer' not in param_state:\n                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n                    else:\n                        buf = param_state['momentum_buffer']\n                        # 计算动量，momentum参数beta_1一般取0.9，相当于之前的动量buf乘以0.9再加上此次梯度\n                        # d_p乘以(1-beta_1)=0.1\n                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n                    if nesterov:\n                      \t# 如果通过nesterov方式更新参数，那么eta_t就相当于g_t+m_t*beta_1\n                        d_p = d_p.add(buf, alpha=momentum)\n                    else:\n                      \t# 如果不通过nesterov方式更新参数，那么\\eta_t就是相当于是上一步计算出的动量m_t\n                        d_p = buf\n\n                p.add_(d_p, alpha=-group['lr']) #最后用学习率更新梯度\n\n        return loss\n```\n\n\n\n## AdaGrad\n\n此前我们都没有用到二阶动量。二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。\n\nSGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到(想想大规模的embedding)。\n\n对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学习一些，即学习率大一些。\n\n怎么样去度量历史更新频率呢？那就是二阶动量——该维度上，迄今为止所有梯度值的平方和：\n$$\nV_t = \\sum_{\\tau=1}^t g_{\\tau}^2\n$$\n我们在回顾一些步骤3中的下降梯度：\n$$\n\\eta_t = \\alpha\\cdot m_t/\\sqrt{V_t}\n$$\n可以看出，此时实质上的学习率由$\\alpha$ 变成了，$\\alpha/\\sqrt{V_t}$。\n\n一般为了避免分母为0，会在分母上加一个小的平滑项。因此$\\sqrt{V_t}$是恒大于0的，而且参数更新越频繁，二阶动量越大，学习率就越小。\n\n这一方法在稀疏数据场景下表现非常好，但也存在一些问题：因为$\\sqrt{V_t}$ 是单调递增的，会使学习率单调递减至0，可能会使训练过程提前结束，即便后续还有数据也无法学到必要的知识。\n\n> 定义优化器：\n\n```\nCLASS torch.optim.Adagrad(params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0,eps=1e-10)\n```\n\n> 参数：\n\n- **params** (iterable) – 优化器作用的模型参数。\n- **lr** (float) – learning rate – 相当于是统一框架中的 。\n- **lr_decay**(float,optional) – 学习率衰减 (默认值：0)\n- **weight_decay** (float, optional) – 权重衰减系数 weight decay (L2 penalty) (默认值：0)\n- **eps**(float,optional)：防止分母为0的一个小数 (默认值：1e-10)\n\n> 源码解读：\n\n```python\n[docs]class Adagrad(Optimizer):\n    \"\"\"Implements Adagrad algorithm.\n\n    It has been proposed in `Adaptive Subgradient Methods for Online Learning\n    and Stochastic Optimization`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-2)\n        lr_decay (float, optional): learning rate decay (default: 0)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-10)\n\n    .. _Adaptive Subgradient Methods for Online Learning and Stochastic\n        Optimization: http://jmlr.org/papers/v12/duchi11a.html\n    \"\"\"\n\n    def __init__(self, params, lr=1e-2, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= lr_decay:\n            raise ValueError(\"Invalid lr_decay value: {}\".format(lr_decay))\n        if not 0.0 <= weight_decay:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        if not 0.0 <= initial_accumulator_value:\n            raise ValueError(\"Invalid initial_accumulator_value value: {}\".format(initial_accumulator_value))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n\n        defaults = dict(lr=lr, lr_decay=lr_decay, eps=eps, weight_decay=weight_decay,\n                        initial_accumulator_value=initial_accumulator_value)\n        super(Adagrad, self).__init__(params, defaults)\n\n        for group in self.param_groups:\n            for p in group['params']:\n                state = self.state[p]\n                state['step'] = 0\n                state['sum'] = torch.full_like(p, initial_accumulator_value, memory_format=torch.preserve_format)\n\n    def share_memory(self):\n        for group in self.param_groups:\n            for p in group['params']:\n                state = self.state[p]\n                state['sum'].share_memory_()\n\n[docs]    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            params_with_grad = []\n            grads = []\n            state_sums = []\n            state_steps = []\n\n            for p in group['params']:\n                if p.grad is not None:\n                    params_with_grad.append(p)\n                    grads.append(p.grad)\n                    state = self.state[p]\n                    state_sums.append(state['sum'])\n                    # update the steps for each param group update\n                    state['step'] += 1\n                    # record the step after step update\n                    state_steps.append(state['step'])\n\n            F.adagrad(params_with_grad,\n                      grads,\n                      state_sums,\n                      state_steps,\n                      group['lr'],\n                      group['weight_decay'],\n                      group['lr_decay'],\n                      group['eps'])\n\n        return loss\n```\n\n## AdaDelta \n\n由于AdaGrad 单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也是就是AdaDelta名称中Delta的来历。\n\n修改思路很简单，前面讲到，指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量：\n$$\nV_t = \\beta_2 * V_{t-1} + (1-\\beta_2)g_t^2\n$$\n接下来还是步骤3：\n$$\n\\eta_t = \\alpha \\cdot g_t/\\sqrt{V_t}\n$$\n这就避免了二阶动量持续累积、导致训练过程提前结束的问题了。\n\n## RMSProp\n\n> 定义优化器：\n\n```\nCLASS torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n```\n\n> 参数：\n\n- **params** (iterable) – 优化器作用的模型参数。\n- **lr** (float) – learning rate – 相当于是统一框架中的 $\\alpha$。\n- **momentum** (float, optional) – 动量参数。(默认值：0)。\n- **alpha**(*float,optional*) – 平滑常数 (默认值：0.99)。\n- **centered**(bool,optional) – if`True`, compute the centered RMSProp, the gradient is normalized by an estimation of its variance，就是这一项是 True 的话就把方差使用梯度作归一化。\n- **weight_decay** (float, optional) – 权重衰减系数 weight decay (L2 penalty) (默认值：0)\n- **eps**(float,optional)：防止分母为0的一个小数 (默认值：1e-10)\n\n> **源码解读：**\n\n```python\nimport torch\nfrom .optimizer import Optimizer\n\n\n[docs]class RMSprop(Optimizer):\n    r\"\"\"Implements RMSprop algorithm.\n\n    Proposed by G. Hinton in his\n    `course <https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>`_.\n\n    The centered version first appears in `Generating Sequences\n    With Recurrent Neural Networks <https://arxiv.org/pdf/1308.0850v5.pdf>`_.\n\n    The implementation here takes the square root of the gradient average before\n    adding epsilon (note that TensorFlow interchanges these two operations). The effective\n    learning rate is thus :math:`\\alpha/(\\sqrt{v} + \\epsilon)` where :math:`\\alpha`\n    is the scheduled learning rate and :math:`v` is the weighted moving average\n    of the squared gradient.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-2)\n        momentum (float, optional): momentum factor (default: 0)\n        alpha (float, optional): smoothing constant (default: 0.99)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        centered (bool, optional) : if ``True``, compute the centered RMSProp,\n            the gradient is normalized by an estimation of its variance\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n\n    \"\"\"\n\n    def __init__(self, params, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0, momentum=0, centered=False):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= momentum:\n            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n        if not 0.0 <= weight_decay:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        if not 0.0 <= alpha:\n            raise ValueError(\"Invalid alpha value: {}\".format(alpha))\n\n        defaults = dict(lr=lr, momentum=momentum, alpha=alpha, eps=eps, centered=centered, weight_decay=weight_decay)\n        super(RMSprop, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RMSprop, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('momentum', 0)\n            group.setdefault('centered', False)\n\n[docs]    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n                if grad.is_sparse:\n                    raise RuntimeError('RMSprop does not support sparse gradients')\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['square_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    if group['momentum'] > 0:\n                        state['momentum_buffer'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    if group['centered']:\n                        state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n\n                square_avg = state['square_avg']\n                alpha = group['alpha']\n\n                state['step'] += 1\n\n                if group['weight_decay'] != 0:\n                    grad = grad.add(p, alpha=group['weight_decay'])\n\n                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n\n                if group['centered']:\n                    grad_avg = state['grad_avg']\n                    grad_avg.mul_(alpha).add_(grad, alpha=1 - alpha)\n                    avg = square_avg.addcmul(grad_avg, grad_avg, value=-1).sqrt_().add_(group['eps']) # 计算当前步的动量\n                else:\n                    avg = square_avg.sqrt().add_(group['eps'])\n\n                if group['momentum'] > 0:\n                    buf = state['momentum_buffer']\n                    buf.mul_(group['momentum']).addcdiv_(grad, avg)\n                    p.add_(buf, alpha=-group['lr'])\n                else:\n                    p.addcdiv_(grad, avg, value=-group['lr'])\n\n        return loss\n```\n\nRMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间\n\n\n\n## Adam\n\n谈到这里，Adam和Nadam的出现就很自然而然了——他们是前述方法的集大成者。\n\nSGDM在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加的二阶动量。\n\n把一阶动量和二阶动量都用起来就是Adam了——Adaptive + Momentum\n\nSGD的一阶动量：\n$$\nm_t = \\beta_1 \\cdot m_{t-1} +(1-\\beta_1)\\cdot g_t\n$$\n加上AdaDelta的二阶动量：\n$$\n\\hat m_t =\\frac{m_t}{1-\\beta_1^t}\n$$\n\n$$\n\\hat V_t = \\frac{V_t}{1-\\beta_2^t}\n$$\n\n优化算法里最常见的两个超参数$\\beta_1,\\beta_2$就都在这里了，前者是控制一阶动量，后者控制二阶动量。\n\n## Nadam\n\n都说Adam是集大成者，但它遗漏了Nesterov，安装NAG的步骤1：\n$$\ng_t = ∇f(w_t-\\alpha\\cdot m_{t-1}/\\sqrt{V_t})\n$$\nNesterov+Adam = Nadam\n\n> 定义优化器：\n\n```\nCLASS torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n```\n\n> 参数：\n\n- **params** (iterable) – 优化器作用的模型参数。\n- **lr** (float) – learning rate – 相当于是统一框架中的 。\n- **betas**(Tuple[float,float],optional) – coefficients used for computing running averages of gradient and its square ((默认值：(0.9, 0.999))\n- **weight_decay** (float, optional) – 权重衰减系数 weight decay (L2 penalty) (默认值：0)\n- **eps**(float,optional)：防止分母为0的一个小数 (默认值：1e-10)\n\n```python\nimport math\nimport torch\nfrom .optimizer import Optimizer\n\n\n[docs]class Adam(Optimizer):\n    r\"\"\"Implements Adam algorithm.\n\n    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False)\n\n    .. _Adam\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, amsgrad=False):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        if not 0.0 <= weight_decay:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, amsgrad=amsgrad)\n        super(Adam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(Adam, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsgrad', False)\n\n[docs]    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n                amsgrad = group['amsgrad']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                if amsgrad:\n                    max_exp_avg_sq = state['max_exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n\n                if group['weight_decay'] != 0:\n                    grad = grad.add(p, alpha=group['weight_decay'])\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n                else:\n                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n\n                step_size = group['lr'] / bias_correction1\n\n                p.addcdiv_(exp_avg, denom, value=-step_size)\n\n        return loss\n```\n\n## AdamW\n\nAdam的另一个改进版AdamW\n\nAdamW就是Adam优化器加上L2正则，来限制参数值不可太大。\n\n以往的L2正则是直接加在损失函数上，比如这样：加入正则， 损失函数就会变成：\n$$\nL_{l_2}(\\theta) = L(\\theta) + 1/2\\gamma||\\theta||^2\n$$\n所以在计算梯度$g_t$时要加上粉色的这一项。\n\n但AdamW稍有不同，如下图所示，将正则加在了绿色位置。\n\n![](https://i.loli.net/2021/08/13/C37Bz8jYiwRI4fF.png)\n\n至于为何这么做？直接摘录BERT里面的原话看看：\n\n> **Just** adding the square of the weights to the loss function is *not* the correct way of using L2 regularization/weight decay with Adam, since that will interact with the m and v parameters in strange ways. Instead we want to decay the weights in a manner that doesn't interact with the m/v parameters. This is equivalent to adding the square of the weights to the loss with plain (non-momentum) SGD. Add weight decay at the end (fixed version).\n\n意思s 如果直接将L2正则加到loss上去，由于Adam优化器的后续操作，该正则项将会与$m_t$和$v_t$产生奇怪的作用。因而，AdamW选择将L2正则项加在了Adam的$m_t$和$v_t$等参数被计算完之后，在于学习率$\\eta$相乘之前，所以这也表明了weight_decay和L2正则虽目的一致、公式一致，但用法还是不同，二者有明显的区别。\n\n以 PyTorch1.7.0 中的AdamW代码为例：\n\n> 定义优化器：\n\n```\nCLASS torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n```\n\n- **params** (iterable) – 优化器作用的模型参数。\n- **lr** (float) – learning rate – 相当于是统一框架中的 。\n- **betas**(Tuple[float,float],optional) – coefficients used for computing running averages of gradient and its square ((默认值：(0.9, 0.999))\n- **weight_decay** (float, optional) – 权重衰减系数 weight decay (L2 penalty) (默认值：0)\n- **eps**(float,optional)：防止分母为0的一个小数 (默认值：1e-10)\n\n```python\nimport math\nimport torch\nfrom .optimizer import Optimizer\n\n\n[docs]class AdamW(Optimizer):\n    r\"\"\"Implements AdamW algorithm.\n\n    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.\n    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False)\n\n    .. _Adam\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _Decoupled Weight Decay Regularization:\n        https://arxiv.org/abs/1711.05101\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=1e-2, amsgrad=False):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        if not 0.0 <= weight_decay:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, amsgrad=amsgrad)\n        super(AdamW, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(AdamW, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsgrad', False)\n\n[docs]    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n\n                # Perform stepweight decay\n                p.mul_(1 - group['lr'] * group['weight_decay'])\n\n                # Perform optimization step\n                grad = p.grad\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n                amsgrad = group['amsgrad']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                if amsgrad:\n                    max_exp_avg_sq = state['max_exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n                else:\n                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n\n                step_size = group['lr'] / bias_correction1\n\n                p.addcdiv_(exp_avg, denom, value=-step_size)\n\n        return loss\n```\n\n与Adam不一样的地方是：\n\nAdam如果使用weight_decay的话，那么相当于目标函数上加了$1/2\\gamma||\\theta||^2$，所以相当于是梯度加上$\\gamma\\theta$故Adam使用了\n\ngrad = grad.add(p, alpha=group['weight_decay'])\n\n而 AdamW 是 p.mul_(1 - group['lr'] * group['weight_decay']) 直接让参数：\n\n$\\theta_t =\\theta_{t-1}-\\alpha\\cdot\\lambda\\cdot\\theta_{t-1}-\\alpha\\cdot\\eta_t$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["ML&DL"]},{"title":"TreeMap红黑树","url":"/2021/08/10/TreeMap红黑树/","content":"\n# TreeMap红黑树\n\n- 前言\n- 二叉查找树BST\n- BST存在的问题\n- 2-3-4树\n- 红黑树\n\n## 前言\n\n除了要学会着色、旋转规则，还要了解为什么变色，为什么旋转。\n\n五大性质的缘由\n\njdk的TreeMap在红黑树上做了一些优化，原版红黑树删除操作它是找的前驱节点替代原删除节点，而TreeMap源码里是用的后继节点替代原删除节点，这两种方案实际效果一样，只不过树的结构不一样，但是对应的红黑树都是平衡的。\n\n\n\n## 二叉查找树(BST)\n\n### 定义\n\n二叉查找树，就是一颗二叉树，他的左节点比父节点小，右节点比父节点大，他的高度决定查找效率。\n\n![](https://i.loli.net/2021/08/10/K6rNGpl3tUue714.png)\n\n### 操作\n\n查找(红黑树通用)：查找每个节点从根开始\n\n- 查找值比当前大，搜右子树\n- 查找值等于当前值，停止查找，返回\n- 查找值比当前值小，则搜右子树\n\n插入：要插入节点，必须先找到插入节点位置，按照搜索的流程，找到左子树或右子树为空的位置插入。\n\n遍历(红黑树通用)：前中后序遍历。\n\n查找最小值(红黑树通用)：沿着根节点的左子树一路查找，直到最后一个不为空的节点\n\n查找最大值(红黑树通用)：沿着根节点的右子树一路查找，直到最后一个不为空的节点\n\n查找前驱节点(红黑树通用)：小于当前节点的最大值\n\n查找后继节点(红黑树通用)：大于当前节点的最小值\n\n![](https://z3.ax1x.com/2021/08/20/fX2DBT.png)\n\n删除 : 本质上是找前驱或后继节点来替代\n\n- 叶子节点直接删除(没有前驱或或后继)\n- 只有一个子节点的用子节点替代(本质上就是找的前驱节点或者后继节点，左节点就是前驱节点，右节点就是后继节点)\n- 有两个子节点的，需要找到替代节点(替代节点也是前驱或者后继)\n\n删除操作和红黑树一样，只不过红黑树多了着色和旋转过程。\n\n\n\n### BST存在的问题\n\nBST存在的问题是，树在插入的时候会导倾斜，不同的插入顺序会导致高度不一样，而树的高度直接影响了树的查找效率。\n\n基于这个问题平衡二叉树产生了，平衡树的插入和删除时，会通过旋转操作将高度保持在LogN。\n\n其中两款有代表性的平衡树分别为\n\n- AVL树（高度平衡树，具备二叉搜索树的全部特性，而且左右子树高度差不超过1）\n- 红黑树\n\n面试题：有了AVL树为什么还要红黑树？\n\nAVL树由于实现比较复杂，而且插入和删除性能差。AVL很多性能耗在旋转操作上\n\n在实际环境下的应用不如红黑树。\n\n红黑树的实际应用范围广，如java中的HashMap和TreeSet，java8中HashMap的实现因为用RBTree代替链表(链表长度大于8时)，性能有提升。\n\n\n\n## 2-3-4树\n\n### 定义\n\n2-3-4树是四阶B树(Balance Tree)，他属于一种多路查找树，他的结构有以下限制：\n\n- 所有叶子节点都拥有相同的深度。\n- 节点只能是2-节点、3-节点、4-节点之一。\n- - 2-节点：包含1个元素的节点，有2个子节点；\n  - 3-节点：包含2个元素的节点，有3个子节点；\n  - 4-节点：包含3个元素的节点，有4个子节点；\n  - 所有节点必须至少包含1个元素\n- 元素始终保持排序顺序，整体上保持二叉查找树的性质，即父结点大于左子节点，小于右子结点；而且结点有多个元素时，每个元素必须大于它左边的和它的右子树中元素。\n\n![](https://i.loli.net/2021/08/14/dVxDmrkuKgUv6M9.png)\n\n### 结点插入\n\n2-3-4树中结点添加需要遵循以下规则：\n\n- 插入都是向最下面一层插入\n- 升元：将插入结点由2-节点升级成3-节点，或由3-结点升级成4-结点\n- 向4-结点插入元素后，需要将中间元素提到父节点升元，原节点变成两个2-节点，再把元素插入2-结点中，如果父节点也是4-结点，则递归向上层升元，直到根节点后将树高加1。\n\n而将这些规则对应到红黑树里，就是：\n\n- 新插入的结点颜色为**红色**，这样才可能不会对红黑树的高度产生影响。\n- 2-结点对应红黑树中的单个黑色结点，插入时直接成功(对应2-结点升元)\n- 3-结点对应红黑树中的**黑+红**子树，插入后将其修复成**红+黑+红**子树\n- 4-结点对应红黑树中的**红+黑+红**子树，插入后将其修复成**红色祖父+黑色父叔+红色孩子**子树，然后再把祖父结点当成新插入的红色结点递归向上层修复，直至修复成功或遇到root结点。\n\n公式：**红黑树+新增一个节点(红色) = 对等的2-3-4树+新增一个节点**\n\n\n\n### 删除结点\n\n2-3-4树的删除可以全部转换为叶子节点的删除\n\n删除原则是先看能不能和下面的叶子节点合并，能合并的直接合并完后删除，不能合并的就要找个元素替换上去，最终都是要保持平衡。\n\n合并-->删除\n\n合并-->替换-->删除\n\n合并-->无法替换-->再合并-->删除\n\n**红色结点一定全部都在多元素节点中**\n\n红黑树的删除要比插入复杂，还是类比2-3-4树：\n\n- 查找最近的叶子结点的元素替代被删除元素，删除替代元素后，从替代元素所处叶子结点开始处理\n- 降元：4-结点变3-结点，3-结点变2-结点。\n- 2-结点中只有一个元素，所以借兄弟结点中的元素来补充删除后的造成的空结点。\n- 当兄弟结点中也没有多个元素可以补充时，尝试将父节点降元，失败时向上递归，直到子树降元成功或root结点树高减一\n\n将这些规则对应到红黑树中即：\n\n- 查找离当前结点最近的叶子结点作为替代节点，(左子树的最右结点或右子树的最左结点都能保证替换后二叉树的节点排序性质，叶子节点的替代结点是自身) 替换掉被删除结点，从替代的叶子结点向上递归修复。\n- 替代结点颜色为红色(对应2-3-4树中 4-节点或3-结点) 时删除子结点直接成功\n- 替代节点为黑色(对应2-3-4树中 2-节点)时， 意味着替代结点所在的子树会降一层，需要依次检验以下三项，以恢复子树高度：\n- - 兄弟结点的子结点中有红色节点(兄弟结点对应3-结点或4-结点) 能够“借用”，旋转过来后修正颜色。\n  - 父结点是红色结点（父结点对应3-结点或4-结点，可以降元）时，将父结点变为黑色，自身和兄弟结点变红色后删除。\n  - 父结点和兄弟结点都是黑色时，将子树降一层后把 **父结点当做替代结点** 递归向上处理。\n\n\n\n### 红黑树对应一颗2-3-4数，一颗2-3-4树对应多颗红黑树\n\n红黑树和2-3-4树的结点添加和删除都有一共基本规则：避免子树高度变化，因为无论是2-3-4树还是红黑树，一旦子树高度有变动，势必会影响其他子树进行调整。所以我们在插入和删除节点时尽量通过子树内部调整来达到平衡。\n\n2-3-4树实现平衡是通过结点的旋转和结点元素变化，红黑树是通过结点旋转和变色。\n\n![](https://i.loli.net/2021/08/14/kgC4yNQ5frcoXBm.png)\n\n2节点全是黑色，3节点有左倾右倾两种情况，4节点上黑下红\n\n2-3-4树的裂变状态: 红黑树新增都是以红色节点进来的，11裂变上去变成红色，下面两个变成黑色\n\n![](https://i.loli.net/2021/08/14/ykrG7CEatD3hULF.png)\n\n整体对比2-3-4树和红黑树\n\n![](https://i.loli.net/2021/08/14/2OrLtl73KcwIunk.png)\n\n![](https://i.loli.net/2021/08/14/5DzbNJyHqQRv8dx.png)\n\n\n\n\n\n\n\n## 红黑树\n\n<img src=\"https://i.loli.net/2021/08/14/CmA461TBsgPtdjG.png\" style=\"zoom:50%;\" />\n\n### 定义\n\n红黑树是一种结点带有颜色属性的二叉查找树，但它在二叉查找树之外还有以下五大性质：\n\n- 结点是红色或黑色\n- 根是黑色\n- 所有叶子都是黑色(叶子是NIL节点，这类节点不可忽视，否则代码会看不懂)\n- 每个红色节点必须有两个黑色子节点（从每个叶子到根的所有路径上不能有两个连续的红色结点）\n- 从任意一节点到其每个叶子的所有简单路径都包含相同数目的黑色结点（黑色平衡）\n\n### 常见操作\n\n**变色、左旋、右旋**\n\n```java\n    private RBNode root;\n\n    /**\n     * 围绕p左旋\n     * @param p\n     *              pf                  pf\n     *            /                   /\n     *           p                   pr(r)\n     *          / \\                 /  \\\n     *         pl  pr(r)    ->     p   rr\n     *             / \\            / \\\n     *            rl rr          pl  rl\n     */\n    private void leftRotate(RBNode p) {\n        if (p != null) {\n            RBNode r = p.right;\n            p.right = r.left;\n            if (r.left != null) {\n                r.left.parent = p;\n            }\n            r.parent = p.parent;\n            if (p.parent == null) {\n                root = r;\n            } else if (p.parent.left == p) {\n                p.parent.left = r;\n            } else {\n                p.parent.right = r;\n            }\n            r.left = p;\n            p.parent = r;\n        }\n    }\n\n    /**\n     * 围绕p右旋\n     * @param p\n     *          pf                        pf\n     *           \\                         \\\n     *            p                         pl(l)\n     *           / \\           ->           /  \\\n     *       pl(l)  pr                    ll    p\n     *       / \\                               / \\\n     *     ll   lr                            lr  pr\n     */\n    private void rightRotate(RBNode p) {\n        if (p != null) {\n            RBNode l = p.left;\n            p.left = l.right;\n            if (l.right != null) {\n                l.right.parent = p;\n            }\n            l.parent = p.parent;\n            if (p.parent != null) {\n                root = l;\n            } else if (p.parent.right == p) {\n                p.parent.right = l;\n            } else {\n                p.parent.left = l;\n            }\n            l.right = p;\n            p.parent = l;\n        }\n    }\n```\n\n**新增** （七种情况，五种情况需要考虑自平衡）\n\n![](https://i.loli.net/2021/08/30/1zoZ7HYrsJDvkUy.png)\n\n分情况讨论，主要是找到插入位置，然后自平衡(左旋或者右旋) 且插入结点是红色节点(如果是黑色的话那么当前分支上就会多出一个黑色结点出来，从而破坏了黑色平衡)，以下分析全部以左子树为例，右子树的情况则相反。\n\n- 情况1、如果插入的是第一个节点(根节点)，红色变黑色\n- 情况2、如果父节点为黑色，则直接插入，不需要变色\n- 如果父节点为红色，叔叔节点也是红色（此种情况爷爷节点一定是黑色），则父节点和叔叔节点变黑色，爷爷节点变红色（如果爷爷节点是根节点，则再变成黑色），爷爷节点此时需要递归（把爷爷节点当做新插入的节点再次进行比较）\n- 如果父节点是红色，没有叔叔节点或者叔叔节点是黑色（此时只能是NIL节点），则以爷爷节点为支点右旋，旋转之后原来的爷爷节点变红色，原来的父节点变黑色。\n\n还是与2-3-4树对比，新增一定在叶子节点上\n\n情况1\n\n![](https://i.loli.net/2021/08/14/cNnouZS7wDbf48r.png)\n\n情况2——右边相当于左右旋了\n\n![](https://i.loli.net/2021/08/14/rWCDmgQLfAUb7s3.png)\n\n与3节点合并情况\n\n![](https://i.loli.net/2021/08/14/rWPIiuGDKaRx8UQ.png)\n\n![](https://i.loli.net/2021/08/14/yDrf4qQhVX2t65a.png)\n\n![](https://i.loli.net/2021/08/14/8D9eqhpGnMKFwTZ.png)\n\n裂变情况\n\n![](https://i.loli.net/2021/08/14/ahr8mFQO14gBfCY.png)\n\n![](https://z3.ax1x.com/2021/08/20/fXRvW9.png)\n\n![](https://z3.ax1x.com/2021/08/20/fXfK39.png)\n\n\n\n**删除** (重点) 五种情况、两种需要考虑自平衡，又细分八种情况，其中四种为镜像情况\n\n先看二叉搜索树的删除：\n\n- 删除叶子节点，直接删除\n- 删除的节点有一个子节点，那么用子节点来替代\n- 如果删除的节点有2个子节点，此时需要找到前驱节点或者后继节点来替代\n\n写代码时，删除方案：\n\n- 找到前驱节点，复制前驱节点值覆盖准备删除的节点值，然后删除前驱节点\n- 找到后继节点，复制后继节点的值覆盖准备删除的节点值，然后删除后继节点\n- 被删除的前驱节点或者后继节点只有两种情况：1、被删除节点是叶子节点。2、被删除节点只有一个孩子。\n\n找前驱后继代码：\n\n```java\n/**\n     * 找到指定节点的前驱节点，即找小于node节点的最大值\n     *\n     * @param node\n     */\n    private RBNode preedecessor(RBNode node) {\n        if (node == null) return null;\n        else if (node.left != null) {\n            RBNode p = node.left;\n            while (p.right != null) {\n                p = p.right;\n            }\n            return p;\n        } else {\n            // 删除时不一定用到，但找前驱时，左子树就得向上找了\n            // 找到第一个左拐的地方\n            RBNode p = node.parent;\n            RBNode ch = node;\n            while (p != null && p.left == ch) {\n                ch = p;\n                p = p.parent;\n            }\n            return p;\n        }\n    }\n\n    /**\n     * 找到指定节点的后继节点，大于节点的最小值\n     *\n     * @param node\n     * @return\n     */\n    private RBNode sucessor(RBNode node) {\n        if (node == null) return null;\n        else if (node.right != null) {\n            RBNode p = node.right;\n            while (p.left != null) {\n                p = p.left;\n            }\n            return p;\n        } else {\n            // 删除时不一定用到，但找前驱时，左子树就得向上找了\n            // 找到第一个左拐的地方\n            RBNode p = node.parent;\n            RBNode ch = node;\n            while (p != null && p.right == ch) {\n                ch = p;\n                p = p.parent;\n            }\n            return p;\n        }\n    }\n```\n\n\n\n红黑树的删除：1先找到节点，2删除\n\n红黑树上面的删除节点一定是2-3-4树上的叶子节点\n\n![](https://z3.ax1x.com/2021/08/20/fXVpDA.png)\n\n三局话：\n\n- **自己能搞定的自己搞定**：\n- - 如果删除的节点对应于2-3-4树的3节点或者4节点，则直接删除，不用和兄弟或父亲借。\n  - 如果删除的是红色节点，则直接删除；如果是黑色节点，则红色节点上来替代，变黑即可\n- **搞不定的找兄弟和父亲帮忙**:\n- - 前提是找到真正的可用的兄弟结点 (真正的兄弟节点是对应于2-3-4树中的兄弟节点，如上图左，5的兄弟节点是8，图右中5的兄弟节点应该是7和7.5，可以通过旋转来找)\n  - 兄弟节点有的借(此时兄弟节点一定是黑色，如果是红色那说明这个节点不是真正的兄弟节点，需要回到上一步找真正的兄弟节点)\n  - 兄弟节点有两个子节点的情况(两个子节点肯定是红色，如果是黑色的话相当于此时兄弟节点对应2-3-4树是2节点，不可能有多余的元素可以借)，此时需要旋转变色\n  - 兄弟节点只有一个子节点的情况，此时需要旋转变色\n- **父亲和兄弟帮不了那有福同享，有难同当(父亲和兄弟自损)**：\n- - 前提还是找到真正的兄弟节点\n  - 兄弟节点没有多余的元素可借（此时兄弟节点一定为黑色的2节点），此时兄弟节点所在分支也要自损一个黑色节点以达到黑色平衡，最快的方式就是兄弟节点直接变红(相当于减少一个黑色节点)，此时以父节点为root的子树又达到了平衡(两边都比之前少了一个黑色)。但是以祖父结点为root的树依然是不平衡的，此时需要递归处理。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["数据结构"]},{"title":"并查集","url":"/2021/08/10/并查集/","content":"\n# 并查集\n\n## 用途\n\n解决元素分组问题，管理一系列不相交的集合\n\n## 操作\n\n- 合并(Union)：把两个不相交的集合合并为一个集合\n- 查询(Find)：查询两个元素是否在同一个集合\n\n### 以一个应用场景为例应用场景\n\n(洛谷P1551) 亲戚\n\n>**题目背景**\n>若某个家族人员过于庞大，要判断两个是否是亲戚，确实还很不容易，现在给出某个亲戚关系图，求任意给出的两个人是否具有亲戚关系。\n>**题目描述**\n>规定：x和y是亲戚，y和z是亲戚，那么x和z也是亲戚。如果x,y是亲戚，那么x的亲戚都是y的亲戚，y的亲戚也都是x的亲戚。\n>**输入格式**\n>第一行：三个整数n,m,p，（n<=5000,m<=5000,p<=5000），分别表示有n个人，m个亲戚关系，询问p对亲戚关系。\n>以下m行：每行两个数Mi，Mj，1<=Mi，Mj<=N，表示Mi和Mj具有亲戚关系。\n>接下来p行：每行两个数Pi，Pj，询问Pi和Pj是否具有亲戚关系。\n>**输出格式**\n>P行，每行一个’Yes’或’No’。表示第i个询问的答案为“具有”或“不具有”亲戚关系。\n\n把所有人划分到若干个不相交的集合中，每个集合里的人彼此是亲戚。为了判断两个人是否为亲戚，只需看它们是否属于同一个集合即可。因此，这里就可以考虑用并查集进行维护。\n\n\n## 思想\n\n**用集合中的一个元素代表集合**。我曾看过一个有趣的比喻，把集合比喻成**帮派**，而代表元素则是**帮主**。接下来我们利用这个比喻，看看并查集是如何运作的。\n\n![](https://i.loli.net/2021/08/10/2xK7tBoWcLDlghy.png)\n\n最开始，每个元素的代表元素是自己。每个元素的最顶端是自己，是一个自环节点。\n\n在比较两人是不是一个帮派的时候，就找自己的帮主，看看是不是一个帮主。每个元素向上找，找到不能再找了就是了。\n\n然而帮派规模大了，肯定会造成等级(树的深度)变深。\n\n有两个操作可以优化一个是路径压缩，一个是按秩压缩。\n\n之前说找帮主来判断两个元素是否在同一集合内。找到帮主一样说明是一个集合里的，不一样把较小集合的帮主指向较大集合的帮主，这样做就是按秩压缩：\n\n![](https://i.loli.net/2021/08/10/QYl1ZJRFnekUhCB.png)\n\n按秩压缩了之后其实还是可以发现树是深度是越来越深的，那么再采取路径压缩。\n\n在每次向上查找的过程中，将路过的节点之间指向帮主，这是通过栈来实现的。\n\n\n\n## 代码\n\n```java\npublic class 并查集 {\n\n    // 样本进来会包一层，叫做元素\n    public static class Element<V> {\n        public V value;\n\n        public Element(V value) {\n            this.value = value;\n        }\n    }\n\n    public static class UnionFindSet<V> {\n\n        public HashMap<V, Element<V>> elementMap;\n        // key 某个元素value 该元素的父\n        public HashMap<Element<V>, Element<V>> fatherMap;\n        // key 某个集合的代表元素， value该集合的大小\n        public HashMap<Element<V>, Integer> sizeMap;\n\n        public UnionFindSet(List<V> list) {\n            elementMap = new HashMap<>();\n            fatherMap = new HashMap<>();\n            sizeMap = new HashMap<>();\n\n            for (V value : list) {\n                Element element = new Element(value);\n                elementMap.put(value, element);\n                fatherMap.put(element, element);\n                sizeMap.put(element, 1);\n            }\n        }\n\n        // 给定一个element，网上找，把代表元素返回\n        private Element<V> findHead(Element<V> element) {\n            Stack<Element<V>> path = new Stack<>();\n            while (element != fatherMap.get(element)) {\n                path.push(element);\n                element = fatherMap.get(element);\n            }\n            while (!path.isEmpty()) {  // 路径铺平\n                fatherMap.put(path.pop(), element);\n            }\n            return element;\n        }\n\n        public boolean isSameSet(V a, V b) {\n            if (elementMap.containsKey(a) && elementMap.containsKey(b)) {\n                return findHead(elementMap.get(a)) == findHead(elementMap.get(b));\n            }\n            return false;\n        }\n\n        public void union(V a, V b) {\n            if (elementMap.containsKey(a) && elementMap.containsKey(b)) {\n                Element<V> aF = findHead(elementMap.get(a));\n                Element<V> bF = findHead(elementMap.get(b));\n                if (aF != bF) {\n                    Element<V> big = sizeMap.get(aF) >= sizeMap.get(bF) ? aF : bF;\n                    Element<V> small = big == aF ? bF : aF;\n                    fatherMap.put(small, big);\n                    sizeMap.put(big, sizeMap.get(aF) + sizeMap.get(bF));\n                    sizeMap.remove(small);\n                }\n            }\n        }\n\n    }\n\n\n}\n```\n\n\n\n## 应用\n\n洛谷P1551亲戚\n\n```java\npublic static void main(String[] args) {\n        Scanner scanner = new Scanner(System.in);\n        int n = scanner.nextInt(); // 人数\n        int m = scanner.nextInt(); // 关系数\n        int p = scanner.nextInt(); // 询问多少个关系\n\n        List<Integer> personList = new ArrayList<>();\n        int[][] relation = new int[m][2];\n        for (int i = 0; i < m; i++) {\n            int p1 = scanner.nextInt();\n            int p2 = scanner.nextInt();\n            if (!personList.contains(p1)) personList.add(p1);\n            if (!personList.contains(p2)) personList.add(p2);\n            relation[i][0] = p1;\n            relation[i][1] = p2;\n//            relationMap.put(p2, p1);\n        }\n\n        UnionFindSet<Integer> unionSet = new UnionFindSet<>(personList);\n\n\n//        for (Map.Entry<Integer, Integer> entry : relationMap.entrySet()) {\n//            unionSet.union(entry.getKey(), entry.getValue());\n//        }\n\n        for (int i = 0; i < m; i++) {\n            unionSet.union(relation[i][0], relation[i][1]);\n        }\n\n        for (int i = 0; i < p; i++) {\n            int p1 = scanner.nextInt();\n            int p2 = scanner.nextInt();\n            if (unionSet.isSameSet(p1, p2)) {\n                System.out.println(\"Yes\");\n            } else {\n                System.out.println(\"No\");\n            }\n        }\n    }\n```\n\n\n\n\n\n\n\n\n\n\n\n","tags":["数据结构"]},{"title":"Docker学习(四) 网络","url":"/2021/08/06/Docker学习-四-网络/","content":"\n# Docker学习(四) 网络\n\n## 理解docker0\n\n![](https://i.loli.net/2021/08/06/Aln7NGbIr8FKL2h.png)\n\n```shell\n# docker 是如何处理容器网络访问的\n\n(base) root@linux:/home/cpss# docker run -d -P --name tomcat01 tomcat\n\n# 查看容器的内部网络地址 ip addr\n# 发现容器启动的时候会得到一个 eth0@if34 ip地址，docker分配的\n(base) root@linux:/home/cpss# docker exec -it tomcat01 ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n33: eth0@if34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default \n    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0\n       valid_lft forever preferred_lft forever\n\n# 思考：linux能不能ping通容器内部\n(base) root@linux:/home/cpss# ping 172.17.0.2\nPING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.\n64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.107 ms\n64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.055 ms\n\n# linux 可以ping通docker容器内部\n```\n\n> 原理\n\n- 我们每启动一个docker容器，docker就会给docker容器分配一个ip，只要安装了docker，就会有一个网卡docker0 桥接模式，使用的技术是 evth-pair技术\n\n  现在再在宿主机执行ip addr，发现多了一个\n\n  ![](https://i.loli.net/2021/08/06/nkoUMcDPLG6KRax.png)\n\n再启动一个容器测试\n\n```sh\n(base) root@linux:/home/cpss# docker run -d -P --name tomcat02 tomcat\n# 每启动一个就多一个网卡\n\n# 发现这个容器带来的网卡，都是一对一对的\n# evth-pair 就是一对的虚拟设备端口，他们都是成对出现的，一段连着协议，一段彼此相连\n# 正因为有这个特性，evth-pair 充当一个桥梁,连接各种虚拟网络设备的\n# Openstac，Docker容器之间的链接，ovs的链接，都是使用这个技术\n```\n\n测试一下tomcat01和tomcat02是否能ping通\n\n```\n(base) root@linux:/home/cpss# docker exec -it tomcat02 ping 172.17.0.2\nPING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.\n64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.135 ms\n64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.086 ms\n64 bytes from 172.17.0.2: icmp_seq=3 ttl=64 time=0.067 ms\n\n\n# 结论 容器之间可以互相ping通的\n```\n\n![](https://i.loli.net/2021/08/06/8x9nU6PVTZepR7b.png)\n\nTomcat01 和tomcat02 是公用的一个路由器，docker0\n\n所有的容器不指定网络的情况下，都是使用docker0路由的，docker会给我们的容器分配一个默认的可用ip\n\n> 小结\n\ndocker 使用的是桥接，宿主机中是一个docker容器的网桥 docker0\n\n![](https://i.loli.net/2021/08/06/bTBl5Qsh7qVCXI4.png)\n\ndocker 中所有的网络端口都是虚拟的，虚拟的转发效率高(内网传递)\n\n> 思考一个场景，编写了一个微服务，database url=ip:, 项目不重启，数据库ip换掉了，我们希望可以处理这个问题，可以通过名字来进行访问容器\n\n###  --link\n\n```shell\n(base) root@linux:/home/cpss# docker exec -it tomcat02 ping tomcat01\nping: tomcat01: Name or service not known\n\n(base) root@linux:/home/cpss# docker run -d -P --name tomcat03 --link tomcat02 tomcat\n6f62526ba5484b2c542bc31b0891a1f06c1baedbdb8667322b9b051a1f443e06\n\n# 通过--link 可以解决\n(base) root@linux:/home/cpss# docker exec -it tomcat03 ping tomcat02\nPING tomcat02 (172.17.0.3) 56(84) bytes of data.\n64 bytes from tomcat02 (172.17.0.3): icmp_seq=1 ttl=64 time=0.108 ms\n64 bytes from tomcat02 (172.17.0.3): icmp_seq=2 ttl=64 time=0.066 ms\n64 bytes from tomcat02 (172.17.0.3): icmp_seq=3 ttl=64 time=0.069 ms\n\n# 反向 ping不通\n(base) root@linux:/home/cpss# docker exec -it tomcat02 ping tomcat03\nping: tomcat03: Name or service not known\n\n\n(base) root@linux:/home/cpss# docker network ls\nNETWORK ID     NAME      DRIVER    SCOPE\n27abb1e4b0d3   bridge    bridge    local\ne69e785a705e   host      host      local\n2412989a4eb3   none      null      local\n\n\n# 其实这个tomcat03就是在本地配置了tomcat02的配置\n# --link 就是我们在hosts配置中增加了一个 映射\n(base) root@linux:/home/cpss# docker exec -it tomcat03 cat /etc/hosts\n127.0.0.1\tlocalhost\n::1\tlocalhost ip6-localhost ip6-loopback\nfe00::0\tip6-localnet\nff00::0\tip6-mcastprefix\nff02::1\tip6-allnodes\nff02::2\tip6-allrouters\n172.17.0.3\ttomcat02 9de7a985a568\n172.17.0.4\t6f62526ba548\n```\n\n现在玩docker已经不建议使用 --link了\n\n自定义网络! 不适用docker0\n\ndocker0问题：他不支持容器名链接访问\n\n\n\n### 自定义网络 （容器互联）\n\n```shell\n(base) root@linux:/home/cpss# docker network --help\nCommands:\n  connect     Connect a container to a network\n  create      Create a network\n  disconnect  Disconnect a container from a network\n  inspect     Display detailed information on one or more networks\n  ls          List networks\n  prune       Remove all unused networks\n  rm          Remove one or more networks\n```\n\n\n\n```\n(base) root@linux:/home/cpss# docker network ls\nNETWORK ID     NAME      DRIVER    SCOPE\n27abb1e4b0d3   bridge    bridge    local\ne69e785a705e   host      host      local\n2412989a4eb3   none      null      local\n```\n\n网络模式\n\n- bridge：桥接docker (默认)\n- none: 不配置网络\n- host: 和宿主机共享网络\n- container：容器内网络连通 (用的少，局限性大)\n\n测试\n\n```shell\n# 我们之间启动的命令 --net bridge 而这个就是我们的docker0\ndocker run -d -P --name tomcat01 tomcat\ndocker run -d -P --name tomcat01 --net bridge tomcat\n\n# docker0 特点。默认，域名不能访问， --link可以打通\n\n# 我们可以自定义个网络\n(base) root@linux:/home/cpss# docker network create --help\nOptions:\n      --attachable           Enable manual container attachment\n      --aux-address map      Auxiliary IPv4 or IPv6 addresses used by Network driver (default map[])\n      --config-from string   The network from which to copy the configuration\n      --config-only          Create a configuration only network\n  -d, --driver string        Driver to manage the Network (default \"bridge\")\n      --gateway strings      IPv4 or IPv6 Gateway for the master subnet\n      --ingress              Create swarm routing-mesh network\n      --internal             Restrict external access to the network\n      --ip-range strings     Allocate container ip from a sub-range\n      --ipam-driver string   IP Address Management Driver (default \"default\")\n      --ipam-opt map         Set IPAM driver specific options (default map[])\n      --ipv6                 Enable IPv6 networking\n      --label list           Set metadata on a network\n  -o, --opt map              Set driver specific options (default map[])\n      --scope string         Control the network's scope\n      --subnet strings       Subnet in CIDR format that represents a network segment\n      \n      \n      \ndocker network create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 mynet\n```\n\n![](https://i.loli.net/2021/08/06/vcuabfSy6KpP9rl.png)\n\n不使用--link 也能连接了\n\n我们自定义的网络docker都已经帮我们维护好了对应的关系\n\n好处：\n\n不同的集群使用不同的网络，保证集群是安全健康的。\n\n\n\n### 网络连通\n\n```sh\n(base) root@linux:/home/cpss# docker network --help\n  connect     Connect a container to a network\n  \n(base) root@linux:/home/cpss# docker network connect --help\nOptions:\n      --alias strings           Add network-scoped alias for the container\n      --driver-opt strings      driver options for the network\n      --ip string               IPv4 address (e.g., 172.30.100.104)\n      --ip6 string              IPv6 address (e.g., 2001:db8::33)\n      --link list               Add link to another container\n      --link-local-ip strings   Add a link-local address for the container\n      \n      \n\n# 测试打通 tomcat01 - mynet\n# 连通之后就是将tomcat01 放到了mynet网络下\n\n```\n\n结论：假设要跨网络操作别人，就需要使用docker network connect\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Docker"]},{"title":"Docker学习(三)","url":"/2021/08/05/Docker学习-三/","content":"\n# Docker学习(三)\n\n## 可视化\n\n### portainer (不常用)\n\nDocker的图形化界面管理工具\n\n```\n# 外部8088 内部9000 \n# -v 挂载\ndocker run -d -p 8088:9000\\\n--restart=always -v /var/run/docker.sock:/var/run/docker.sock --privileged=true portainer/portainer\n```\n\n### Rancher(CI/CD再用)\n\n\n\n---\n\n## 镜像是什么\n\n是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件。它包含某个软件所需的所有内容，包括代码、运行时、库、环境变量和配置文件。\n\n如何得到：\n\n- 远程仓库下载\n- 拷贝\n- 自己制作一个镜像DockerFile\n\n### 镜像加载原理\n\n>UnionFS(联合文件系统)\n\n我们下载的时候看到的一层层就是这个\n\nUnionFS是一种分层、轻量级且高性能的文件系统，它支持文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下.\n\n特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。\n\ndocker的镜像实际上由一层一层的文件系统组成，这种层级文件系统就是UnionFS\n\nbootfs(boot file system)，在Docker镜像的最底层是bootfs，这一层与典型的Linux、Unix系统是一样的，它主要包含bootloader和kernel，bootloader主要是引导加载kernel，Linux刚启动时会加载bootfs文件系统。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载掉bootfs\n\nrootfs(root file system)，在bootfs之上，包含的是Linux系统中的/dev /proc /bin /etc 等标准目录和文件，rootfs就是各种不同的操作系统发行版，如ubantu，centos\n\n对于一个精简的OS，rootfs可以很小，只包含最基本的命令，因为底层直接用host的kernel。\n\n\n\n## 分层理解\n\n> 分层的镜像\n\n下载的日志输出，可以看到是一层一层的在下载\n\n![](https://i.loli.net/2021/08/05/SmgOoLaUYiwG6sr.png)\n\n为什么采用这种分层结构 ？\n\n最大的好处就是资源共享，比如有多个镜像都从相同的Base镜像构建而来，那么宿主机只需在磁盘上保留一份base镜像，同时内存中也只需要加载一份base镜像，这样就可以为所有的容器服务了，而且镜像的每一层都可以被共享。\n\n查看镜像分层方式可以通过docker image inspect命令\n\n> 特点\n\nDocker镜像都是只读的，当容器启动时，一个新的可写层被加载到镜像的顶部。\n\n这一层就是通常说的容器层，容器之下的都叫镜像层。\n\n![](https://i.loli.net/2021/08/05/Bo3v1qXJnadp5Kh.png)\n\n## 如何commit镜像\n\n```\ndocker commit 提交容器成为一个新的副本\n# 和git类似\ndocker commit -m=\"提交的描述信息\" -a=\"作者\" 容器id 容器镜像名:[TAG]\n```\n\n## 容器数据卷\n\n数据？如果数据都在容器中，那么容器删除，数据就会丢失！需求：数据可以持久\n\n容器之间可以有一个数据共享的技术，docker容器中产生的数据，同步到本地\n\n目录挂载，将容器内的目录挂载到linux上。\n\n![](https://i.loli.net/2021/08/06/cCQPLRghqlwpUZr.png)\n\n总结一句话，容器的持久化和同步操作，容器间也是可以数据共享的。\n\n### 使用\n\n> 方式1：直接使用命令挂载 -v\n\n```\n-v 主机目录:容器内目录\n-p 主机端口:容器内端口\n\ndocker run -it -v /home/ceshi:/home centos /bin/bash\n```\n\n![](https://i.loli.net/2021/08/06/TpZcrXgRi3SV9hB.png)\n\n是双向的同步，哪怕容器已经停止。\n\n好处：以后修改只需在本地修改即可。\n\n### 安装mysql\n\n思考：mysql的数据持久化问题 \n\n```shell\n(base) root@linux:/home/cpss# docker pull mysql:5.7\n\n# 运行，需要数据挂载\n# 安装启动mysql时，需要配置密码的！\n# -d 后台运行\n(base) root@linux:/home/cpss# docker run -d -p 3310:3306 -v /data2/mysql/conf:/etc/mysql/conf.d -v /data2/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root --name mysql01 mysql:5.7\n72180bd20207e871aebdc0a06fddfe10e30d39561620c78558328b9ac0a30b9c\n```\n\n### 具名和匿名挂载\n\n```shell\n# 匿名挂载\n-v 容器内路径\ndocker run -d -P --name nginx01 -v /etc/nginx nginx\n\n# 查看所有的volume情况，匿名卷挂载\n(base) root@linux:/data2/mysql# docker volume ls\n# 这种就是匿名挂载，在-v只写了容器内的路径，没有写容器外的路径！\n\n# 具名挂载\ndocker run -d -P --name nginx02 -v juming-nginx:/etc/nginx nginx\n(base) root@linux:/data2/mysql# docker volume ls\nDRIVER    VOLUME NAME\nlocal     juming-nginx\n\n# 通过-v 卷名：容器内路径\n# 查看一下这个卷\n\n```\n\n![](https://i.loli.net/2021/08/06/HStUJkLaIY9sXm7.png)\n\n所有的docker容器内的卷，没有指定目录的情况下都是在 /var/lib/docker/volumes/xxxxx/_data\n\n通过具名挂载可以方便的找到一个卷，大多数情况在使用的是具名挂载。\n\n```sh\n# 如何确定是具名挂载还是匿名挂载， 还是指定路径挂载\n-v 容器内路径 # 匿名挂载\n-v 卷名：容器内路径 # 具名挂载\n-v /宿主机路径:容器内路径 # 指定路径挂载\n```\n\n扩展\n\n```shell\ndocker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:ro nginx\ndocker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:rw nginx\n\n# 通过 -v 容器内路径:ro rw改变读写权限\n#  容器对我们挂载出来的内容就有限定了\n# ro readonly 只要看到ro就说明这个路径只能通过宿主机来操作，容器内部是无法操作的。\n```\n\n> 方式二、dockerfile 创建镜像时就挂载出来\n\nDockerfile 就是用来构建docker镜像的构建文件\n\n通过脚本可以生成镜像，镜像是一层一层的，脚本一个个的命令，每个命令都是一层\n\n```shell\n(base) root@linux:/data2/docker-volume# pwd\n/data2/docker-volume\n(base) root@linux:/data2/docker-volume# vim dockerfile1\n(base) root@linux:/data2/docker-volume# cat dockerfile1 \nFROM centos\n\nVOLUME [\"volume01\",\"volume02\"]\n\nCMD echo \"---end---\"\n\nCMD /bin/bash\n(base) root@linux:/data2/docker-volume# docker build -f dockerfile1 -t zuo/centos:1.0 .\nSending build context to Docker daemon  2.048kB\nStep 1/4 : FROM centos\n ---> 300e315adb2f\nStep 2/4 : VOLUME [\"volume01\",\"volume02\"]\n ---> Running in 26da05b75834\nRemoving intermediate container 26da05b75834\n ---> 5ae4812f35a4\nStep 3/4 : CMD echo \"---end---\"\n ---> Running in 29c52fec2f47\nRemoving intermediate container 29c52fec2f47\n ---> cb1793533f3d\nStep 4/4 : CMD /bin/bash\n ---> Running in c4bc1543fe44\nRemoving intermediate container c4bc1543fe44\n ---> c635584bb2a8\nSuccessfully built c635584bb2a8\nSuccessfully tagged zuo/centos:1.0\n\n\n(base) root@linux:/data2/docker-volume# docker images\nREPOSITORY      TAG       IMAGE ID       CREATED          SIZE\nzuo/centos      1.0       c635584bb2a8   59 seconds ago   209MB\n\n\n\n# 创建dockerfile文件，名字可以随机\n# 文件中的内容 指令(大写) 参数\nFROM centos\n每个命令就是镜像的一层\n```\n\n![](https://i.loli.net/2021/08/06/9LG7RVIYzOsK4Sc.png)\n\n这个卷是匿名挂载，一定有外部的目录\n\n![](https://i.loli.net/2021/08/06/g9H4moazDAIJUTj.png)\n\n### 数据卷容器\n\n两个mysql同步数据 --volumes-from\n\n![](https://i.loli.net/2021/08/06/nZx8FVqjUbBS1I2.png)\n\n```shell\n# 启动3个容器，通过我们刚才自己的写镜像启动\n(base) root@linux:/data2/docker-volume# docker run -it --name docker01 zuo/centos:1.0\n\n(base) root@linux:/data2/docker-volume# docker run -it --name docker02 --volumes-from docker01 zuo/centos:1.0\n```\n\n容器之间配置信息的传递，数据卷容器的生命周期一直持续到没有容器为止\n\n一旦持久化到了本地，本地的数据是不会删除的。\n\n\n\n\n\n## DockerFile\n\ndockerfile 是用来构建docker镜像的文件，命令参数脚本\n\n构建步骤：\n\n- 编写一个dockerfile文件\n- docker build构建成为一个镜像\n- docker run 运行镜像\n- docker push 发布镜像(docker hub，阿里云镜像仓库)\n\n### 构建过程\n\n基础知识\n\n- 每个保留关键字指令都是大写字母\n- 执行从上到下顺序执行\n- 每个指令都会创建提交一个新的镜像层\n\n![](https://i.loli.net/2021/08/06/3JdmShcx4r6znqw.png)\n\ndockerfile是面向开发的，我们以后要发布项目做镜像，要写。\n\n### 指令\n\n```dockerfile\nFROM          # 基础镜像，一切从这里开始构建\nMAINTAINER    # 镜像是谁写的，姓名+邮箱\nRUN           # 镜像构建时要运行的命令\nADD           # 步骤，添加内容\nWORKERDIR     # 镜像的工作目录\nVOLUME         # 挂载的目录\nEXPOSE         # 暴露端口\nCMD            # 指定容器启动时需要运行的命令,只有最后一个会生效，可被替代\nENTRYPOINT     # 指定容器启动时需要运行的命令，可以追加命令\nONBUILD        # 当构建一个被继承 Dockerfile \nCOPY           #  类似ADD 将文件拷贝到镜像中\nENV            # 构建的时候设置环境变量\n```\n\n\n\n```dockerfile\nFROM ubuntu:18.04  # 指定基础镜像 如果为scratch代表从下一行开始是镜像的第一层\nRUN echo '<h1>Hello, Docker!</h1>' > /usr/share/nginx/html/index.html # RUN指令用来执行命令，每一行代表新建docker的一个layer\n#能在一个layer内执行的指令就通过&& 进行联接，并可应用shell中的换行符\\\n#在dockerfile每层都要检查，下载，展开的多余文件，以及缓存等能删除的尽量都去掉\n\nCOPY #COPY 指令将从构建上下文目录中 <源路径> 的文件/目录复制到新的一层的镜像内的 <目标路径> 位置。\nCOPY package.json /usr/src/app/ # 将当前上下文路径的json文件复制到image的指定路径下\n\nAND #丰富了COPY的功能，但是会降低构件image速度，如果不需要自动解压缩，则不推荐使用该指令\n\nCMD # ？？？？？？？？？ 还没理解\n\nENTRYPOINT # 当存在 ENTRYPOINT 后，CMD 的内容将会作为参数传给ENTRYPOINT，从而达到了我们预期的效果。\n\nENV #用来设置环境变量  ENV <key> <value> 或 ENV <key1>=<value1> <key2>=<value2>...\nENV VERSION=1.0 DEBUG=on \\\n    NAME=\"Happy ONE\"\n\nENV LD_LIBRARY_PATH=\\\n$LD_LIBRARY_PATH:\\\n$NAME/alpha\n\nARG # ARG <参数名>[=<默认值>] Dockerfile 中的 ARG 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令 docker build 中用 --build-arg <参数名>=<值> 来覆盖\n\nARG DOCKER_USERNAME=library # 注意：在FROM之前定义的ARG参数，会消失，在FROM后需要重新定义\n# ARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。但是不要因此就使用 ARG 保存密码之类的信息，因为 docker history 还是可以看到所有值的。\n\nVOLUME # 用于指定image启动时挂载到容器中的默认卷，而不是写入容器存储层\nVOLUME /data # VOLUME [\"<路径1>\", \"<路径2>\"...] 或 VOLUME <路径>\n在image启动时可替换\ndocker run -d -v mydata:/data xxxx #其中的 -v mydata:/data 就是挂载宿主机的卷到容器内\n\nEXPOSE # EXPOSE <端口1> [<端口2>...] EXPOSE 指令是声明容器运行时提供服务的端口，这只是一个声明，在容器运行时并不会因为这个声明应用就会开启这个端口的服务\n# 在 Dockerfile 中写入这样的声明有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口\n\nWORKDIR # WORKDIR <工作目录路径> 使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。\n\nUSER  # USER <用户名>[:<用户组>] 指定当前用户\nHEALTHCHECK\nONBUILD\nLEBEL\nSHELL #SHELL 指令可以指定 RUN ENTRYPOINT CMD 指令的 shell，Linux 中默认为 [\"/bin/sh\", \"-c\"]   \nDockerfile 多阶段构建\n```\n\n> 创建一个字节的centos\n\n```dockerfile\n# 1 编写dockerfile文件\nFROM centos\nMAINTAINER zuo<com>\n\nENV MYPATH /usr/local\nWORKERDIR $MYPATH\n\nRUN yum -y install vim  # 你想让他干啥\n\nEXPOSE 80\n\nCMD echo $MYPATH\nCMD echo \"----end----\"\nCMD /bin/bash\n\n# 2通过这个文件构建镜像\ndocker build -f mydockerfile-centos -t mycentos:0.1 .\n\n# 3测试运行\ndocker run -it mycentos\n\n```\n\n查看docker镜像构建历史\n\n![](https://i.loli.net/2021/08/06/VuB78orZcqWGS5v.png)\n\n> CMD 和 ENTRYPOINT 的区别\n\n![](https://i.loli.net/2021/08/06/rpFZ8xUYwROiauC.png)\n\nENTRYPOINT 是可以追加命令的\n\n### 做一个tomcat镜像\n\n- 准备镜像文件，tomcat压缩包，jdk的压缩包\n- 编写dockerfile文件，官方命名 Dockerfile，build会自动寻找这个文件，就不需要-f 指定了\n\n```dockerfile\nFROM centos\nMAINTAINER zuo<com>\n\nCOPY readme.txt /usr/local/readme.txt\nADD jdk-8ull-linux-x64.tar.gz /usr/local/   # 会自动解压\nADD apache-tomcat-9.0.22.tar.gz /usr/local/ \n\nRUN  yum -y install vim\nENV MYPATH /usr/local\nWORKDIR $MYPATH\n\nENV JAVA_HOME /user/local/jdk1.8.0_11\nENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\nENV CLASSPATH_HOME /usr/local/apache-tomcat-9.0.22\nENV CLASSPATH_BASH /usr/local/apache-tomcat-9.0.22\nENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin\n\nEXPOSE 8080\n\nCMD /usr/local/apache-tomcat-9.0.22/bin/startup.sh && tail -F /usr/local/apache-tomcat-9.0.22/bin/logs\n```\n\n构建镜像\n\n```dockerfile\ndocker build -t diytomcat .\n\ndocker run -d -p 9090:8080 --name zuotomcat -v /home/zuo/build/tomcat/test:/usr/local/apache-tomcat-9.0.22/webapps/test -v /home/zuo/build/tomcat/tomcatlogs/:/usr/local/apache-tomcat-9.0.22/logs diytomcat\n```\n\n## 发布自己的镜像\n\n```dockerfile\n(base) root@linux:/home/cpss# docker login --help\n\nUsage:  docker login [OPTIONS] [SERVER]\n\nLog in to a Docker registry.\nIf no server is specified, the default is defined by the daemon.\n\nOptions:\n  -p, --password string   Password\n      --password-stdin    Take the password from stdin\n  -u, --username string   Username\n  \n  \n(base) root@linux:/home/cpss# docker login -u zzuuoo666\nPassword: \nWARNING! Your password will be stored unencrypted in /root/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n\nLogin Succeeded\n\n\ndocker push zzuuoo666/diytomcat:1.0 # 就可以了 不加信息可能会被拒绝\n# 如果镜像上传过被拒绝，可以添加一个tag\ndocker tag ID zzuuoo666/tomcat:1.0\ndocker push zzuuoo666/tomcat:1.0\n```\n\n\n\n## 小结\n\n![](https://i.loli.net/2021/08/06/VmDSgJkyvzeHXtZ.png)\n\n\n\n\n\n## \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Docker"]},{"title":"Docker学习(二)例子练习","url":"/2021/08/01/Docker学习-二-例子练习/","content":"\n# Docker学习(二)例子练习\n\n## 部署Nginx\n\n- 搜索镜像去docker hub上\n- 下载镜像 docker pull nginx\n- docker run -d 后台运行 --name nginx01 -p 10024:80\n\n```\n(base) root@localhost:/home/cpss# docker run -d --name nginx01 -p 10024:80 nginx\n84960293d8409dc9f7e70be88027c2149ece57d7cf02dc4d71eb81fe1651fc96\n```\n\n```\n(base) root@localhost:/home/cpss# docker ps\nCONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS                                     NAMES\n84960293d840   nginx     \"/docker-entrypoint.…\"   21 seconds ago   Up 20 seconds   0.0.0.0:10024->80/tcp, :::10024->80/tcp   nginx01\n```\n\n```\n(base) root@localhost:/home/cpss# curl localhost:10024\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n</style>\n</head>\n<body>\n<h1>Welcome to nginx!</h1>\n<p>If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.</p>\n\n<p>For online documentation and support please refer to\n<a href=\"http://nginx.org/\">nginx.org</a>.<br/>\nCommercial support is available at\n<a href=\"http://nginx.com/\">nginx.com</a>.</p>\n\n<p><em>Thank you for using nginx.</em></p>\n</body>\n</html>\n```\n\n-p 暴露端口的概念\n\n![](https://i.loli.net/2021/08/04/4Bx8PzGlrD1T6vA.png)\n\n\n\n```shell\ndocker exec -it nginx01 /bin/bash 进入容器\nroot@84960293d840:/# whereis nginx\nnginx: /usr/sbin/nginx /usr/lib/nginx /etc/nginx /usr/share/nginx\n\n\n(base) root@localhost:/home/cpss# docker ps\nCONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS                                     NAMES\n84960293d840   nginx     \"/docker-entrypoint.…\"   8 minutes ago   Up 8 minutes   0.0.0.0:10024->80/tcp, :::10024->80/tcp   nginx01\n(base) root@localhost:/home/cpss# docker stop 84960293d840\n84960293d840\n```\n\n思考：每次改动nginx配置文件，都需要进入容器内部，十分麻烦\n\n可以在容器外部提供一个映射路径，达到在容器修改文件名，内部容器就可以自动修改。\n\n这个技术是  -v 数据卷技术  \n\n\n\n\n\n\n\n## 部署 ES+Kibana\n\nES暴露端口很多，也耗内存，数据一般需要放到安全目录，挂载\n\n```shell\n# --net somenetwork 网络配置\n# --rm 用完就删掉\n\n# 启动 elasticsearch 比较耗内存\ndocker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" elasticsearch:7.14.0\n\n\nCONTAINER ID   NAME            CPU %     MEM USAGE / LIMIT     MEM %     NET I/O           BLOCK I/O       PIDS\na9057d9c6e50   elasticsearch   2.54%     32.49GiB / 125.8GiB   25.83%    11.1kB / 1.94kB   100MB / 292MB   98\n\n(base) root@linux:/home/cpss# curl localhost:9200\n{\n  \"name\" : \"a9057d9c6e50\",\n  \"cluster_name\" : \"docker-cluster\",\n  \"cluster_uuid\" : \"lkLPT_ssQ2CV30B55gn4bg\",\n  \"version\" : {\n    \"number\" : \"7.14.0\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"docker\",\n    \"build_hash\" : \"dd5a0a2acaa2045ff9624f3729fc8a6f40835aa1\",\n    \"build_date\" : \"2021-07-29T20:49:32.864135063Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"8.9.0\",\n    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n\n# 增加内存限制，修改卑职文件 -e 环境修改\ndocker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e ES_JAVA_OPTS=\"-Xms64m -Xmx512m\"   elasticsearch:7.14.0\n\nCONTAINER ID   NAME             CPU %     MEM USAGE / LIMIT     MEM %     NET I/O       BLOCK I/O         PIDS\ne40105c39e81   elasticsearch1   281.21%   676.5MiB / 125.8GiB   0.53%     2.84kB / 0B   26.9MB / 1.22MB   103\n\n```\n\n![](https://i.loli.net/2021/08/05/PiIaTA4C1DsMZz3.png)\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Docker"]},{"title":"Docker学习(一)","url":"/2021/07/30/Docker学习-一/","content":"\n# Docker学习(一)\n\n文档：https://docs.docker.com/\n\nHub : https://hub.docker.com/\n\n## 路线\n\n- Docker概述\n- Docker安装\n- Docker命令\n- - 镜像命令\n  - 容器命令\n  - 操作命令\n  - ......\n- Docker镜像\n- 容器数据卷\n- DockerFile\n- Docker网络原理\n- Docker Compose\n- Docker Swarm\n- CI\\CD Jenkins\n\n## Docker概述\n\nDocker为什么会出现？\n\n> 环境配置十分麻烦，每个机器都要部署环境，很难跨平台，集群环境更浪费时间。项目能不能带上环境打包(镜像)。\n\n能干嘛？\n\n> 之前的虚拟机技术,浪费资源比较多\n\n![](https://i.loli.net/2021/07/30/cG1EzWbCX5ArsTf.png)\n\n缺点：\n\n- 资源占用多\n- 冗余步骤多\n- 启动很慢\n\n> 容器化技术\n\n不是模拟一个完整的操作系统\n\n![](https://i.loli.net/2021/07/30/MojI6CL2lqRapYW.png)\n\n不同之处：\n\n- 传统虚拟机，虚拟出一套硬件，运行一个完整的操作系统，然后在这个系统上运行安装软件\n- 容器的应用直接运行在宿主机上的内核中，容器是没有自己的内核的，没有虚拟硬件，比较轻便\n- 每个容器间是互相隔离的，每个容器内都有一共自己的文件系统，互不影响。\n\n> DevOps（开发 运维）\n\n更快速的交付和部署\n\n传统：一堆帮助文档，安装程序\n\nDocker: 打包镜像发布测试，一键运行\n\n更便捷的升级和扩容缩容，更高效的计算资源利用，测试环境都高度一致。\n\nDocker是内核级别的虚拟化，可以再一个物理机上运行很多的容器实例。\n\n## Docker 基本组成\n\n![](https://i.loli.net/2021/07/30/XMPGFCdjvi96tey.png)\n\n从左到右，依次是客户端、服务器和仓库。\n\n- 镜像（Image）：docker镜像就好比是一个模板，可以通过这个模板来创建容器服务。如：tomcat镜像--->run--->tomcat01容器。通过这个镜像可以创建多个容器，最终服务运行或者项目运行就是在容器中的\n\n- 容器（Containers）：Docker利用容器技术，可以独立运行一个或者一组应用，通过镜像来创建。启动，\n\n  停止，删除基本命令。目前可把这个容器简单理解为就是一个简易的linux系统。\n\n- 仓库（Repository）：存放镜像的地方。分为公有仓库和私有仓库，和GitHub差不多。Docker hub默认是国外的，可以配阿里云镜像加速。\n\n\n\n## Hello World\n\n![](https://i.loli.net/2021/07/30/oyDtNArhC1qmlXV.png)\n\n如何查看hello world镜像\n\n```\n(base) root@localhost:/home/cpss# docker images\nREPOSITORY      TAG       IMAGE ID       CREATED         SIZE\nhello-world     latest    d1165f221234   4 months ago    13.3kB\nstudyfang/hgn   latest    37553493935b   10 months ago   8.88GB\n```\n\ndocker默认工作路径\n\n```\n(base) root@localhost:/home/cpss# ls /var/lib/docker/\nbuildkit  containers  image  network  overlay2  plugins  runtimes  swarm  tmp  trust  volumes\n```\n\n## 镜像加速\n\n创建或修改 /etc/docker/daemon.json 文件，修改为如下形式\n\n```\n{\n  \"registry-mirrors\": [\n    \"https://registry.docker-cn.com\",\n    \"http://hub-mirror.c.163.com\",\n    \"https://docker.mirrors.ustc.edu.cn\"\n  ]\n}\n```\n\n```\n(base) root@localhost:/etc/docker# vim daemon.json\n(base) root@localhost:/etc/docker# systemctl daemon-reload\n(base) root@localhost:/etc/docker# systemctl restart docker\n(base) root@localhost:/etc/docker# systemctl status docker\n```\n\n使用docker info 查看镜像改变。\n\n![](https://i.loli.net/2021/07/30/AJiXGsvMfug9jc5.png)\n\n\n\n## run流程原理\n\n ![](https://i.loli.net/2021/07/30/6jbSgOziPhAYNkd.png)\n\ndocker是怎么工作的？\n\n> docker是一个client-server结构的系统，docker的守护进程运行在主机上，通过socket从客户端访问。\n>\n> docker server 接收到docker client的指令就会执行这个命令\n\n![](https://i.loli.net/2021/07/30/vFZk7yGAHVgfad1.png)\n\ndocker为什么比VM快？\n\n>docker有比虚拟机更少的抽象层。\n>\n>docker利用的是宿主机的内核，vm需要Guest OS\n\n<img src=\"https://i.loli.net/2021/07/30/wormWUzExPMNBsh.png\" style=\"zoom:150%;\" />\n\n所以新建一个容器的时候，docker不需要向虚拟机一样重新加载一个操作系统内核。避免引导操作，虚拟机是加载GuestOS，docker是利用宿主机的操作系统，省略了这个复杂的过程。\n\n<img src=\"https://i.loli.net/2021/07/30/ode6RsjDH1Y5yF3.png\" style=\"zoom:150%;\" />\n\n## Docker的常用命令\n\n```bash\ndocker version     # docker的版本信息\ndocker info\t\t\t\t # 显示docker的系统信息，包括镜像和容器的数量\ndocker 命令 --help  # 帮助命令\n```\n\n---\n\n### 镜像命令\n\ndocker images\n\n```sh\n(base) root@localhost:/home/cpss# docker images\nREPOSITORY      TAG       IMAGE ID       CREATED         SIZE\nhello-world     latest    d1165f221234   4 months ago    13.3kB\nstudyfang/hgn   latest    37553493935b   10 months ago   8.88GB\n```\n\n- REPOSITORY 镜像的仓库源\n\n- TAG 镜像的标签\n\n- IMAGE ID 镜像的id\n\n- CREATED 镜像的创建时间\n\n- SIZE 镜像大小\n\n  ```shell\n  Options:\n    -a, --all             Show all images (default hides intermediate images)\n    -q, --quiet           Only show image IDs\n  ```\n\ndocker search 搜索镜像\n\n```shell\n(base) root@localhost:/home/cpss# docker search hotpotqa\nNAME                                  DESCRIPTION                     STARS     OFFICIAL   AUTOMATED\nqipeng/hotpotqa-eval                                                  0                    \nstudyfang/hotpotqa                                                    0                    \nqipeng/hotpotqa-base                                                  0                    \ntuming1990/hotpotqa-docker                                            0                    \nhamishivi/hotpotqa-base               Hotpotqa with extra packages.   0                    \nqipeng/hotpotqa_submission_cuda10.2                                   0                    \ntswings2018/hotpotqa                  by deng                         0        \n```\n\ndocker pull\n\n```\n# 下载镜像 docker pull 镜像名[:tag]\n```\n\n![](https://i.loli.net/2021/07/30/r9qWyoDSOTk7QKh.png)\n\ndocker rmi 删除镜像\n\n可通过id 或者 名称来删\n\n```\ndocker rmi -f 镜像id\n```\n\n\n\n### 容器命令\n\n有了镜像才可以创建容器\n\n这里下载一个centos镜像来测试学习\n\n```\ndocker pull centos\n```\n\n新建容器并启动\n\n```shell\ndocker run [可选参数] image\n# 参数说明\n--name=\"Name\" 容器名字  tomcat01 tomcat02 用来区分容器\n-d            后台方式运行 nohup\n-it           使用交互方式运行，进入容器查看内容\n-p            指定容器的端口  ip:主机端口:容器端口 主机端口:容器端口(常用)   容器端口\n-P\t\t\t\t\t\t随机指定端口\n# 测试 启动并进入容器\n(base) root@linux:/home/cpss# docker run -it centos /bin/bash\n[root@ef41db25d696 /]# 容器内就是自己的服务器环境\n\ndocker ps # 查看正在运行的容器\ndocker ps -a # 查看曾经运行过的容器\ndocker ps -a -n=1 # 显示个数\ndocker ps -aq # 只显示编号\n```\n\n退出容器\n\n```shell\nexit # 直接退出容器并停止\nctrl +p +q # 容器不停止退出\n```\n\n删除容器\n\n删除容器\n\n```shell\ndocker rm 容器id                # 删除指定的容器 不能删除正在运行的容器 -f强制删除\ndocker rm -f $(docker ps -aq)  # 删除所有的容器\n\ndocker ps -a -q|xargs docker rm # 删除所有的容器\n```\n\n启动和停止容器的操作\n\n```sh\ndocker start 容器id\ndocker restart 容器id\ndocker stop 容器id\ndocker kill 容器\n```\n\n### 常用其他命令\n\n后台启动容器\n\n```shell\ndocker run -d centos\n# 问题 docker ps时发现centos停止了\n# 常见的坑，docker 容器使用后台运行，就必须要有一个前台进程。docker发现没有应用就会自动停止。\n# 容器启动后，发现自己没有提供服务，就会立即停止\n```\n\n查看日志\n\n```shell\n(base) root@linux:/home/cpss# docker logs --help\nOptions:\n      --details        Show extra details provided to logs\n  -f, --follow         Follow log output\n      --since string   Show logs since timestamp (e.g. 2013-01-02T13:23:37Z) or relative (e.g. 42m for 42 minutes)\n  -n, --tail string    Number of lines to show from the end of the logs (default \"all\")\n  -t, --timestamps     Show timestamps\n      --until string   Show logs before a timestamp (e.g. 2013-01-02T13:23:37Z) or relative (e.g. 42m for 42 minutes)\n\n\n# -tf 显示日志\n# --tail number 要显示日志条数\ndocker logs -tf --tail 10 f3c59b35b738\n# 容器没有日志\n# 自己写一段shell\n(base) root@linux:/home/cpss# docker run -d centos /bin/sh -c \"while true;do echo 111;sleep 1;done\"\nba0ae87cb0949d44e179f03e2bb3e25a38b394bb98b7aa0f4a1a2b9ad68ca86d\n(base) root@linux:/home/cpss# docker ps\nCONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS        PORTS     NAMES\nba0ae87cb094   centos    \"/bin/sh -c 'while t…\"   3 seconds ago   Up 1 second             determined_bouman\n(base) root@linux:/home/cpss# docker logs -tf --tail 10 ba0ae87cb094\n```\n\n查看容器中的进程信息\n\n```\ndocker top 容器id\n```\n\n查看镜像的元数据\n\n```shell\ndocker inspect 容器id\n\n(base) root@linux:/home/cpss# docker inspect ba0ae87cb094\n[\n    {\n        \"Id\": \"ba0ae87cb0949d44e179f03e2bb3e25a38b394bb98b7aa0f4a1a2b9ad68ca86d\",\n        \"Created\": \"2021-08-01T03:10:14.298411164Z\",\n        \"Path\": \"/bin/sh\",\n        \"Args\": [\n            \"-c\",\n            \"while true;do echo 111;sleep 1;done\"\n        ],\n        \"State\": {\n            \"Status\": \"exited\",\n            \"Running\": false,\n            \"Paused\": false,\n            \"Restarting\": false,\n            \"OOMKilled\": false,\n            \"Dead\": false,\n            \"Pid\": 0,\n            \"ExitCode\": 137,\n            \"Error\": \"\",\n            \"StartedAt\": \"2021-08-01T03:10:15.270494437Z\",\n            \"FinishedAt\": \"2021-08-01T03:12:01.287526932Z\"\n        },\n        \"Image\": \"sha256:300e315adb2f96afe5f0b2780b87f28ae95231fe3bdd1e16b9ba606307728f55\",\n        .....\n]\n\n```\n\n进入当前正在运行的容器\n\n```shell\n# 我们通常容器都是使用后台方式运行的，需要进入容器，修改一些配置\n# 方式1\ndocker exec -it 容器id bashShell\n# 方式2\ndocker attach 容器id \n# 区别\n# attach 正在执行的代码 进入正在执行的终端，不会启动新的进程\n# exec 进入容器后开启一个新的终端，可以在里面操作\n```\n\n从容器内拷贝文件到主机上\n\n```sh\n# 容器停止也可以拷贝，容器在数据就在\ndocker cp 容器id:容器内路径 目的的主机路径\n# 拷贝是一个手动过程，以后可以使用 -v 卷的技术 可以实现自动同步\n```\n\n### 命令小结\n\n![](https://i.loli.net/2021/08/01/ha7fdJZE2jnNIOU.png)\n\n现在学的是Images 和 Cotainer里的命令，其他的还没学\n\n![](https://i.loli.net/2021/08/01/FH1ZyqXoS3wx2Dg.png)\n\n![](https://i.loli.net/2021/08/01/YTrjW8M1c3I5sQ9.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Docker"]},{"title":"HotpotQA Submission Guide","url":"/2021/07/28/HotpotQA-Submission-Guide/","content":"\n# HotpotQA Submission Guide\n\n记录如何提交模型在HotpotQA test\n\n## codalab安装与注册\n\n先去注册 https://worksheets.codalab.org/\n\n首先安装codalab\n\n```\npip install codalab -U \n```\n\n如果ERROR: Cannot uninstall 'PyYAML'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n\n使用 pip install codalab -U --ignore-installed PyYAML\n\nCodalab wiki ： https://github.com/codalab/codalab-worksheets/wiki\n\n注册安装完成后可以再命令行登录：\n\n```bash\n$ cl work\nRequesting access at https://worksheets.codalab.org\nUsername: guest1\nPassword:\nCurrently on worksheet https://worksheets.codalab.org::home-guest1(0x39729afdca6140869a11e055e4cc0649).\n```\n\n`cl work`命令的意思就是切换工作表（worksheet），默认的工作表指向主页工作表 （`home-<username>`）。\n\n## 先一个例子提交Hotpot QA的baseline\n\ndistractor setting 是需要提交代码的，full wiki不需要。先主要攻克 distractor setting吧\n\n尝试完baseline再上传我自己的模型。\n\n> 你的分数想要在排行榜上出现，需要预留最多一个月的时间。\n\n在干扰项设置中，要求您将代码提交给Codalab，并根据隐藏的测试集对其进行评估。您应该首先确保您的代码能够正确生成dev的输出和评估结果，以便可以更容易地将您的设置转移到测试集。下面，提供一个提交baseline模型的示例。\n\n### Step 1: Preparing code and data\n\n首先将基线模型的GitHub存储库克隆到Codalab包中（在codalab上代码是公开的，想要不公开。。）\n\n在命令行中运行\n\n```bash\ncl run --request-network 'git clone https://github.com/hotpotqa/hotpot.git' -n repo\n```\n\n--request-network：需要网络环境， -n是添加别名为repo，以后可以更容易地引用它(而不是每次都使用长UUID)。\n\n注意这里git克隆的要是https的ssh的路径会失败。\n\n成功后刷新网页控制台：\n\n![](https://i.loli.net/2021/07/28/mo35I4yMLEDawFH.png)\n\n然后，我们上传对训练集进行预处理后生成的词汇映射文件。\n\n创建包含所有必要预处理文件的mappings.zip文件，即idx2char.json、idx2word.json、char2idx.json、word2idx.json、char_emb.json和word_emb.json。这个是baseline运行所需要的。\n\n作者提供了下载mappings.zip的下载地址：http://curtis.ml.cmu.edu/datasets/hotpot/mappings.zip\n\n要上传数据到Codalab CLI，只需运行\n\n```bash\ncl upload mappings.zip -n mappings\n```\n\n完全上传后，Codalab会为您解压zip文件。\n\n当然，还需要上传预先训练好的模型文件。我们已经准备好了预先训练好的文件model.pt。\n\n下载地址：http://curtis.ml.cmu.edu/datasets/hotpot/model.pt\n\n```bash\ncl upload model.pt -n model\n```\n\n如果超时就重新执行\n\n![](https://i.loli.net/2021/07/28/eDSWJn71hkAIlGr.png)\n\n### Step 2: Preparing the environment\n\n现在基本已经准备好对新的输入进行预测。我们只需要设置代码需要在其中运行的适当环境。\n\n> 要做到这一点，最简单的方法是使用Docker镜像，我们在[qipeng/hotpotqa-base](https://hub.docker.com/r/qipeng/hotpotqa-base)上提供了一个镜像，其中预装了nvidia GPU相关库和Anaconda 3。我们还安装了运行此docker映像中的基线模型所需的所有软件包，这样我们就不必在Codalab包中安装所有东西。\n\n如果确实忘记了环境中的某些内容，也可以在Codalab中轻松设置：\n\n```bash\ncl run -n download_spacy_model --request-docker-image qipeng/hotpotqa-base:gpu --request-network :repo 'cp -r repo/hotpot .; python -m spacy download en'\n```\n\n注意：由于评估期间禁止使用网络，此捆绑包仅用于演示目的。对于需要下载的软件依赖项，强烈建议下载到您准备的Docker镜像中。\n\n![](https://i.loli.net/2021/07/28/JzasC3nmrFNPI1U.png)\n\n### Step 3: Running evaluation\n\n现在，继续根据刚刚上传的模型进行预测，并评估输出。\n\n要在dev集上运行上传的基线模型的预测，我们运行以下命令：\n\n```bash\ncl run -n predict --request-docker-image qipeng/hotpotqa-base:gpu --request-gpus 1 --request-cpus 4 --request-memory 32g repo:download_spacy_model :mappings input.json:0xbdd8f3 :model 'cp -r repo/hotpot .; cp mappings/* hotpot; mkdir hotpot/model; cp model hotpot/model/model.pt; cp input.json hotpot; cd hotpot; python main.py --mode prepro --data_file input.json --para_limit 2250 --data_split dev; python main.py --mode test --data_split dev --para_limit 2250 --batch_size 24 --init_lr 0.1 --keep_prob 1.0 --sp_lambda 1.0 --save model --prediction_file pred.json; cp pred.json ../;'\n```\n\n> 让我们看看上面的命令中发生了什么。在第一部分中，我们使用-n predic命名包，并使用指定所需的资源\n>\n> --request-docker-image qipeng/hotpotqa-base:gpu\n>\n> --request-gpus 1\n>\n> --request-cpus 4\n>\n> --request-memory 32g\n>\n> 请注意，您不能在此捆绑包中使用--request-network\n>\n> 然后指定对其他包的依赖关系。repo:download_spacy_model表示将包download_spacy_model别名为repo\n>\n> :mappings指定对捆绑包mapping的依赖关系，而不使用别名。\n>\n> input.json:hotpotqa-data//dev_distractor_input_v1.0将输入json文件重命名为input.json，其中包id指向dev json文件。0xbdd8f3\n>\n> 请不要上传您自己版本的开发集文件并使用它，因为我们依赖官方的开发文件UUID来确定在评估期间用测试集替换什么(如果您使用自己的开发集文件，评估将失败)。\n\n然后，使用一系列cp命令从不同的包复制文件，并以我们的预测脚本可以处理的方式组织它们。\n\n请注意，可以将每个引用的捆绑包视为当前捆绑包中的一个目录。例如，通过cp -r repo/hotpot。我们将repo捆绑包中的hotot子目录复制到当前捆绑包的“根”目录(开始运行捆绑包中的代码时所在的目录，而不是/root！)。\n\n然后，我们调用main.py两次，第一次使用--mode prepro预处理dev集，第二次使用--mode test进行预测。如果您使用代码，则可以相应地更改此设置。\n\n请注意，如果您的代码涉及预训练的特征提取(例如，Elmo或BERT)，则应该将其合并为此处命令的一部分，而不是作为包上传，因为您事先没有访问测试集的权限。（也就是说要预处理测试集的话要在一个捆绑包里进行多次运行python文件吧）\n\n还要注意，您的模型不应该依赖于键类型和级别来进行预测，因为这些键没有出现在测试集中。\n\n之后，我们将文件pred.json复制到当前包的“根”目录。请注意，文件名必须命名为pred.json，并且该文件必须放在包的“根”目录下，评估命令才能正常工作。\n\n![](https://i.loli.net/2021/07/28/cvhExjYFiz8bka9.png)\n\n使用以下命令（将predict替换为您自己的prediction bundle的名称），确保您能够在dev set上评估您的模型而不会遇到任何问题：\n\n```bash\ncl macro hotpotqa-utils//dev-eval-distractor-v1.0 predict -n evaluate\n```\n\n![](https://i.loli.net/2021/07/28/EJ1SVlTsnHdZ9rq.png)\n\n![](https://i.loli.net/2021/07/28/jkswd5B198ZbqDi.png)\n\n\n\n### Step 4: Describe and tag your submission 描述并标记您的提交\n\n准备好后，编辑预测捆绑包的说明，以反映在排行榜上显示所需的信息：\n\n```\nModel name (Affiliation) (single model / ensemble) [paper name](paper link) (code link)\n```\n\n如果您在使用深渊翻滚时愿意，可以使用匿名Anonymous作为您的从属关系，之后可以通过编辑您的深渊翻滚捆绑包的描述来修改它。\n\n[paper名称] 和(代码链接)部分是可选的 \n\n请注意，虽然[paper名称]和(paper链接)之间没有空格，因为这会造成代码链接的歧义。以下是一些示例\n\n```\nBaseline Model (Carnegie Mellon University, Stanford University, & Universite de Montreal) (single model) [(Yang, Qi, Zhang, et al. 2018)](https://arxiv.org/pdf/1809.09600.pdf) (https://github.com/hotpotqa/hotpot)\n\nMy Awesome Model (Awesome Institute) (ensemble) [(Awesome et al., 2018)](https://arxiv.org/pdf/1812.12345.pdf)\n\nPotLuck (Culinary University) (single model) [](https://arxiv.org/pdf/1901.12345.pdf) (https://github.com/potluck/potluck)\n```\n\n第一个示例正是我们用来描述基线模型的。第二个没有代码链接，第三个没有指定论文名称。\n\n请注意，Codalab在捆绑描述中使用非ASCII字符有问题，因此请避免使用它们。\n\n要提交您的捆绑包，请使用hotpotqa-diditor-test-submit标记您的预测捆绑包(这可以在Web UI上通过选择捆绑包并修改右侧面板上的标签来完成)，然后向彭琪(pengqi@cs.stanford.edu)发送一封简短的电子邮件，其中包含您的捆绑包UUID(0x后跟32个字符)或指向您的捆绑包的链接(不是您的工作表或工作表UUID！)。\n\n请确保您的预测所依赖的所有捆绑包都是公开可读的(这是Codalab中的默认可见性)。\n\n> 重要信息：\n>\n> 1.请仅在dev集合上执行改善模型性能所需的任何模型选择或消融。不能在测试集上支持同一模型的多个提交。\n>\n> 2.请避免删除您的Submission捆绑包，即使在填写排行榜条目之后也是如此。这是您更新与您的Submission相关的信息的最佳方式，包括但不限于其名称、隶属关系、纸质链接、代码链接等。\n>\n> 3.如果你提交了多份报告(单一模型和集成模型)，请确保你的预测捆绑包有不同的名称。例如，predict-single和`predict-ensemble`。这是唯一一种我们在30天内容纳的多次提交。\n\n\n\n## 1总结\n\n- 从github下载代码\n- 上传模型和需要的数据\n- 设置环境用docker\n- run\n\n![](https://i.loli.net/2021/08/01/S6ik5vh4ImFxqCt.png)\n\nhttps://zhuanlan.zhihu.com/p/196343938","tags":["nlp"]},{"title":"Heterogeneous Graph Transformer for Graph-to-Sequence Learning","url":"/2021/07/23/Heterogeneous-Graph-Transformer-for-Graph-to-Sequence-Learning/","content":"\n# Heterogeneous Graph Transformer for Graph-to-Sequence Learning\n\nGraph2Seq学习的目的是将图结构的表示转换为单词序列，以便生成文本。\n\nAMR-to-text是从抽象意义表示(AMR)图中生成文本的任务，其中节点表示语义概念，边表示概念之间的关系。\n\n传统GNN只考虑了直接相连节点之间的关系，而忽略了远距离节点之间的间接关系。\n\nGraph2Seq的其他两个和Graph Transformer的论文\n\n- Graph transformer for graph-to-sequence learning AAAI 2020\n\n- Modeling graph structure in transformer for better AMR-to-text gen- eration  EMNLP 2019\n\n使用节点之间的最短关系路径来编码语义关系。但是，它们忽略了关系路径中节点的信息，对直接关系和间接关系没有区别地进行编码。当从直接邻居那里聚集信息时，可能会干扰信息的传播过程。\n\n作者使用Heterogeneous Graph Transformer来独立地建模原始图的各个子图中的不同关系，包括节点之间的直接关系、间接关系和多种可能的关系。\n\n\n\n## Input Graph Transformer\n\n为了缓解语料库中的数据稀疏问题，作者将进一步将字节对编码(BPE)引入Levi图。\n\n将原始节点拆分成多个子词节点。除了添加缺省连接外，我们还在子词之间添加了反向边和自循环边。\n\n如下图：\n\n![](https://i.loli.net/2021/07/23/fswuO1n4J7bptHG.png)\n\n例如，图中的单词Country被分割为co@@、un@@、try  它们之间有三种类型的边。\n\n\n\n<img src=\"https://i.loli.net/2021/07/23/LUmRx6udzNeJyAr.png\" style=\"zoom:67%;\" />\n\n该任务一般先将抽象概念图(上图a)，转换成Levi图(上图b)。将AMR图转换为扩展的Levi图，该图可以看作是一个异构图，因为它具有不同类型的边。\n\n## Heterogeneous Graph Transformer\n\n![](https://i.loli.net/2021/07/23/MvH7kQdb2KFwert.png)\n\n给定一个经过预处理的扩展Levi图，根据其异构性将扩展Levi图分成多个子图。\n\n在每个Graph Encoder中，基于其在当前子图中的相邻节点来更新不同子图中的节点表示。然后，将该节点在不同子图中的所有表示组合在一起，以获得其最终表示。\n\n### Graph Encoder\n\n与其他Graph Transformer不同的是仅使用相对位置编码来隐藏结构信息。\n\n在更新每个节点的表示时，直接屏蔽了非相邻节点的注意力。mask attention $\\alpha_{ij}\\notin N_i$  ，此外这个作者还尝试用了加性注意力这就和GAT几乎很像了。\n\n因此，给定输入序列 $x=(x_1,...,x_n)$，每个关注头中表示为 $z_i$ 的节点i的输出表示如下计算：\n$$\nz_i = \\sum_{j\\in N_i} \\alpha_{ij}(x_j W^V)\n$$\n\n### Heterogeneous Mechanism\n\n在多头机制成功的激励下，提出了异质机制。考虑到一个句子，多头注意允许模型隐含地注意到来自不同位置的不同表示子空间的信息。相应地，异构机制使得模型显式地关注不同子图中的信息，对应于图的不同表示子空间，从而增强了模型的编码能力。\n\n首先将所有的边类型组合成一个单一的边类型，从而得到一个同质连通子图。该连通子图实际上是一个包含原始图中完全连通信息的无向图。除了学习直连关系，还引入了一个完全连通子图来学习间接连接节点之间的隐含关系。\n\n每个编码层中的输出z计算如下：\n$$\n\\begin{equation}\\begin{split} \n z &= FFN(concat(z^{G^{sub}_1},...,z^{G_M^{sub}})W^O)\\\\\n z_i^{G_m^sub} &= \\sum_{j\\in N_i^{G^{sub}_m} }\\alpha_{ij}(x_jW^V), m\\in[1,M] \n    \\end{split}\\end{equation}\n$$\n$W^O\\in R^{Md_z\\times d_z}$参数矩阵 \n\n作者还采用了子层之间的残差连接、FFN以及层归一化。\n\n### Layer Aggregation\n\n编码层之间更好的信息传播可能带来更好的性能。\n\n因此，我们研究了三种不同的Layer Aggregation方法，如图3所示。\n\n![](https://i.loli.net/2021/07/23/gfL3GKQDxzCamqX.png)\n\n当更新第 $l$ 层节点的表示时，最近的方法是先聚合邻居，然后将聚合结果与来自 $(l−1)$ 层的节点表示相结合。此策略可视为不同图层之间跳过连接的一种形式。\n$$\n\\begin{equation}\\begin{split} \n z_{N_i}^{(l)} &= AGGREGATE(\\{z_j^{(l-1)}, \\forall j \\in N_i\\})\\\\\n z_i^{(l)} &= COMBINE(z_{N_i}^{(l)}, z_i^{(l-1)})\n    \\end{split}\\end{equation}\n$$\n\n\n残差连接是另一种著名的跳跃连接，它使用identity mapping作为组合函数来帮助信号传播，但这些跳跃连接不能独立自适应地调整最后一层表示的邻域大小。\n\n如果我们为$z_i^{(l)}$ skip一个层，则所有后续的单元例（如使用此表示的$z_i^{(l+j)}$) 都将隐式的使用此skip\n\n因此，为了有选择地聚合前几层的输出，我们在模型中引入了跳跃体系。\n\n在编码器的最后一层L，通过concat的方式组合前几个编码层的所有输出，以帮助模型有选择地聚合所有这些中间表示。\n$$\nz_i^{final} = Concat(z_i^{(L)},...,z_i^{(1)},x_i) W_{jump}\n$$\n$W_{jump}\\in R^{(Ld_z+d_x)\\times d_z}$\n\n此外，为了更好地改善信息传播，还可以引入稠密连通性。通过密集连接，l层中的节点不仅从第(l−1)层获取输入，而且还从所有前面的层提取信息： \n$$\nz_i^{(l)} = Concat(z_i^{(l-1)},..,z_i^{(1)},x_i) W^{(l)}_{dense}\n$$\n$W^{(l)}_{dense} \\in R^{d^{(l)}\\times d_z}, d^{(l)}=d_x+d_z\\times(l-1)$\n\n![](https://i.loli.net/2021/07/23/kqOvxdBz6YrnXj8.png)\n\n\n\n\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"Graph Transformer for Graph-to-Sequence Learning","url":"/2021/07/23/Graph-Transformer-for-Graph-to-Sequence-Learning/","content":"\n# Graph Transformer for Graph-to-Sequence Learning\n\n这篇论文应用于在基于抽象语义表示(AMR)的文本生成和基于句法的神经机器翻译，句法机器翻译并入源端语法信息可以提高翻译质量。如图给出了AMR到文本生成的示例。\n\n![](https://i.loli.net/2021/07/23/QDi75OPI64YbHgA.png)\n\n论文应用Graph Transformer，其与限制近邻之间信息交换的图神经网络不同，Graph Transformer使用显式关系编码，允许两个远距离节点之间的直接通信。它为全局图结构建模提供了一种更有效的方法。\n\n这篇论文想解决的是打破传统GNN的局部邻接特性，使用高效的全局信息。\n\n## Methed\n\n对于n个节点的图，以前的图神经网络将节点表示$v_i$计算为输入节点 $i$ 及其所有一阶邻域 $N(i)$的函数。图结构由每个节点表示的感受野隐式反映。然而，这种本地通信设计对于远程信息交换可能是低效的。\n\n所以引入Graph Transformer，它提供了一种截然不同的范例，可以实现关系感知的全球通信。\n\n![](https://i.loli.net/2021/07/23/FtimYg4NB5MTkoS.png)\n\n作者提出的是关系增强的全局注意力机制，和Graphromer一样任何节点对之间的关系被描述为它们之间的最短关系路径。\n\n### Graph Encoder\n\n责将输入图形转换为一组相应的节点嵌入。核心问题是如何在允许全连通通信的同时保持图的拓扑结构。\n\n 作者的想法是将两个节点之间的显式关系表示融入到它们的表示学习中。在标准的多头注意中，元素 $x_i$ 和元素 $x_j$之间的注意分数简单地分别是它们的查询向量和键向量的点积：\n$$\ns_{ij} = f(x_i,x_j) =x_iW^T_qW_kx_j\n$$\n假设我们已经学习了节点i和节点j之间的关系 $r_{ij}$ 的矢量表示，我们将其称为关系编码。\n$$\n[r_{i\\to j};r_{j\\to i}] = W_r r_{ij}\n$$\n$r_{i\\to j};r_{j\\to i}$ 为正向和反向关系编码。\n\n如果把正反两个关系编码加到节点embedding中，注意力分数计算可以为：\n$$\n\\begin{equation}\\begin{split} \ns_{ij} &= g(x_i, x_j, r_{ij})\\\\\n\t\t\t& = (x_i + r_{i\\to j})W_q^TW_k(x_j + r_{j\\to i})\\\\\n\t\t\t&= \\underbrace{x_iW_q^TW_kx_j}_{(a)} +  \\underbrace{x_iW_q^TW_kr_{j\\to i}}_{(b)} \\\\\n\t\t\t&+ \\underbrace{r_{i\\to j}W_q^TW_kx_j}_{(c)} + \\underbrace{r_{i\\to j}W_q^TW_kr_{j\\to i}}_{(d)}\n \n    \\end{split}\\end{equation}\n$$\n直观上，等式项意义:\n\n- (a) 捕获纯粹基于内容的content-based addressing,，这是普通注意力机制中的原始term。\n- (b) 依赖于源节点的关系偏置。\n- (c) 依赖于目标节点的关系偏置。\n- (d) 对通用的关系偏差进行编码。\n\n在这里作者使用的是节点间的最短路径来表示关系。\n\n#### Relation Encoder\n\n从概念上讲，关系编码为模型提供了关于应该如何收集和分发信息的全局指导，即在哪里关注。\n\n对于NLP中的大多数图形结构，边标签传达了相邻节点之间的直接关系(例如，概念到概念所扮演的语义角色，以及两个单词之间的依存关系)。\n\n作者将这种单跳关系定义扩展到多跳关系推理中，以刻画任意两个节点之间的关系。\n\n例如第一个图中 want-01 到 girl的最短路径概念为，$\\text{want-01} \\to^{ARG1} \\text{believe-01}\\to^{ARG0} girl$ 传达girl是wanted的目标。\n\n直观地说，两个节点之间的最短路径给出了它们之间最密切且可以说是最重要的关系\n\n作者使用GRU将关系序列转换为分布表示。$i$ 到 $j$ 的最短路径关系为: $sp_{i\\to j}=  [e(i,k_1), e(k1,k2),...,e(k_n,j)]$\n\n其中$e(,)$是边标签，$k_{1:n}$ 是中继节点。\n$$\n\\begin{equation}\\begin{split} \n \\overrightarrow s_t &= GRU_f(\\overrightarrow s_{t-1}, sp_t)\\\\\n \\overleftarrow s_t &= GRU_f(\\overleftarrow s_{t+1},sp_t)\n    \\end{split}\\end{equation}\n$$\nconcat最终关系表达为$r_{ij} = [\\overrightarrow s_n; \\overleftarrow s_0]$\n\n#### Bidirectionality\n\n因为应用任务常是DAG，作者给做成理论双向交互的。反转边连接与原始边相同的两个节点，但方向不同，并使用反转标签。\n\n此外作者还在每个图中引入一个额外的全局节点和自环边，该节点具有特殊标签GLOBAL与其他所有节点都有一条直接边。全局节点的最终表示$x_{global}$用作整个图表示。\n\n#### Absolute Position\n\n除了成对关系之外，一些绝对位置信息也是有益的。例如，AMR图的根作为整体焦点的粗略表示，使得到根节点的最小距离部分地反映了相应概念在整句语义中的重要性。 \n\n位置嵌入添加到编码器堆栈底部的输入embedding中。例如，第一个中的Want-01是AMR图的根节点，因此其索引应该为0。也将全局节点的索引表示为0。\n\n\n\n### Sequence Decoder\n\n和普通Transformer Decoder没什么大的区别\n\n特殊的一点，使用全局图形表示$x_{global}$来初始化每个时间步的隐藏状态。\n\n然后，通过在编码器的输出上交错多轮关注来更新每个时间步骤t处的隐藏状态 $h_t$\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"Rethinking Graph Transformers with Spectral Attention","url":"/2021/07/21/Rethinking-Graph-Transformers-with-Spectral-Attention/","content":"\n# Rethinking Graph Transformers with Spectral Attention\n\n提出了*Spectral Attention Network*(SAN)，它使用学习的位置编码(LPE)，可以利用全拉普拉斯频谱来学习给定图中每个节点的位置。通过利用拉普拉斯的全谱，模型在理论上具有强大的区分图形的能力，并且可以更好地从它们的共振中检测出相似的子结构。\n\n在这项工作中，作者还是研究如何将Transformer体系结构应用于图形表示学习。开发了强大的可学习的位置编码方法，这些方法植根于谱图理论。 谱注意力网络(SAN)架构解决了先前图形转换器工作中的关键理论限制，并且明显超过了标准消息传递GNN的表达能力。\n\nSAN方法的优势对比：\n\n![](https://i.loli.net/2021/07/26/x7U849s2RSCdQau.png)\n\n- 保持注意中的局部结构\n- 使用边特征\n- 连接非相邻节点\n- 使用基于特征向量的PE进行注意\n- 使用具有结构信息的PE\n- 考虑特征值的排序\n- 特征向量的范数不变量\n- 考虑特征值的谱               (SAN独有)\n- 考虑特征向量的变量#       (SAN独有)\n- 意识到特征值的多重性    (SAN独有)\n- 对特征向量的符号不变\n\n也就是说SAN结合了稀疏和稠密GT的特性，并且还考虑了特征值的谱、征向量的变量#、意识到特征值的多重性。\n\n## 基于特征函数的绝对和相对位置编码\n\n因为不存在对节点进行排序或定义轴的规范方法。在本节中，作者将研究如何使用拉普拉斯的特征函数来定义图形中的绝对和相对PE，测量节点之间的物理相互作用，并使特定的子结构能够“听到”-类似于鼓的声音，揭示其结构。\n\n### 特征向量等价于图上的正弦函数\n\n在Transformer架构中，一个基本方面是使用正弦和余弦函数作为序列的PE。然而，对于任意图形，sinusoids正弦不能被清楚地定义，因为沿轴的位置没有清晰的概念。取而代之的是，它们的等价性由图Laplacian L的特征向量 $\\Phi$ 给出。\n\n事实上，在欧几里得空间中，拉普拉斯算子对应于梯度的散度，其特征函数是正弦/余弦函数，平方频率对应于特征值(我们有时从这里起将这两个概念互换)。因此，在图域中，图的Laplacian的特征向量与正弦函数自然等价，并且这一直觉被用于最近的多项工作中，这些工作将特征向量用作GNN(Benchmarking graph neural networks)、定向流(Directional graph networks. ICML2021)和GT的PE。\n\n在与正弦函数等价的情况下，我们很自然地发现，$\\mathcal{F}[f]$的傅里叶变换函数应用于图$\\mathcal{F}[f](\\lambda_i) = \\langle f, \\phi_i \\rangle$，其中特征值被认为是该图的傅立叶域中的一个位置。因此，最好将特征向量视为位于特征值轴上的向量，而不是矩阵的组成部分，如图所示。\n\n![](https://i.loli.net/2021/07/26/Bg3QbcITZuMRLjs.png)\n\n### 关于相对位置，特征函数告诉我们什么？(物理应用)\n\n除了模拟正弦函数外，拉普拉斯函数的特征向量还包含有关系统物理的重要信息，可以揭示距离度量。因为拉普拉斯运算符是物理学中的一个基本运算符，在麦克斯韦方程和热扩散中都有显著的应用。\n\n在电磁理论中，拉普拉斯的(伪)逆，在数学上称为拉普拉斯的格林函数，表示电荷的静电势。\n\n在图中，相同的概念使用拉普拉斯G的伪逆，并且可以通过其特征函数来计算。\n\n如下公式，$G(j_1,j_2)$ 是节点$j_1,j_2$ 之间的电势。 $\\hat \\phi_i,\\hat \\lambda_i$ 为对称Laplacian$D^{\\frac{-1}{2}}LD^{\\frac{-1}{2}}$第 $i$个特征值和特征向量。\n$$\nG(j_1,j_2) = d_{j_1}^{\\frac{1}{2}}d_{j_2}^{\\frac{-1}{2}}\\sum_{i>0}\\frac{(\\hat \\phi_{i,j_1},\\hat \\phi_{i,j_2})^2}{\\hat \\lambda_i}\n$$\n此外，傅立叶给出的热方程的原始解依赖于被称为傅立叶级数的正弦/余弦的和。由于拉普拉斯函数的特征向量是这些函数在图中的近似，我们找到了近似的解。热核与随机游走相关，我们利用两个热核之间的相互作用在下面方程中定义节点$j_1,j_2$之间的扩散距离$d_D$。类似的二次谐波距离$d_B$是一种不同的距离测量方法。这里我们使用正则拉普拉斯L的特征函数：\n$$\n\\begin{equation}\\begin{split} \n d^2_D(j_1,j_2) &= \\sum_{k>0} e^{-2t\\lambda_i}(\\phi_{i,j_1} - \\phi_{i,j_2})^2\\\\ d_B^2(j_1,j_2)&=\\sum_{i>0}\\frac{(\\phi_{i,j_1} - \\phi_{i,j_2})^2}{\\lambda_i^2}\n    \\end{split}\\end{equation}\n$$\n这个方程，首先强调了在提供有关图中相对位置的信息时将特征向量与其对应的特征值配对的重要性。其次，我们注意到特征向量的乘积与静电相互作用成正比，而减法与扩散距离和重谐距离成正比。最后，所有3个方程都有一个一致的模式：在确定节点之间的距离时，频率/特征值越小，权重越大。\n\n\n\n### 听图的形状及其子结构\n\n特征值的另一个众所周知的性质是它们如何用于区分不同的图结构和子结构，因为它们可以解释为图的共振频率。\n\n这就引出了一个著名的问题，即我们是否能从鼓的特征值中听到鼓的形状，同样的问题也适用于几何物体和3D分子。\n\n通过将特征函数用于部分功能对应、算法理解几何和样式对应。分子图的特征向量的例子如图所示。\n\n![](https://i.loli.net/2021/07/26/o4M9SwJnLWTKCsj.png)\n\n\n\n##  Laplace Eigenfunctions的规范\n\n在欧几里德空间和序列中，使用正弦波作为PE是很简单的：我们可以简单地选择一组频率，计算正弦波，并将它们添加或拼接到输入嵌入，就像在原始变压器中所做的那样。然而，在任意图中，复制这些步骤并不那么简单，因为每个图都有一组唯一的特征函数。\n\n在接下来的部分中，将介绍谱图理论中的关键原则，在为图构造PE时要考虑这些原则，这些原则大部分被以前的方法忽略了。包括正则化，特征值及其多样性的重要性，特征向量的数量是可变的，以及符号模糊性。作者的LPE架构旨在解决这些问题。\n\n\n\n**Normalization** 给定拉普拉斯的特征值，就有一个维数大于1的相关特征空间。为了在模型中利用这些信息，必须选择一个单一的特征向量。在我们的工作中，我们使用L2正则化，因为它与格林公式也就是上面的第一个公式的定义是兼容的。因此，我们将始终选择特征向量$\\phi$，使$⟨\\phi，\\phi⟩=1$。\n\n**Eigenvalues** 另一个基本方面是与每个特征向量相关联的特征值提供了有价值的信息。基于特征向量的特征值的排序在序列中起作用，因为频率是预先确定的。然而，这一假设在图中不起作用，因为它们的谱中的特征值可以改变。例如，在上图中，我们观察到排序如何忽略两个分子在 $λ = 1$ 以不同方式共振的事实。\n\n**Multiplicities** 选择特征函数的另一个重要问题是特征值高度多样的可能性，即当一个特征值多次作为特征多项式的根出现时。在这种情况下，相关联的特征空间可以具有2维或更多维，因为我们可以从具有相同特征值的任何特征向量的线性组合中生成有效的特征向量。这进一步复杂化了选择用于算法计算的特征向量的问题，并突出了拥有能够处理这种歧义的模型的重要性。\n\n**Variable number of eigenvectors** 图 $G_i$ 至多可以有 $N_i$ 个线性独立的特征向量，其中 $N_i$ 是它的节点数。最重要的是，$N_i$ 可以在数据集中的所有的 $G_i$ 都有所不同。GT选择了固定数目的k个特征向量给每个图，其中 $k≤N_i$，$∀i$。当数据集中最小的图的节点比最大的图少得多时，这就产生了一个主要的瓶颈，因为很小比例的特征向量将用于大型图。这不可避免地造成信息丢失，并激发了对构建k维固定PE的模型的需求，其中k不依赖于图中的特征向量的数目。\n\n**Sign invariance** 如前所述，特征向量存在符号歧义。由于φ的符号与它的正则化无关，在选择图的k个特征向量时，我们只剩下2k个可能的符号组合。以前的工作已经提出通过随机反转特征向量的符号来进行数据增强，虽然当k较小时可以工作，但是对于较大的k会变得困难。\n\n\n\n## Model Architecture\n\n我们提出了一种体系结构，它可以使用特征函数作为PE，同时解决上述规范中提出的问题。我们的 *Spectral Attention Network* (SAN)模型输入图的特征函数，并将其投影到固定大小的学习位置编码(LPE)中。LPE允许网络使用每个图的整个拉普拉斯频谱，学习频率如何交互，并决定哪些频率对给定任务最重要。\n\n![](https://i.loli.net/2021/07/26/8uAUjDZ5EdNQpFs.png)\n\n如图分为两步学习过程。\n\n图中的(c-d-e)描述了第一步，在每个节点的特征函数上应用一个Transformer，为每个图生成一个LPE矩阵。\n\n然后将LPE连接到节点嵌入图中(g-h)，然后将其传递给Graph Trabsformer (i)。如果任务涉及图分类或回归，则最终节点嵌入随后将传递到最终池化层。\n\n### LPE Transformer Over Nodes\n\n使用拉普拉斯编码作为节点特征在有关该主题的文献中是普遍存在的。LPE的想法受到上面第二个图的启发，其中特征向量 $\\phi$ 被表示为一个非均匀序列，特征值λ是频率轴上的位置。使用此表示法，Transformers是处理它们并生成固定大小PE的自然选择。\n\nLPE结构如图所示：\n\n![](https://i.loli.net/2021/07/26/yxhrKLAwXcvBTgD.png)\n\n学习位置编码(LPE)结构，模型通过考虑m个特征值和特征向量来学习图的拉普拉斯谱，其中允许 $m≤N$，其中N表示节点数。\n\n首先，我们通过将m个最低特征值与其关联的特征向量连接起来，为每个节点$j$ 创建一个大小为 $2×m$ 的嵌入矩阵。这里，m是要计算的特征向量的最大数目的超参数，并且类似于标准变压器的可变长度序列。对于 $m>N$ 的图，只需添加掩码填充。注意，要捕获所有图的整个谱，只需选择m，使其等于图在数据集中具有的最大节点数。然后在大小为2的维度上应用线性层以生成大小为k的新嵌入。然后，Transformer编码器对长度为m且隐藏维数为k的序列计算self-attention。最后，sum pooling将该序列简化为固定的k维节点嵌入。\n\n通过将特征值与归一化特征向量连接起来，该模型直接处理前三个规范。即将特征向量归一化，将特征向量与其特征值配对，并将特征向量的个数作为变量。此外，该模型意识到了多重性，并且有可能线性组合或忽略一些重复的特征值。\n\n然而，这种方法仍然没有解决预先计算的特征向量的符号是任意的限制。为了解决这个问题，我们像以前的工作[13，12]所采用的那样，在训练过程中随机反转预先计算的特征向量的符号，以促进符号歧义的不变性。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["GNN"]},{"title":"Heterogeneous Graph Transformer","url":"/2021/07/09/Heterogeneous-Graph-Transformer/","content":"\n# Heterogeneous Graph Transformer\n\n提出了一种用于Web规模异构图建模的异构图Transformer(HGT)体系结构。\n\n其一是设计了节点和边类型相关的参数来表征对每条边的异构attention，使得HGT能够维护不同类型的节点和边的专用表示。\n\n其二为了处理Web规模的图形数据，我们设计了异构小批量图形采样算法HG Samples，以实现高效和可扩展的训练\n\n作者使用的是OAG学术图，其存在的异构关系如下图：\n\n![](https://z3.ax1x.com/2021/07/09/RxMqUA.png)\n\n## 要解决的问题\n\nGNN以前可以处理异质图是基于元路径的方法有PathSim, methpath2vec等。GNN火起来以后也出现了好多处理异质图的工作。\n\n作者认为面临着几个问题：首先，它们大多涉及为每种类型的异构图设计元路径，需要特定的领域知识；其次，它们要么简单地假设不同类型的节点/边共享相同的特征和表示空间，要么只针对节点类型或边类型保持不同的非共享权重，使得它们不足以捕捉异构图的属性；最后，其固有的设计和实现使得它们无法对Web规模的异构图进行建模。\n\n作者的目标是：保持节点和边类型的依赖表示，避免定制的元路径，并且能够扩展到Web规模的异构图。\n\n### 做法：\n\n#### 异质处理\n\n为了处理图的异构性，引入了节点和边型依赖的注意机制。HGT中的异构相互关注度不是参数化的，而是通过基于其元关系三元组分解每条边e=(s，t)来定义的，即 <s的节点类型、s&t之间的e的边类型、t的节点类型>。上图说明了异质学术图的元关系。使用这些元关系来参数化权重矩阵，以计算每条边上的关注度。因此，允许不同类型的节点和边保持其特定的表示空间。\n\n同时，不同类型的连接节点仍然可以交互、传递和汇聚消息，而不受其分布差距的限制。由于其体系结构的本质，HGT可以通过跨层的消息传递来融合来自不同类型的高阶邻居的信息，这可以被认为是“软”元路径。也就是说，即使HGT只将其一跳边作为输入，而不需要人工设计元路径，所提出的注意机制也可以自动和隐式地学习和提取对不同下游任务重要的“元路径”。\n\n#### 异质子图采样法\n\n为了对Web规模的异构图进行建模，设计了第一个用于小批量GNN训练的异构子图采样算法HG Samples。它的主要思想是对不同类型节点比例相近的异构子图进行采样。此外，它还被设计成保持采样子图的稠密性，以最大限度地减少信息损失。有了HG-sample，所有的GNN模型都可以在任意大小的异构图上进行训练和推断。\n\n## 方法\n\n思想：利用异构图的元关系来参数化异构相互关注、消息传递和传播步骤的权重矩阵。\n\n有向图 $G = (V,E,A,R)$ ,  节点 $v \\in V$，每个边$e \\in E$ 。他们的类型映射函数为 $\\tau(v):V \\to A$ 、$\\phi(e):E\\to R$ \n\n### 元关系\n\n对于一个边 $e = (s,t)$ ，元关系定义为 $<\\tau(s),\\phi(e),\\tau(t)>$ 。$\\phi(e)^{-1}$ 是关系的反向表达。\n\n### HGT架构\n\n![](https://i.loli.net/2021/07/09/8CepfwW4dgEzsjc.png)\n\n主要的三个组件：Heterogeneous Mutual Attention、Heterogeneous Message Passin和特定于Target-Specific Aggregation。\n\n定义第$l$ 层的输出为 $H^l$, 也是第$l+1$层的输入。\n\n#### Heterogeneous Mutual Attention\n\n首先计算源节点 s 到目标节点 t 之间的 Mutual Attention。\n\n针对问题是：通过使用一个权重矩阵W来假设s和t具有相同的特征分布。这种假设对于异构图通常是不正确的，因为在异构图中，每种类型的节点都可以有自己的特征分布。\n\n给出目标节点 t ，以及它的邻居节点 $s \\in N(t)$ 它们可能属于不同的分布。通过元关系三元组 $<\\tau(s),\\phi(e),\\tau(t)>$, 计算mutual attention。\n\n将目标节点t映射为query向量，将源节点s映射为key向量，并计算它们的点积作为关注度。\n\n与Vanilla Transformer相比关键区别在于，Vanilla Transformer对所有单词使用一组投影映射，HGT的每个元关系都应该有一组不同的投影权重。\n\n为了在保持不同关系特性的同时最大限度地实现参数共享，提出将交互算子的权重矩阵参数化为源节点投影、边投影和目标节点投影。\n\n对每个边$e=(s,t)$进行h heads attention :\n$$\n\\begin{equation}\\begin{split} \n Attention_{HGT}(s,e,t) &= Softmax_{\\forall s\\in N(t)}(  \\|_{i\\in[1,h]} \\text{ATT-head}^i(s,e,t) )\\\\\n\\text {ATT-head}^i(s,e,t) &=(K^i(s) W^{ATT}_{\\phi(e)}Q^i(t)^T) \\cdot \\frac{\\mu<\\tau(s),\\phi(e),\\tau(t)>}{\\sqrt d} \\\\\nK^i(s) &= \\text{K-Linear}_{\\tau(s)}^i(H^{(l-1)}[s])\\\\\nQ^i(t) &= Q-Linear_{\\tau(t)}^i (H^{(l-1)}[t])\n    \\end{split}\\end{equation}\n$$\n$\\text{ATT-head^i(s,e,t)}$ 是第 $i$ 个注意力头。$\\text{K-Linear}^i_{\\tau{(s)}}:R^d \\to R^{\\frac{d}{h}}$ 编码了源接地那s类型$\\tau(s)$ 意味着每每个类型节点有独一无二的线性映射最大限度地对分布差异进行建模。\n\n然后计算Query和Key的相似度， 异构图的一个独特特征是在节点类型对之间可能存在不同的边类型(关系)，例如 $τ(S)$和$τ(T)$ 。因此，与直接计算查询和键向量之间的点积的Vanilla Transformer不同，我们为每个边类型$\\phi(e)$保留了一个不同的基于边的矩阵 $W^{ATT}_{\\phi(e)}\\in R^{\\frac{d}{h}\\times \\frac{d}{h}}$ 。这样，即使在相同的节点类型对之间，该模型也可以捕获不同的语义关系。\n\n此外，由于不是所有的关系对目标节点的贡献相等，我们增加了一个先验张量 $\\mu\\in R^{|A|\\times |R|\\times |A|}$ 表示每个元关系三元组的一般意义，作为对注意力的自适应缩放。\n\n最后，我们将注意力集中在一起，以获得每个节点对的attention向量。\n\n#### Heterogeneous Message Passing\n\n在计算Mutual Attention的同时，将信息从源节点传递到目标节点。\n\n与attention过程类似，希望在消息传递过程中加入边的元关系，以缓解不同类型节点和边的分布差异。\n\n对于一对节点 $e=(s,t)$，我们通过以下公式计算其多头 Message:\n$$\n\\begin{equation}\\begin{split} \n Message_{HGT(s,e,t)} &= \\|_{i\\in [1,h]} \\text{MSG-head}^i(s,e,t)\\\\\n \\text{MSG-head}^i(s,e,t) &= \\text{M-Linear}_{\\tau(s)}^i(H^{(l-1)}[s])W^{MSG}_{\\phi(e)}\n    \\end{split}\\end{equation}\n$$\n\n#### Target-Specific Aggregation\n\n$$\n\\hat H^{(l)}[t] =  \\oplus_{\\forall s\\in N(t)} (Attention_{HGT}(s,e,t) \\cdot Message_{HGT}(s,e,t))\n$$\n\n这将来自不同特征分布的所有近邻(源节点)的信息聚集到目标节点 t。\n\n最后一步是将目标节点t的向量映射回按其节点类型τ(T)索引的特定于类型的分布。为此，我们将线性投影A-线性τ(T)应用于更新后的向量H􏰅(L)[t]，随后是非线性激活和剩余连接[5]，如下所示：\n\n\n\n### HGSampling\n\n![](https://i.loli.net/2021/07/09/5yz2s4vhCZPINX9.png)\n\n\n\n\n\n\n\n","tags":["GNN"]},{"title":"Transformer的辅助","url":"/2021/07/04/Transformer的辅助/","content":"\n# Transformer的辅助\n\n转载：https://zhuanlan.zhihu.com/p/149634836\n\n## [为什么Transformer需要进行Multi-head Attention](https://www.zhihu.com/question/341222779/answer/814111138)\n\nAttention is all you need论文中讲模型分为多个头，形成多个子空间，每个头关注不同方面的信息。\n\n如果Multi-Head作用是关注句子的不同方面，那么不同的head就应该关注不同的Token；当然也有可能是关注的pattern相同，但是关注的内容不同，即V不同。\n\n但是大量的paper表明，transformer或Bert的特定层有独特的功能，底层更偏向于关注语法；顶层更偏向于关注语义。\n\n所以对Multi-head而言，同一层Transformer_block关注的方面应该整体是一致的。不同的head关注点也是一样。但是可视化同一层的head后，发现总有那么一两个头独一无二的，和其他头的关注不一样。\n\n众多研究表明Multi-Head其实不是必须的，去掉一些头效果依然有不错的效果（而且效果下降可能是因为参数量下降），这是因为在头足够的情况下，这些头已经能够有关注位置信息、关注语法信息、关注罕见词的能力了，再多一些头，无非是一种enhance或noise而已。\n\n### 相关paper \n\n- A Multiscale Visualization of Attention in the Transformer Model [https://arxiv.org/pdf/1906.05714.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1906.05714.pdf)\n- What Does BERT Look At? An Analysis of BERT’s Attention [https://arxiv.org/pdf/1906.04341v1.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1906.04341v1.pdf)\n- Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention [https://arxiv.org/pdf/1908.11365.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1908.11365.pdf)\n- Adaptively Sparse Transformers[https://arxiv.org/pdf/1909.00015.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1909.00015.pdf)\n- Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned [https://arxiv.org/pdf/1905.0941](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1905.09418.pdf)\n\n\n\n## [Transformer为什么Q和K使用不同的权重矩阵生成，为什么不能使用同一个值进行自身的点乘？（注意和第一个问题的区别）](https://www.zhihu.com/question/319339652)\n\n既然K和Q差不多（唯一区别是W_k和W_Q权值不同），直接拿K自己点乘就行了，何必再创建一个Q？创建了还要花内存去保存，浪费资源，还得更新参数。\n\n**为什么要计算Q和K的点乘？**\n\n我们知道K和Q的点乘是为了得到一个attention score 矩阵，用来对V进行提纯。K和Q使用了不同的W_k, W_Q来计算，可以理解为是在不同空间上的投影。正因为有了这种不同空间的投影，增加了表达能力，这样计算得到的attention score矩阵的泛化能力更高。这里解释下我理解的泛化能力，因为K和Q使用了不同的W_k, W_Q来计算，得到的也是两个完全不同的矩阵，所以表达能力更强。\n\n 但是如果不用Q，直接拿K和K点乘的话，你会发现attention score 矩阵是一个对称矩阵。因为是同样一个矩阵，都投影到了同样一个空间，所以泛化能力很差。这样的矩阵导致对V进行提纯的时候，效果也不会好。\n\n## [为什么在进行softmax之前需要对注意进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解](https://www.zhihu.com/question/339723385/)\n\n**（**论文中解释是：向量的点积结果会很大，将softmax函数push到梯度很小的区域，scaled会缓解这种现象。怎么理解将sotfmax函数push到梯度很小区域？还有为什么scaled是维度的根号，不是其他的数？**）**\n\n以数组为例，2个长度是len，均值是0，方差是1的数组点积会生成长度是len，均值是0，方差是len的数组。而方差变大会导致softmax的输入推向正无穷或负无穷，这时的梯度会无限趋近于0，不利于训练的收敛。因此除以len的开方，可以是数组的方差重新回归到1，有利于训练的收敛。\n\n@LinT成功人士（） 以下感谢分享\n\n**1. 为什么比较大的输入会使得softmax的梯度变得很小？**\n\n对于一个输入向量$x\\in R^d$, softmax函数将其映射/归一化到一个分布$\\hat y\\in R^d$。在这个过程中softmax先用一个自然底数$e$ 将输入中的元素检举先“拉大”，然后归一化为一个分布。假设某个输入x 中最大的元素下班是k，如果输入的数量级变大(每个元素都很大)，那么$\\hat y_k$会非常接近1。\n\n举个例子$x$ 的数量级对输入最大元素对应的预测概率$\\hat y_k$的影响。假定输入 $x = [a,a,2a]^T$, 我们看看不同量级的$a$ 产生的$\\hat y_3$有什么区别。\n\n- ![[公式]](https://www.zhihu.com/equation?tex=a%3D1+) 时， ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D_3%3D0.5761168847658291) ;\n- ![[公式]](https://www.zhihu.com/equation?tex=a%3D10++) 时， ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D_3%3D0.999909208384341);\n- ![[公式]](https://www.zhihu.com/equation?tex=a%3D100++) 时， ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D_3%5Capprox+1.0) (计算机精度限制)。\n\n可以看出，第三个元素里的值随着数量级的增加而接近于1。而我们知道softmax层后的每个元素之后为1。也就是说，向量里最大值索引的元素基本上占据所有的概率了。为了理解方便，可视化如下。可以看出，随着向量里最大元素的数量级的增大，它就越近于1，相当于整个输出变成了one-hot编码了[y = [0,0,1\\]](#)\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nx  = np.linspace(0,100,100)\nf = lambda x: np.exp(x * 2) / (np.exp(x) + np.exp(x) + np.exp(2 * x))\ny = [f(temp) for temp in x]\nplt.plot(x,y)\n```\n\n![](https://i.loli.net/2021/07/05/25qZGxyD17w9UgM.png)\n\n![](https://i.loli.net/2021/07/05/NZpiYSf8y7vVWEL.png)\n\n**2. 维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩？**\n\n![v2-493286fbea075e160bf3bac214d2ac60_720w](https://pic1.zhimg.com/80/v2-493286fbea075e160bf3bac214d2ac60_720w.jpg)\n\n![](https://i.loli.net/2021/07/05/5BnlfzVdOjh14EA.png)\n\n----\n\n## **在计算注意力分数的时候如何对padding做mask操作？**\n\nmask是将一些不要用的值掩盖掉，使其不产生作用。有两种mask，第一种是**padding mask**，在所有scaled dot-product attention都用到；第二种是**sequence mask，**在decoder的self-attention里面用到。\n\n**padding mask：**因为一个批量输入中，所有序列的长度使不同的。为了符合模型的输入方式，会用padding的方式来填充（比如填0），使所有序列的长度一致。但填充部分是没有意义的，所以在计算注意力的时候，不需要也不应该有注意力分配到这些填充的值上面。所以解决方式就是在填充的位置赋予一个**很小的负值/负无穷（-np.inf）**的值，**经过softmax后的得分为0**，即没有注意力分配到这个上面。\n\n```python\ndef padding_mask(seq_k, seq_q):\n  # shape(seq_k)=(B,L_k) , shape(seq_q) = (B, L_q)\n  # 因为要计算seq_k和seq_q的相似程度，来表示注意力的得分\n  # padding mask要作用在 QK^T上，所以padding mask是跟seq_k和seq_q序列长度相关的矩阵\n  # shape(padding mask) = (B, L_q, L_k)\n  len_q = seq_q.size(1)\n  # PAD is 0 这里要计算seq_k序列中，padding为0的地方，并将相应位置变为True，方便后续处理\n  pad_mask = seq_k.eq(0)\n  # 将每个seq_k序列扩展len_q次，shape[B, L_q, L_k]\n  pad_mask = pad_mask.unsqueeze(1).expand(-1, len_q, -1)\n  return pad_mask\n  \n```\n\n以上方法为大部分padding mask的计算形式，但实际上，这里做了seq_q全部有效的假设（没有padding），并不够精确 。自己的看法：上述代码expand操作，只是将seq_k中padding的部分重复了L_q次，并没有注意到，seq_q也有padding的部分。即在一个(L_q,L_k)矩阵中，只有最后几列需要掩码，实际矩阵的最后几行也需要掩码。（以后上图更形象）\n\n**sequence mask：**在decoder部分，因为不能见到下文信息（防止泄漏），所以用mask的方式掩盖掉当前时刻t及之后的下文信息。具体，可产生一个对角线为0的上三角矩阵，将其作用到每个decoder的输入列上。代码如下：\n\n```python\ndef sequence_mask(seq):\n    batch_size, seq_len = seq.size()\n    mask = torch.triu(torch.ones((seq_len, seq_len), dtype=torch.uint8),\n                    diagonal=1)\n    mask = mask.unsqueeze(0).expand(batch_size, -1, -1)  # [B, L, L]\n    # 三角矩阵中，为1的部分是需要被掩码掉的\n    return mask\n```\n\ndecoder-block有两个multi-head attention，下面的multi-head attention是目标输入的self-attention，需要用到1.padding mask：去除padding位置的影响；2.sequence mask：去掉下文穿越的影响。上面的multi-head attention只需要padding mask，因为下面的多头注意力已经磨平了下文信息。当**encoder和decoder的输入序列长度一样时**，可以通过padding mask+sequence mask作为scaled dot-product attention的attn_mask来实现。\n\n其他情况的attn_mask（代码中的表达）等于padding mask\n\n\n\n## [为什么在进行多头关注的时候需要对每个head进行切割？](https://www.zhihu.com/question/350369171)\n\n@何妨吟啸且徐行 感谢回答\n\nTransformer的多头注意力看上去是借鉴了CNN中同一卷积层内使用多个卷积核的思想，原文中使用了 8 个“scaled dot-product attention”，在同一“multi-head attention”层中，输入均为“KQV”，**同时**进行注意力的计算，彼此之前**参数不共享**，最终将结果**拼接**起来，这样可以允许模型在**不同的表示子空间里学习到相关的信息**，在此之前的 [A Structured Self-attentive Sentence Embedding](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1703.03130) 也有着类似的思想。简而言之，就是希望每个注意力头，只关注最终输出序列中一个子空间，互相**独立**。其核心思想在于，抽取到更加丰富的**特征信息**。\n回到题主的问题上来，如果只使用 one head 并且维度为 ![[公式]](https://www.zhihu.com/equation?tex=d_%7Bmodel%7D) ，相较于 8 head 并且维度为 ![[公式]](https://www.zhihu.com/equation?tex=d_%7Bmodel%7D+%2F+8)。首先存在计算量极大的问题，并且高维空间下的学习难度也会相应提升，这就难免文中实验出现的参数量大且效果不佳的情况，于是将原有的高维空间转化为多个低维空间并再最后进行拼接，形成同样维度的输出，借此丰富特性信息，降低了计算量，而且取得了更好的效果，十分巧妙。\n\n## **为何在获取输入词向量之后需要对矩阵乘以embeddding size的开方？意义是什么？**\n\nembedding matrix的初始化方式是xavier init，这种方式的方差是1/embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。\n\n## **你还了解些关于位置编码的技术，各自的优缺点是什么？**\n\nhttps://zhuanlan.zhihu.com/p/105001610\n\n## [Transformer 为什么使用 layer normalization，而不是其他的归一化方法？](https://www.zhihu.com/question/395811291)\n\nhttps://www.zhihu.com/question/395811291/answer/1251829041\n\n\n\n\n\n## **Transformer如何并行化的？**解码器端可以做并行化吗？\n\nTransformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，在self-attention模块，对于某个序列![[公式]](https://www.zhihu.com/equation?tex=x_%7B1%7D%2C+x_%7B2%7D%2C+%5Cdots%2C+x_%7Bn%7D)，self-attention模块可以直接计算![[公式]](https://www.zhihu.com/equation?tex=x_%7Bi%7D%2C+x_%7Bj%7D)的点乘结果，而RNN系列的模型就必须按照顺序从![[公式]](https://www.zhihu.com/equation?tex=x_%7B1%7D)计算到![[公式]](https://www.zhihu.com/equation?tex=x_%7Bn%7D)。\n\n\n\n\n\n## **简单描述一下wordpiece model和字节对编码？**\n\nhttps://zhuanlan.zhihu.com/p/86965595\n\n\n\n\n\n## **Transformer训练的时候学习率是如何设定的？**\n\n![](https://i.loli.net/2021/07/05/FDESyq27uWr1lvJ.png)\n\n\n\n## [Transformer中multi-head机制是如何实现每个head提取的信息空间互斥的？](https://www.zhihu.com/question/357565475)\n\nTransformer的多头注意力看上去是借鉴了CNN中同一卷积层内使用多个卷积核的思想，原文中使用了 8 个“scaled dot-product attention”，在同一“multi-head attention”层中，输入均为“KQV”，**同时**进行注意力的计算，彼此之前**参数不共享**，最终将结果**拼接**起来，这样可以允许模型在**不同的表示子空间里学习到相关的信息**。简而言之，就是希望每个注意力头，只关注最终输出序列中一个子空间，互相**独立**。其核心思想在于，抽取到更加丰富的**特征信息**。\n\n\n\n## **Transformer的细节到底是怎么样的？**\n\nhttps://www.zhihu.com/question/362131975/answer/945357471\n\n\n\n## **[为什么Bert的三个Embedding可以进行相加？](https://www.zhihu.com/question/374835153/answer/1040767499)**\n\n(Token Embedding、Segment Embedding、Position Embedding三个向量为什么可以相加呢？相加后向量的大小和方向就变了，语义不就变了吗？) 深度神经网络里变得非常复杂，本质上神经网络中每个神经元收到的信号也是“权重”相加得来。这三个向量为什么可以相加呢？因为三个embedding相加等价于三个原始one-hot的拼接再经过一个全连接网络。 相加后向量的大小和方向就变了，语义不就变了吗？这里不是语义变了，而是在训练的时候就是这几个向量相加进行训练的，训练完之后，将lookup后的向量进行相加，就能得到比较好的表示了。 从梯度的角度解释：\n$$\n(f+g+h)' = f'+g'+h'\n$$\n\n## **[为什么BERT输入的最大长度要限制为512？](https://www.zhihu.com/question/395903256)**\n\n个人推断是考虑了计算与运行效率综合做出的限制。\n\nBERT输入的最大长度限制为512, 其中还需要包括[CLS]和[SEP]. 那么实际可用的长度仅为510.**但是别忘了,**每个单词tokenizer之后也有可能被分成好几部分. 所以实际可输入的句子长度远不足510.\n\nBERT由于position-embedding的限制只能处理最长512个词的句子。如果文本长度超过512，有以下几种方式进行处理：\n\n**a）直接截断：**从长文本中截取一部分，具体截取哪些片段需要观察数据，如新闻数据一般第一段比较重要就可以截取前边部分；\n\n**b）抽取重要片段：**抽取长文本的关键句子作为摘要，然后进入BERT；\n\n**c）分段：**把长文本分成几段，每段经过BERT之后再进行拼接或求平均或者接入其他网络如lstm。\n\n另外transformer-xl 、[LongFormer：用稀疏自注意力拓展模型文本容纳量](https://zhuanlan.zhihu.com/p/133491514)等优秀设计也可以解决长文本。\n\n\n\n## **为什么BERT选择mask掉15%这个比例的词，可以是其他的比例吗？**\n\n来自@海晨威的算法屋\n\nBERT采用的Masked LM，会选取语料中所有词的15%进行随机mask，论文中表示是受到完形填空任务的启发，但其实**与CBOW也有异曲同工之妙**。\n\n从CBOW的角度，这里 ![[公式]](https://www.zhihu.com/equation?tex=p%3D15%5C%25) 有一个比较好的解释是：在一个大小为 ![[公式]](https://www.zhihu.com/equation?tex=1%2Fp%3D100%2F15%5Capprox7) 的窗口中随机选一个词，类似CBOW中滑动窗口的中心词，区别是这里的滑动窗口是非重叠的。\n\n那从CBOW的滑动窗口角度，10%~20%都是还ok的比例。\n\n上述非官方解释，是来自我的一位朋友提供的一个理解切入的角度，供参考。\n\n来自@Serendipity\n\n15%的概率是通过实验得到的最好的概率，xlnet也是在这个概率附近，说明在这个概率下，既能有充分的mask样本可以学习，又不至于让segment的信息损失太多，以至于影响mask样本上下文信息的表达。然而因为在下游任务中不会出现token“<mask>”，所以预训练和fine-tune出现了不一致，为了减弱不一致性给模型带来的影响，被mask的token有80%的概率用“<mask>”表示，有10%的概率随机替换成某一个token，有10%的概率保留原来的token，这3个百分比也是多次实验得到的最佳组合，在这3个百分比的情况下，下游任务的fine-tune可以达到最佳的实验结果。\n\n## **为什么BERT在第一句前会加一个[CLS]标志?**\n\nbert在token序列之前加了一个特定的token“[cls]”，这个token对应的向量后续会用在分类任务上；如果是句子对的任务，那么两个句子间使用特定的token“[seq]”来分割。\n\n为什么选它呢，因为与文本中已有的其它词相比，这个无明显语义信息的符号会**更“公平”地融合文本中各个词的语义信息**，从而更好的表示整句话的语义。\n\n这里补充一下bert的输出，有两种：\n\n一种是get_pooled_out()，就是上述[CLS]的表示，输出shape是[batch size,hidden size]。\n\n一种是get_sequence_out()，获取的是整个句子每一个token的向量表示，输出shape是[batch_size, seq_length, hidden_size]，这里也包括[CLS]，因此在做token级别的任务时要注意它。\n\n\n\n## **Bert和Transformer在loss上的差异**\n\ntransformer的loss是在decoder阶段计算的。bert预训练的loss由2部分构成，一部分是NSP的loss，就是token“[cls]”经过1层Dense，然后接一个二分类的loss，其中0表示segment B是segment A的下一句，1表示segment A和segment B来自2篇不同的文本；另一部分是MLM的loss，segment中每个token都有15%的概率被mask，而被mask的token有80%的概率用“<mask>”表示，有10%的概率随机替换成某一个token，有10%的概率保留原来的token，被mask的token经过encoder后乘以embedding matrix的转置会生成在vocab上的分布，然后计算分布和真实的token的one-hot形式的cross entropy，最后sum起来当作loss。这两部分loss相加起来当作total loss，利用adam进行训练。bert fine-tune的loss会根据任务性质来设计，例如分类任务中就是token“[cls]”经过1层Dense，然后接了一个二分类的loss；例如问题回答任务中会在paragraph上的token中预测一个起始位置，一个终止位置，然后以起始位置和终止位置的预测分布和真实分布为基础设计loss；例如序列标注，预测每一个token的词性，然后以每一个token在词性的预测分布和真实分布为基础设计loss。\n\nbert在encoder之后，在计算NSP和MLM的loss之前，分别对NSP和MLM的输入加了一个Dense操作，这部分参数只对预训练有用，对fine-tune没用。而transformer在decoder之后就直接计算loss了，中间没有Dense操作。\n\n\n\n## **为什么bert需要额外的segment embedding?**\n\n\n\n因为bert预训练的其中一个任务是判断segment A和segment B之间的关系，这就需要embedding中能包含当前token属于哪个segment的信息，然而无论是token embedding，还是position embedding都无法表示出这种信息，因此额外创建一个segment embedding matrix用来表示当前token属于哪个segment的信息，segment vocab size就是2，其中index=0表示token属于segment A，index=1表示token属于segment B。\n\n\n\n## **为什么transformer的embedding后面接了一个dropout，而bert是先接了一个layer normalization，再接dropout?**\n\nLN是为了解决梯度消失的问题，dropout是为了解决过拟合的问题。在embedding后面加LN有利于embedding matrix的收敛。\n\n\n\n## **BERT模型有什么调参技巧?**\n\nhttps://www.zhihu.com/question/373856698/answer/1034691809\n\n## **Transformer中warm-up和LayerNorm的重要性？**\n\nhttps://zhuanlan.zhihu.com/p/84614490\n\n\n\n## [Bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？](https://www.zhihu.com/question/318355038)\n\n\n\nhttps://www.zhihu.com/question/318355038\n\n\n\n","tags":["nlp"]},{"title":"A Generalization of Transformer Networks to Graphs","url":"/2021/06/29/A-Generalization-of-Transformer-Networks-to-Graphs/","content":"\n# A Generalization of Transformer Networks to Graphs\n\n作者提出了一种适用于任意图的Transformer结构的。 \n\n## 针对问题:\n\n最初的Transformer相当于一个在所有单词之间都有连接的全连通图上操作，但这样的体系结构没有利用图的连通感应偏差，并且当图拓扑结构重要时，没有被编码到节点特征汇总，性能不好。\n\n## 解决方案\n\n作者提出四个新特性：\n\n- 首先，注意机制是图中每个节点的邻域连通性的函数。\n- 其次，位置编码由Laplacian特征向量来表示，用在了原始Transformer在NLP中常用的正弦位置编码。\n- 第三，用batch normalization层代替layer normalization，提供了更快的训练速度和更好的泛化性能。\n- 最后，将对任务至关重要的边缘特征表示，加入到该graph-transformer结构中。\n\n## 相关工作\n\n### Graph Transformer-2019\n\n首先作者提出在2019年Graph Transformer那篇文章，为了捕捉全局信息将attention应用在全图节点上替代了局部邻居。\n\n但作者认为这样限制了稀疏性的有效利用，这是在图数据上学习比较重要的ductive bias。 为了获取全局信息的目的，作者认为还有其他方法可以合并相同的信息，而不是放弃稀疏性和局部上下文。\n\n> 例如，使用特定于图形的位置特征(Zhang et al.2020)，或节点拉普拉斯位置特征向量(Belkin和Niyoi 2003；Dwivedi等人)。2020)或相对可学习的位置信息(You、Ying和Leskovec 2019)、虚拟节点 (Li et al. 2015)等。\n\n### Graph-BERT-2020\n\n其次作者在相关工作中评价了Graph-Bert那篇文章，其强调预先训练和并行化学习，使用一种子图批处理方案，创建固定大小的无链接子图，将其传递给模型，而不是原始图。\n\nGraph-BERT采用几种位置编码方案的组合来捕获绝对节点结构和相对节点位置信息。由于原始图不直接用于Graph-BERT，并且子图在节点之间没有边(即，无链接)，所提出的位置编码的组合试图在节点中保留原始图的结构信息。\n\n### HGT-2020、GTN-2019\n\nGraph Transformer Networks(GTN)学习异构图，目标是将给定的异构图转换为基于元路径的图，然后执行卷积。\n\n他们使用attention背后的重点是为了预置生成的meta-paths，除了能够处理任意数目的节点和边类型外，HGT还以基于中心节点和消息传递节点的时间戳差异的相对时间位置编码的形式捕获了异质图中信息流的动态变化。\n\n\n\n### 本文贡献\n\n提出稀疏性和位置编码是图变形器开发中的两个关键方面。与为特定的图形任务设计一个性能最佳的模型相反，这篇的工作是尝试一个通用的graph-Transformer模型。\n\n- 提出了一种将Transformer网络可以用于任意结构同质图的方法，即Graph Transformer，并提出了一种具有边特征的扩展Graph Transformer，它允许使用显式的域信息作为边特征。\n- 利用图数据集的拉普拉斯特征向量来融合节点位置特征， 与文献的比较表明，对于任意同质图，拉普拉斯特征向量比任何现有的编码节点位置信息的方法都要好。\n- 实验证明其好于传统GNN\n\n\n\n## 方法\n\n### 图的稀疏性\n\n在NLP Transformer中，句子被视为完全连通的图形，这种选择有两个原因：\n\n- 很难在句子中的单词之间找到有意义的稀疏交互或联系。例如，句子中的一个词对另一个词的依赖性可以随上下文、用户的视角和特定应用而变化。一个句子中的词之间可能存在许多似是而非的基本事实连接，因此，句子的文本数据集没有显式的词交互可用。因此，让一个句子中的每个单词相互关注其他单词是有意义的，就像Transformer架构所遵循的那样。\n- 在NLP Transformer中考虑的所谓的图通常具有少于数十或数百个节点。这在计算上是可行的，大型变压器模型可以在这种完全连通的文字图上进行训练。\n\n在高达数百万或数十亿的节点大小。可用的结构为我们提供了丰富的信息源，可以作为神经网络中的归纳偏差加以利用，而节点大小实际上使得这样的数据集不可能有一个完全连通的图。\n\n### 图的位置编码\n\n在NLP中，大多数情况下，基于Transformer的模型由每个字的位置编码补充。这对于确保每个单词的唯一表示以及甚至保留距离信息是至关重要的。\n\n> 对于图，唯一节点位置的设计是具有挑战性的，**因为存在防止规范节点位置信息的对称** 。事实上，大多数GNN学习的是节点位置不变的structural node信息。\n>\n> 这就是为什么简单的基于注意力的模型，如GAT，其中attention是局部邻域连通性的函数，而不是全图连通性。\n\n为了更好地对距离感知信息进行编码，(附近节点具有相似的位置特征，而较远的节点具有不同的位置特征)使用拉普拉斯特征向量作为图变换中的PE。\n\n 作者在训练过程中随机反转特征向量的符号，遵循 Benchmarking graph neural networks 的做法 。预先计算了数据集中所有图的拉普拉斯特征向量。通过对图的拉普拉斯矩阵进行因式分解来定义特征向量；\n$$\n\\Delta = I - D^{-1/2}AD^{-1/2}=U^T\\Lambda U\n$$\n使用节点的k个最小特征向量作为其位置编码，并对节点 $i$用 $λ_i$表示。\n\n### Graph Transformer Architecture\n\n![](https://z3.ax1x.com/2021/06/29/RwVhq0.png)\n\n左边的模型是为没有明确边属性的图设计的，右边的模型维护一个指定的边特征，以结合可用的边信息并在每一层维护它们的抽象表示。\n\n#### 输入\n\n图 $G$ 的节点特征 $\\alpha_i \\in R^{d_n \\times 1}$ ，节点$i,j$ 对应的边特征$\\beta_{ij}\\in R^{d_e \\times 1}$\n\n$\\alpha_i, \\beta_{ij}$  通过线性映射成为$d$ 维隐层特征 $h_i^0 , e_{ij}^0$ :\n$$\n\\hat h_i^0 = A^0\\alpha_i + a^0 ; e_{ij}^0 = B^0\\beta_{ij}+b^0\n$$\n其中，$A^0\\in R^{d\\times d_n}, B^0\\in R^{d\\times d_e}, a,b\\in R^d$\n\n将位置编码线性映射后加入节点特征:\n$$\n\\lambda_i^0 = C^0\\lambda_i +c^0; h_i^0 = \\hat h_i^0 + \\lambda_i^0\n$$\n其中，$C^0\\in R^{d\\times k}, c^0 \\in R^d$ . 请注意，拉普拉斯位置编码仅添加到输入层的节点特征，而不是在中间层。\n\n#### Graph Transformer Layer\n\n第 $l$ 层节点更新:\n$$\n\\begin{equation}\\begin{split} \n \\hat h_i^{l+1} &= O^l_h \\parallel_{k=1}^H \\left ( \\sum_{j\\in N_i} w_{ij}^{k,l} V^{k,l} h^l_j \\right )\\\\\n where, w_{i,j}^{k,l} &= softmax_j(\\frac{Q^{k,l} \\cdot K^{k,l}h_j^l}{\\sqrt{d_k}})\n    \\end{split}\\end{equation}\n$$\n其中，$Q^{k,l}, K^{k,l}, V^{k,l} \\in R^{d_k \\times d}, O^{l}_h\\in R^{d\\times d}$ ，$k$ 注意力haed数。\n\n为了数值稳定性，在取softmax 的输出被限制介于−5到+5之间。\n\n然后，将attention输出$h^{l+1}$传递给FFN，然后是残差和归一化层，如下所示：\n$$\n\\begin{equation}\\begin{split} \n \\hat{\\hat {h}}_i ^{l+1} &= Norm(h_i^l + \\hat h^{l+1}_i) \\\\\n \\hat{\\hat{\\hat{h_i}}}^{l+1} &= W_2^l ReLU(W_1^l \\hat{\\hat {h}}_i ^{l+1}) \\\\\n h^{l+1}_i &= Norm(\\hat{\\hat {h}}_i ^{l+1}+\\hat{\\hat{\\hat{h_i}}}^{l+1})\n    \\end{split}\\end{equation}\n$$\n其中 $W_1^l\\in R^{2d \\times d}, W_2^l \\in R^{d\\times 2d}$ 为了说明清楚，省略了偏执项。\n\n\n\n#### Graph Transformer Layer with edge features\n\n旨在更好地利用图数据集中以边属性形式提供的丰富特征信息。\n\n![](https://z3.ax1x.com/2021/06/30/RwzFXD.png)\n\n\n\n这些边特征是对应于节点对的相关分数，所以将这些可用的边特征与通过 pairwise attention计算的隐含边分数联系起来。换言之，假设在query 和 key 特征投影相乘之后，当节点 $i$ 关注节点 $j$ 时，计算在softmax注意分数 $\\hat w_{ij}$， 将该分数  $\\hat w_{ij}$ 视为关于边 $<i，j>$的隐含信息。\n\n利用边特征改进已经计算的隐式注意分数 $\\hat w_{ij}$ 。通过简单地将两个值 $\\hat w_{ij}$ 和 $e_{ij}$ 相乘来实现的\n\n指定的节点对称的边特征表示管道，用于将边属性从一层传播到另一层：\n$$\n\\begin{equation}\\begin{split} \n  \\hat h_i^{l+1} &= O^l_h \\parallel_{k=1}^H \\left ( \\sum_{j\\in N_i} w_{ij}^{k,l} V^{k,l} h^l_j \\right )\\\\ \\\\\n  \\hat e^{l+1}_{ij} &= O^l_e \\parallel_{k=1}^H(\\hat w_{ij}^{k,l}),\\\\  \\\\\n  where, w_{ij}^{k,l} &= softmax_j(\\hat w_{ij}^{k,l}) ,\\\\ \\\\\n  \\hat w_{ij}^{k,l} &= \\left(\\frac{Q^{k,l} \\cdot K^{k,l}h_j^l}{\\sqrt{d_k}} \\cdot E^{k,l}e^{l}_{ij} \\right)\n  \n    \\end{split}\\end{equation}\n$$\n其中 $Q^{k,l}, K^{k,l}, V^{k,l} ,E^{k,l} \\in R^{d_k\\times d} , O^l_h,O^l_e\\in R^{d\\times d}$\n\n其余d 也是要经过Transformer架构中的其他成分。\n\n\n\n## 实验\n\n![](https://z3.ax1x.com/2021/06/30/R0NJln.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"Do Transformers Really Perform Bad for Graph Representation?","url":"/2021/06/23/Do-Transformers-Really-Perform-Bad-for-Graph-Representation/","content":"\n# Do Transformers Really Perform Bad for Graph Representation?\n\n作者提出了一个建立在Transformer体系结构上可以对图进行出色表征的模型——Graphormer。\n\n近来Transformer和Graph进行结合尝试的文章有：\n\n- Graph transformer for graph-to-sequence learning. AAAI 2020.\n- A generalization of transformer networks to graphs. AAAI 2021. (GT)\n- Heterogeneous graph transformer.  2020.\n- Re-thinking graph transformers with spectral attention.   2021.  (>GT)\n- Graph transformer.  2019.\n- Direct multi-hop attention based graph neural network. 2020\n- Graph transformer networks.   2019.\n- Transformers are Graph Neaurl Networks. 2020\n- Graph-bert: Only attention is needed for learning graph representations. 2020\n\n虽然有很多利用Transformer进入图形领域的尝试，但唯一有效的方法是用Softmax attention 取代经典GNN变体中的一些关键模块(例如，特征聚合)。因此，Transformer体系结构是否适合于对图进行建模，以及如何使其在图表示学习中发挥作用，仍然是一个悬而未决的问题。\n\n## 本文方法\n\nHow Transformers could perform well for graph representation learning?\n\n作者解决办法：关键是要正确地将图的结构信息融入到模型中。\n\n1.  *Centrality Encoding*：捕捉节点在图中的重要性。在图中，不同的节点可以具有不同的重要性，例如，名人被认为比社交网络中的大多数网络用户更有影响力。然而，这种信息没有反映在self-attention模块中，因为它主要使用节点语义特征来计算相似度。\n\n   1.1. *degree centrality* ：利用度中心性进行Centrality Encoding，其中根据每个节点的度将可学习向量分配给每个节点，并将其添加到输入层中的节点特征。\n\n2. *Spatial Encoding* ：提出了一种新的空间编码来捕捉节点之间的结构关系。将图形结构化数据与其他结构化数据(例如语言、图像)区分开来的一个值得注意的几何属性是不存在用于嵌入图形的规范网格，结点只能位于非欧几里德空间中，并且由边连接。作者使用任意两个节点之间的**最短路径距离**作为示例，将其编码为Softmax关注度中的偏置项，以帮助模型准确地捕捉图中的空间相关性。\n\n   2.1. 此外，有时edge特征中还包含额外的空间信息，例如分子图中两个原子之间的键类型。设计了一种新的Edge Encoding方法，将这种信号进一步带入Transformer layers。\n\n\n\n## Graphormer\n\n### Structural Encodings in Graphormer\n\n三种有效的Graphormer编码：Centrality Encoding、Spatial Encoding、Edge Encoding in the Attention\n\n![](https://i.loli.net/2021/06/24/CoMykaSRbVxvtUc.png)\n\n#### 特征层面 Centrality Encoding\n\n普通的attention机制是基于节点之间的语义相关性来计算注意力分布的，节点的中心性可以衡量节点在图中的重要性，但这种信息在目前的attention计算中被忽略了。(attention不是也可以计算出什么比较重要吗，这里有点疑问)\n\n在Graphormer中，作者使用了degree中心性，作为节点中心性的度量。根据每个节点的入度和出度为每个节点分配两个实值embedding向量。对每个节点都应用中心性编码，因此需将其作为输入添加到节点特征中。\n$$\nh_i^{(0)} = x_i + z_{deg^{-}(v_i)}^- + z^+_{deg^+(v_i)}\n$$\n通过在输入中使用中心性编码，Softmax注意力既可以捕捉到查询和关键字中的节点重要性信号，又能捕捉到节点的重要性。\n\n```python\nself.in_degree_encoder = nn.Embedding(512, hidden_dim, padding_idx=0)\nself.out_degree_encoder = nn.Embedding(512, hidden_dim, padding_idx=0)\nnode_feature = node_feature + self.in_degree_encoder(in_degree) + self.out_degree_encoder(out_degree)\n```\n\n\n\n#### 注意力层面 Spatial Encoding\n\noriginal的transformer的注意力机制使得它具有全局的感受野，前提是对于每一个token需要指定一个位置，比如一个句子中不同词的位置可以用 $1,2,3,4......$ 来代表序列中不同位置的编码。对于图来说并不存在序列这样的位置特性，那么应该如何考虑不同节点的位置信息呢？\n\n图结构通常是在非欧空间下的，两个节点的位置关系可以用节点之间的最短路径 $\\phi(v_i,v_j):V \\times V \\to R$ 来表示(无关联的两个节点之间的最短路径为-1)。 函数 $\\phi$ 可以由图中节点之间的连通性来定义。在这篇文章中作者选用的是最短路径长度SPD。并进行可学习参数映射为 $b_{\\phi(v_i,v_j)}$  ，在所有层之间共享。\n\n将这一特征融入注意力矩阵，使得注意力系数也包含了图中节点的相对位置（连接关系）信息：\n$$\nA_{ij} = \\frac{(h_iW_Q)(h_jW_K)^T}{\\sqrt{d}} + b_{\\phi}(v_i,v_j)\n$$\n\n\n这样做的好处：\n\n- 首先，与传统GNN相比，其中接受场仅限于邻居，可以在公式中看到这一点。transformer层提供了全局信息，每个节点都可以关注图中的所有其他节点。\n- 其次，通过使用 $b_{\\phi(v_i,v_j)}$，单个transformer层中的每个节点可以根据图的结构信息自适应地关注所有其他节点。比如学到的b是个单调减函数，那就表达了**最短路径越大的节点之间的关系越小**，这也是图神经网络中的基本思想：**越近的邻居的信息越重要**。\n\n\n\n#### 注意力层面 Edge Encoding in the Attention\n\n此外，graph相关的任务中通常还有边的信息，这些特征对于图形表示很重要。\n\n以前工作中的有两种编码方式：\n\n- 在第一种方法，边缘特征被添加到关联节点的特征。\n- 在第二种方法，对于每个节点，其关联边的特征将与节点特征一起aggregation使用。\n\n但这种使用边特征的方法只将边信息传播到其关联的节点，这可能不是利用边信息来表示整个图的有效方式。\n\n作者提出了一个新的edge编码方式，\n\n注意机制需要估计每个节点对 $(v_i，v_j)$ 的相关性，作者认为应该在相关性中考虑连接它们的边 (像多跳图网络那样)。对于每个有序节点对 $(v_i，v_j)$，从 $v_i$ 到 $v_j$ 的最短路径为 $SP_{ij}=(e_1，e_2，...，e_n)$，则可以用路径的加权平均得到两个节点之间边的相关信息：\n$$\ne_{ij} = \\frac{1}{N} \\sum_{n=1}^N x_{e_n} (w_n^E)^T\n$$\n其中 $x_{e_n}$ 是第n个边 $e_n$ 的特征，$w_n^E \\in R^{d_E}$ 是第n个权重embedding，$d_E$ 是边特征的维度。\n\n将这个信息也一起融入到注意力机制中：\n$$\nA_{ij} = \\frac{(h_iW_Q)(h_jW_K)^T}{\\sqrt{d}} + b_{\\phi}(v_i,v_j) + e_{ij}\n$$\n\n```python\n# Scaled Dot-Product Attention.\n# Attention(Q, K, V) = softmax((QK^T)/sqrt(d_k))V\nq = q * self.scale\nx = torch.matmul(q, k)  # [b, h, q_len, k_len]\nx = x + attn_bias\nx = torch.softmax(x, dim=3)\nx = self.att_dropout(x)\nx = x.matmul(v)  # [b, h, q_len, attn]\n```\n\n> 把 $A_{ij}$  的三部分进行拆分，三个分支的网络分别学习不同层面的信息，然后再后面再对三个分支的网络输出进行concat或者attention是否有帮助？\n\n\n\n### 图池化的一个细节\n\n想要表示图级别的embedding，Graphormer采用了一个额外写的特殊节点 $[VNode]$ , 让它和图中每个节点都链接。\n\n有点像bert中的CLS一样，整个图$h_G$的表示将是最终层中的 $[VNode]$ 的节点特征\n\n\n\n\n\n## *Do these modifications make Graphormer more powerful than other GNN variants?* \n\n### Fact 1.\n\n通过选择合适的权值和距离函数 $\\phi$，Graphormer layer可以表示GIN、GCN、GraphSAGE等流行的广义网络模型的AGGREGATE和 COMBINE 步骤。\n\n1. Spatial encoding 使得 self-attention 模块能够区分节点 $v_i$ 的邻居集合 $N(v_i)$，从而SoftMax函数可以计算 $N(v_i)$ 上的平均统计量\n2. 通过了解节点的度，平均邻域可以转化为邻域之和\n3. 利用多头注意力和FFN，可以分别处理 $v_i$ 和 $N(v_i)$ 的表示，并且将其组合在一起。 \n\n### 证明1  \n\n#### 采用空间编码的自我注意模块可以表示均值聚合\n\n$$\nA_{ij} = \\frac{(h_iW_Q)(h_jW_K)^T}{\\sqrt{d}} + b_{\\phi}(v_i,v_j)\n$$\n\n\n\n如果 $\\phi=1$ ，则设置 $b_{\\phi}=0$ ，否则设置 $b_{\\phi}=−∞$，其中 $\\phi$ 是 SPD。意思是如果直接相连就考虑其原本的注意力分数，如果不是直接相连注意力分数就为负无穷，就和普通的GNN一样的了。\n\n设置 $W_Q=W_K=0$ 且 $W_V$ 为单位矩阵，$softmax(A)V$ 表示邻居表示的平均值。\n\n\n\n\n\n## 实验\n\n OGB-LSC quantum chemistry regression (i.e., PCQM4M-LSC) challenge\n\n这是目前最大的图形级预测数据集，总共包含超过380万个图表\n\n还有其他三个任务：ogbg- molhiv, ogbg-molpcba and ZINC\n\n主要有两种模型大小：\n\nGraphormer (L = 12, d = 768) ， $Graphormer_{SMALL}$ (L = 6,d = 512).\n\n\n\n![](https://i.loli.net/2021/06/25/QJ9CO7jATVIXzxE.png)\n\n\n\n![](https://i.loli.net/2021/06/25/h3dgUJ65qlzEAuX.png)\n\n\n\n### 消融实验\n\n将以前使用的位置编码(PE)与作者提出的空间编码进行了比较，这两种编码的目的都是对变压器的不同节点关系信息进行编码。\n\n以前的基于变压器的GNN采用了各种PE，例如Weisfeiler-Lehman-PE(WL-PE)和Laplacian PE。采用的是Laplacian ，有文章证明一系列PE相比，它的性能很好。\n\n采用空间编码的transformer体系结构的性能优于基于位置编码的transformer体系结构，说明了利用空间编码捕获节点空间信息的有效性。\n\n![](https://i.loli.net/2021/06/25/5INfs8M9XHSmqDK.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"Avoiding Reasoning Shortcuts- Adversarial Evaluation, Training, and Model Development for Multi-Hop QA","url":"/2021/06/13/Avoiding-Reasoning-Shortcuts-Adversarial-Evaluation-Training-and-Model-Development-for-Multi-Hop-QA/","content":"\n# Avoiding Reasoning Shortcuts- Adversarial Evaluation, Training, and Model Development for Multi-Hop QA\n\n这篇文章作者发现在HotpotQA中经常包含Reasoning Shortcuts。也就是说模型没有真正的理解文章并进行推理，而是通过将问题与上下文中的句子进行词匹配来直接定位答案。\n\n作者主要做了两件事：\n\n- 构建干扰文档数据，证明了存在推理shortcut现象。\n- 设计一个新模型来缓解这个问题\n\n\n\n## 对抗验证\n\n![](https://z3.ax1x.com/2021/06/14/27n5O1.png)\n\n\n\n问题是What was the father of Kasper Schmeichel voted to be by the IFFHS in 1992? （卡斯珀·舒梅切尔的父亲1992年被IFFHS投票选为什么？）\n\n模型需要从两个文档中考虑信息，找出隐含的推理链条。\n\nKasper Schmeichel $\\rightarrow^{son}$  Peter Schemeichel $\\rightarrow^{voted}$ World’s Best Goalkeeper\n\nKasper Schmeichel是Peter Schemeicher, Peter Schemeicher 被投票为 World’s Best Goalkeeper 世界最佳守门员。\n\n在该示例中，也可以通过将问题中的几个关键字(“voted to be by the IFFHS in 1992——1992年投票的IFFHS”)与上下文中的相应事实相匹配来得到正确的回答，而无需通过第一跳推理来找到“Kasper Schmeichel的父亲”，因为两个distractor文档都不包含足够分散注意力的信息。 \n\n因此，一个在现有评估上表现良好的模型并不一定表明它具有很强的复合推理能力。\n\n随着对抗扰动文本被添加到上下文中，使用单跳shotcut方式不再可能找到正确的答案，这现在导致了两个可能的答案(“世界最佳守门员”和“世界最佳防守人”)。\n\n添加干扰后模型预测答案为 IFFHS World's Best Defender 最佳防守人。\n\n\n\n## 如何构建对抗数据\n\n**要构建这种对抗的需求**\n\n> 作者指出HotpotQA从维基百科中选择距离目标问题最短的bigram TF-IDF的前8个文档作为干扰项，形成总共10个文档的上下文。由于在生成问题时没有向群组工作人员提供导向文档，因此不能保证在给定整个上下文的情况下，两个支持文档都是必要的来推断答案。\n>\n> 多跳假设可以通过两种方式由不完整的分心文档来实现。\n>\n> 1. 其中一个选定的干扰项可能包含推断答案所需的所有证据，从经验上讲，在HotpotQA中没有发现这样的情况，因为关于一个主题的Wiki文章很少讨论另一个主题的细节\n>\n> 2. 整个分散注意力的文档池可能不包含真正分散读者/模型注意力的信息。\n\n作者把绕过推理回答问题这种方式，叫做shortcut 。其经常出现在HotpotQA中的桥接问题中，比较问题一般不能匹配而得出。作者采样了50个桥接问题，发现其中26个有这种问题。\n\n-----\n\n设原内容、问题答案为 $(C,q,a)$ 是可能包含shortcut问题的数据，作者是想将其变为$(C',q,a)$ \n\n$q,a$ 不变，$C'$ 变成接近 $C$ 的文章。在HotpotQA中是提供两个支持文档$P$的， 其中$P\\subset C$ 。\n\n 在构建对抗文本时也就是ADDDoc。就是利用新的 $P'$ ，混合$(C,P')$ 构成新的数据集。\n\n**那么 $P'$ 如何来的？**\n\n假设 $p2\\in P$ 是包含答案的支持文档，$p1\\in P$ 是包含线索的文档。\n\nADDDoc是利用词或短语级别的干扰，将 $p2$ 替换成$p'2$ ，其包含满足推理快捷方式但不与整个问题的答案相矛盾的假答案。\n\n**那么词语是如何替换的？**\n\n首先，对于答案中的每个非停用词，都会在GloVe100维向量空间中找到最接近的10个替代词，它的子串与原始答案的重叠子串长度不超过3个。如(Mumbai → Delhi, Goalkeeper → Defender)。如果这个过程失败，就从HotpotQA dev集合的整个答案池中随机抽样一个候选者 。 \n\n如果原始答案有多个单词，我们将答案中的一个非停用词与相应的抽样答案单词替换，以创建假答案(“World’s Best Goalkeeper → World’s Best Defender”)。\n\n![27EFfA.png](https://z3.ax1x.com/2021/06/14/27EFfA.png)\n\n问：Sachin Wamer作为软件工程师所在的公司的总部在哪里？\n\n由此产生的段落 $p'2$ 提供了一个满足推理捷径的答案，但也与整个问题的真实答案相矛盾，因为它形成了另一个有效的推理链，将问题与假答案连接起来 (Sachin Warrier $\\rightarrow^{work}$ TCS $\\rightarrow^{at}$ Delhi)。\n\n为了打破这个矛盾的推理链，我们需要用另一个实体替换连接两个证据的桥梁实体（在这种情况下为“Tata Consultancy Services”），这样生成的答案就不再作为有效答案 。\n\n用从 HotpotQA 开发集中所有文档标题中随机采样的候选者替换 $p'2$ 的标题。 如果 $p1$ 的标题出现在$p'2$ 中，我们也将其替换为另一个采样标题，以彻底消除 $p'2$ 和 $p1$ 之间的联系。\n\n如上图将 Tata Consultancy Services  替换为  Valencia Street Circuit \n\n\n\n## 模型方法\n\n\n\n### Encoding\n\n对于cotnext 和question 使用v 维Highway Network 合并 字符嵌入和GloVe词嵌入。\n\n得到 $x\\in R^{J\\times v}$ 和 $q\\in R^{S\\times v}$ 其中 J 是文章长度 S 为问题长度\n\n整体结构和BiDAF那个文章相似。\n\n[![27xDFP.png](https://z3.ax1x.com/2021/06/14/27xDFP.png)](https://imgtu.com/i/27xDFP)\n\n\n\n### Single-Hop Baseline\n\n使用bi-attention + self-attention ，给定上下文和问题encoding  $h,u$ 经过 context-to-query $BiAttn(h,u)$ 计算得到一个相似矩阵 $M^{S\\times J}$ :\n$$\n\\begin{equation}\\begin{split} \n M_{s,j} &= W_1u_s +W_2h_j + W_3(u_s\\odot h_j) \\\\\n p_{s,j} &= \\frac {exp(M_{s,j})}{\\sum_{s=1}^S exp(M_{s,j})}\\\\\n c_{q_j} &= \\sum_{s=1}^S p_{s,j} u_s\n    \\end{split}\\end{equation}\n$$\n$\\odot$ 对应元素相乘。\n\n然后query-to-context 注意力：\n$$\n\\begin{equation}\\begin{split} \nm_j &= max_{1\\le s\\le S} M_{s,j} \\\\\n p_{s,j} &= \\frac {exp(m_{j})}{\\sum_{j=1}^J exp(m_{j})}\\\\\n q_c &= \\sum_{j=1}^J p_{j} h_j\\\\\n h'_j &= [h_j ;c_{q_j}; h_j\\odot c_{q_j}; c_{q_j} \\odot q_c]\\\\\n h^1 &= BiLSTM(h')\n    \\end{split}\\end{equation}\n$$\n\n\n\n\n\n## 实验\n\n![](https://i.loli.net/2021/06/23/mPiX5Rkz1o9tCSl.png)\n\n\n\n\n\n![](https://i.loli.net/2021/06/23/yQODEbgUcaeZApK.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["nlp"]},{"title":"TRANSFORMER-XH: MULTI-EVIDENCE REASONING WITH EXTRA HOP ATTENTION","url":"/2021/06/10/TRANSFORMER-XH-MULTI-EVIDENCE-REASONING-WITH-EXTRA-HOP-ATTENTION/","content":"\n# TRANSFORMER-XH: MULTI-EVIDENCE REASONING WITH EXTRA HOP ATTENTION\n\n## 摘要\n\n提出Transformer-XH配有eXtra Hop attention以完全数据驱动的方式实现了结构化文本的内在建模。\n\n完全数据驱动应该是指不仅可以处理序列结构的数据，还可以处理图结构。\n\neXtra Hop attention 除了关注每个序列内的记号外，还可以连接的文本序列之间跳跃\n\n因此文档之间传播信息和构建全局上下文表示来更好地进行联合多证据推理。\n\neXtra Hop attention的作用：\n\n- 当每段文本与其他证据相关时能够更全局地表示每段文本所贡献的证据\n- 以一种自然的方式，通过必要的边信息传递对证据图联合推理\n\n## 方法\n\n面临的第一个挑战就是由于transformer的softmax计算在所有token对上，很难处理长文本。\n\n Transform-XL(eXtra Long) 通过将更长的文本(多段落文档) 分解成文本段序列  $\\lbrace X_1,...,X_r,...,X_{\\zeta} \\rbrace$ 来解决。\n\n使用如下的公式计算在相邻文本段之间传播信息：\n$$\n\\hat H_r^{l-1} = [cat(Freeze(H_{r-1}^{l-1}) ,H_r^{l-1})]\n$$\n其中 $H_r^{l-1}$ 表示第 r 个文本段的第 $l-1$ 层Transformer表达。\n\n新注意力机制中的 $Q,K,V$ 就表达为：\n$$\n\\hat Q^T; \\hat K^T; \\hat V^T =  W^q\\cdot \\hat H^{l-1}_r; W^k\\cdot \\hat H^{l-1}_r; W^v \\cdot \\hat H^{l-1}_r\n$$\n之后就还是送到缩放点积中。这一点处理长文本可能是参考了TransformerXL。\n\n> Transformer-XL 的重要组件之一，**Segment Recurrence Mechanism（段循环机制）**想做的就是，能不能在前一段计算完后，将它计算出的隐状态都保存下来，存到一个 Memeory 中，之后在计算当前段的时候，**将之前存下来的隐状态和当前段的隐状态拼起来，作为 Attention 机制的 K 和 V，从而获得更长的上下文信息**\n\n![](https://i.loli.net/2021/06/10/5zH2Avr7IiXOl6d.png)\n\n然而，在许多情况下，文本段被组织在线性序列之外的结构中。例如，文档由图形结构中的超链接连接，这种图形结构不容易简化为形成线性序列，从而禁止了Transformer-XL的递归方法。\n\n\n\n下面将引出eXtra Hop attention 如下图\n\n![](https://i.loli.net/2021/06/10/lZicWgnPv4L1dz9.png)\n\n\n\n图a 表示 三个链接的文档 $d_2,d_1,d_3$ . Transform-XH使用eXtra Hop attention沿图形边缘传播信息，从而在连接的文本序列之间实现信息共享。\n\n结构化文本包含一些列节点 $ X =\\lbrace X_1,...,X_r,...X_{\\zeta} \\rbrace$ , 对应于一个文本序列。 \n\n目标是生产表达 $X =\\lbrace \\hat H_1,...,\\hat H_r,...\\hat H_{\\zeta} \\rbrace$  ，其不仅合并了每个序列 $X$ 中的本地信息，而且还合并了关于整个结构化文本 ${X，E}$的全局上下文。\n\nTransform-XH通过两种注意机制来实现这一点：in-sequence attention 和 eXtra Hop attention。\n\nin-sequence attention 和 Transformer一样， 在 $l$ 层，第 $i$ 个token 收集从同一文本段τ内的其他 token 的信息：\n$$\nh_{r,i}^l = \\sum_j softmax_j (\\frac {q_{r,i}^T \\cdot k_{r,j}}{ \\sqrt d_k}) \\cdot v_{r,j}\n$$\neXtra Hop attention, 使用第 CLS token 作为 attention hub ，在 $l$ 层，第 $\\tau$ 个文本序列，如果 $τ$ 文本序列与另一个文本序列η之间存在边( $e_{τη}=1 $) :\n$$\n\\hat h_{r,0}^l = \\sum_{\\eta ; e_{r\\eta}=1} softmax_{\\eta} (\\frac {\\hat q_{r,0}^T \\cdot \\hat k_{\\eta,0}}{\\sqrt d_k}) \\cdot \\hat v_{\\eta,0}\n$$\n节点 $τ$  使用hop query $\\hat q_{r,0}$ 和 key $\\hat k_{\\eta, 0}$ 计算其邻居 $η$ 上的关注度权重，然后乘以邻居的value $\\hat v_{\\eta,0}$ ，最后将两个注意力机制聚合起来得到新的 $l$ 层的表达:\n\n\n$$\n\\begin{equation}\\begin{split} \n \\hat h_{r,0}^l &= Linear(cat[h_{r,0}^l, \\hat h_{r,0}^l ])\\\\\n \\hat h_{r,i}^l &= h^l_{r,i} ; \\forall i \\ne 0\n    \\end{split}\\end{equation}\n$$\n$ i \\ne 0$ 是 non-hub tokens \n\n一层 eXtra Hop attention 可视为沿着边 E 信息传递的 single-step \n\n例如，在图a中，文档节点 $d_3$ 通过使用hop attention ,  $d_1→d_3$ 从其邻居d1收集信息来更新其表示。当多个Transformer-xh层被堆叠时，$d_1$ 中的该信息包括来自其in-sequence attention 的 $d_1$的本地上下文，以及来自 $ l-1$ 层的 hop attention ，$d_2−d_1$ 的交叉序列信息。因此，L 层 Transformer-XH可以处理最多 L 跳以外的信息。\n\n总之 Transformer-XH共有三个主要属性，可对原始结构化文本数据进行有效建模：\n\n- 信息沿边的传播\n- 信息的重要性 (hop attention)\n- 序列内和跨序列信息的平衡 (attention combination)\n\n在 $H$ 中学习的表示可以天生地表达结构化文本中的细微差别，这些细微差别是复杂的推理任务(如多跳QA和自然语言推理)所需的。\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"旋转数组的最小数字&搜索旋转排序数组","url":"/2021/06/07/旋转数组的最小数字-搜索旋转排序数组/","content":"\n# 旋转数组的最小数字&搜索旋转排序数组\n\n\n\n## 旋转数组最小数字\n\n### 题目\n\n把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。\n\n输入一个升序的数组的一个旋转，输出旋转数组的最小元素。\n\n例如数组 {3,4,5,1,2}{3,4,5,1,2} 为 {1,2,3,4,5}{1,2,3,4,5} 的一个旋转，该数组的最小值为 11。\n\n数组可能包含重复项。\n\n**注意**：数组内所含元素非负，若数组大小为 00，请返回 −1−1。\n\n样例\n\n```\n输入：nums = [2, 2, 2, 0, 1]\n\n输出：0\n```\n\n### 解法：二分查找\n\n题目中有排序两字，自然较优的解是涉及到二分法的\n\n假设我们用下图表示数组，水平线代表数字相同，横坐标代表数字下标\n\n![](https://i.loli.net/2021/06/07/iFZJBWC4teHIxKR.png)\n\n我们发现除了最后水平的一段（黑色水平那段）之外，其余部分满足二分性质：\n\n竖直虚线左边的数满足 $numbers[i] ≥ numbers[0]$ ；\n\n而竖直虚线右边的数不满足这个条件。\n\n我们要找的便是不满足上诉性质的那段中的最小值。\n\n所以我们先将最后水平的一段删除 , 使得右半段不满足 $numbers[i] ≥ numbers[0]$ ，而是严格满足 $numbers[i] < numbers[0]$。\n\n另外，如果处理数组完全单调的情况：\n\n当删除最后一段后，如果剩下的最后一个大于等一第一个数，说明数组完全单调。\n\n```java\nclass Solution {\n    public int findMin(int[] numbers) {\n        if(numbers == null || numbers.length==0) return -1;\n        int left=0;\n        int right = numbers.length-1;\n        // 去除第二段有序数组中最后的和第一段第一个数相同的数\n        // 使得第二段有序数组严格满足numbers[i]<numbers[0]\n        while(right>0 && numbers[right]==numbers[0]){\n            right--;\n        }\n        // 如果此时整个数组都有序了，那么numbers[0]就是最小值\n        if(numbers[0] < numbers[right]){\n            return numbers[0];\n        }\n        \n        while(left<right){\n            int mid = left+right>>1;\n            if(numbers[mid] < numbers[0]){ // 说明mid落在了右半段，最小值在[left,mid]里\n                right = mid;\n            }else{\n                left = mid +1;\n            }\n        }\n        return numbers[left];\n    }\n}\n```\n\n## 搜索旋转数组\n\n![](https://i.loli.net/2021/06/07/wMJ9hGK68eZspRy.png)\n\n数组旋转后可画出如下图：\n\n![](https://i.loli.net/2021/06/07/zudQlKM7UBmpbhq.png)\n\n橙色线表示的就是旋转后数组的左右两个部分。不难发现，如果下标落在右半部分，则一定有 $nums[mid] <= nums[nums.length-1]$\n\n判断 $nums[mid] <= nums[nums.length-1]$ 是否成立\n\n- 成立：说明当前mid落在了数组的右半部分，而我们要找的最小值其实就是右半部分的开头，故更新区间为 $[l,mid]$ 。\n- 否则：说明mid落在了旋转数组的左半部分，那么右半部分的起点则在 $[mid+1, r]$\n\n总之，要找满足 $nums[mid] <= nums[nums.length-1]$ 的最小值\n\n\n\n第二阶段\n\n找到最小值后，假如最小值的下标是 min ，数组便可以分为有序的两半 $[l, min-1]$  和 $[min, r]$  此时判断 $target<=nums[nums.length-1]$  。\n\n若成立，可以再右半部分中找target，因为target如果在右半部分的话，一定大于 $nums[nums.length-1]$， 那么久应该去左半边 $[l, min-1]$ 中找 target\n\n```java\nclass Solution {\n    public int search(int[] nums, int target) {\n        if(nums==null || nums.length==0) return -1;\n        //1,找出数组中的最小值,即左右两边的分界点，便可以将数组分为有序的左右两边\n        //2,判断target <= nums[nums.length - 1]是否成立\n        //  成立：target在旋转后的右半边\n        //  不成立：target在旋转数组的左半边\n        int l =0 ;\n        int r = nums.length-1;\n        while(r>0 && nums[r]==nums[0]){\n            r--;\n        }\n\n        while(l<r){\n            int mid = l+r >>1;\n            if(nums[mid] <= nums[nums.length-1]){\n                r=mid;\n            }else{\n                l=mid+1;\n            }\n        }\n        //上面while结束后，l = r，都指向旋转数组中的最小值\n        if(target<=nums[nums.length-1]){\n            // target在右边， l本身就是指向右边起点的，不用更新，更新r为右边终点。\n            r = nums.length-1;\n        }else{\n            //target在左半边\n            l = 0;//左半边的起点\n            r--;//让r指向最小值的前一个位置，即左半边的终点\n        }\n        //定好了区间[l,r]后，可以在里面找target了\n        //使用二分模板即可，找满足nums[mid] >= target的最小值\n        while(l<r){\n            int mid = l+r>>1;\n            if(nums[mid] >= target){\n                r=mid;\n            }else{\n                l=mid+1;\n            }\n        }\n        // 判断最终找到的num[l]是否等于target\n        if(nums[l] == target) return l;\n        return -1;\n\n\n    }\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["刷题"]},{"title":"GRAPH-BERT: Only Attention is Needed for Learning Graph Representations","url":"/2021/06/04/GRAPH-BERT-Only-Attention-is-Needed-for-Learning-Graph-Representations/","content":"\n# GRAPH-BERT: Only Attention is Needed for Learning Graph Representations\n\n当前GNNs的主要方法是过度依赖图中的连接关系，这样造成了三大问题。\n\n1. 模型假死 (*suspended animation problem*) : 随着神经网络层数的不断加深，模型对于输入的数据开始不进行反应。这个问题的原因论文没写，个人理解是由于层之间的非线性使得数据分布变换导致梯度消失。\n2. 过平滑 (*over-smoothing problem*) : 由于GNN大多依靠聚合操作 (mean,max,sum) 的信息更新方式，这样随着层的不断堆叠，每个节点都会大量收到其他信息节点的影响，从而使得每个节点的embedding预测趋同。\n3. 难以并行计算：由于内存的限制，尤其是在大型图里面，图中的关联关系难以并行计算。\n\n根据以上问题作者提出了一种新的图神经网络，即Graph-Based BERT，它完全基于注意力机制，不需要任何图卷积或聚集操作。\n\n在模型输入部分，不会把一整个大图输入给模型，而是先采样得到大图的一些无边子图，只是抽取子节点，而不考虑这些节点之间的边关系。这样就解决了GNN不能并行的问题。\n\n传统GNN由于图的结构多样性，不能进行跨任务的预训练工作，但Graph-Bert不考虑边之间的联系，因此并不受限于图结构，可以很好地进行预训练和迁移学习。\n\n$$\ne_j^{(r)} = \\left[ sin(\\frac {WL(v_j)}{10000^{\\frac{2l}{d_h}}}) , cos(\\frac {WL(v_j)}{10000^{\\frac{2l+1}{d_h}}}) \\right]_{l=0}^{[\\frac {d_h}{2}]}\n$$\n\n$$\n\\begin{equation}\\begin{split} \nH^{(l)} &=  \\mathrm{G\\text{-}Transformer}(H^{(l-1)})\\\\ \n&=\\mathrm{softmax}(\\frac{QK^T}{\\sqrt d_h})V + \\mathrm{G\\text{-}Res}(H^{(l-1)},X_i)+ \\mathrm{features...}\n    \\end{split}\\end{equation}\n$$\n\n$$\n\\begin{equation}\\begin{split} \n H^{(l)} &=  \\mathrm{G\\text{-}Transformer}(H^{(l-1)})\\\\ \n&=\\mathrm{softmax}(\\frac{QK^T}{\\sqrt d_h})V + \\mathrm{features...}\n    \\end{split}\\end{equation}\n$$\n\n## Method\n\n### 符号定义\n\n\n\n\n\n![](https://i.loli.net/2021/06/04/lRKbH2Aa71m3peP.png)\n\n### 无边子图采样\n\n\n\n\n\n### 输入节点向量Embedding\n\n\n\n\n\n#### 原始特征embedding\n\n\n\n#### Weisfeiler-Lehman 绝对角色 Embedding\n\n\n\n#### 基于亲密度的相对位置Embedding\n\n\n\n#### 基于相对距离的Hop Embedding\n\n\n\n### Graph Transformer Encoder\n\n\n\n### 预训练任务\n\n\n\n#### 节点原始属性重构\n\n\n\n\n#### 图结构恢复\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"Attention","url":"/2021/05/31/Attention/","content":"\n# 从RNN到LSTM到Transformer再到GNN\n\n\n\n## RNN\n\n\n\n\n\n\n\n## LSTM\n\n\n\n\n\n\n\n## Attention\n\n### 单头注意力\n\n![](https://i.loli.net/2021/05/31/PkepyKTAUWOI3wB.png)\n\n将句子 $S$ 中第 $i$ 个词的隐藏特征 $h$ 从 $l$ 层更新到 $l+1$ 层 :\n$$\n\\begin{equation}\\begin{split} \n h^{l+1}_i &= Attention(Q^lh_i^l, K^lh_j^l, V^lh_j^l)\\\\\n i.e., \\ h_i^{l+1} &= \\sum_{j\\in S} softmax_j(Q^lh^l_i,K^lh_j^l)\n    \\end{split}\\end{equation}\n$$\n$Q^l, K^l , V^l$ 是可学习的线性权重（分别表示注意力计算中的Query，Key，Value）。句子中的每个单词并行执行注意力机制，从而可以一次性获得他们已更新的特征——这是Transformer相对RNNs的另一个加分点，它使得模型能够逐字更新特征。\n\n![](https://i.loli.net/2021/05/31/vx6helFfAStIKj4.png)\n\n### 多头注意力\n\n事实证明，要让这种点积注意力机制起作用是很难的——如果随机初始化处理得不好会使得整个学习过程失去稳定性。我们可以通过并行执行多个注意力“头”并将结果连接起来（现在每个注意力头都有单独的可学习权重）来克服这个问题：\n$$\n\\begin{equation}\\begin{split} \n h^{l+1}_i &= Concat(head_1,...,head_K) O^l\\\\\n head_k &= Attention(Q^{k,l} h_i^l, \\ K^{k,l} h_j^l, \\ V^{k,l}h_j^l)\n    \\end{split}\\end{equation}\n$$\n其中， $Q^{k,l} ,K^{k,l}, V^{k,l}$ 是第 $k$ 个注意力投的可学习权重，而 $O^l$ 是一个向下的投影，可以用以匹配跨层的 $h_i^{l+1}$ 和 $h^l_i$ 的尺寸。此外多头注意力形成多个子空间，可以让模型去关注不同方面的信息。\n\n## Transformer架构\n\n![](https://i.loli.net/2021/05/31/F8obteYNsUVGpPZ.png)\n\n\n\n下面是上文的多头Attention结构，但为什么Transformer的结构为什么是这样的？\n\n注意力机制之后的词的特征可能在不同尺度或重要性上:\n\n1. 这可能是由于某些词在将其他词的特征累加时具有非常集中或非常分散的注意力权重 $w_{ij}$\n\n2. 在单个特征/向量输入级别，跨多个注意力头 (每个可能会以不同的比例输出值) 进行级联可以导致最终向量 $h_i^{l+1}$ 的输入具有一个大范围的值。遵循传统的机器学习思路，在上述流程中增加一个归一化层似乎是合理的选择。\n\n对于上面的两个问题，Transformer用LayerNorm客服了问题2，LayerNorm在特征层级上进行归一化并学习一种仿射变换。  [batchNormalization与layerNormalization的区别](https://zhuanlan.zhihu.com/p/113233908)\n\n对于问题1，通过求特征维度的平方根，来做缩放点积。\n\n在LayerNorm之后，是FF-MLP\n\n是一个控制尺度问题的技巧，具有特殊结构的考虑位置的双层MLP，在多头注意力之后，他们通过一个可学习的权重将 $h_i^{l+1}$  投影到一个更高的维度，在该维度中， $h_i^{l+1}$ 经过ReLU 非线性变换，然后投影回其原始维度，然后再进行另一个归一化操作。\n$$\nh^{l+1}_i = LN (MLP(LN(h_l^{l+1})))\n$$\n不确定超参数化前馈子层背后的确切理由是什么，似乎也没有人对此提出疑问！我认为LayerNorm和缩放的点积不能完全解决突出的问题，因此大型MLP是一种可以相互独立地重新缩放特征向量的手段。\n\nTransformer架构也非常适合非常深的网络，使NLP界能够在模型参数和扩展数据这两方面进行延伸。每个多头注意力子层和前馈子层的输入和输出之间的残差连接是堆叠Transformer层的关键（但为了清楚起见，在上图中省略了）。\n\n\n\n\n\n## GNN\n\n图神经网络（GNNs）或图卷积网络（GCNs）在图数据中建立节点和边的表示。它们是通过邻域聚合（或消息传递）来实现的，在邻域聚合中，每个节点从其邻域收集特征，以更新其周围的局部图结构表示。通过堆叠多个GNN层使得该模型可以将每个节点的特征传播到整个图中，从其邻居传播到邻居的邻居，依此类推。\n\n![](https://i.loli.net/2021/05/31/VgT8NGlDXIxLB3H.png)\n\n以这个表情符号社交网络为例：由GNN产生的节点特征可用于预测性任务，例如识别最有影响力的成员或提出潜在的联系。\n\n在他们最基本的形式中，GNNs通过以下方法来更新节点 $i$ 在 $l$ 层的隐藏层特征 $h$ 。\n\n也就是先将节点自身特征 $h_i^{l}$ 和每个邻居节点 $j \\ \\ (j\\in N(i))$  特征 $h_j^{l}$ 的聚合相加，然后在整体做一个非线性变换， 如下:\n$$\nh_i^{l+1} = \\sigma (U^{l} h_i^l + \\sum_{j\\in N(j)}(V^l h_j^l))\n$$\n其中， $U^l, V^l$ 是GNN 层的可学习权重矩阵。\n\n邻居节点 $j\\in N(i)$ 上的求和可以被其他输入大小不变的聚合函数代替，例如简单的 均值/最大值函数或其他更强大的函数（如通过注意机制的加权和）。\n\n如果是GAT的话其实就变成了Transformer了\n\n![](https://i.loli.net/2021/05/31/YRcsK5h38HoS4On.png)\n\n如果我们要执行多个并行的邻域聚合头，并且用注意力机制（即加权和）替换领域 上的求和 ，我们将获得图注意力网络（GAT）。加上归一化和前馈MLP，瞧，我们就有了Graph Transformer！\n\n\n\n### 在NLP中，句子就是由词全连接而成的图\n\n为了使连接更加清晰，可以将一个句子看作一个完全连接的图，其中每个单词都连接到其他每个单词。现在，我们可以使用GNN来为图（句子）中的每个节点（单词）构建特征，然后我们可以使用它来执行NLP任务。\n\n![](https://i.loli.net/2021/05/31/eV8aQgnZ64rsNyv.png)\n\n广义上来讲，这就是Transformers正在做的事情：Transformers是以多头注意力作为邻聚合函数的GNNs。标准GNNs从其局部邻域节点 $j\\in N(i)$ 聚合特征，而NLP的Transfors 将整个句子视为局部邻域，在每个层聚合来自每个单词 $j\\in S$的特征。 而NLP的Transformers将整个句子视为局部邻域，在每个层聚合来自每个单词 $j\\in S$ 的特征。\n\n重要的是，各种特定于问题的技巧（如位置编码、因果/掩码聚合、学习率表和大量的预训练）对于Transformers的成功至关重要，但在GNN界中却很少出现。同时，从GNN的角度看Transformers可以启发我们摆脱模型结构中的许多花哨的玩意。\n\n\n\n### 全连接图是NLP的最佳输入格式吗？\n\n在统计NLP和ML之前，Noam Chomsky等语言学家致力于发展语言结构的最新理论，如语法树/图。Tree LSTMs已经尝试过这一点，但是也许Transformers/GNNs是可以让语言理论和统计NLP的领域结合得更加紧密的更好的架构？\n\n\n\n###  **如何学习到长期依赖？**\n\n完全连通图使得学习词与词之间非常长期的依赖关系变得非常困难，这是完全连通图的另一个问题。这仅仅是因为图中的边数与节点数成二次方关系，即在n个单词的句子中，Transformer/GNN 将在 $n^2$ 上对单词进行计算，如果n很大，那将会是一个非常棘手的问题。\n\nNLP界对长序列和依赖性问题的看法很有意思，例如，使用注意力机制在输入大小方面稀疏或自适应，在每一层中添加递归或压缩，以及使用对局部性敏感的哈希法进行有效的注意，这些都是 优化Transformers 有希望的想法。\n\n有趣的是，还可以看到一些GNN界的想法被混入其中，例如使用句子图稀疏化的二进制分区似乎是另一种令人兴奋的方法。\n\n![](https://i.loli.net/2021/05/31/k1XmCxvfYMwuNIt.png)\n\n\n\n### Transformers在学习神经网络的句法吗？\n\nNLP界有几篇关于Transformers可能学到什么的有趣论文。其基本前提是，对句子中的所有词对使用注意力机制（目的是确定哪些词对最有趣），可以让Transformers学习特定任务句法之类的东西。\n\n多头注意力中的不同头也可能“关注”不同的句法属性。\n\n从图的角度来看，通过在完全图上使用GNN，我们能否从GNN在每一层执行邻域聚合的方法中恢复最重要的边线及其可能带来的影响？我还不太相信这种观点。\n\n\n\n### **为什么要用多头注意力？为什么要用注意力机制？**\n\n\n\n我更赞同多头机制的优化观点——拥有多个注意力可以改进学习，克服不好的随机初始化。例如，这些论文表明，Transformers头可以在训练后“修剪”或“删除”，并且不会产生重大的性能影响。\n\n\n\n多头邻聚合机制在GNNs中也被证明是有效的，例如在GAT使用相同的多头注意力，MoNet使用多个高斯核来聚合特征。虽然多头技巧是为了稳定注意力机制而发明的，但它能否成为提炼出额外模型性能的标准？\n\n\n\n相反，具有简单聚合函数（如sum或max）的GNNs不需要多个聚合头来维持稳定的训练。如果我们不需要计算句子中每个词对之间的成对兼容性，对Transformers来说不是很好吗？\n\n\n\nTransformers能从抛弃注意力中获益吗？Yann Dauphin和合作者最近的工作提出了另一种ConvNet架构。Transformers也可能最终会做一些类似于ConvNets的事情。\n\n![](https://i.loli.net/2021/05/31/6nlhQ2GLpBSeudq.png)\n\n\n\n### **为什么Transformers这么难训练？**\n\n阅读新的Transformer论文让我觉得，在确定最佳学习率表、预热策略和衰减设置时，训练这些模型需要一些类似于黑魔法的东西。这可能仅仅是因为模型太大，而且所研究的NLP任务非常具有挑战性。\n\n但是最近的结果表明，这也可能是由于结构中归一化和残差连接的特定组合导致的。\n\n在这一点上我很在意，但是也让我感到怀疑：我们真的需要代价昂贵的成对的多头注意力结构，超参数化的MLP子层以及复杂的学习计划吗？\n\n我们真的需要具有大量碳足迹的（译者注：有人提出现在训练一个模型相当于5辆汽车一天的排碳量）大规模模型吗？\n\n具有良好归纳偏差的架构难道不容易训练吗？\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## 参考文献\n\n[为什么说Transformer就是图神经网络？](https://zhuanlan.zhihu.com/p/110805093)\n\n\n\n","tags":["nlp"]},{"title":"Do Multi-Hop Question Answering Systems Know How to Answer the Single-Hop Sub-Questions?","url":"/2021/05/31/Do-Multi-Hop-Question-Answering-Systems-Know-How-to-Answer-the-Single-Hop-Sub-Questions/","content":"\n# Do Multi-Hop Question Answering Systems Know How to Answer the Single-Hop Sub-Questions?\n\n这是一篇比较有意思的工作，但是出发点是多跳阅读理解的本质问题。\n\n多跳QA需要一个模型来检索和整合来自多个段落的信息来回答问题，作者认为现有的评估标准，EM和F1并不能证明在多大程度上学会了多跳推理能力。\n\n所以作者根据多跳QA中的桥接实体生成了一千个相关的子问题，来测试模型的能力，并期望这样能说明一些问题。\n\n## 做法\n\n当设计一个多跳问题时，我们要求模型去检索一系列句子作为证据，然后对他们进行推理来回答问题。作者设计了一个HotpotQA干扰项集，的子问题集，期望模型如果具有了多跳的推理能力，多跳的问题可以回答的话，那么单跳问题也可以回答。但是这个单跳问题不是凭空出现的和原问题不相关的问题。如下图所示：\n\n![](https://i.loli.net/2021/05/31/YEQ2HtyLSrqU1FD.png)\n\n这是一个典型的桥接问题，问题是：罗斯为阿诺德·施瓦辛格饰演的前纽约警探主演的一部电影做宣传是在哪一年？\n\n想要回答问题，我们就必须先知道施瓦辛格在哪个电影里扮演了纽约警探，也就是必须找到桥梁实体 Gold Para2 中的电影《End of Days》才能回答。\n\n那么子问题的建立就很自然的可以分为，\n\n1、施瓦辛格正在哪个电影里扮演了纽约警探？\n\n2、罗斯在那一年为电影《End of Days》做了宣传？\n\n第一个问题的答案正好是桥梁实体，第二个问题的答案是最终答案。\n\n作者认为只有模型能够完整的回答这些问题，说明模型就具备了多跳推理能力。\n\n生成方法是半自动的：\n\n- 首先，我们通过预测断点将每个源问题分解成若干子串\n- 其次，进行post processed，生成两个子问题。使用一些启发式方法从段落中提取子问题的答案。\n- 最后，将生成的候选评价实例发送给人工验证。\n\n## 实验\n\n作者认为有些预测答案，虽然部分匹配EM=0但是语义上是正确的，也应该被算作预测正确。如下这种：\n\n![](https://i.loli.net/2021/05/31/8xBnLoPbdfKmDiI.png)\n\n新的评估标准：给定黄金答案文本跨度 $a_g$ 和预测答案文本 $a_p$，如果满足以下两个要求之一，则它们部分匹配：\n$$\nf1 > 0.8\\\\\nf1>0.6 \\land \\{(a_g\\ contains\\ a_{p} ) \\lor (a_p \\ contains \\ a_g) \\}\n$$\n$f1$ 值大于0.8，直接认为符合要求， 或者，$f1$ 大于0.6 ，且标准答案文本跨度包含了预测文本的答案或预测答案包含了标准答案。\n\n\n\nBaseline 选用开源的CogQA、DFGN、DecompRC\n\n![](https://i.loli.net/2021/05/31/FI9gEo7fLJ3UjBO.png)\n\n实验结果发现CogQA稍微好一些\n\n\n\n![](https://i.loli.net/2021/05/31/jq1aPVeZrJYBEQu.png)\n\n模型有很高的概率没有答对其中一个问题。\n\n作者将这些示例称为模型故障案例：模型故障案例在所有正确回答的多跳问题中所占的百分比被定义为模型故障率。\n\nCogQA PM下的故障率: $(6.1+16.5+3.4)/(40.9+6.1+16.5+3.4) \\times100\\% = 38.86\\%$ \n\n![](https://i.loli.net/2021/05/31/TfhW2lJBSLtbRP3.png)\n\n所评估的所有三个模型都有很高的模型失败率，这表明这些模型学会了回答复杂的问题，而没有探索推理过程的多个步骤。当使用EM和PM分数进行评估时，也会出现同样的现象。\n\n> After analyzing the model failure cases, we ob- serve a common phenomenon that there is a high similarity between the words in the second sub- question and the words near the answer in the con- text. The model has learned to answer multi-hop question by local pattern matching, instead of going through the multiple reasoning steps. For the ex- ample presented in Figure 1, the model may locate the answer “*1999*” for the multi-hop question by matching the surrounding words “ *Guns N Roses*” in the second sub-question. Despite answering the multi-hop question correctly, the model fails to identify the answer of the first sub-question which it is expected to retrieve as a multi-hop QA system.\n\n\n\n\n\n\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"Pytorch RNN之pack_padded_sequence()和pad_packed_sequence()","url":"/2021/05/29/Pytorch-RNN之pack-padded-sequence-和pad-packed-sequence/","content":"\n# Pytorch RNN之pack_padded_sequence()和pad_packed_sequence()\n\n\n\n## 为什么有pad和pack操作？\n\n先看一个例子，这个batch中有5个sample\n\n![](https://i.loli.net/2021/05/29/AZFV4WzLUKsgpfa.png)\n\n如果不用pack和pad操作会有一个问题，什么问题呢？\n\n比如上图，句子“Yes”只有一个单词，但是padding了多余的pad符号，这样会导致LSTM对它的表示通过了非常多无用的字符，这样得到的句子表示就会有误差，更直观的如下图：\n\n![](https://i.loli.net/2021/05/29/hT5ab7rQgunimtB.png)\n\n\n\n那么我们正确的做法应该是怎么样呢？\n\n在上面这个例子，我们想要得到的表示仅仅是LSTM过完单词\"Yes\"之后的表示，而不是通过了多个无用的“Pad”得到的表示：如下图：\n\n![](https://i.loli.net/2021/05/29/9efQa4sVrN8ulFd.png)\n\n\n\n\n\n## torch.nn.utils.rnn.pack_padded_sequence()\n\n这里的`pack`，理解成压紧比较好。 将一个 填充过的变长序列 压紧。（填充时候，会有冗余，所以压紧一下）\n\n其中pack的过程为：（注意pack的形式，不是按行压，而是按列压）\n\n![](https://i.loli.net/2021/05/29/KTaQHmkbOIC3h8x.png)\n\n​                                      （下面方框内为`PackedSequence`对象，由data和batch_sizes组成）\n\npack之后，原来填充的 PAD（一般初始化为0）占位符被删掉了。\n\n输入的形状可以是(T×B×* )。`T`是最长序列长度，`B`是`batch size`，`*`代表任意维度(可以是0)。如果`batch_first=True`的话，那么相应的 `input size` 就是 `(B×T×*)`。\n\n`Variable`中保存的序列，应该按序列长度的长短排序，长的在前，短的在后。即`input[:,0]`代表的是最长的序列，`input[:, B-1]`保存的是最短的序列。\n\n> `NOTE：` 只要是维度大于等于2的`input`都可以作为这个函数的参数。你可以用它来打包`labels`，然后用`RNN`的输出和打包后的`labels`来计算`loss`。通过`PackedSequence`对象的`.data`属性可以获取 `Variable`。\n\n参数说明:\n\n- input (Variable) – 变长序列 被填充后的 batch\n- lengths (list[int]) – `Variable` 中 每个序列的长度。\n- batch_first (bool, optional) – 如果是`True`，input的形状应该是`B*T*size`。\n\n返回值:\n\n一个`PackedSequence` 对象。\n\n## torch.nn.utils.rnn.pad_packed_sequence()\n\n填充`packed_sequence`。\n\n上面提到的函数的功能是将一个填充后的变长序列压紧。 这个操作和pack_padded_sequence()是相反的。把压紧的序列再填充回来。填充时会初始化为0。\n\n返回的Varaible的值的`size`是 `T×B×*`, `T` 是最长序列的长度，`B` 是 batch_size,如果 `batch_first=True`,那么返回值是`B×T×*`。\n\nBatch中的元素将会以它们长度的逆序排列。\n\n参数说明:\n\n- sequence (PackedSequence) – 将要被填充的 batch\n- batch_first (bool, optional) – 如果为True，返回的数据的格式为 `B×T×*`。\n\n返回值: 一个tuple，包含被填充后的序列，和batch中序列的长度列表\n\n## 例子\n\n![](https://i.loli.net/2021/05/29/ok9UMKLO5RYAlWP.png)\n\n![](https://i.loli.net/2021/05/29/rkuqZlphAoPFyi7.png)\n\n此时PackedSequence对象输入RNN后，输出RNN的还是PackedSequence对象 \n\n## 参考\n\nhttps://www.cnblogs.com/lindaxin/p/8052043.html\n\nhttps://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence\n\nhttps://zhuanlan.zhihu.com/p/34418001?edition=yidianzixun&utm_source=yidianzixun&yidian_docid=0IVwLf60\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["pytorch"]},{"title":"Multi-hop Attention Graph Neural Networks","url":"/2021/05/27/Multi-hop-Attention-Graph-Neural-Networks/","content":"\n# Multi-hop Attention Graph Neural Networks\n\nGAT中的attention运算只能关注节点相连节点表达，这种机制不考虑不直接相连但又有很重要信息的节点表达。\n\n所以提出了多跳注意力图神经网络(MAGNA)，这是一种将多跳上下文信息融入到注意力计算的每一层的方法。\n\n其将注意力分数分散到整个网络，相当于增加了每一层的GNN的“感受野”。\n\n![](https://i.loli.net/2021/05/27/xCrtn9LhXwiZ7sK.png)\n\n如左图，考虑A和D节点，普通的attention层只计算直接相连节点的注意力分数，如 $ \\alpha_{A,D} $   , 但如果C的信息很重要，   $ \\alpha_{C,D}=0 $  关注度却为0。并且，单个GAT层中A和D节点之间的运算只依赖于自己的表达，而不依赖于它们的图邻域上下文。其相当于每一层只关注了一阶邻居范围的感受野，虽然堆叠多层GNN可以扩大这个范围，但GNN层数一多就会有过平滑的问题。\n\n再看右图，MAGNA 层的改进方法是\n\n- 通过扩散多跳注意力捕捉  $ \\alpha_{D,C}' $  表达为  $ \\alpha_{D,C}' = f([\\alpha_{B,C},\\alpha_{D,B}]) $\n- 基于图邻接矩阵的权值，通过分散注意力来考虑节点之间的所有路径，从而增强图结构学习。MAGNA利用D的节点特征进行A和B之间的注意力计算，这意味着MAGNA中的两跳注意力是基于上下文的。\n\n总之，GAT中的一跳注意机制限制了探索更广泛的图形结构与注意权重之间关系的能力。\n\n\n\n本文提出了多跳注意图神经网络(MAGNA)，这是一种针对图结构数据的有效的多跳自注意机制。Magna使用了一种新颖的图形注意力扩散层(图1)，其中我们首先计算边上的注意力权重(用实心箭头表示)，然后使用边上的注意力权重通过注意力扩散过程计算断开的节点对之间的自我注意力权重(虚线箭头)。\n\n## 方法\n\n\n\n### 参数定义\n\n图 $G =(V,E)$ , $E\\in V\\times V$ ,  V 节点集有 $N_n$ 个 ，E 边集有 $N_e$ 个\n\n节点 v 到其类型的映射为 $ \\phi: V \\rightarrow \\Tau $ , 边e 到其关系类型的映射 $\\psi : E \\rightarrow R$\n\n节点的embedding : $X \\in \\mathbb{R}^{N_n\\times d_n}$ , 边的embedding: $R\\in \\mathbb{R}^{N_r\\times d_r}$\n\n其中 $N_n = |V|, N_r=|R|$ , $d_n,d_r$ 是节点和边类型的embedding维度。\n\nEmbedding 的每行 $x_i = X[i:]$ 表示节点 $v_i (1\\le i\\le N_n)$ 的embedding ， $r_j=R[j:] ,  r_j(1\\le j\\le N_r)$  \n\n首先看一下MAGNA 模块的整体结构\n\n![](https://i.loli.net/2021/05/27/sd32RDYOLyvK64l.png)\n\n有点像Transformer block，现在GNN的包装越来越往Transformer based模型上靠了。\n\n他传入节点和关系embedding，会首先经历一个对于每个节点的多头注意力层（这里和GAT一样），然后是注意力扩散、 Layer Norm、前向传播层和两个残差链接。\n\n \n\n\n\n### Multi-hop Attention Diffusion\n\nAttention diffusion是每一层中用于计算MAGNA‘s的attention分数。首先第一阶段，计算每一条边上的attention分数。第二阶段，用扩散注意力计算多条邻居的注意力。\n\n#### Edge Attention Computation.\n\n在每一层 $l$ 处，为每个三元组 $(v_i，r_k，v_j)$ 计算矢量消息。 为了计算在 $l+1$ 层的表示，将关联的三元组的所有消息聚合成一条消息，然后使用该消息更新 $v_j^{l+1}$。\n\n在第一阶段， 一个边 $(v_i,r_k,v_j)$ 的注意力分数是由以下计算而来：\n$$\ns_{i,k,j}^{(l)} = LeakyRelu(v_a^{(l)} tanh(W_h^{(l)} h_i^{(l)} || W_t^{(l)}h_j^{(l)} || W_r^{(l)}r_k))\n$$\n$ W_h^{(l)} , W_t^{(l)}\\in \\mathbb{R}^{d^{(l)}\\times d^{(l)}} , W_r^{(l)}\\in \\mathbb{R}^{d^{(l)}\\times d_r} , v_a^{(l)}\\in \\mathbb{R }^{1\\times 3d^{(l)}}$   可共享的可训练参数。\n\n$h_i^{(l)}\\in \\mathbb{R}^{d^{(l)}}$ 是第 $l$ 层第 $i$ 个节点的embedding。 $h_i^{(0)} = x_i$\n\n$r_k (1\\le k \\le N_r)$ 是可训练的第 $k$ 个关系类型的embedding\n\n将上式应用到graph中的每一条边后，得到 attention score matrix $ S^{(l)}$:\n$$\nS^{(l)}_{i,j} =\n\\begin{cases}\ns_{i,j,k}^{(l)}, &\\ (v_i,r_k,v_j)\\ appears\\ in\\ G\\\\\n\\infty, &otherwise\n\\end{cases}\n$$\n随后，我们通过对得分矩阵 $S^{(l)}$ 执行逐行Softmax来获得注意力矩阵 $A^{(l)}_{i,j} = Softamax(S^{(l)})$\n\n$A^{(l)}_{i,j}$ 就定义为在第 $l$ 层中当 从节点 $j$ 和 节点 $i$ 聚合消息时的关注值。\n\n这里其实和GAT差不多 只是多了不同种边和节点。 \n\n#### Attention Diffusion for Multi-hop Neighbors\n\n通过以下注意力扩散过程，在网络中将计算不直接连接的节点之间的注意力。\n\n该过程基于1-hop 注意力矩阵A的幂 为：\n$$\nA = \\sum^{\\infty}_{i=0}\\theta_iA^i \\ \\ \\ \nwhere \\sum_{i=0}^{\\infty}\\theta_i = 1 \\ and \\ \\theta_i \\gt 0\n$$\n其中 $\\theta_i$ 是 attention decay factor 并且 $\\theta_i \\gt \\theta_{i+1}$ ，\n\n注意矩阵的幂 $A^i$ 给出了从节点 $h$ 到节点 $t$ 的长度为 $i$ 的关系路径的数量，从而增加了注意的感受野。\n\n重要的是，该机制允许两个节点之间的注意力不仅取决于它们之前的层表示，而且还考虑到节点之间的路径，从而有效地在不直接连接的节点之间创建 attention shotcuts\n\n在实现过程中，作者使用几何分布 (geometric distribution)    $θ_i=α(1−α)^i$，其中 $α∈(0，1]$  。\n\n该选择基于the inductive bias ，即较远的节点在消息聚合中应该被较小的权重，并且具有到目标节点的不同关系路径长度的节点以独立的方式被顺序加权。\n\n此外，请注意，如果定义$θ_0=α∈(0，1] ，A_0=I$ ，则上面的公式。利用关注矩阵A和移动概率 $α$ ，给出了图上的[Personal Page Rank](https://blog.csdn.net/likeyou1314918273/article/details/106895794/)。因此，扩散注意力权重 $A_{i,j}$ 可以看作是节点 $j$ 对节点 $i$ 的影响。 \n\n同时对与目标节点关系路径长度不同的节点权重应该相互独立。因此，本文定义了基于特征聚合的graph attention diffusion：\n$$\nAttDiff(G,H^{(l)}, \\Theta) = A H^{(l)}\n$$\n 其中 $\\Theta$ 为注意力参数集合。 \n\n#### Approximate Computation for Attention Diffusio\n\n对于大图，公式（3）的计算开销巨大，而DAGCN需要通过 $AH^l$进行信息聚合，本文通过定义一个数列 $Z^K$, 当 $K \\rightarrow \\infty$时，该数列能收敛到$AH^l$的值：\n$$\nZ^0 = H^L, Z^{k+1} = (1-\\alpha)AZ^{(k)} + \\alpha Z^0 \\\\\nlim_{K\\rightarrow \\infty} Z^{K} = AH^{l}\n$$\n证明请参考原文。上述的近似使得attention的复杂度保持在$O(|E|)$。很多真实世界网络具有小世界（small-world ）特征，在这种情况下，较小的K值就足够。对于具有较大直径的图，选择较大的K和较小 $\\alpha$ 。\n\n\n\n### Multi-hop Attention based GNN Architecture\n\n图2提供了可多次堆叠的MAGNA block 的架构概览。\n\n#### Multi-head Graph Attention Diffusion Layer\n\n在不同的视角联合关注来自不同表示子空间的信息。\n$$\n\\begin{equation}\\begin{split} \n \\hat H^{(l)} &= MultiHead(G, \\hat H^{(l)}) =(||_{i=1}^M head_i) W_o \\\\\nhead_i &=  AttDiff(G, \\hat H^{(l)}, \\Theta_i) \\\\\n\\hat H^{(l)} &= LayerNorm(H^{(l)})\n    \\end{split}\\end{equation}\n$$\n方程中以递归的方式计算注意力扩散。增加了层归一化，有助于稳定递归计算过程。\n\n#### Deep Aggregation\n\n此外，还包含一个完全连接的前馈子层，它由两层前馈网络组成。我们还在两个子层中添加了层标准化和残差连接，从而为每个block提供了更具表现力的聚合步骤\n$$\n\\begin{equation}\\begin{split} \n \\hat H^{(l+1)} &= \\hat H^{(l)} + H^{(l)} \\\\\n H^{(l+1)} &= W_2^{(l)} ReLU(W_1^{(l)} LayerNorm(\\hat H^{(l+1)})) + \\hat H^{(l+1)}\n    \\end{split}\\end{equation}\n$$\n\n\n## 实验\n\n![](https://i.loli.net/2021/05/28/rVp3kq5IsGweYgR.png)\n\n\n\n![](https://i.loli.net/2021/05/28/ZzVJehsHGlPoFiY.png)\n\n\n\n## Reviewer\n\n> The central question of the reviewers' discussion was whether the contribution of this paper was significant enough or too incremental. The discussion emphasized relevant literature which already considers multi-hop attention (e.g. https://openreview.net/forum?id=rkKvBAiiz [Cucurull et al.], https://ieeexplore.ieee.org/document/8683050 [Feng et al.], https://arxiv.org/abs/2001.07620 [Isufi et al.]), and which should have served as baseline. In particular, the experiment suggested by R3 was in line with some of these previous works, which consider \"a multi-hop adjacency matrix \" as a way to increase the GAT's receptive field. This was as opposed to preserving the 1-hop adjacency matrix used in the original GAT and stacking multiple layers to enlarge the receptive field, which as noted by the authors, may result in over-smoothed node features. The reviewers acknowledged that there is indeed as slight difference between the formulation proposed in the paper and the one in e.g. [Cucurull et al.]. The difference consists in calculating attention and then computing the powers with a decay factor vs. increasing the receptive field first by using powers of the adjacency matrix and then computing attention. Still, the multi-hop GAT baseline of [Cucurull et al.] could be extended to use a multi-hop adjacency matrix computed with the diffusion process from [Klicpera 2019], as suggested by R3. In light of these works and the above-mentioned missing baselines, the reviewers agreed that the contribution may be viewed as rather incremental (combining multi-hop graph attention with graph diffusion). The discussion also highlighted the potential of the presented spectral analysis, which could be strengthened by developing new insights in order to become a stronger contribution (see R2's suggestions).\n\n>Proposed methodology being more powerful than GAT is arguable:\n>When the attention scores for indirectly connected neighbors are still computed based on the immediate neighbors' attention scores, it is not convincing enough to be argued as more powerful than GAT, which learns attention scores over contextualized immediate neighbors.Also, the approximate realization of the model described in Eqn: 5 follows a message-passing style to propagate attention scores. Suppose it is to be argued that standard message-passing-based diffusion is not powerful enough to get a good immediate neighbor representation that encodes neighbors' information from far away. In that case, it is not immediately clear how a similar diffusion, when used for propagating attention scores from immediate neighbors to neighbors multiple hops away, will be more powerful. \n\n\n\n","tags":["GNN"]},{"title":"GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training","url":"/2021/05/24/GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training/","content":"\n# GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training\n\n一个自监督的利用对比学习来学习GNN内在可迁移的先验知识的预训练框架，目的是想得到一个可迁移性强，迁移领域广的，通用的表达。为了捕捉跨多个网络的通用网络拓扑特性，预训练任务为，跨网络子图实例判别。所以预训练的重点在结构相似性层面上，并且不带节点属性。\n\n\n\n## 基本思想\n\n基本思想是对输入图中的实例进行采样，将每个实例视为自己的一个不同的类，并学习对这些实例进行编码和区分。\n\n具体地说，作者认为GCC需要回答三个问题，才能学习到可转移性好的结构模式：\n\n- 实例是什么？\n- 分辨规则是什么？\n- 如何对实例进行编码？\n\n对于者三个问题作者展开研究。\n\n## 预训练任务——子图实例判别\n\n任务的目标是根据顶点的局部结构来区分它们。\n\n![](https://i.loli.net/2021/05/24/vH3s8Rm45liJPxh.png)\n\n对于每个顶点，从它的多跳ego网络中抽取子图作为实例。\n\nGCC的目的是区分从某个顶点采样的子图和从其他顶点采样的子图。并且不假设顶点和子图来自同一个图，所以图编码器被迫捕获不同输入图的通用模式。\n\n\n\n### 定义子图实例\n\n对比学习框架的成功很大程度上取决于数据实例的定义。CV和NLP任务可以直接将实例定义为图像或句子。\n\n 但是，这些想法不能直接扩展到图形数据，因为图形中的实例没有明确定义。\n\n因为节点是无属性的节点，所以要表示一个节点，就要采用以他为中心的局部结构。\n\n具体地说，对于某个顶点v，定义一个实例为它的r-ego网络：\n\n对于一个 r-ego 网络 $G = (V,E)$ ，$V$ 是节点集并且 $E \\subseteq V\\times V$\n\n对于其中心点v， 他的 r-neighbors 定义为 $S_v=\\{u:d(u,v)\\le r\\}$ , 其中 $d(u,v)$ 为邻居u节点到v节点的最短路径。\n\n顶点v的r-ego图，记为 $G_v$，是由 $S_v$ 引出的子图。 下图就是一个2-ege图，右边是预训练过程。\n\n![](https://i.loli.net/2021/05/25/Epkna1vfKVIU4hG.png)\n\n### 定义实例相似性判别准则\n\n在cv中，同一图像的两个随机数据增加(例如，随机裁剪、随机调整大小、随机颜色抖动、随机翻转等)被视为相似的实例对。\n\n在GCC中将同一r-ego网络的两个随机数据扩充看作一个相似实例对，并将数据扩充定义为图采样。\n\nGCC的图采样遵循三个步骤\n\n- 重新启动的随机行走(RWR) ：从ego图的节点v出发，随机采样子图结构，并以一定概率返回到v节点。得到的采样子图可以被认为是一种数据扩增，像cv那样。\n- 子图归纳：导出子图随机游走抽样(ISRW)。\n- 匿名化：匿名化被采样的子图 $\\hat G_v$ ,并重新排序。\n\n\n\n### 定义图编码器\n\n给定两个采样子图 $x^q$ 和 $x^k$，GCC分别通过两个图神经网络编码器 $f_q$ 和 $f_k$ 对其进行编码。从技术上讲，任何图神经网络都可以作为这里的编码器，而GCC模型对不同的选择并不敏感。因为不考虑节点属性，而大多数GNN模型需要把节点特征/属性作为输入。为了弥补这一差距，作者建议利用每个采样子图的图结构来初始化顶点特征。\n\n目标Loss采用对比学习经典的InfoNCE:\n$$\nL = -log \\frac{exp(q^Tk_+/\\tau)}{\\sum_{i=0}^K exp(q^Tk_i/\\tau)}\n$$\n其中 $q = f_q(x^q) , k=f_k(x^k)$ ，$q$ 是query 对应的目录为 $K+1$ 个编码的keys: $\\{k_0,...,k_K\\}$\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["GNN"]},{"title":"Leetcode63_DP走方格","url":"/2021/05/23/Leetcode63-DP走方格/","content":"\n# Leetcode63_DP走方格\n\n题目：https://leetcode-cn.com/problems/unique-paths-ii/\n\n## 状态定义\n\n$dp[i][j]$ 表示走到格子 $(i,j)$ 的路径数\n\n## 状态计算\n\n如果网格 $(i,j)$ 上有障碍物，则 $dp[i][j]$ 值为0，走到这个格子的方法数为0\n\n否则网格 $(i,j)$ 可以从网格 $(i-1,j)$ 或者网格 $(i,j-1)$ 走过来，因此走到该格子的方法数为走到网格 $\n(i-1,j)$ 和网格$(i,j-1)$ 的方法数之和，即: $dp[i,j] = dp[i-1,j] + dp[i,j-1]$\n$$\ndp[i][j] = \n\\begin{cases}\ndp[i-1,j] + dp[i,j-1], & \\text{(i,j) 上无障碍物}  \\\\\n0, & \\text{(i,j)上有障碍物}\n\\end{cases}\n$$\n\n## 初始条件\n\n第 1 列的格子只有从其上边格子走过去这一种走法，因此初始化 $dp[i][0]$ 值为 1，存在障碍物时为 0；\n\n第一行的格子只有从其左边格子走过去这一种走法，因此初始化$dp[0][j]$ 值为1，存在障碍物事为0；\n\n```java\nint[][] dp = new int[m][n];\nfor (int i = 0; i < m && obstacleGrid[i][0] == 0; i++) {\n    dp[i][0] = 1;\n}\nfor (int j = 0; j < n && obstacleGrid[0][j] == 0; j++) {\n    dp[0][j] = 1;\n}\n\n```\n\n## 具体实现\n\n```java\npublic int uniquePathWithObstacles(int[][] grid){\n  \tif (obstacleGrid == null || obstacleGrid.length == 0) {\n       return 0;\n    }\n\t\tint m=grid.length;\n  \tint n=grid[0].length;\n  \t\n  \tint[][] dp=new int[m][n];\n  \tfor(int i=0;i<m && grid==0;i++){\n       dp[i][0] = 1;\n    }\n  \tfor(int i=0;i<n && grid==0;i++){\n      \tdp[0][j] = 1;\n    }\n  \n  \tfor(int i=1;i<m;i++){\n      \tfor(int j=1;j<n;j++){\n          if(grid[i][j]==0){\n            dp[i][j] = dp[i-1][j] + dp[i][j-1];\n          }\n        }\n    }\n  return dp[m-1][n-1];\n}\n```\n\n同理没有障碍物就删掉判断条件。\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["DP"]},{"title":"Dynamically Fused Graph Network for Multi-hop Reasoning","url":"/2021/05/22/Dynamically-Fused-Graph-Network-for-Multi-hop-Reasoning/","content":"\n# Dynamically Fused Graph Network for Multi-hop Reasoning\n\n受人类分步推理行为的启发，提出了动态融合图网络(DFGN)，回答那些需要多个分散证据并在这些证据上进行推理的问题。不依赖于任何额外的预定义知识基础，能回答开放领域中的问题。\n\n## 大体过程\n\n![](https://i.loli.net/2021/05/22/OzWcZmVIGBq9yut.png)\n\n给出了一个问题和三个段落。DFGN通过从多个段落构造实体图，预测动态掩码选择子图，沿着图传播信息，最后将图中的信息传回文本来定位答案，从而对事实进行多步推理。节点是实体引用，带颜色节点表示潜在实体。边由共现关系构造而成，每一步都由DFGN选择灰色圆圈内的子图来处理。\n\n## 挑战\n\n1. 由于并不是每个文档都包含相关信息，基于多跳文本的问答需要从多个段落中滤除噪声并提取有用信息。\n\n> 作者思路: 通过DFGN这种多轮迭代的动态实体图来解决，如上图DFGN每一轮都通过掩码预测模块在动态图上生成和推理，其中不相关的实体被屏蔽，只有推理远被保留也就是灰色圆圈内的子图，这种做法缓解了误差的传播问题。\n>\n> 此外，DFGN的预测mask的过程可隐含地导出推理链，可以解释推理结果。针对开放域语料库基本真值推理链难以定义和标注的问题，提出了一种可行的弱监督掩码学习方法。提出了一种新的度量来评估预测推理链和构建的实体图的质量。\n\n但这样做有用的信息被mask了怎么办？怎么确定的mask范围？其实是用了注意力机制计算实体的权重，后文写。\n\n2. 不能直接从实体图中提取出答案，现实中，答案可能不在所提取的实体图的实体中。\n\n> 作者思路：在DFGN中设计了一个fusion处理模块，不仅将信息从文档聚合到实体图(Doc2graph)，还将实体图的信息传播回文档表示(Raph2doc)。通过文档token和实体在每一跳迭代地执行融合过程，然后从文档令牌获得最终结果答案。Doc2graph和Graph2doc的融合过程以及动态实体图共同改善了文档信息和实体图之间的交互性，从而减少了噪声，从而提高了答案的准确性。\n\n相当于在tokens的表达中加入了推理图中的推理信息，这个思想还是挺不错的。\n\n## 具体过程\n\n模仿人类对QA的推理过程。从查询感兴趣的实体开始，聚焦于开始实体周围的单词，连接到在邻居中发现的或由相同表面信息链接的某些相关实体，重复该步骤以形成推理链，并且落在可能是答案的某个实体或片段上。\n\nDFGN包含五个组件：\n\n![](https://i.loli.net/2021/05/22/DMpXHmNs6PYVbad.png)\n\n- 段落选择子网络\n- 实体图构建模块\n- 编码层\n- 用于多跳推理的融合模块\n- 最终预测层\n\n\n\n### Fusion Block\n\n这里只着重写一下Fusion Block\n\n在为查询Q和上下文C计算嵌入后，剩下的挑战是如何识别支持实体和潜在答案的文本跨度。\n\nFusion Block从 $Q_0$和 $C_0$ 开始，寻找一步支持实体。\n\n1. 通过计算实体嵌入从tokens将信息传递到实体(Doc2Graph Flow)\n2. 在实体图上进行信息传递\n3. 传递信息从实体图到文本tokens(Graph2Doc flow)\n\n![](https://i.loli.net/2021/05/22/zlremMsBCHVwILJ.png)\n\n#### Doc2Graph Flow\n\n由于通过NER工具识别每个实体，因此利用与实体相关联的文本跨度来计算实体嵌入(Doc2Graph)。\n\n为此，作者定义了一个 M 是01矩阵，$M_{i,j}$ 的值意思是，如果第 $i$ 个token在第j个实体的范围内，则 $M_{i,j}=1$\n\n这个 M 用于选择与实体相关联的文本范围。这其实是一个池化操作，将上下文C 的嵌入变成了实体E的嵌入 。\n\n$E_{t-1} = [e_{t-1,1},...,e_{t-1,N}] \\in R^{2d_2\\times N}$  这个模块作者定位 Tok2Ent，就是上图的左边部分。\n\n\n\n#### Dynamic Graph Attention\n\n然后是图中的中间部分，动态图注意力部分。\n\n在从输入上下文Ct−1获得实体嵌入后，我们应用图神经网络将节点信息传播给它们的邻居。我们提出了一种动态图注意机制来模仿人类的循序渐进的探索和推理行为。与q越相关，邻居节点从附近接收的信息越多。\n\n首先通过在实体上创建 Soft Mask 来识别与查询相关的节点。它充当信息看门人，即只允许与查询有关的那些实体节点传播信息。\n\n使用查询嵌入和实体嵌入之间的注意力网络来预测 Soft Mask $m_t$，其目的是表示第 t 个推理步骤中的开始实体：\n$$\n\\begin{equation}\\begin{split} \n \\hat q^{(t-1)} &= MeanPooling(Q^{t-1}) \\\\\n \\gamma^{(t)}_i &= \\frac {\\hat q^{(t-1)} V^{(t)} e_i^{(t-1)} }{\\sqrt{d_2}}\\\\\n m^{(t)} &= \\sigma([\\gamma^{(t)}_1,...,\\gamma_{N}^{(t)}]) \\\\\n \\hat E^{(t-1)} &= [m_1^{(t)}e_1^{(t-1)} ,..., m^{(t)}_N e_{N}^{(t-1)}]\n    \\end{split}\\end{equation}\n$$\n其实就是用注意力机制计算每个实体嵌入的权重。$V_t$ 是线性映射矩阵。\n\n总之就是通过Soft Mask，得到想要的开始推理的实体，并将它送入图中初始化。噪声信息不放入图中，相当于过滤掉。\n\n> 此外，作者引入一个弱监督信号来诱导每个 Fusion Block 处的软掩码来匹配启发式掩码。对于每个训练案例，启发式掩码包含从查询中检测到的开始掩码，并且通过对相邻矩阵应用广度优先搜索(BFS)获得的附加BFS掩码给出开始掩码。然后，将预测的软掩码和启发式之间的二进制交叉熵损失添加到目标。（跳过那些无法从查询中检测到起始掩码的情况）。\n\n在送入图后的信息聚合方式是使用的GAT，但有一点作者和以前的GAT不同，\n\n在Dynamic Graph Attention中，每个节点隐层的列进行求和，形成一个新的实体状态，其中包含它从邻居收到的全部信息：\n$$\ne_i^{(t)} =ReLU(\\sum_{j\\in B_i} \\alpha_{j,i}^{(t)} h_j^{(t)}) \n$$\n其中 $B_i$ 是邻居实体集合中的第 i 个实体，所以一次更新后的实体表达为 $E^{(t)} =[e_1^{(t)},...,e_N^{(t)}] $\n\n#### Updating Query\n\n一条推理链包含多个步骤，每一步新访问的实体就是下一步的起始实体。\n\n为了预测下一步期望的起始实体，引入了一种Updating Query机制，通过当前步骤的实体嵌入来更新查询嵌入。\n$$\nQ^{(t)} = Bi-Attention(Q^{(t-1)},E^{(t)})\n$$\n\n#### Graph to Document Flow\n\n利用Tok2Ent和动态图关注度，实现了实体级的推理步骤。然而，不受限制的答案仍然无法追溯。\n\n为了解决这个问题，开发了一个Graph2Doc模块来保持信息从实体回流到上下文中的tokens。因此，与答案有关的文本跨度可以在上下文中本地化。\n\n使用Doc2Graph Flow中一样的M矩阵，将$C_{t-1}$ 中的先前tokens嵌入和 对应于该令牌的关联实体嵌入对应回来。\n\nM中的每一行对应一个令牌，因此如果该令牌出现在实体的提及中，就使用它从 $E_t$ 中选择一个实体的嵌入。利用LSTM层进一步处理该信息，以产生下一级上下文表示： $C^{(t)} = LSTM([C^{(t-1)}, ME^{(t)T}])$\n\n\n\n\n\n### Prediction\n\n有四个输出维度，1.支持句，2.答案的开始位置，3.答案的结束位置，4.答案的类型。\n\n使用四个同构的LSTM $F_i$ 是逐层堆叠的。最后Fusion Block的上下文表示被发送到第一个LSTM $F_0$。每个$F_i$输出的logits为$  O∈R^{m×d2}$   ，并计算这些logit上的交叉熵损失。\n\n![](https://i.loli.net/2021/05/22/6ov3shgtbaNZWDP.png)\n\n\n\n## 实验\n\n![](https://i.loli.net/2021/05/22/udk43YL7pFQDCZX.png)\n\n\n\n### 作者提出的推理链质量的衡量标准\n\nESP(实体级支持)分数\n\n> 推理链是实体图上的一条有向路径，因此高质量的实体图是良好推理的基础。由于NER模型的精度有限和图结构的不完备性，31.3%的发展集中的情况不能进行完整的推理过程，其中至少有一个支持语句不能通过实体图到达，即在这个句子中没有实体被NER模型识别。我们将这类情况命名为“缺失支撑实体”，这种情况的比率可以用来评价图的构造质量。\n> 下面，在给出ESP(实体级支持)分数之前，我们首先给出几个定义。\n\n![](https://i.loli.net/2021/05/22/Eft6vqQFXSKu7ek.png)\n\n\n\n### 案例分析\n\n![](https://i.loli.net/2021/05/22/ryUW4c5s7VbQwKx.png)\n\n\n\n\n\n\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"120三角形最小路径和","url":"/2021/05/22/120三角形最小路径和/","content":"\n# 120三角形最小路径和\n\n## 题目描述\n\n```\n给定一个三角形，找出自顶向下的最小路径和。\n每一步只能移动到下一行中相邻的结点上。\n\n相邻的结点 \n在这里指的是 下标 与 上一层结点下标 相同或者等于 上一层结点下标 + 1 的两个结点。\n\n \n\n例如，给定三角形：\n\n[\n     [2],\n    [3,4],\n   [6,5,7],\n  [4,1,8,3]\n]\n自顶向下的最小路径和为 11（即，2 + 3 + 5 + 1 = 11）。\n\n \n\n说明：\n\n如果你可以只使用 O(n) 的额外空间（n 为三角形的总行数）来解决这个问题，\n```\n\n## 分析\n\n若定义 f(i,j) 为 (i, j)点到底边的最小路径和，则递归求解式为：\n$$\nf(i,j) = mian(f(i+1,j) , f(i+1,j+1)) + triangle[i][j]\n$$\n由此，将任一点到底边的最小路径和，转化为了与该点相邻两点到底边的最小路径和中的较小值，再加上该点本身的值。\n\n得出(解一) 递归解法。\n\n### 解一（递归）\n\n```java\npublic int minimumTotal(List<List<Integer>> tri){\n  return dfs(tri,0,0);\n}\nprivate int dfs(List<List<Integer>> tri, int i, int j){\n  if (i==tri.size()) return 0;\n  return Math.min(dfs(tri,i+1,j), dfs(tri,i+1, j+1)) + tri.get(i).get(j);\n}\n```\n\n缺点：暴力搜索会有大量重复计算，引出解法二，结合记忆化数组进行优化。\n\n### 解法二：(递归+记忆化)\n\n定义一个二位数组用来记忆化\n\n```java\nInteger[][] memo;\n\npublic int minimumTotal(List<List<Integer>> tri){\n  memo = new Integer[tri.size()][tri.size()];\n  return dfs(tri,0,0);\n}\nprivate int dfs(List<List<Integer>> tri, int i, int j){\n  if(i==tri.size()) return 0;\n  if(memo[i][j]!=null) return memo[i][j];\n  return memo[i][j] = Math.min(dfs(tri, i+1, j), dfs(tri, i+1, j+1)) + tri.get(i).get(j);\n}\n```\n\n时间复杂度：$O(N^2)$，N为三角形的行数。\n空间复杂度：$O(N^2)$，N为三角形的行数。\n\n### 动态规划\n\n定义二维 dp 数组，将解法二中「自顶向下的递归」改为「自底向上的递推」。\n\n状态表示：$dp[i][j]$ 表示从点 $(i,j)$ 到底边的最小路径和\n\n状态计算：$dp[i][j] = min(dp[i+1][j] , dp[i+1][j+1]]) + tri[i][j]$\n\n```java\npublic int minmumTotal(List<List<Integer>> tri){\n  int n = tri.size();\n  // dp[i][j] 表示从点(i,j)到底边的最小路径和\n  int[][] dp = new int[n+1][n+1];\n  // 从三角形的最后一行开始递推\n  for(int i=n-1;i>=0;i++){\n    for(int j=0;j<=i;j++){\n      dp[i][j]=Math.min(dp[i+1][j], dp[i+1][j+1]) + tri.get(i).get(j);\n    }\n  }\n  return dp[0][0];\n}\n```\n\n时间复杂度：$O(N^2)$，N为三角形的行数。\n空间复杂度：$O(N^2)$，N为三角形的行数。\n\n\n\n### 空间优化\n\n上一个解定义了一个N行N列的 dp数组\n\n但实际递推中发现，计算 $dp[i][j]$ 时，只用到了下一行的 $dp[i+1][j]  和 dp[i+1][j+1]$\n\n因此dp 数组不需要定义N行，只需定义1行\n\n把 $i$ 所在维度去掉，将就可以将 $O(N^2)$ 的空间复杂度优化成$ O(N)$\n\n```java\npublic int minimumTotal(List<List<Integer>> tri) {\n \tint n = tri.size();\n  int[] dp = new int[n+1];\n  for(int i=n-1; j>=0; i--){\n    for(int j=0;j<=i; j++){\n      dp[j] = Math.min(dp[j], dp[j+1]) + tri.get(i).get(j);\n    }\n  }\n  return dp[0];\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["DP"]},{"title":"Unsupervised Multi-hop Question Answering by Question Generation","url":"/2021/05/21/Unsupervised-Multi-hop-Question-Answering-by-Question-Generation/","content":"\n#  Unsupervised Multi-hop Question Answering by Question Generation\n\n第一个研究无监督多跳QA的。\n\nMQA-QG致力于探索在不参考任何人工标记的多跳问答对的情况下训练性能良好的多跳QA模型的可能性。\n\n无标签数据源分为同构和异构，即作者考虑了两种数据源，一种是结构化的表格文本数据，一种是纯文本数据。\n\n如果推理链中只有一种数据源的叫同构，两种数据源的叫异构。\n\n仅使用生成的训练数据，和有监督的性能做对比，对于Hybridge QA和HotpotQA数据集分别有61%和83%的有监督学习性能。其中hotpotQA(同构数据)，Hybridge QA(异构数据)。\n\n## 大体过程\n\n从每个数据源中选择或生成相关信息，将多个信息整合成一个问题。\n\n- 首先定义一系列operators去检索或生成相关信息。\n\n- 然后定义六个推理图每个对应于一种类型的多跳问题，且是建立在operators之上的计算图。\n\n![](https://i.loli.net/2021/05/21/dz9iH1rRSjg68ox.png)\n\n如上图，是生成table2text的问题。给出输入$(table, text)$ ，桥梁实体Jenson Button被operators找出来，他链接了文本数据和表格数据。\n\n然后再用一个叫 (QGwithEnt operator)的操作生成了右边的一个简单的问题：简森巴顿是什么时候出生？\n\n左边被 (DescribeEnt operator) 生成了一个描述桥梁实体的句子：简森巴顿是2004年美国大奖赛排名第4位的车手。\n\n最后再由 BridgeBlend operator 混合成一个多跳问题：2004年美国大奖赛排名第四的车手是什么时候出生的？\n\n\n\n## 具体方法\n\n给一个问题 q 和一系列文本 $C = \\{C_1,...,C_n\\}$ 其中 $C_i$ 可能是文章、table，如果推广到多模态还可能是image。\n\nQA model 表示为 $p_{\\theta}(a|q,C)$\n\n在本文中，作者只考虑两跳问题，并将所需的上下文表示为 $C_i$ 和 $C_j$。\n\n主要有三个成分分别是 \n\n- operators：由规则或现成的预训练模型实现的原子操作，用于从输入上下文 $(C_i、C_j)$ 检索、生成或融合相关信息。\n- reasoning graphs：不同的推理图定义了不同类型的以operators构建的多跳QA推理链。通过执行推理图生成训练 (q，a) 对。\n- question filtration：去除不相关和不自然的(q，a)对，给出多跳问答的最终训练集D。\n\n![](https://i.loli.net/2021/05/21/zQhscxV4MLRoWwn.png)\n\n\n\n### Operators\n\n定义了8个基本operator ，分为三种类型：\n\n- 选择：从单个上下文中检索相关信息\n- 生成：从单个上下文中生成信息\n- 融合：将多个检索或生成的信息进行融合，以构造多跳问题。\n\n#### FindBridge\n\n大多数多跳问题依赖于连接不同输入上下文的实体整合多条信息，即桥梁实体。\n\nFindBridge将两个上下文 $(C_i、C_j)$ 作为输入，并提取出现在 $C_i$ 和 $C_j$ 中的实体作为桥实体。如在第一个图中，提取“Jenson Button”作为桥实体。\n\n\n\n### FindComEnt\n\n在生成比较类型的多跳问题时，我们需要决定为桥实体比较什么属性。\n\nFind-Coment提取潜在的可比性，从文本中提取具有NER类型的实体作为比较属性(国籍、位置、日期时间和数字)。\n\n![](https://i.loli.net/2021/05/21/desLTyfBGOnzgqR.png)\n\n如上图的 #1 步骤\n\n\n\n#### QGwithAns和QGwithEnt\n\n这两个都是生成简单的单跳问题的， 随后会被用来合成多跳问题。\n\n作者使用预培训好的Google T5模型微调在SQuAD来实现这两个操作员。给定上下文-问题-答案三元组训练集D={(c，q，a)}，我们在两个任务上联合微调模型。给定上下文-问题-答案三元组训练集 $D={(c，q，a)}$ ，在两个任务上联合微调模型。\n\n- QGwithAns的目标是生成一个问题Q，其中a为答案，给定(c，a)为输入。\n- QGwithEnt旨在生成包含特定实体e的问题Q，给定(c，e)作为输入。\n\n![](https://i.loli.net/2021/05/21/IR6M35PrYKt91ba.png)\n\n#### CompBlend\n\n基于两个单跳问题Q1和Q2组成比较型多跳问题。这两个问题询问两个不同实体e1和e2的相同比较属性p。我们通过将p、e1和e2填入预定义模板来形成多跳问题。\n\n![](https://i.loli.net/2021/05/21/yiafYuGkUSeWl4B.png)\n\n\n\n#### DescribeEnt\n\n给定table $T$ 和表中的目标实体 e，DescribeEnt operator 基于T中的信息生成描述实体 e 的语句。使用GPT-TabGen模型，该模型首先使用模板将T 变成文档 $P_T$，然后将 $P_T$ 送到GPT-2生成输出句子 Y\n\n![](https://i.loli.net/2021/05/22/W9gHwV2PKpRtulr.png)\n\n为了避免 $P_T$中存在不相关信息，应用了一个仅描述目标实体所在行的模板。然后，在 ToTTo 数据集上通过最大化 $p(Y|P_T;β)$ 的来微调模型，β表示模型参数。ToTTo 数据集是一种受控的表格到文本生成的大规模数据集。\n\n\n\n#### QuesToSent\n\n通过应用《Transforming question answering datasets into natural language inference datasets》定义的语言规则将问题Q转换成其陈述形式s。\n\n\n\n#### BridgeBlend\n\n基于1)桥接实体e。2)包含e的单跳问题q。3)描述e的句子s。组成桥接型多跳问题。\n\n![](https://i.loli.net/2021/05/22/Rqj6PJW5kCOnS7x.png)\n\n作者通过应用简单但有效的规则来实现这一点，该规则将 Q中的桥接实体 e替换为 “the [MASK] that s”，并采用预训练的Bert-Large来填充[MASK]。\n\n\n\n### Reasoning Graphs\n\n基于以上的operators ，作者定义了6种类型的推理图，生成不同类型的问题。\n\n 每个问题都是有向无环图 G，每个G对应一个operator。\n\n一共有四类：\n\n- Table-to-Text：表格和文本之间的桥接式问题，答案来自文本。\n- Text-to-Table ：表格和文本之间的桥接式问题，答案来自表格。\n- Text-to-Text : 桥接类型，两边都是文本。\n- Comparison：基于两篇文章的比较型问题。\n\n通过执行每个推理图来生成QA对。通过定义新的算子和推理图，可以很容易地扩展到其他模态和推理链。\n\n![](https://i.loli.net/2021/05/22/9fWtj7U2vu4lbIi.png)\n\n![](https://i.loli.net/2021/05/22/aAs6NjF2TlPYiQd.png)\n\n![](https://i.loli.net/2021/05/22/mnT1JuCsK25ZrVx.png)\n\n\n\n### Question Filtration\n\n使用了两种方法来提炼生成的QA对的质量\n\n- Filtration：使用预先训练的gpt-2模型来过滤解决那些不流利或不自然的问题。选择困惑度最低的前N个样本作为生成的数据集来训练多跳QA模型。\n- Paraphrasing：基于BART模型训练一个问题解释模型来解释每个生成的问题。\n\n实验表明，过滤给QA模型带来了明显的改进。然而，在实验中展示了释义产生了更多类似人类的问题，但是引入了语义漂移问题，从而损害了QA性能。\n\n为了生成更自然的问题，我们试图训练一个基于BART的问题释义模型，以对每个生成的问题进行语法分析。观察到，通过将原问题的冗余部分改写成更简洁的表达，释义确实产生了更多的流行性问题。然而，释义引入了“语义漂移”问题，即释义后的疑问句改变了原疑问句的语义。我们认为这会影响QA性能，因为它会产生问答不一致的嘈杂样本。\n\n作者认为在无监督多跳问答中，对于生成的问题，语义的忠实性比流利性更重要。这就解释了为什么设计手工制作的推理图来保证语义的忠实性。然而，如何在保持语义忠实性的同时生成流畅的类人问题是未来的一个重要方向。\n\n\n\n## 实验\n\n对于Hybridge QA，问题按其答案是来自表格(56%)还是来自段落(44%)进行划分。大约80%的Hybridge QA问题需要桥接式推理。\n\n![](https://i.loli.net/2021/05/22/TpgkmquVjYtrPMf.png)\n\n\n\n在Hybridge QA和HotpotQA上的QA性能。\n\n![](https://i.loli.net/2021/05/22/IUMvlYxrGH4j8WC.png)\n\n\n\n![](https://i.loli.net/2021/05/22/AKh2cWJMs9UYrn3.png)\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"Is Graph Structure Necessary for Multi-hop Question Answering?","url":"/2021/05/19/Is-Graph-Structure-Necessary-for-Multi-hop-Question-Answering/","content":"\n# Is Graph Structure Necessary for Multi-hop Question Answering?\n\n图结构对多跳问答有多大贡献？\n\n这是一篇只有6页的类似实验报告的论文。是在作者在做实验时发现，图结构和邻接矩阵都是与任务相关的先验知识，graph-attention可以看作是self-attention的特例。实验和可视化分析表明，self-attention和transformer可以代替graph-attention或整个图形结构，并且效果无明显变化，从而对图网络在自然语言处理任务上的应用能力提出了质疑。并希望未来引入图结构纳入自然语言处理任务的工作，应说明其必要性和优越性。\n\n作者使用 **Dynamically Fused Graph Network ** 那篇文章作为baseline开展了研究。\n\n## Baseline\n\n首先描述baseline模型，证明了只有当预先训练的模型以feature-based的方式使用时，图结构才能发挥重要作用。虽然在fine-tuning方法中使用预先训练的模型，但图结构可能没有帮助。\n\n作者复现了DFGN，并修改了预训练模型的使用。该模型首先利用检索器从候选集合中选择相关段落，并将其提供给基于图形的阅读器。实体图中的所有实体都由独立的NER模型抽取而来。\n\n- 检索器：在HotpotQA任务中使用Roberta Large模型来计算查询与每个候选段落之间的相关得分。我们对得分小于0.1%的段落进行过滤，最大入选段落数为3，入选段落拼接为Context C\n\n- 编码层：我们将查询Q和上下文C连接起来，并将序列提供给另一个Roberta，结果被进一步送到Bi-attention layer 以从编码层获得表示。\n\n- 图形融合块：给定第 t-1 跳的上下文表达 $C_{t-1}$ , 将token送入到mean-max pooling得到实体图$H_{t-1}\\in R^{2d\\times N}$, 其中N是实体的数量。之后采用的是图注意力层更新节点表达:\n  $$\n  \\begin{equation}\\begin{split} \n   \\beta_{i,j}^{(t)} &= LeakyReLU(W_t^T[h_i^{(t-1)},h_j^{(t-1)}])\\\\\n   \\alpha_{i,j}^{(t)} &= \\frac {exp(\\beta^{(t)}_{i,j})}{\\sum_{k\\in N_i}\\alpha^{(t)}_{i,k}h_k^{(t-1)}}\\\\\n   h_i^{(t)} &= ReLU(\\sum_{k\\in N_i} \\alpha^{(t)}_{i,k}h_k^{(t-1)})\n      \\end{split}\\end{equation}\n  $$\n  还有查询实体关注、查询更新机制、弱监督等模块。\n\n- 构建实体图：上下文中具有相同提及文本的实体被连接、同一句中出现的实体是相连的。\n\n- 最终预测层\n\n作者将将预训练语言模型的输出直接送到预测层，由于基线模型与DFGN的主要区别在于我们在fine-tuning方法中使用了large的预训练模型，而不是基于feature-based的方法，因此在两种不同的设置下进行了实验。在HotpotQA上提交的结果，并进行了消融实验对比如下：\n\n![](https://i.loli.net/2021/05/19/Kmq726TuL9aU1wF.png)\n\n结果表明在fine-tuning下有没有图结构效果不明显，在feature-based下图结构是有明显作用的。\n\n\n\n## graph-attention是self-attention的一种特例\n\n基于人工定义的规则和图结构的邻接矩阵可以看作是先验知识边，可以通过self-attention或Transformers来学习\n\n基于以上的实验结果表明，自我注意或变形金刚在多跳问题回答中可能具有优势。\n\n> 解决多跳问题的关键是通过查询在原文中找到对应的实体。然后，构建从这些起始实体到其他相同或共现实体的一条或多条推理路径。如图1所示，以前的工作通常从多个段落中提取实体，并将这些实体建模为实体图。邻接矩阵是由人工定义的规则构建的，这些规则通常是实体之间的共现关系。从这个角度看，图的结构和邻接矩阵都可以看作是与任务相关的先验知识。实体图结构限制了模型只能基于实体进行推理，邻接矩阵辅助模型忽略一跳中的非相邻节点。然而，可能是在没有任何先验知识的情况下，模型仍然可以学习实体到实体的注意模式。\n\n> 此外从上文图注意力的公式来看，不难发现图注意力与自我注意具有相似的形式。在前向传播中，实体图中的每个节点都会计算与其他连接节点的关注度得分。如图1所示，当图中的节点完全连接时，图注意力将退化为普通的自我关注层。因此，图形注意可以看作是自我注意的一种特例。\n\n基于以上的想法，作者将图结构做成全连接的实体和self-attention做了一次实验比较，为了验证整个图结构能否被transformer取代。\n\n![](https://i.loli.net/2021/05/19/TAMlO8FU5KeLpma.png)\n\n实验结果如图，与自我注意相比，图形注意并没有显示出明显的优势。\n\n![](https://i.loli.net/2021/05/19/RGcaHXL1o9QOUzI.png)\n\n对于图形注意和自我注意在不同密度区间的结果。尽管邻接矩阵的密度不同，但图形注意与自我注意的结果是一致的。这意味着自我关注可以学会忽略不相关的实体。\n\n此外，transformer显示出强大的推理能力。只有叠加两层变压器才能获得与DFGN相当的效果。\n\n\n\n并且作者分布从Entity2Entity、Attribute2Entity、Coreference2Entity、Entity2Sentence角度可实话了注意力权重在预训练语言模型中效果：\n\n![](https://i.loli.net/2021/05/19/sDHM6uNOE4tIX5S.png)\n\n认为基于实体的图网络忽略了后三种链接的信息，我认为可能异质图更多的解决这个问题。\n\n\n\n\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"Navicat配置远程连接sqlite","url":"/2021/05/19/Navicat配置远程连接sqlite/","content":"\n# Navicat配置远程连接sqlite\n\n\n\n![](https://i.loli.net/2021/05/20/qdk2IJAea8BhGxf.png)\n\n远程sqlite其实是需要php环境的\n\n因为配置远程需要一个php文件当做通道，而且要能在浏览器上可访问这个文件\n\n所以首先找到这个文件，这个文件一般在Navicat的目录下，把它放在db的同目录下\n\n![](https://i.loli.net/2021/05/19/YCiGenoyAQvwIDE.png)\n\n然后就是安装apache2 和php\n\nhttps://blog.csdn.net/qq_37264323/article/details/90586239\n\n我是按这个装的很简单\n\n按步骤按完 apache2和php就可以了其余的不用装\n\n然后就是配置可访问，先打开80端口的访问网址看看Apache启动成功否\n\n之后就是配置虚拟路径，因为一般我们的db都不是放在Apache默认的www目录下\n\n我的服务器系统是ubantu\n\nApache安装目录在：/etc/apache2\n\n我要配置的位置是 :vim /etc/apache2/sites-available/000-default.conf  \n\n![](https://i.loli.net/2021/05/19/PsukcIXBlH6U9pS.png)\n\n在virtualhost内放置上面 alias 和directory 配置好路径\n\nservice apache2 restart\n\n但是访问http://10.12.1.150/data1/ntunnel_sqlite.php \n\n会出现forbidden\n\n我是参考这个解决的 \n\n![](https://i.loli.net/2021/05/19/vMgWrR8ayShqTHu.png)\n\nhttps://www.cnblogs.com/starof/p/4685999.html\n\n最终可访问：\n\n![](https://i.loli.net/2021/05/19/MHCveaGhg59qc3y.png)\n\n但目前存在一个问题 SQLite3 class available No 链接不成功\n\n![](https://i.loli.net/2021/05/19/DrnsE9yZ8PT6jvC.png)\n\n我怀疑是php对sqlite3的某个配置没有配好\n\nvim /etc/php/7.2/apache2/php.ini\n\n![](https://i.loli.net/2021/05/20/bci5VXT3GdEpmZW.png)\n\n找到这两个地方把注释去掉 \n\n> apt-get install php7.2-sqlite3\n\n![](https://i.loli.net/2021/05/20/ISnMmaUVirpyYG8.png)\n\n\n\n![](https://i.loli.net/2021/05/20/FKb1XUYksjNRinw.png)\n\n![](https://i.loli.net/2021/05/20/BCE6i4lpxnStk8I.png)\n\n\n\n另参考：[Navicat使用HTTP通道远程连接SQLite](https://ya2.top/articles/navicat%E4%BD%BF%E7%94%A8http%E9%80%9A%E9%81%93%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5sqlite/)\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["配置记录"]},{"title":"从01背包问题一维优化到多重背包问题二进制、单调队列优化总结","url":"/2021/05/17/从01背包问题一维优化到多重背包问题二进制、单调队列优化总结/","content":"\n# 从01背包问题一维优化到多重背包问题二进制、单调队列优化总结\n\n\n\n背包问题很经典，但从来都没有从头到尾总结过。\n\n01背包问题，是给一个容量大小为V的背包和N件物品，每件物品有各自的价值w，且每个物品只能被选择1次。要求在有限的背包容量下，装入物品总价值最大。\n\n多重背包问题的变动是，每个物品不止可以选择1次了，但要求还是在有限容量下装入最大的价值。\n\n相当于问题除了给出背包容量V，每种物品的价值W，之外，还给了每种物品的可选数量S\n\n多重背包问题的做法有\n\n- 将多重背包问题拆分为01背包问题，每种物品的每个我都选择一下0或1选与不选，这种做法时间复杂度较高。\n\n  适用数据范围为：\n\n  $0<N,V≤1000$\n  $0<v_i,w_i≤1000$ (因为题目一般的可解计算量为$10^7$ )\n\n- 范围超了有，二进制优化方法\n\n  适用数据范围为：\n\n  $0<N \\le 1000$\n\n  $0<V \\le 2000$\n\n  $0<v_i,w_i,s_i≤2000$\n\n- 再大还有单调队列优化方法\n\n  适用数据范围为：\n\n  $0<N \\le 1000$\n\n  $0<V \\le 20000$\n\n  $0<v_i,w_i,s_i≤20000$\n\n\n\n## 01背包问题\n\n题目：https://www.acwing.com/problem/content/2/\n\n不断对第i个物品做出决策，[0-1] 代表选与不选两种抉择\n\n![](https://i.loli.net/2021/05/17/xM8coeUv3Guh1LX.png)\n\n将状态$f[i][j]$优化到一维$f[j]$，实际上只需要做一个等价变形。\n\n为什么可以这样变形呢？我们定义的状态$f[i][j]$可以求得任意合法的 $i$ 与 $j$ 最优解，但题目只需要求得最终状态$f[n][m]$，因此我们只需要一维的空间来更新状态。\n\n1. 状态$f[j]$ 定义：N件物品，背包容量 $j$ 下的最优解\n2. 注意枚举背包容量 $j$ 必须从 $V$ 开始\n3. 为什么一维情况下枚举背包容量需要逆序？ 在2维情况下，状态 $f[i][j]$ 是由上一轮 $i-1$ 的状态得来的， $f[i][j]$ 与 $f[i-1][j]$ 是相互独立的。而优化到1维后，如果还是正序遍历，则有 $f[较小体积]$ 更新到 $f[较大体积]$， 则有可能本应该用第 $i-1$ 轮的状态却用的是第 $i$ 轮的状态\n4. 例如，一维状态第$i$ 轮对体积为3的物品进行决策，则$f[7]$ 由 $f[4]$ 更新而来，这里的$f[4]$ 正确应该是 $f[i-1][4]$，但从后小岛大枚举 $j$ 这里的 $f[4]$ 在第 $i$ 轮却成了 $f[i][4]$。 当逆序枚举背包容量 $j$ 时， 我们求$f[7]$ 同样由 $f[4]$ 更新。这里的 $f[4]$ 还没有在第 $i$ 轮计算，所以实际计算的 $f[4]$ 仍是 $f[i-1][4]$\n5. 简单来说，一维情况下正序更新状态 $f[j]$ 需要用到前面计算的状态已经被污染，逆序则不会有这样的问题\n6. 状态转移方程为 $f[j] = max(f[j], f[j-v[i]]+ w[i])$\n\n```java\npublic class Main{\n    \n    public static void main(String[] args) {\n        Scanner sc = new Scanner(System.in);\n        int N = sc.nextInt();\n        int V = sc.nextInt();\n        \n        int[] v=new int[N+1];\n        int[] w=new int[N+1];\n        \n        for(int i=1;i<=N;i++){\n            v[i] = sc.nextInt();\n            w[i] = sc.nextInt();\n        }\n        \n         //dp1(v,w, N, V);// 无优化数组\n      \t dp2(v,w, N, V);// 优化为1维数组\n    }\n    \n    public static void dp1(int[] v,int [] w, int N, int V){\n        int[][] dp = new int[N+1][V+1];\n        dp[0][0]= 0;\n        for(int i=1;i<=N;i++){\n            for(int j=1;j<=V;j++){\n                if(j<v[i]) dp[i][j]=dp[i-1][j];\n                else{\n                    dp[i][j] = Math.max(dp[i-1][j], dp[i-1][j-v[i]]+w[i]);\n                }\n            }\n        }\n        System.out.println(dp[N][V]);\n    }\n  \n    public static void dp2(int[] v,int [] w, int N, int V){\n          int[] dp = new int[V+1];\n          dp[0]= 0;\n          for(int i=1;i<=N;i++){\n              for(int j=V;j>=v[i];j--){\n                  // if(j<v[i]) dp[j]=dp[j];\n                  // else{\n                  dp[j] = Math.max(dp[j], dp[j-v[i]]+w[i]);\n                  // }\n              }\n          }\n          System.out.println(dp[V]);\n      }\n\n}\n```\n\n\n\n还可以优化输入\n\n处理数据时，我们是一个物品一个物品，一个体积一个体积的去枚举\n\n因此可以不必开两个数组去记录体积和价值，而是边输入边处理。\n\n```java\npublic class Main{\n    \n    public static void main(String[] args) {\n        Scanner sc = new Scanner(System.in);\n        int N = sc.nextInt();\n        int V = sc.nextInt();\n        int[] f=new int[V+1];\n        \n        for(int i=1;i<=N;i++){\n            int v = sc.nextInt();\n            int w = sc.nextInt();\n            for(int j = V;j >= v;j--){\n                f[j] = Math.max(f[j], f[j-v]+w);\n            }\n        }\n        System.out.println(f[V]);\n    }\n}\n```\n\n\n\n## 多重背包问题1\n\n题目：https://www.acwing.com/problem/content/4/\n\n多重背包问题，在给出每个物品的体积V和价值W的基础上，让每个物品不只可选1次\n\n完全背包和01背包的区别是完全背包中每个物品可以用无限次，而多重背包不是无限次用。\n\n最直接也最耗时的思路是，所有的可选的物品种类和次数都询问一次选或不选，也就是当成01背包问题来做。\n\n但也比01背包问题多了一个数量级, 相对暴力\n\n![](https://i.loli.net/2021/05/18/UWfLp2TGBw4jPvn.png)\n\n```java\nimport java.util.Scanner;\n\npublic class Main{\n    public static void main(String[] args) {\n        Scanner scanner = new Scanner(System.in);\n        int N = scanner.nextInt();\n        int V = scanner.nextInt();\n\n        int[] f = new int[110];\n\n        for (int i = 0; i < N; i++) {\n            int v = scanner.nextInt();\n            int w = scanner.nextInt();\n            int s = scanner.nextInt();\n            for (int j = V; j >= 0; j--) {\n                for (int k = 1; k <= s && k * v <= j; k++) {\n                    f[j] = Math.max(f[j], f[j - k * v] + k * w);\n                }\n            }\n        }\n\n        System.out.println(f[V]);\n    }\n}\n```\n\n\n\n## 多重背包问题2\n\n题目：https://www.acwing.com/problem/content/5/\n\n这道题和多重背包问题1其实是一样的，只不过数量级有变化，要求你用二进制优化的方法来解。\n\n那么什么是二进制优化法？\n\n上一道题是将每种物品拆成单份的01背包去求解的\n\n```text\n 即 v,w,s = v,w,7 时：\n 正常拆分：-> (v,w)，(v,w)，(v,w)，(v,w)，(v,w)，(v,w)，(v,w)\n 二进制拆分：-> (v,w),(v<<1,w<<1),(v<<2,w<<2)\n\n 7 : 1 ,2, 4\n 0\n 1\n 2\n 3 = 1+2\n 4\n 5= 1+4\n 6 =2+4\n 7=1+2+4\n <p>\n s - 1 -2 - 4 - 8  ....\n 减 2的幂  减到不能减为止 用s - (1+2+4+8...)\n 就可以把物品分成log(s)份 而不是s份\n <p>\n log(2000)=11\n 1000*11*2000 = 2*10^7\n 所以将每个物品拆成log份\n \n 模拟\n1 2 4 8 16 32 64 128 256 512 1024\n这十一个数可以拼凑出0-2047间的所有整数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16................\n1 2 1+2 4 4+1 4+2 4+2+1 8 8+1 8+2 8+2+1 8+4 8+4+1 8+4+2 8+4+2+1 16................\n所以在使用二进制将si个i物品拆包组装成一个个大包之后我们总归可以通过01背包的枚举方式来得到一个正确的i物品选用数量，比如说应该选67件i物品，那么体现成我们选取了 价值为64w的物品一件 + 价值为2w的物品一件 + 价值为1*w的物品一件\n\n```\n\n为什么这么拆有用？\n\n上一题的状态转移方程是\n$$\n\\begin{equation}\\begin{split} \n f[j] &= max(f[j-1], f[j-v[i]]+w[i], f[j-2*v[i]]+2*w[i],... ) \\\\\n    \\end{split}\\end{equation}\n$$\n我们首先确认三点：\n\n（1）我们知道转化成01背包的基本思路就是：判断每件物品我是取了你好呢还是不取你好。\n\n（2）我们知道任意一个实数可以由二进制数来表示，也就是$2^0 - 2^k$其中一项或几项的和。\n\n（3）这里多重背包问的就是每件物品取多少件可以获得最大价值。\n\n如果仍然不是很能理解的话，取这样一个例子:要求在一堆苹果选出n个苹果。我们传统的思维是一个一个地去选，选够n个苹果就停止。这样选择的次数就是n次\n\n二进制优化思维就是：现在给出一堆苹果和10个箱子，选出n个苹果。将这一堆苹果分别按照1,2,4,8,16,.....512分到10个箱子里，那么由于任何一个数字x ∈[1,1024]\n都可以从这10个箱子里的苹果数量表示出来，但是这样选择的次数就是 ≤10次 。\n\n这样利用二进制优化，时间复杂度就从$O(n^3)降到O(n^2logS)$,从$4*10^9$降到了$2*10^7$。\n\n```java\nimport java.util.Scanner;\n\npublic class Main{\n   \tScanner scanner = new Scanner(System.in);\n    \n    void run1(){\n        int N = scanner.nextInt();\n        int V = scanner.nextInt();\n        \n        int[] v_arr = new int[12010];\n        int[] w_arr = new int[12010];\n        \n        int cnt = 0; // 分组的组别\n        for(int i=1;i<=N;i++){\n            int v = scanner.nextInt();\n            int w = scanner.nextInt();\n            int s = scanner.nextInt();\n            int k =1; //组别里的类别个数\n            while(k<=s){\n                cnt++; //组别先增加\n                v_arr[cnt] = v*k; //整体体积\n                w_arr[cnt] = w*k; //整体价值\n                s-=k;   \n                k*=2;\n            }\n            //剩余一组\n            if(s>0){\n                cnt++;\n                v_arr[cnt]= v*s;\n                w_arr[cnt]= w*s;\n            }\n        }\n        \n        int[] f = new int[V+1];\n        N = cnt;\n        // 01背包\n        for(int i=1;i<=N;i++){\n            for(int j=V;j>=v_arr[i];j--){\n                f[j] = Math.max(f[j] , f[j-v_arr[i]]+w_arr[i]);\n            }\n        }\n        System.out.println(f[V]);\n    }\n\n    public static void main(String[] args) {\n        new Main().run1();\n    }\n}\n```\n\n\n\n\n\n## 多重背包问题3\n\n题目：https://www.acwing.com/problem/content/description/6/\n\n$0<N \\le 1000$\n\n$0<V \\le 20000$\n\n$0<v_i,w_i,s_i≤20000$\n\n如果还以上面的二进制优化来做，复杂度为 $1000 * log(20000) * 20000 = 3*10^8$  会超时。\n$$\n\\begin{equation}\\begin{split} \n 原: f[j]=max(f[j],f[[j-kv[i]]+kw[i]);\\\\\n    \\end{split}\\end{equation}\n$$\n\n\n```java\nfor(int i=1;i<=N;i++)  //第一重循环\n        for(int j=0;j<=V;j++)  //第二重循环\n            for (int k = 0; k <= s[i] && k * v[i] <= j; k ++ )  //第三重循环\n```\n\n考虑到对于每次层 $i，j$  只与 j % v+kv 有关，k 的范围 $[0,s]$\n\n优化二三重循环,将每一层 j 按 j%v 分成v组，节省了第二重循环中 $ j+v…j+kv$ 的时间，将两重循环优化为遍历一次 m；\n\n$f[i][j]=max(f[i][j],f[i-1][j-kv[i]]+k*w[i]) $相当于求每一组在s个范围内的最大值，单调队列O（1）时间即可；\n\n时间复杂度应该是O(NV)\n\n\n\n```java\nimport java.util.Scanner;\n\npublic class Main{\n   \tScanner scanner = new Scanner(System.in);\n    \n    void run1(){\n        int N = scanner.nextInt();\n        int V = scanner.nextInt();\n        \n        int[] v_arr = new int[V+1];\n        int[] w_arr = new int[V+1];\n        \n        int[] f = new int[V+1];\n        int[] g = new int[V+1];\n        int[] q = new int[V+1];\n        \n        for(int i=1;i<=N;i++){\n            int v = scanner.nextInt();\n            int w = scanner.nextInt();\n            int s = scanner.nextInt();\n            for(int j=0;j<v;j++){\n                int hh,tt;\n                hh=0,tt=-1;\n                for(int k=j;k<=m;k+=vi){\n                    g[k] = f[k];//每次f[k]都可能会更新， 预先保存f[i-1, k]的值 \n                    if(hh<=tt&&(k-q[hh])/vi>si) hh++;//保证保证不超前si个\n                    while(hh<=tt&&g[q[tt]]+(k-q[tt])/vi*wi <f[k]) tt--;//单调队列入队方法\n                    q[++tt] = k;\n                    f[k] = g[q[hh]]+(k-q[hh])/vi*wi;\n                }\n            }\n        }\n        \n    }\n\n    public static void main(String[] args) {\n        new Main().run1();\n    }\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["DP"]},{"title":"Adam & AdamW 原论文","url":"/2021/05/16/Adam-AdamW-原论文/","content":"\n# Adam & AdamW 原论文\n\n---\n\n# Adam\n\n一种基于低阶矩估计的随机目标函数一阶梯度优化算法。该方法也适用于非平稳目标和具有非常强噪声和/或稀疏梯度的问题。特点有：实现简单、计算高效、低内存要求、对梯度的对角重新缩放不变，并且很适合于数据和/或参数较大的问题。\n\n","tags":["ML&DL"]},{"title":"GPT-GNN: Generative Pre-Training of Graph Neural Networks","url":"/2021/05/15/GPT-GNN-Generative-Pre-Training-of-Graph-Neural-Networks/","content":"\n# GPT-GNN: Generative Pre-Training of Graph Neural Networks\n\nSelf-Supervised Learning分成两种方法:一种是生成式模型，一种是判别式模型(对比学习)。\n\n以输入图片信号为例，生成式模型，输入一张图片，通过Encoder编码和Decoder解码还原输入图片信息，监督信号是输入输出尽可能相似。判别式模型，输入两张图片，通过Encoder编码，监督信号是判断两张图是否相似(例如，输入同一个人的两张照片，判断输入相似，输出1；输入两个人的照片，判断输入不相似，输出0)。\n\n## 文章贡献\n\n继上一文 [Strategies for Pre-training Graph Neural Networks](https://coding-zuo.github.io/2021/05/12/Strategies-for-Pre-training-Graph-Neural-Networks/) 对预训练GNN做了大规模的实验，并提出提出了一种结合节点级和图级表示的预训练方法，优化了单单使用一种级别做预训练后产生的负迁移效果。\n\n又以生成式自监督的方式，来在预训练阶段捕捉图数据的结构信息和语义信息。分别是边生成任务和属性生成任务。\n\n它们联合优化等价于最大化整个属性图的概率似然，这样预训练模型可以捕捉到节点属性与图结构之间的内在依赖关系。\n\n预训练的GNN网络目标主要是异质单个(大规模)图上预训练，并进行节点级迁移。\n\n然后优化了预训练模型可以让其处理大规模的图采样子图，采用的是通过自适应的嵌入队列，减轻负采样带来的不准确损失。\n\n接下来主要介绍两种自监督任务和这个优化方法。\n\n\n\n## 行文逻辑\n\n通过行文逻辑，学习怎么写论文。\n\n首先作者先是说GNN有用，预训练GNN刚刚被证明有用！接下来从充分利用无标签数据做无监督任务说，大规模的图数据标记成本昂贵。NLP的数据也一样标注昂贵，所以有了bert那样的预训练语言模型，并且提高了下游任务性能。同样在cv领域也是。\n\n列举了GAE、GraphRNN、半监督GCN等图生成技术，但他们不适合用于预训练GNN。因为：首先，它们大多只关注于生成无属性的图结构，而没有定义节点属性与图结构之间的底层模式，图结构是GNNs中卷积聚合的核心。其次，它们被设计用来处理迄今为止的小图形，限制了它们在大规模图形上进行预训练的潜力。\n\n然后介绍了下预训练和finetuning的流程，就不多说了。\n\n然后切入正题介绍他的贡献，上文介绍过。\n\n----\n\n然后是准备工作和相关工作，介绍GNN的传统机制，信息传递和信息聚合的基本原理，不多介绍。\n\n和GNN发展历史，其中有一个Graph Infomax 最近可能要学习一下，最大化了从GNNs获得的节点表示和图pooling表示之间的互信息，也就是节点级和图级。作者认为其，在纯监督学习环境下表现出改进，但学习任务通过强迫附近节点具有相似的嵌入来实现，而忽略了图的丰富语义和高阶结构。\n\n介绍预训练在cv和nlp的成功。不过我最近听说cv圈有一篇文章，最近2021的有一篇预训练CNN其效果并不比基于transformer的模型差。\n\n介绍生成预训练任务的数学定义，之后是具体细节和模型方法，再到实验结论等等。\n\n\n\n## 关于生成式预训练任务的框架流程\n\n形式上给出图数据 $G = (V,E,X)$  和GNN模型 $f_{\\theta}$\n\n我们通过这个GNN将此图上的可能性建模为 $p(G;θ)$ -----表示G中的节点是如何属性化和连接其他节点的(可以理解为先验知识)。\n\n其目的是通过最大化图的似然，得到参数 $θ^∗=max_{θ}p(G;θ)$ 来预先训练广义神经网络模型。\n\n那么问题变成了如何对 $p(G;\\theta)$ 进行适当的建模。\n\n现在大多的现有图生成方法都是遵循自回归方式来分解目标概率分布，也就是图中的节点是按顺序来的，并且边是通过将每个新到达的节点连接到现有节点来生成的。什么是自回归？\n$$\nX_t = c+\\sum_{i=1}^p\\phi_iX_{t-i}+\\epsilon_t\n$$\n如上式，c 为常数项，$\\epsilon$ 为随机误差，概况来说就是X的当前期值等于一个或数个前期值的线性组合加常数项和睡觉误差。\n\n\n\n类似的作者也通过一个排列向量 $\\pi$ 来确定节点顺序，其中 $i^{\\pi}$ 表示向量中第i个位置的节点id。因此，图的分布$p(G;\\theta)$ 等价于所有可能排列上的期望似然：\n$$\np(G;\\theta) = \\mathbb{E}_{\\pi} [p_{\\theta}(X^{\\pi},E^{\\pi})]\n$$\n其中$X^{\\pi} \\in R^{|V|\\times d}$ ，$E$ 是边集 ，$E_{i}^{\\pi}$ 表示所有连接节点$i^{\\pi}$ 的边。\n\n为简单起见，假设观察到任何节点排序 $π$ 的概率相等，并且在下面的章节中说明一个排列的生成过程时也省略了下标 $π$。给定一个排列顺序，我们可以将对数似然率自动回归分解-每次迭代生成一个节点，如下所示：\n$$\nlogp_{\\theta}(X,E) = \\sum_{i=1}^{|V|}logp_{\\theta}(X_i,E_i|X_{\\lt i},E_{\\lt i})\n$$\n在第i步，使用所有 i 之前已生成的节点，他们的属性和边分别是 $X_{\\lt i}$ ，$E_{\\lt i}$ ，给定 $X_{\\lt i}$ $E_{\\lt i}$ 生成节点 i 的概率log加和。\n\n从本质上讲，等式中的目标。描述了属性图的自回归生成过程。问题变成：如何对条件概率 $p_θ(X_i，E_i|X_{<i}，E_{<i})$ 建模？\n\n### 因式分解属性图生成\n\n为了计算 $p_{\\theta}(X_i,E_i|X_{\\lt i},E_{\\lt i})$ ，一种天真的解决方案可以是简单地假设 $X_i$ 和 $E_i$是独立的，即 :\n$$\np_{\\theta}(X_i,E_i|X_{\\lt i},E_{\\lt i}) = p_{\\theta}(X_i|X_{\\lt i},E_{\\lt i}) \\cdot p_{\\theta}(E_i|X_{\\lt i},E_{\\lt i})\n$$\n然而通过这样的分解，对于每个节点，其属性和连接之间的依赖性被完全忽略。\n\n然而，被忽略的依赖性是属性图的核心性质，也是GNNs中卷积聚集的基础。因此，这种天真的分解不能为训练前的GNN提供信息指导。\n\n就比如，物以类聚人以群分，我和相似的人右边是因为我们有相似的属性。\n\n为了解决这个问题，作者提出了属性图生成过程的依赖感知分解机制。具体地说，当估计一个新节点的属性时，我们会得到它的结构信息，反之当估计一个新的结构边信息时，我们会考虑到它的属性信息。在该过程中，可以将生成分解为两个耦合部分：\n\n- 1.给出观测边的边，生成节点属性\n- 2.给出观测边和1中已经生成的节点属性，生成剩余的边\n\n通过这种方式，模型可以捕获每个节点的属性和结构之间的依赖关系。\n\n正式的定义如何建模，定义一个变量 $o$ , 表示$E_i$内所有观测边的索引向量。\n\n$E_{i,o}$ 是已观测的边，$\\lnot o$表示要生成的所有mask边的索引。通过所有的已观测边来重写条件概率作为一个期望似然如下：\n\n![](https://i.loli.net/2021/05/16/qIjAo2HyNkF5WcS.png)\n\n这里的理解非常重要，第一个等式中，把 $E_i$ 拆成了$E_{i,¬o}$和 $E_{i,o}$ ，也就是说指定了哪些边是观测边，哪些边是masked边。需要注意的是，当o确定下来了，$\\lnot o$ 也是确定的。因此等式外面加上了对o的累加，这里可以理解为类似于全概率公司去对所有可能的o求和。\n\n此外，这里要注意  $E_i, E_{<i},E_{i,o},E_{i,\\lnot o}$  四个符号分别表示什么：\n\n- 现在位于step i，$E_{<i}$ 是指在step i 之前生成的边\n- $E_i$ 指在step i 将会生成的边 (与节点i 相连，有好多边)\n- 将 $E_i$ 的边生成过程拆分成一件生成的和将要生成的两部分，即 $E_{i,o},E_{i,\\lnot o}$\n\n在第二个等式中，把p 看成是概率分布，写作对于o 期望的形式。\n\n最后把 $X_i$ 和 $E_{i,\\lnot o}$ 看做独立的过程，拆成两个概率分布。\n\n这种分解的优势在于，没有忽略 $X_i$ 和 $E_{i,o}$ 的联系。第一项表示given观测边，聚合目标节点i的邻居信息来生成其属性$X_i$ 。第二项表示given观测边和刚生成的属性$X_i$，预测$E_{i,¬o}$中的边是否存在。\n\n![](https://i.loli.net/2021/05/16/hTuqivsBcRC1fon.png)\n\n如上图所示，给出一个例子。对于academic图，我们要去生成一个paper node，它的属性为title。我们要去生成一个paper node，它的属性为title，并且其和author，publish venue，reference相连。上图中的实线部分为已经观测到的边，首先生成节点的属性，即title。然后基于author1，author2，author3和刚生成的节点属性title，预测剩下的边，即虚线部分。\n\n### 高效的属性和边生成 \n\n出于效率考虑希望：\n\n- 对于输入图只跑一次GNN就能计算节点属性生成和边生成过程的loss\n- 希望节点属性生成和边生成能同时运行\n\n然而边生成需要用到节点属性信息，如果两个生成过程同时进行，会导致信息泄露。为避免这个问题，将节点分成两种类型：\n\n- 属性生成节点，mask住这些节点的属性，用一个公用的dummy token，并学习一个共享向量$X^{init}$来代替 和$X_i$ 维度相同。\n- 边生成节点，对于这些节点，保留他们的属性。\n\n需要注意的是，同一个节点在不同阶段扮演不同的角色，可能是属性生成节点也可能是边生成节点。只在某一阶段，一个节点有一个确定的角色。\n\n在graph上训练GNN 来生成各个节点的embedding，用$h_{attr}$ 和 $h_{edge}$ 来分别表示属性生成节点和边生成节点的embedding。由于属性生成节点的属性被mask了，因此$h_{attr}$中包含的信息通畅会少于 $h_{edge}$。\n\n因此，在GNN的message passing过程中，只使用$h_{edge}$ 作为向其他节点发送的信息。 也就是说，对于每个节点，其聚合邻居 $h_edge$ 的信息和自身信息来生成新的embedding。之后使用不同的decoder来生成节点属性和边。（注意，节点的embedding和节点属性不是一回事。通俗理解，在GNN中节点的属性是input，节点的embedding是hidden layer。）\n\n\n\n对于属性生成，用$Dec^{Attr}(\\cdot)$ 来表示decoder，输入$h_{attr}$ 来生成节点属性。decoder的选择依赖于节点属性的类型，如果是text类型的节点属性，可以使用LSTM等。如果节点属性是vector，可以使用MLP。\n\n定义一个距离函数来度量生成属性和真实属性之间的差异，对于text类型属性，可以使用perplexity困惑度，对于vector属性，可以使用L2距离。因此，可以计算属性生成过程中的loss\n$$\nL_i^{Attr} = Distance(Dec^{Attr}(h_i^{Attr}, X_i))\n$$\n\n\n最小化生成属性和真实属性之间的差异，等价于对generate attribute做MLE，也就是最大化 $p_{\\theta}(X_i|E_{i,o},X_{<i},E_{<i})$ 从而捕捉了图中的节点属性信息。\n\n\n\n对于边生成过程，假设每条边的生成过程和其他边是独立的，由此对likelihood分解：\n$$\np_{\\theta} (E_{i,\\lnot o}|E_{i,o},X_{\\le i},E_{\\le i}) = \\prod_{j^+\\in E_{i,\\lnot o}} p_{\\theta}(j^+|E_{i,o},X_{\\le i},E_{\\le i})\n$$\n得到$h_{edge}$ 后，如果节点i和节点j相连，则使用\n$$\nDec^{Edge} (h_i^{Edge},h_j^{Edge})\n$$\n进行建模，$Dec^{Edge}$ 是一个pairwise score function\n\nloss定义为：\n$$\nL_i^{Edge} = - \\sum_{j^+\\in E_{i,\\lnot o}} log \\frac{exp(Dec^{Edge}(h_i^{Edge},h_{j^+}^{Edge}))}{\\sum_{j\\in S_i^-\\bigcup{j^+} }exp(Dec^{Edge}(h_i^{Edge},h_j^{Edge}))}\n$$\n$S_i^-$ 是指没有和节点i相连的节点\n\n下面是作者给出的属性图生成过程的说明性示例。\n\n![](https://i.loli.net/2021/05/16/i8IYhQ2NbSfEAKe.png)\n\n- a) 对于input graph 确定排列 $\\pi$\n- b) 随机挑选一部分与节点i相连的边作为已观测的$E_{i,o}$ ,剩下的作为masked edges $E_{i,\\lnot o}$ 并删除masked edges\n- c) 把节点分为属性生成节点和边生成节点\n- d) 计算节点 3，4，5的embedding，包括他们的属性生成节点和边生成节点。\n- d)-e) 通过对每个节点并行进行节点属性预测和masked预测来训练一个GNN模型\n\n\n\n具体算法流程：\n\n![](https://i.loli.net/2021/05/16/tGbrz7QJfmKEhdw.png)\n\n输入一个属性图，每次采样一个子图 $\\hat G$作为训练的实例进行训练。首先决定permutation order π。同时，我们希望能够并行化训练，只做一次前向传播，就能得到整个图的embedding，由此可以同时计算所有节点的loss。因此，根据permutation order π来移除边，也就是使每个节点只能从更低order的节点处获得信息。\n 之后，需要决定哪些边被mask。对于每个节点，获得其所有的出边，随机挑选一部分边被mask住，这一过程对应上述line4。\n 之后，对节点进行划分，得到整个图中节点的embedding，用于之后loss的计算，对应line5。\n line 7-9进行loss的计算。\n line 8中，通过整合采样图中未连接的节点和Q中以前计算的节点embedding来选择负样本，这种方式能够减轻对于采样图优化和对于整个图优化的差距。\n 在line11-12中，优化模型并更新Q。\n\n\n\n## GPT-GNN 对于异质的大图\n\n对于异构图，即包含不同类型的点和边的图，唯一的不同在于不同类型的点和边采用不同的decoder。\n 对于大规模的图，可以采样子图来进行训练，即上述算法流程中Sampler的作用。为了计算 $L_{edge}$ 这一loss，需要遍历输入图的所有节点。然而，我们只能在采样的子图上计算这个loss。为了缓解这一差异，提出了adaptive queue，其中存储了之前采样的子图的节点embedding作为负样本。每次采样一个新的子图时，逐步更新这个队列，增加新的节点embedding，移除旧的节点embedding。通过引入adaptive queue，不同采样子图中的节点也能为全局的结构提供信息。\n\n## 实验效果\n\n![](https://i.loli.net/2021/05/16/xER1ftIsSWcaK72.png)\n\n![](https://i.loli.net/2021/05/16/ZNcLJsHUqRGOhCk.png)","tags":["GNN"]},{"title":"Strategies for Pre-training Graph Neural Networks","url":"/2021/05/12/Strategies-for-Pre-training-Graph-Neural-Networks/","content":"\n# Strategies for Pre-training Graph Neural Networks\n\n目前深度学习各个领域的预训练都搞的热火朝天，GNN也是肯定要搞的。那么预训练之后下一个热潮会是什么呢？\n\nICLR2020 首次系统的探索了大规模GNN预训练\n\n提出了一种结合节点级和图级表示的预训练方法来训练模型。\n\n在节点级，使用了两种自监督方法，即上下文预测和属性预测。\n\n在图形级，使用有监督的图级属性预测和结构相似性预测\n\n同时作者建立了两个新的预训练数据集，2M graph的化学数据集和一个有395K graph的生物数据集。\n\n接下来介绍作者这么做的理由\n\n## 发现\n\n因为对于特定任务的有标签数据是很稀少的，但无标签数据却有很多，所以为了充分利用无标签数据，各种自监督方法开始兴起。\n\n所以作者分别在图级和节点级层面上提出了两大类预测方法\n\n- 属性预测：属性mask(节点)、有监督的属性预测(图级)\n- 结构预测：上下文预测(节点)、结构相似性预测(图级)\n\n以往的一些研究表明(Xu et al., 2017; Ching et al., 2018; Wang et al., 2019),一个成功的迁移学习不仅仅是增加与下游任务来自同一领域的标注好的预训练数据集的数量。相反，它需要大量的领域专业知识来仔细选择与感兴趣的下游任务相关的示例和目标标签。否则，知识从相关的预训练任务转移到新的下游任务可能会损害泛化，这被称为负迁移(Rosenstein等人，2005年)，并极大地限制了预训练模型的适用性和可靠性。\n\n作者研究发现朴素的策略要么在整个图的层面上预先训练GNN，要么在单个节点层面上预先训练GNN，所给出的改进有限，甚至可能导致许多下游任务的负迁移。在只有图级的预训练下大约有1/4的任务出现了负迁移。\n\n![](https://i.loli.net/2021/05/12/z5CEtxbX9Tj1WwN.png)\n\n图(a.i)当仅使用节点级预训练时，可以很好地分离不同形状的节点(语义上不同的节点)，但汇集节点级嵌入创建的结果，图嵌入是不可分离的(图嵌入由+和−表示)\n\n图(a.ii)仅在图级预训练的情况下，图嵌入可以很好地分离，但是单个节点的嵌入并不一定捕获它们特定于领域的语义。\n\n图(a.iii) 高质量的节点嵌入使得不同类型的节点能够很好地分开，同时嵌入空间也是可组合的。这允许对整个图形进行准确和健壮的表示，并允许将预先训练的模型健壮地传输到各种下游任务。\n\n\n\n## 预训练策略\n\n在预训练策略的技术核心是在单个节点以及整个图的级别预先训练。这一概念鼓励GNN在两个级别捕获特定域的语义。\n\n\n\n### 节点级预训练\n\n两种自监督方法，上下文预测和属性mask。\n\n![](https://i.loli.net/2021/05/12/RFS46a2tozyNGkp.png)\n\n图(a)在上下文预测中，子图是所选中心节点周围的K跳邻域，其中K是GNN层的数量，上图中设置为K=2。环境定义为中心节点r1-和r2-Hop之间的周围图结构，上图中使用r1=1和r2=4。\n\n图(b) 在属性mask中，输入节点/边属性(例如，分子图中的原子类型)被随机mask，并且要求GNN预测它们。\n\n#### 上下文预测：利用图结构的分布性\n\n使用子图来预测其周围的图结构。目标是预先训练GNN，以便它将出现在类似结构上下文中的节点映射到附近的嵌入。\n\n通过三个步骤：\n\n- 邻居节点和上下文图\n\n  对于每个节点v，定义v的邻居和上下文图。因为GNN信息聚合的是K层邻居，所以节点v的嵌入$h_v$ 依赖于距离v至多k跳节点。上下文图由两个超参数r1和r2来描述，并且它表示远离v的r1跳和r2跳之间的子图(即它是宽度为r2−r1的环)。并且r1<K，以便在邻域和上下文图之间共享一些节点，我们将这些节点称为上下文锚节点。这些锚节点提供关于邻居图和上下文图如何彼此连接的信息。\n\n- 使用一个辅助GNN把上下文编码成固定向量\n\n  由于图的组合性，直接预测上下文图是很困难的。这与自然语言处理不同，在自然语言处理中，单词来自固定和有限的词汇表。为了实现上下文预测，将上下文图编码为固定长度的向量。为此，引入一个上下文GNN作为辅助编码，就是图中的GNN‘。首先用其获得上下文图中的节点嵌入，然后对上下文锚点的嵌入进行平均，得到固定长度的上下文嵌入。对于图G中的节点v，将其对应的上下文嵌入表示为$c^G_v$\n\n- 负采样\n\n  主要的GNN编码邻居节点获取节点的embedding—— $h_v^{(K)}$ ，上下文GNN编码上下文图获取上下文embedding——$c^G_v$。学习目标是一个二分类：是否特定邻域和特定上下文图是否属于同一节点。\n  $$\n  \\sigma(h^{(k)T}_v c_{v'}^{G'}) \\approx 1 \\{\\text{v and v' are the same nodes}\\}\n  $$\n  \n\n  让v‘=v并且G’=G(即正例)，或者我们从随机选择的图G‘中随机抽样v’(即负例)。\n\n#### 属性mask:利用图属性的分布性\n\n目标是通过学习图结构上节点/边属性的分布规律来获取领域知识。\n\n属性mask有节点mask和属性mask两类\n\n工作原理：掩蔽节点/边缘属性，然后让GNN基于相邻结构预测这些属性，这参考了bert的mask。\n\n具体地说，通过用特殊的屏蔽指示符替换输入节点/边属性(例如分子图中的原子类型)来随机屏蔽它们。然后应用GNNs来获得相应的节点/边嵌入(边嵌入:为边的端点的节点嵌入之和来获得)。\n\n最后，在嵌入的基础上应用线性模型来预测被mask的节点/边属性。有趣的是bert的mask其实相当于在全连通的token图上应用了消息传递。\n\n在图结构数据中是对非全连通图进行操作，目的是捕捉节点/边属性在不同图结构上的分布规律。\n\n\n\n### 图级别预训练\n\n我们的目标是确保节点和图嵌入都是高质量的，以便图嵌入是健壮的，并且可以跨下游任务传输。\n\n有两个用于图级预训练的选项：预测整个图的特定于域的属性(监督标签)，或者预测图结构。\n\n#### 有监督的图级属性预测\n\n由于图形级表示 $h_G$ 直接用于对下游预测任务进行微调，希望将特定于域的信息直接编码成 $h_G$。\n\n考虑了一种对图表示进行预训练的实用方法：图级多任务监督预训练，用于联合预测单个图的不同监督标签集。例如，在分子性质预测中，我们可以预先训练GNN来预测到目前为止实验测量的分子的所有性质。在蛋白质功能预测中，目标是预测给定的蛋白质是否具有给定的功能，我们可以预先训练GNN来预测到目前为止已经验证的各种蛋白质功能的存在。\n\n重要的是，单独进行大量的多任务图级预训练可能无法给出可转移的图级表示。(问题来了)\n\n这是因为一些有监督的预训练任务可能与下游感兴趣的任务无关，甚至会损害下游的绩效（负迁移）。一种解决办法是选择“真正相关的”有监督的训练前任务，只对这些任务进行训练前GNN训练。然而，这样的解决方案成本极高，因为选择相关任务需要大量的领域专业知识，并且需要针对不同的下游任务分别进行预训练。\n\n为了缓解这个问题，作者的见解是，多任务监督的预训练只提供图形级的监督；因此，创建图形级嵌入的本地节点嵌入可能没有意义。这种无用的节点嵌入可能会加剧负迁移问题，因为许多不同的预训练任务在节点嵌入空间中更容易相互干扰。受此启发，在执行图级预训练之前，先通过上文描述的节点级预训练方法在单个节点级别对GNN进行正则化。正如作者所料，组合策略产生了更多可转移的图形表示。并且在没有专家选择监督的预训练任务的情况下稳健地改善了下游性能。\n\n\n\n#### 结构相似性预测\n\n目标是对两个图的结构相似性进行建模\n\n此类任务的示例包括对图形编辑距离进行建模(Bai等人，2019年)或预测图形结构相似性(Navarin等人，2018年)。\n\n这里好像作者感觉比较难没有全部实现，留到了以后的工作中\n\n\n\n### 总体预训练策略\n\n预训练策略是首先进行节点级的自监督预训练，然后进行图级多任务监督的预训练。当GNN预训练完成后，我们对下游任务的预训练GNN模型进行微调。具体地说，我们在图级表示的基础上添加线性分类器来预测下游的图标签。随后以端到端的方式微调整个模型，即预先训练的GNN和下游线性分类器。\n\n\n\n## 进一步相关工作\n\n关于图中单个节点的无监督表示学习的文献非常丰富，大致分为两类。\n\n第一类是使用基于局部随机行走的目标的方法(Grover&Leskovec，2016；Perozzi等人，2014；Don等人，2015)以及例如通过预测边的存在来重建图的邻接矩阵的方法。\n\n在第二类中是诸如Deep Graph Infomax的方法，其训练最大化局部节点表示和聚集的全局图表示之间的互信息的节点编码器。(基于对比学习互信息的最近也要研究研究)\n\n这两种方法都鼓励附近的节点具有相似的嵌入表示，最初是针对节点分类和链路预测提出和评估的。然而，这对于图级预测任务来说可能是次优的，在图级预测任务中，捕捉局部邻域的结构相似性通常比捕捉图中节点的位置信息更重要\n\n所以该预训练策略既考虑了节点级的预训练任务，也考虑了图级的预训练任务，并且正如在实验中所显示的，为了使预训练模型获得良好的性能，必须同时使用这两种类型的任务。\n\n\n\n## 实验\n\n![](https://i.loli.net/2021/05/12/cFBosvWCfURYdhx.png)\n\n阴影单元格表示负迁移，即预训练模型的ROC-AUC比未预训练模型的ROC-AUC差。借此说明两个级别共用的重要性。\n\n\n\n![](https://i.loli.net/2021/05/12/HvFtBiY5RadqGMw.png)\n\n在有无预培训的情况下测试不同GNN架构的ROC-AUC(%)性能。\n\n这里表达能力越强的结构预训练效果越好，表达能力较弱的GNN收益较小，甚至有时未负。这一发现证实了先前的观察结果(例如，Erhan等人)。(2010))，使用富有表现力的模型对于充分利用预培训至关重要，当用于表达能力有限的模型(如GCN、GraphSAGE和GAT)时，预培训甚至会影响性能。\n\n并且GAT的表现反而下降了不少。作者认为GAT属于表达能力有限的模型，还有人认为GAT attention的参数比较多，模型结构比较复杂导致。\n\n\n\n![](https://i.loli.net/2021/05/12/kFYCKXIvcU2iLeZ.png)\n\n\n\n\n\n\n\n\n\n","tags":["GNN"]},{"title":"Meta Learning(李宏毅)","url":"/2021/05/09/Meta-Learning-李宏毅/","content":"\n# Meta Learning\n\n李宏毅：https://www.bilibili.com/video/BV15b411g7Wd?p=57&spm_id_from=pageDriver\n\n一个不错的科普：https://www.bilibili.com/video/BV1KB4y1c7gg?from=search&seid=2922012165894972973\n\n## 什么是元学习\n\nMeta Learning = Learn to Learn (学习如何去做学习这件事)\n\n机器在学习了很多task后，在获得过去的任务下所汲取的经验后，学习到了更多的学习技巧，成为了一个更厉害的学习者。\n\n从而有一个新任务，他可以学的更快更好。\n\n比如：task1你教机器去学语音识别，task2你教他去做图片识别，那么task3你让他去学习文字识别，那么他可能学的会更好。\n\n元学习的输入是训练数据，输出的是可以用于下一个任务的function，function也就是万能函数模拟器神经网络的模型参数\n$$\n\\begin{equation}\\begin{split} \n f^* = F(D_{train})\n    \\end{split}\\end{equation}\n$$\n其中F 代表元学习算法，D是数据，f就是function。理解下图：\n\n![](https://i.loli.net/2021/05/09/rMLgmoSywHJcq5u.png)\n\n### 和机器学习的区别\n\n机器学习：定义一系列function--->定一个function好坏的指标-----> 用gradient decent找到一个最好的function\n\n元学习(也是找一个function)：定义一系列大Function----->定一个评价大Function好坏的指标----->找到一个最好的大Function\n\n\n\n### 和终身学习(Life-long learning)有些像？\n\n[持续/终身学习](https://blog.csdn.net/zyy617532750/article/details/104217399)：是让同一个模型可以同时学会很多任务技能\n\n而元学习是不同的任务仍然有不同的模型，我们期待的是模型通过以前的学习经历可以让他在未来别的任务上学的好。\n\n\n\n\n\n## 元学习过程\n\n### 定义一系列学习算法\n\n为什么是一系列学习算法，其实不同的模型参数、不同的结构、不同的学习参数的组合都是不同的学习算法。\n\n![](https://i.loli.net/2021/05/09/P6iANVszETHWB37.png)\n\n以梯度下降法为例，首先定义一个网络结构，初始化一个参数，通过训练数据计算一个梯度g，再通过学习率更新参数。\n\n迭代多次最后得到最终参数$\\hat \\theta$\n\n但上图中红色框框内的都是人为定义的。元学习就是想让这红框内的东西，不让人来设计，让机器根据先验知识来自己学习设计。\n\n### 评估function参数好坏\n\n让模型先学一些任务，去解一些问题看看。\n\n比如Task1：用一些$D_{train}$ 数据去训练模型得到$f_1$ ,再用Task1的$D_{test}$ 去衡量 $f_1$ 得到一个loss $l_1$\n\n一个任务不够，再多找些任务来\n\nTask2：用一些$D_{train}$ 数据去训练模型得到$f_2$ ,再用Task2的$D_{test}$ 去衡量 $f_2$得到一个loss $l_2$\n\n最后得到评价F好坏的Loss：\n$$\n\\begin{equation}\\begin{split} \n L(F) &= \\sum_{n=1}^Nl_n\\\\\n F^* &= argmin_FL(F)\n    \\end{split}\\end{equation}\n$$\nN 为任务数\n\nmeta learning 通常会把task的Train叫做Suppot set，Test叫做Query set\n\n\n\n## MAML(Model Agnostic Meta-Learning)\n\n学一个初始化的参数\n$$\n\\begin{equation}\\begin{split} \n L(\\phi) = \\sum_{n=1}^N l^n(\\hat \\theta^n)\n    \\end{split}\\end{equation}\n$$\n$\\phi$ 输入的初始化参数，$\\hat \\theta^n$ 在第n个task上学出来的model，$\\hat \\theta^n$ 取决于$\\phi$ \n\n$l^n(\\hat \\theta^n)$: 把$\\hat \\theta^n$这组参数拿到第n个task的测试集中去看看效果怎么样\n\n怎么确定初始化的参数好不好，就用初始化参数到不同task上去做训练\n\n最小化$L(\\phi)$ : $\\phi \\gets \\phi-\\alpha ▽_{\\phi}L(\\phi)$\n\n### 和迁移学习(Transfer learning) 预训练有些像？\n\n迁移学习：某一个任务的数据很少，但另外一个任务的数据多。就把model预训练在多的数据上，再fine-tuning在少的数据上。\n\n他的loss function：\n$$\n\\begin{equation}\\begin{split} \n  L(\\phi) = \\sum_{n=1}^N l^n(\\phi)\n    \\end{split}\\end{equation}\n$$\n在MAML里面loss是用$\\phi$ 训练完后的model计算出来的，是训练过后的model\n\n在pretrain里是用现在这个model直接去下游任务中衡量表现怎么样。\n\n有的文章把预训练改成MAML的形式，以缓解预训练任务和下游任务直接目标不同产生的gap。\n\n\n\n在MAML中，我们不在意$\\phi$ 在training task上的表现，在意的是用$\\phi$ 训练出来的$\\hat \\theta^n$的表现如何\n\n（面向的是**学习的过程**，并不是**学习的结果**）\n\n![](https://i.loli.net/2021/05/10/7V2Uua4g8e9R1tk.png)\n\n![](https://i.loli.net/2021/05/10/epRfZzxlFTgSjVI.png)\n\n如上图虽然$\\phi$ 本身表现不够好，但$\\phi$经过训练以后可以变得很强 (潜力如何)\n\n而pretrain在意的是现在这个$\\phi$表现的怎么样，是在找寻在所有task都最好的$\\phi$, 并不保证训练以后会得到好的 $ \\hat \\theta^n$ （现在表现如何）\n\n并且MAML只训练很少的步数，因为\n\n- 为了快速\n- 希望在训练一步就得到很好的结果\n- 在使用算法模型时可以多update\n- 为了适应Few-shot learning \n\n### Toy Example\n\n每一个任务：\n\n- 给一个目标sin函数 $y = a sin(x+b)$ 其中 a、b 都是随机数，每一组 a、b 对应一条正弦曲线\n- 从目标函数中采样k个点\n- 使用采样点去估计目标函数\n\n希望拟合的y越好越好。随机采样不同的a和b就可以得到不同的任务。\n\n![](https://i.loli.net/2021/05/10/9YnTfrxqoBDgVCU.png)\n\n\n\n\n\n\n\n## 参考文献\n\n[元学习-总结](https://zhuanlan.zhihu.com/p/367684934)\n\n[元学习（Meta-learning）——李宏毅老师教学视频笔记](https://zhuanlan.zhihu.com/p/108503451)\n\n[[meta-learning] 对MAML的深度解析](https://zhuanlan.zhihu.com/p/181709693)\n\n","tags":["ML&DL"]},{"title":"Learning to Pre-train Graph Neural Networks","url":"/2021/05/08/Learning-to-Pre-train-Graph-Neural-Networks/","content":"\n# Learning to Pre-train Graph Neural Networks\n\n\n\n## 动机与挑战\n\n图神经网络也是有预训练模型的，预训练之所以可以提升，可以解释为获取了有用的先验知识，并迁移到任务中。\n\n常规的GNN预训练步骤和其他网络一样分为两个步骤：\n\n- 在大量未标记的图数据上预先训练GNN模型，其导出编码固有图属性的通用可转移知识\n- 在特定于任务的图形数据上对预先训练的GNN模型进行微调，以使通用知识适用于下游任务。\n\n但之前有人已经研究过直接进行fine-tuning效果不提反降，产生负迁移效果。应该是出自(Strategies for Pre-training Graph Neural Networks 如何解决的以后看了这篇论文再说) \n\n而这篇文章的主要想解决的是由于预训练和fine-tuning优化目标的不同，两者之间存在明显差距，损害了模型的泛化效果。\n\n引出了第一个挑战：如何缩小不同优化目标带来的差距？ --->>元学习思想\n\n那GNN的预训练模型的特点是不仅要考虑局部的节点级先验知识还要获取图级别的全局先验知识 (现有方法要么只考虑节点级的预训练，或者仍然需要用有监督的图级预训练)\n\n引出了第二个挑战：如何利用完全未标记的图数据同时保留节点级和图形级信息？\n\n提出了L2P-GNN，计了节点级和图级的双重自适应机制，并且是完全自监督的方式。\n\n\n\n## 设计\n\n### GNN\n\n首先定义一个图 $G = (V,E,X,Z)$ , 其中 $V$ 是节点、$E$ 是边、$X \\in R ^{|V|\\times d_v}$ 是节点特征、 $Z \\in R^{|E|\\times d_e}$ 是边的特征。\n\nGNN 一般包含两个关键的计算，一个是聚合信息的操作AGGREGATE，另一个是更新操作UPDATE\n\n节点表示：节点v的l层表示由下式给出：\n$$\n\\begin{equation}\\begin{split} \n h_v^l &= \\Psi (\\psi, A, X,Z)^l\\\\\n &= \\text{UPDATAE}(h_v^{l-1}, AGGREGATE(\\{(h_v^{l-1}, h_u^{l-1}, z_{uv}): u\\in N_v\\}))\n\\end{split}\\end{equation}\n$$\n其中 $z_{uv}$ 是u到v的边特征向量，A是邻接矩阵 ，$N_v$ 是v的邻居节点。$\\Psi$ 是聚合和更新操作的定义，$\\psi$ 是可学习参数。\n\n图级的表示：通常用READOUT \n$$\n\\begin{equation}\\begin{split} \n h_G = \\Omega(w ; H^l) = \\text{READOUT} (\\{h_v^l| v\\in V\\})\n    \\end{split}\\end{equation}\n$$\n其中$H^l = [h_v^l]$ 是节点级表达矩阵。READOUT的典型实现有sum、max、mean池化，或者用其他复杂一点的方法。\n\n### 常规GNN的预训练\n\n1. 预训练：定义 $D^{pre}$ 为预训练图数据，$L^{pre}$ 预训练的loss ，优化目标为：\n\n$$\n\\theta_0 = argmin_{\\theta}  L^{pre} (f_\\theta; D^{pre})\n$$\n\n2. fine-tuning：目标是，在对下游任务的训练集图数据$D^{tr}$进行微调之后，最大化下游测试集图数据$D^{te}$上的表现 \n\n所谓的微调根据预先训练的参数$\\theta_0$来初始化模型，并且用在(通常是批处理的)$D_{tr}$上的多步梯度下降来更新GNN模型 $f_{\\theta}$。\n$$\n\\theta_1 = \\theta_0 - \\eta ▽_{\\theta_0} L^{fine}(f_{\\theta_0};D^{tr})\n$$\n其中 $ \\eta$ 学习率\n\n可见常规的预训练和finetuing是解耦的，参数$\\theta_0$ 和下游没有适应性的联系形式。\n\n为此，作者提出通过构建预训练阶段来模拟下游任务的微调过程，从而直接优化预训练模型对下游任务的快速适应性。\n\n### 新的预训练方法\n\n其实就是元学习的思想 参考上文 [Meta Learning(李宏毅)](https://coding-zuo.github.io/2021/05/09/Meta-Learning-%E6%9D%8E%E5%AE%8F%E6%AF%85/)\n\n现有$G\\in D^{pre}$ 从中采样一些子图 定义为$D^{tr}_{T_G}$ 作为模拟下游任务$T_G$的训练数据——元学习中的support sets，再采样一些子图作为$D^{te}_{T_G}$ 作为模拟的验证集——元学习中的query sets。\n$$\n\\theta_0 = argmin_{\\theta} \\sum_{G\\in D^{pre}} L^{pre}(f_{\\theta - \\alpha ▽_{\\theta}L^{pre}(f_{\\theta}; D^{tr}_{T_G})}; D^{te}_{T_G})\n$$\n$\\theta - \\alpha ▽_{\\theta}L^{pre}(f_{\\theta}; D^{tr}_{T_G})$ 相当于在$D^{tr}_{T_G}$ 预训练的测试集先进行了一次fine-tuning\n\n作者认为：因此，预培训输出$θ_0$并不是为了直接优化任何特定任务的训练或测试数据。相反，θ0通常是最佳的，因为它允许快速适应新任务。\n\n我认为：这类似元学习的思想，还可以从元知识的角度来描述。还有一个点，这个预训练数据集和下游任务相不相关呢？如果相关度不大会不会有用，如果相关会不会更好？\n\n\n\n## L2P_GNN\n\n两个特点：\n\n- 从局部和全局角度捕捉图形中的结构和属性\n- 套用MAML获得元学习的先验知识可以适应新的任务或图表\n\n### 任务实施\n\n定义每个任务的图数据随机采样得到 $T_G = (S_G,Q_G)$ , $S_G$ 为Support set ，$Q_G$ 为Query set\n\n![](/Users/zuoyuhui/Library/Application Support/typora-user-images/image-20210510173143614.png)\n\n多个任务的支持集合查询集为: $S_G =(S_G^1,S_G^2,...,S_G^k) ,Q_G =(Q_G^1,Q_G^2,...,Q_G^k)$\n\n在给定父任务和子任务的情况下，作者设计了一个节点级聚集和图级汇集的自监督基本GNN模型，分别学习节点和图的表示。其核心思想是利用无标签图数据的内在结构作为节点级和图级的自我监督。\n\n\n\n节点级：自监督预测u和v节点有边链接的目标函数\n$$\nL^{node}(\\psi;S_G^c) = \\sum_{(u,v)\\in S_G^e} -ln(\\sigma(h_u^Th_v)) -ln(\\sigma(-h_u^Th_{v'}))\n$$\n其中 $v'$ 是负采样节点，是没有和u有边的节点。\n\n图级：通过图池化获得图表达$h_G$，每个任务的支持集图表达为 $h_{S_G^c} = \\Omega(w;\\{h_u|\\forall u,\\exists v:(u,v) \\in S_G^c\\})$\n$$\nL^{graph} (w; S_G) = \\sum_{c=1}^k -log(\\sigma(h_{S_G^c}^T h_G)) -log(\\sigma(-h^T_{S_G^c}h_{G'}))\n$$\n两个级别的loss综合到一起：\n$$\nL_{T_G}(\\theta;S_G) = L^{graph}(w;S_G) + \\frac{1}{k} \\sum_{c=1}^k L^{node}(\\psi;S_G^c)\n$$\n其中$\\theta = \\{\\psi,w\\}$ 是可学习参数，就是可迁移的先验知识\n\n### 双重适应(图级和节点级)\n\n\n\n![](https://i.loli.net/2021/05/10/ahQdOwrHvFgKIxy.png)\n\n节点级：支持loss采用一个或几个梯度下降步骤，以获得子任务的适应先验 $ψ$。例如，当使用一个具有节点级学习率$α$的梯度更新时:\n$$\n\\psi' = \\psi - \\alpha \\frac{\\partial\\sum_{c=1}^k L^{node}(\\psi;S_G^c)}{\\partial\\psi}\n$$\n图级：\n$$\nw' = \\psi - \\beta \\frac{\\partial  L^{graph}(w;S_G^c)}{\\partial w}\n$$\n所有任务的更新参数过程\n$$\n\\theta \\gets \\theta - \\gamma\\frac{\\partial\\sum_{G\\in D^{pre}}L_{T_G}(\\theta';Q_G) }{\\partial \\theta}\n$$\n\n## 实验\n\n实验的主要目的：要验证有没有缩小预训练和微调的gap图级和节点级预训练策略是否奏效\n\n作者对预训练的GNN模型在下游任务微调前后（命名为model-P和model-F）进行了对比分析，并考虑了三个比较视角：model-P和model-F参数之间的中心核对齐相似性（CKA），训练损失（delta损失）和下游任务测试性能（delta RUC-AUC或Micro-F1）的变化。\n\n![](https://i.loli.net/2021/05/10/wiGa9eSAcgNy62u.png)\n\n如图所示，观察到L2P-GNN参数在微调前后的CKA相似性通常比基线的CKA相似性小，这表明L2P-GNN经历了更大的变化，以便更好地适应下游任务。\n\nCKA 是测量神经网络表示相似性的，可以对迁移学习任务进行评估，值越小越相似。\n\n此外，L2P-GNN的训练损失变化较小，说明L2P-GNN通过快速适应可以很容易地达到新任务的最优点。\n\n\n\n\n\n\n\n## 参考\n\n### GNN预训练的论文\n\nHu, W.; Liu, B.; Gomes, J.; Zitnik, M.; Liang, P.; Pande, V. S.; and Leskovec, J. 2020. Strategies for Pre-training Graph Neural Networks. In *Proceedings of ICLR*.\n\nHu, Z.; Fan, C.; Chen, T.; Chang, K.; and Sun, Y. 2019. Pre-Training Graph Neural Networks for Generic Structural Feature Extraction. *CoRR* abs/1905.13728.\n\nNavarin, N.; Tran, D. V.; and Sperduti, A. 2018. Pre-training Graph Neural Networks with Kernels. *CoRR* abs/1811.06930.\n\n### 元学习\n\nFinn, C.; Abbeel, P.; and Levine, S. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. In *Pro- ceedings of ICML*, 1126–1135.\n\nLu, Y.; Fang, Y.; and Shi, C. 2020. Meta-learning on Hetero- geneous Information Networks for Cold-start Recommenda- tion. In *Proceedings of KDD*, 1563–1573.","tags":["GNN"]},{"title":"DP——最大子序和","url":"/2021/05/06/DP——最大子序和/","content":"\n# DP——最大子序和\n\nhttps://www.acwing.com/problem/content/137/\n\n输入一个长度为 n 的整数序列，从中找出一段长度不超过 m 的连续子序列，使得子序列中所有数的和最大。\n\n**注意：** 子序列的长度至少是 1。\n\n#### 输入格式\n\n第一行输入两个整数 n,m。\n\n第二行输入 n 个数，代表长度为 n 的整数序列。\n\n同一行数之间用空格隔开。\n\n#### 输出格式\n\n输出一个整数，代表该序列的最大子序和。\n\n#### 数据范围\n\n1≤n,m≤300000\n\n#### 输入样例：\n\n```\n6 4\n1 -3 5 1 -2 3\n```\n\n#### 输出样例：\n\n```\n7\n```\n\n\n\n\n\n## 解\n\n![](https://i.loli.net/2021/05/06/QHsdPGY1q7FrtpB.png)\n\n\n\n状态转移方程：集合代表的喊一声所有以i结尾的子段，如果i=3的话，那么集合可能是{1,num[i]}、{1,-3,num[i]}、{1,5,num[i]}、{num[i]} ，目标是求这些集合中的最大值，因为每个集合都有num[i]可先不考虑num[i]。\n\n所以只要考虑f[i-1]+num[i] ,和只有num[i]的集合的最大值。\n\n也就是考虑f[i-1]和0谁最大。\n\n最终的答案是所有集合的值取最大\n\n## 代码\n\nLeetCode53 不限制最大子序列长度\n\n```java\nclass Solution {\n    public int maxSubArray(int[] num) {\n        int last = 0;\n        int res = Integer.MIN_VALUE;\n        for (int i = 0; i < num.length; i++) {\n            int now = Math.max(last, 0) + num[i];\n            res = Math.max(res, now);\n            last = now;\n        }\n        return res;\n    }\n}\n```\n\n\n\nacwing 135 限制最大子序列长度\n\n不同的是，对于每一个i，要求前面长度为m这个段内，求一个最小值\n\n![](https://i.loli.net/2021/05/06/KfOS1Mzt4GxnoUd.png)\n$$\nmax\\{Sum_i - Sum_j\\} , i-m 到 i-1\n$$\n可以用一个队列来维护m个数\n\n每次i向后移动，就插入一个数同时队首出列\n\n- 用一个单调队列\n- 把没用的数删去\n- 变成单调递增的序列\n- 用$0(1)$ 把 min或max找出\n\n```java\n\npublic class Main{\n\n    void run(){\n        int n = jin.nextInt();\n        int m = jin.nextInt();\n        nums.add(0);\n        for (int i = 0 ; i < n ; i++) nums.add(jin.nextInt());\n        for (int i = 1 ; i <= n ; i++) nums.set(i, nums.get(i)+nums.get(i-1));\n\n        int res = Integer.MIN_VALUE;\n        for (int i = 1; i <= n ; i++){\n            while(!queue.isEmpty() && i - queue.peekFirst() > m) queue.removeFirst();\n\n            if (!queue.isEmpty()) res = Math.max(res, nums.get(i) - nums.get(queue.peekFirst())); // why not peekF -1 ?\n            else res = Math.max(res, nums.get(i));                                              // 差点漏掉了\n            while(!queue.isEmpty() && nums.get(i) <= nums.get(queue.peekLast())) queue.removeLast();\n            queue.offerLast(i);\n        }\n        res = Math.max(res, nums.get(n) - nums.get(queue.peekFirst()-1));\n        System.out.println(res);\n    }\n\n    List<Integer> nums = new ArrayList<>();\n    Deque<Integer> queue = new LinkedList<>();\n    private Scanner jin = new Scanner(System.in);\n    public static void main(String[] args) throws Exception {new Main().run();}\n}\n\n```\n\n\n\n\n\n\n\n\n\n\n\n","tags":["DP"]},{"title":"海华阅读理解比赛复盘","url":"/2021/05/01/海华阅读理解比赛复盘/","content":"\n# 海华阅读理解比赛复盘\n\n比赛详情、EMA、Baseline，本文主要记录提分点和模型改进的验证\n\n参考上文 [海华中文阅读理解比赛梳理/多卡并行/transformers](https://coding-zuo.github.io/2021/04/06/%E6%B5%B7%E5%8D%8E%E4%B8%AD%E6%96%87%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%AF%94%E8%B5%9B%E6%A2%B3%E7%90%86-%E5%A4%9A%E5%8D%A1%E5%B9%B6%E8%A1%8C-transformers/)\n\n[github](https://github.com/Coding-Zuo/MRC_multiChoice)\n\n\n\n## 数据增强\n\n数据增强的办法很多参考 https://zhuanlan.zhihu.com/p/145521255\n\n我只采用了句子乱序和数据回译，都是将增强数据和原始数据挨着放到数据集中，在训练的时候停用shuffle。(可能有其他方法：每条数据根据概率来选择性增强)，我这种可能会让数据集臃肿，质量下降。\n\n### 句子乱序\n\n没有提分，也没有降很多。\n\n原因参考：[从知觉谈中文乱序不影响阅读的原因](https://zhuanlan.zhihu.com/p/107594976)\n\n代码：https://github.com/Coding-Zuo/MRC_multiChoice/blob/main/train/data_process.py 中的data_enhancement_sentence_order\n\n### 数据回译\n\n和句子乱序一样和回译到的数据和原始数据挨着放到数据集，没有提分，可能是回译到的数据质量不好。\n\n使用的是百度API，百度限制一个账户免费200万字符，如果超了就多注册几个账户薅羊毛。\n\n代码：https://github.com/Coding-Zuo/MRC_multiChoice/blob/main/TranslateAPI.py\n\n\n\n### 在训练集上打伪标签\n\n由于时间问题，没有直接提交伪标签训练的结果，就直接模型融合。验证集有提高。\n\n用训练好的模型去inference测试集，取了模型认为有百分之85概率认为是正确答案的数据打上伪标签，加入到训练集训练。\n\n\n\n## 优化训练\n\n### EMA\n\n滑动平均exponential moving average\n\n没有提分，反而效果变差。具体原因，还在探索，可能和优化方法有关？\n\n我一直使用的都是adamw，[比较Adam 和Adamw](https://www.cnblogs.com/tfknight/p/13425532.html) [一文告诉你Adam、AdamW、Amsgrad区别和联系](https://zhuanlan.zhihu.com/p/39543160)，AdamW是在Adam+L2正则化的基础上进行改进的算法。\n\n可以和sgd搭配看看效果。(这方面因为时间问题没有尝试充足)\n\n[PyTorch指数移动平均(EMA)手册](https://blog.csdn.net/weixin_43002433/article/details/113531466)\n\n指数移动平均EMA是用于估计变量的局部均值的，它可以使变量的更新不只取决于当前时刻的数据。\n\n而是加权平均了近期一段时间内的历史数据，是的变量的更新更平滑，不易受到某次异常值的影响。\n\n\n\n### labelSmoothing\n\n精度提升不明显，但是缓解了验证集的loss上升。\n\n```python\nclass LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, eps=0.1, reduction='mean'):\n        super(LabelSmoothingCrossEntropy, self).__init__()\n        self.eps = eps\n        self.reduction = reduction\n\n    def forward(self, output, target):\n        c = output.size()[-1]\n        log_preds = F.log_softmax(output, dim=-1)\n        if self.reduction == 'sum':\n            loss = -log_preds.sum()\n        else:\n            loss = -log_preds.sum(dim=-1)\n            if self.reduction == 'mean':\n                loss = loss.mean()\n        return loss * self.eps / c + (1 - self.eps) * F.nll_loss(log_preds, target, reduction=self.reduction)\n```\n\n\n\n### 对抗训练\n\n提升两个点以上\n\n可参考我的 [ppt](https://coding-zuo.github.io/adversary/index.html) 和以前文章\n\n主要使用了fgm和pgd两个，都有提升的效果\n\n但有时候pgd并没有提升，可能是在有些参数和加了伪标签的数据情况下，学习困难？\n\n\n\n### 早停\n\nbert的早停不太好控制，有时候一两个epoch之后还会更新，可能跟参数有关。\n\n\n\n\n\n## 模型改进\n\n### 尝试用LongFormer\n\n因为文本比较长，但因为没有时间测试而没有跑，不过已经基本调通，日后跑一跑。\n\n\n\n### 复现DUMA\n\n用co-attention 来分别处理 bert输出的文章编码和问题答案对编码，分别送到co-attention中。\n\n我的方法是分别为文章和问题答案设置一个maxlen， 多的截掉，因为我机器只能最大总长度跑到400，而数据文章又比较长，可能这也会导致学习瓶颈的出现。\n\n我的另一个实现想法但是没有时间做的是，把文章和问题答案拼在一起用sep分割送入bert，输出时只要找到sep的timesteps进行分割，对于得到的两个不等长的向量，在经过对其。送入co-attention。\n\n训练刚开始有一个比较好的提分劲头，但随着深入训练后期效果乏力。可能是因为参数没有调好？DUMA那篇论文没有复现细节。\n\n\n\n### 尝试其他比赛前排模型\n\n![](https://i.loli.net/2021/05/01/f1QIsuWtSVXCcBx.png)\n\n移植后问题：训练集准确率很低，具体问题还需探究。\n\n\n\n### 尝试在bert后加self-attention层\n\n用pool_output,投入自注意力，没有明显提升\n\n在bert后加多层线性也没有明显提升。不过可以尝试加highway network。\n\n\n\n## 模型融合\n\n组合不同参数和结构的打包模型，用argmax的方法融合了九个，达到最好的51.7分，晋级分数最终为52分，遗憾落榜。\n\n还尝试用实现vote投票来融合，并没有最终提交。\n\n以后将会尝试实现bert的stacking融合。\n\n\n\n## 遇到的难题\n\n1. bert换成roberta后始终不收敛，因为没有经验，学习率试过1e-5, 1e-6, 2e-5,和不同batch32、64、128进行组合都不收敛(浪费了很多时间)。最终发现学习率在1e-5,2e-5 ,batch 在8或16才会收敛。\n\n   并参照roberta论文附录中的参数，收敛了，但是效果没有达到预期，不过听说好多人也是用了roberta。\n\n![](https://i.loli.net/2021/05/01/7vZQHiFus6DqJI2.png)\n\n2. 调参没经验，浪费了很多时间。\n\n\n\n## 总结\n\n用了将近一个月的时间来做这个比赛，对模型训练体系、模型理解、微调下游任务、多卡并行、对抗训练。还有好多理论需要通过实践来加深理解。\n\n\n\n\n\n\n\n\n\n\n\n","tags":["DataGame"]},{"title":"阅读理解文献梳理","url":"/2021/04/29/阅读理解文献梳理/","content":"\n# 阅读理解文献梳理\n\n\n\n## 多跳QA\n\n\n\n### 模型在任务中学习的多跳推理行为。\n\nQFE (Nishida et al., 2019) regards evidence extraction as a query-focused summarization task, and reformulates the query in each hop.    将证据提取作为以查询为中心的摘要任务，并在每一跳中重构查询。—— HGN\n\nKosuke Nishida, Kyosuke Nishida, Masaaki Nagata, Atsushi Otsuka, Itsumi Saito, Hisako Asano, and Junji Tomita. 2019. Answering while summarizing: Multi-task learning for multi-hop qa with evidence extraction. In *ACL*.\n\n---\n\n DecompRC (Min et al., 2019b) decomposes a compositional question into simpler sub-questions and leverages single-hop MRC mod- els to answer the sub-questions.  将作文问题分解为更简单的子问题，并利用单跳MRC模型答复子问题—— HGN\n\nSewon Min, Victor Zhong, Luke Zettlemoyer, and Han- naneh Hajishirzi. 2019b. Multi-hop reading compre- hension through question decomposition and rescor- ing. In *ACL*.\n\n---\n\nA neural modular network is also proposed in Jiang and Bansal (2019b), where neural modules are dynamically assembled for more interpretable multi-hop rea- soning.一种神经模块网络，其中神经模块被动态地组装起来，以便更好地解释多跳推理。—— HGN\n\nYichen Jiang and Mohit Bansal. 2019b. Self- assembling modular networks for interpretable multi-hop reasoning. In *EMNLP*.\n\n----\n\n其他\n\nJifan Chen and Greg Durrett. 2019. Understanding dataset design choices for multi-hop reasoning. In *NAACL*.—— HGN\n\nSewon Min, Eric Wallace, Sameer Singh, Matt Gard- ner, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019a. Compositional questions do not necessitate multi-hop reasoning. In *ACL*.—— HGN\n\nYichen Jiang and Mohit Bansal. 2019a. Avoiding rea- soning shortcuts: Adversarial evaluation, training, and model development for multi-hop qa. In *ACL*.—— HGN\n\n-----\n\n### 与GNN相关的\n\nCoref-GRN (Dhingra et al., 2018) construct an entity graph based on co-reference reso- lution or sliding windows.基于共引用解决方案或滑动窗口构建实体图。—— HGN\n\nBhuwan Dhingra, Qiao Jin, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. 2018. Neural models for reasoning over multiple mentions using coreference. In *NAACL*.\n\n----\n\nEntity-GCN (De Cao et al., 2019) considers three different types of edges that connect different entities in the entity graph.考虑连接实体图中不同实体的三种不同类型的边。—— HGN\n\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2019. Question answering by reasoning across documents with graph convolutional networks. In *NAACL*.\n\n---\n\n**(已读)**HDE-Graph (Tu et al., 2019) enriches information in the entity graph by adding document nodes and creating interactions among documents, entities and answer candidates.通过添加文档节点并在文档、实体和候选答案之间创建交互，丰富了实体图中的信息。——HGN\n\n---\n\n**(已读)**Cognitive Graph QA employs an MRC model to predict answer spans and possible next-hop spans, and then organizes them into a cognitive graph.使用MRC模型预测答案跨度和可能的下一跳跨度，然后将它们组织到认知图中。——HGN\n\n----\n\nDFGN (Xiao et al., 2019) constructs a dynamic entity graph, where in each reasoning step irrelevant en- tities are softly masked out and a fusion module is designed to improve the interaction between the entity graph and documents.构建了一个动态实体图，在每个推理步骤中，不相关的实体被软屏蔽，并设计了一个融合模块来改善实体图与文档之间的交互性。——HGN\n\nYunxuan Xiao, Yanru Qu, Lin Qiu, Hao Zhou, Lei Li, Weinan Zhang, and Yong Yu. 2019. Dynamically fused graph network for multi-hop reasoning. In *ACL*.\n\n-----\n\n**(已读)**SAE (Tu et al., 2020) defines three types of edge in the sentence graph based on the named entities and noun phrases appearing in the question and sentences 根据问题和句子中出现的命名实体和名词短语，定义句子图中的三种边——HGN\n\n----\n\nC2F Reader (Shao et al., 2020) uses graph attention or self-attention on entity graph, and argues that this graph may not be necessary for multi-hop reasoning. 在实体图上使用图注意或自我注意，并认为该图对于多跳推理可能不是必需的。——HGN\n\nNan Shao, Yiming Cui, Ting Liu, Wang, and Guop- ing Hu Hu. 2020. Is graph structure necessary for multi-hop reasoningt. *arXiv preprint arXiv:2004.03096*.\n\n------\n\nAsai et al. (2020) proposes a new graph-based recurrent method to find evidence documents as reasoning paths, which is more focused on information retrieval.提出了一种新的基于图的递归方法来寻找证据文档作为推理路径，更侧重于信息检索。——HGN\n\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In *ICLR*.\n\n----\n\n**(已读)**HGN 2020 提出的模型构建了一个层次图，有效地探索了不同粒度之间的关系，并使用不同的节点来执行不同的任务。\n\n\n\n## GNN\n\n### GNN结构机制\n\n- GCN\n- SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS(ICLR 2017)\n- GAT\n- GraphSage\n- MPGNN\n- HGN\n- HAN\n\n### 预训练GNN\n\nHu, W.; Liu, B.; Gomes, J.; Zitnik, M.; Liang, P.; Pande, V. S.; and Leskovec, J. 2020. Strategies for Pre-training Graph Neural Networks. In *Proceedings of ICLR*.——L2P-GNN\n\n提出了不同的策略来预训练图神经网络的节点和图级，虽然在图级需要标记的数据。\n\n----\n\nHu, Z.; Fan, C.; Chen, T.; Chang, K.; and Sun, Y. 2019. Pre-Training Graph Neural Networks for Generic Structural Feature Extraction. *CoRR* abs/1905.13728..——L2P-GNN\n\n使用三个非监督任务预先培训图形编码器，以捕获图形的不同方面。\n\n-----\n\nNavarin, N.; Tran, D. V.; and Sperduti, A. 2018. Pre-training Graph Neural Networks with Kernels. *CoRR* abs/1811.06930..——L2P-GNN\n\n利用图内核进行预培训\n\n\n\n## 元学习及应用\n\nFinn, C.; Abbeel, P.; and Levine, S. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. In *Pro- ceedings of ICML*, 1126–1135.\n\nLu, Y.; Fang, Y.; and Shi, C. 2020. Meta-learning on Hetero- geneous Information Networks for Cold-start Recommenda- tion. In *Proceedings of KDD*, 1563–1573.\n\n\n\n## 预训练语言模型\n\n- ALBERT\n- Roberta\n- Bert\n- LongFormer\n\n## 对抗训练\n\n- FGM\n- PGD\n- FreeLB\n\n\n\n\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"Hierarchical Graph Network for Multi-hop Question Answering","url":"/2021/04/29/Hierarchical-Graph-Network-for-Multi-hop-Question-Answering/","content":"\n# Hierarchical Graph Network for Multi-hop Question Answering\n\nhttps://arxiv.org/pdf/1911.03631.pdf\n\n这篇文章也是HotpotQA数据集上的关于解决多跳问答场景的，在干扰项排行榜和全维基排行榜都曾是前列。\n\n多跳QA和HotpotQA数据集 : [HotpotQA数据集：A Dataset for Diverse, Explainable Multi-hop Question Answering](https://coding-zuo.github.io/2021/03/23/HotpotQA%E6%95%B0%E6%8D%AE%E9%9B%86/)\n\n其特点是\n\n- 通过在问题、段落、句子、实体等不同粒度上构建层次图(HGN)\n- 通过HGN这种节点粒度层次可以区分，进一步进行多任务推理：段落选择、支持句预测、实体抽取、最终答案预测\n\n## 如何构图\n\n首先来介绍下作者是如何构图的。\n\n![](https://i.loli.net/2021/05/01/sux1NI4eMgf2bcz.png)\n\n段落由句子组成，每个句子包含多个实体。这个图自然是以层次结构编码的，它也激发了作者构建层次图的动机。\n\n四种节点类型：\n\n- 问题节点Q\n- 实体节点E\n- 段落节点P：对于每个段落节点，在段落中的所有句子之间添加边。\n- 句子节点S：对于每个句子节点，提取句子中的所有实体，并在段落句子节点和这些实体节点之间添加边。\n\n七种边类型：\n\n- 问题节点和定位段落节点有边\n- 问题节点和问题中的实体节点有边\n- 段落节点和段落中的句子节点有边\n- 句子节点与其链接的段落节点之间的边(超链接链接)\n- 句子节点和句子中所提取的实体节点有边\n- 段落和段落之间有边(论文是取和问题最相关的前两个段落)\n- 存在同一个段落的句子节点\n\n\n\n## 挑战与动机\n\nHotpotQA的方案一般是先用一个检索器去找到包含正确答案的段落。然后在用MRC模型去选择段落去预测答案。\n\n目前的挑战：即使通过多个段落成功地确认了推理链，如何从分散的段落中收集不同粒度级别的证据共同回答并支持事实预测，仍然是一个关键的挑战。\n\n作者认为多跳阅读推理直观的步骤：\n\n- 找到与问题相关的段落\n- 在段落中选择强有力的证据\n- 从获得的证据中查明正确答案\n\n作者也是这么实现的，并创新的采用了多个层级的粒度信息去构图推理。\n\n\n\n## HGN \n\n![](https://i.loli.net/2021/05/05/6eL2w9WqRcPdIMj.png)\n\n模型包含四个模块：图构造模块、上下文编码模块、图推理模块、多任务预测模块\n\n### 图构造模块\n\n就是构造上文的七种边四种节点，形成层级图\n\n一共要考虑两步：\n\n- 选择相关段落：\n\n  第一跳：用预训练模型加一个二分类判断段落中是否包含支撑事实，\n\n  如果返回多个段落则选择排名靠前的两个作为段落节点。\n\n  如果标题匹配没有结果，则进一步搜索段落中包含问题实体的。\n\n  如果还是搜索失败，将会从段落排序中选择排名最高的段落。\n\n  确定第一跳后：下一步就是段落中找到可以通向其他相关段落的事实和实体(不再依赖实体链接，这可能会很引入噪音，而是在第一跳段落中使用超链接来发现第二跳段落。)\n\n  \n\n- 添加表示所选段落内的句子/实体之间的连接的边。\n\n### 上下文编码模块\n\n给出构建的层次图，下一步是获得所有图节点的初始表示。首先将所有选定的段落合并到上下文C中，将其与问题Q连接起来，输入Roberta。\n\n$C = \\{c_0,c_1,...,c_{n-1}\\} \\in \\text{R}^{n\\times d } , Q =\\{q_0,q_1,...,q_{m-1}\\}\\in \\text{R}^{m\\times d}$\n\n问题Q随后是一个双向注意力层。(2017. Bidirectional attention flow for machine comprehension. *ICLR*.)\n\n在上下文表示C之上用BiLSTM，并且从BILSTM的输出中提取不同节点的表示，表示为$M∈R^{n×2d}$。\n\n在BiLSTM后通过预测开始和结束位置来得到句子和实体节点。\n\n$p_i$ 第i段落节点、$s_i$ 第i句子节点、 $e_i$ 第i个实体节点、q 问题节点 $\\in \\text{R}^d$\n$$\n\\begin{equation}\\begin{split} \n p_i &= MLP_1 ([M[P^{(i)}_{start}][d:]; M[P^{(i)}_{end}][:d] ])\\\\\n  s_i &= MLP_2 ([M[S^{(i)}_{start}][d:]; M[S^{(i)}_{end}][:d] ])\\\\\n  e_i &= MLP_3 ([M[E^{(i)}_{start}][d:]; M[E^{(i)}_{end}][:d] ])\\\\\n  q &= \\text{max-pooling}(Q)\n    \\end{split}\\end{equation}\n$$\n\n\n\n\n### 图推理模块\n\n获得层次图所需要的节点：\n\n- 段落节点：$P = \\{p_i\\}^{n_p}_{i=1} , n_p=4$   \n- 句子节点：$S = \\{s_i\\}^{n_s}_{i=1}, n_s=40$\n- 实体节点：$E = \\{e_i\\}^{n_e}_{i=1}, n_e=60$\n\n定义图的临界矩阵为$H =  \\{q,P,S,E\\} \\in \\text{R}^{g\\times d }  , g= n_p+n_s+n_e+1$\n\n 经过GAT后，得到更新过后的每个节点表示$P',S',E',q'$\n\n为了让图信息进一步提取上下文答案跨度，这里还用更新后的节点表示H‘和之前的上下文表示M，通过一个门控注意力机制，用于答案跨度的预测。\n\n具体表示为：\n$$\n\\begin{equation}\\begin{split} \n C &= Relu(MW_m) \\cdot Relu(H'W'_m)^T\\\\\n \\hat H &= Softmax(C)\\cdot H'\\\\\n G &= \\sigma([M;\\hat H]W_s) \\cdot Tanh([M;\\hat H]W_t)\n    \\end{split}\\end{equation}\n$$\n其中：$W_m \\in \\text{R}^{2d\\times 2d}$ ，$W’_m \\in \\text{R}^{2d\\times 2d}$ ，$W_s \\in \\text{R}^{4d\\times 4d}$ , $W_t\\in \\text{R}^{4d\\times 4d}$\n\n\n\n### 多任务预测模块\n\n- 基于段落节点的段落选择\n- 基于句子节点的支撑事实预测\n- 基于实体节点和上下文G表示的答案预测\n\n最终目标函数\n$$\n\\begin{equation}\\begin{split} \n \\text{L}_{joint} = \\text{L}_{start} +\\text{L}_{end} + \\lambda_1\\text{L}_{para} + \\lambda_2\\text{L}_{sent} + \\lambda_3\\text{L}_{entity} + \\lambda_4 \\text{L}_{type}\n    \\end{split}\\end{equation}\n$$\n其中$\\lambda_{1,2,3,4}$ 超参数权重\n\n$L_{type}$ 是预测答案类型的损失\n\n\n\n## 错误分析\n\n![](https://i.loli.net/2021/05/05/ephOUw7zXAcK3bH.png)\n\n作者在这分析了模型的弱点(为将来的工作提供了见解)，在dev集中随机抽样了100个答案f1为0的示例。\n\n作者总结了六类错误\n\n- Annotation批注：数据集中提供的批注不正确 \n\n  上图第一行：“Tonka”和“101只斑点狗”是在同一个十年上映的吗？数据集给的答案和实际情况不一样？这种应该是数据集错误吧， 这种错误占了9%。这种问题应该不是模型的弱点吧？\n\n- Multiple-Answers：问题可能有多个正确答案，但数据集中只提供一个答案\n\n  迈克尔·J·亨特取代了成为哪家机构管理员的律师？答案EPA是预测答案的缩写，这种问题也比较难解决，占了24%是比重最多的。\n\n- Discrete Reasoning:  这种类型的错误经常出现在“比较”题中，需要离散推理才能正确回答问题； 16%\n\n  在Mastodon和Hole这两个乐队中，哪个成员更多？ 可能是已知这两个乐队人数，要比较这两个数的大小\n\n- Commonsense & External Knowledge： 要回答这类问题，需要常识或外部知识\n\n  迷你专辑Code#01的艺人第二次延长演出的名字是什么？\n\n- Multi-hop：模型不能进行多跳推理，从错误的段落中找到最终答案 16%\n\n  这部根据5：15出现的摇滚歌剧改编的电影是谁导演的？\n\n- MRC:  模型正确地找到了支持段落和句子，但预测了错误的答案跨度。 20%\n\n  艾达·洛夫莱斯，第一位计算机程序员，在“恰尔德·拜伦”中与拜伦勋爵有什么关系？答案是他的女儿，模型回答成紧张的关系，说明模型没有完全理解问题中的关系。\n\n\n\n可以看出HGN对于阅读理解的进行鲁棒性的回答还是有所不足，面对相同答案的多样性还有进一步的改进空间。\n\n对于句子理解和推理定位还不够特别准确。\n\n\n\n\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"二分模板","url":"/2021/04/29/二分模板/","content":"\n# 二分模板\n\nhttps://www.acwing.com/problem/content/791/\n\n二分的本质不是单调性, 单调性的题目一定可以二分, 可以二分的题目不一定有单调性\n\n二分的本质是边界\n二分法用于查找, 每次都选择答案所在的区间再次进行查找, 当区间长度为 1时, 就是答案\n\n![](https://i.loli.net/2021/04/29/Hy4vGqOtus8lQXp.png)\n\n1. 根据 check(mid)来判断 r和 l的取值范围\n2. 根据取值范围选择 mid是否有 + 1操作\n   - mid归于左边, r = mid, mid选择 不 +1\n   - mid归于右边, l = mid, mid选择 +1\n\n```java\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.util.Arrays;\n\npublic class 模板_二分 {\n\n    public static void main(String[] args) throws IOException {\n        BufferedReader input = new BufferedReader(new InputStreamReader(System.in));\n        int[] line1 = Arrays.asList(input.readLine().split(\" \")).stream().mapToInt(Integer::parseInt).toArray();\n        int n = line1[0];\n        int q = line1[1];\n        int[] line2 = Arrays.asList(input.readLine().split(\" \")).stream().mapToInt(Integer::parseInt).toArray();\n\n        while (q-- != 0) {\n            int target = Integer.parseInt(input.readLine());\n            // 查找左边界 用第一个模板\n            int index_l = bsearch_1(line2, 0, n - 1, target);\n            if (line2[index_l] != target) {\n                System.out.println(\"-1 -1\");\n            } else {\n                System.out.print(index_l + \" \");\n                // 查找右边界 用第二个模板\n                int index_r = bsearch_2(line2, 0, n - 1, target);\n                System.out.print(index_r + \"\\n\");\n            }\n        }\n    }\n\n    public static int bsearch_1(int[] arr, int l, int r, int target) {\n        while (l < r) {\n            int mid = l + r >> 1;\n            if (arr[mid] >= target) {\n                r = mid;\n            } else {\n                l = mid + 1;\n            }\n        }\n        return l;\n    }\n\n    public static int bsearch_2(int[] arr, int l, int r, int target) {\n        while (l < r) {\n            int mid = l + r + 1 >> 1;\n            if (arr[mid] <= target) {\n                l = mid;\n            } else {\n                r = mid - 1;\n            }\n        }\n        return l;\n    }\n\n}\n```\n\n","tags":["刷题"]},{"title":"归并排序模板","url":"/2021/04/28/归并排序模板/","content":"\n# 归并排序模板\n\n分治思想\n\n![](https://i.loli.net/2021/04/28/8g1qixHscOBTv6Q.png)\n\n1. 确定分界点：$mid = (l+r)/2$\n\n2. 先递归分成左右两边\n\n3. 将两个有序数组合并成一个有序序列——归并\n\n   使用两个指针：这个过程时间复杂度为$O(n)$\n\n![](https://i.loli.net/2021/04/28/d23pUiKgLOswDZm.png)\n\n整体时间复杂度$O(nlogn)$\n\n因为分层用了$logn$次\n\n![](https://i.loli.net/2021/04/28/WeSTDRHymJbg5KN.png)\n\n```java\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\n\npublic class 模板_归并排序 {\n\n\n    public static void main(String[] args) throws IOException {\n        BufferedReader input = new BufferedReader(new InputStreamReader(System.in));\n        int n = Integer.parseInt(input.readLine());\n        int[] q = new int[n];\n        String[] linelist = input.readLine().split(\" \");\n        for (int i = 0; i < linelist.length; i++) {\n            q[i] = Integer.parseInt(linelist[i]);\n        }\n\n        merge_sort(q, 0, q.length - 1);\n\n        for (int i = 0; i < q.length; i++) {\n            System.out.print(q[i]);\n            System.out.print(\" \");\n        }\n\n    }\n\n\n    public static void merge_sort(int[] q, int l, int r) {\n        if (l >= r) return;\n        // 确定分界点\n        int mid = l + ((r - l) >> 1);\n        // 递归\n        merge_sort(q, l, mid);\n        merge_sort(q, mid + 1, r);\n\n        int[] tmp = new int[r - l + 1]; // 辅助数组\n        // 归并\n        int k = 0; // 表示tmp中有多少个数\n        // 两个指针\n        int i = l, j = mid + 1;\n        while (i <= mid && j <= r) {\n            if (q[i] <= q[j]) {\n                tmp[k++] = q[i++];\n            } else {\n                tmp[k++] = q[j++];\n            }\n        }\n        // 剩余\n        while (i <= mid) tmp[k++] = q[i++];\n        while (j <= r) tmp[k++] = q[j++];\n        // 放回\n        for (i = l, j = 0; i <= r; i++, j++) q[i] = tmp[j];\n    }\n}\n```","tags":["刷题"]},{"title":"Ubantu18.04安装NVIDIA驱动+cuda10.1+cuDNN+Tensorflow2.1.0","url":"/2021/04/26/Ubantu18-04安装NVIDIA驱动-cuda10-1-cuDNN-Tensorflow2-1-0/","content":"\n# Ubantu18-04安装NVIDIA驱动-cuda10-1-cuDNN-Tensorflow2-1-0\n\n注意：TensorFlow2.1 要求 你的GPU算力要达到3.5，检查自己GPU算力\n\n## 安装和卸载NVIDIA驱动\n\n首先要确保驱动已经卸载干净\n\n```\nsudo apt-get purge nvidia*\nsudo apt-get autoremove\n```\n\n检查自己GPU版本，之后到官网去下载，这种办法安装比较稳妥，其他网络安装办法有时候出错不知道咋回事。\n\n```\nlshw -numeric -C display\n```\n\n\n\n![](https://i.loli.net/2021/04/26/tDVckAHU8B7dnla.png)\n\n下载驱动网址：https://www.nvidia.cn/Download/index.aspx?lang=cn\n\n![](https://i.loli.net/2021/04/26/ruzlX3qQ6Ndat9I.png)\n\n禁用Nouveau\n\n```\nNouveau驱动禁用方法：\n\nsudo gedit /etc/modprobe.d/blacklist.conf\n或者\nsudo vim /etc/modprobe.d/blacklist.conf\n \n在最后两行添加：\n \nblacklist nouveau\noptions nouveau modeset=0     // 禁用nouveau第三方驱动，之后也不需要改回来\n \n执行\n \nsudo update -initramfs -u   // 更新内核\n```\n\n\n\n关闭lightdm\n\n\n\n```\nsudo service lightdm stop\n\n　sudo init 3 # 遇见X Server报错执行 \n\n rm -rf /tmp/.X*\n\n ./NVIDIA-Linux-x86_64-418.165.02.run #开始安装驱动 遇见continue就continue 遇见ok就ok\n```\n\n![](https://i.loli.net/2021/04/26/Wf7Imlx8PyKqtUG.png)\n\n## 安装cuda10.1\n\nhttps://tensorflow.google.cn/install/source#linux\n\n在这个网站上对好版本，版本不对可不行，全是坑 \n\nhttps://developer.nvidia.com/cuda-toolkit-archive 选择版本\n\n然后在这里下载cuda 我用的是deb的办法也是本地下载后安装的。**（我这个网络可能是不行，总是apt-get update 总是报错 所以这个方法没成功用runfile成功了。。。）参考一下吧** \n\n![](https://i.loli.net/2021/04/26/wVXLYjvD5zHTNRu.png)\n\n安装\n\n```\nsudo dpkg -i cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb\nsudo apt-key add /var/cuda-repo-<version>/7fa2af80.pub\nsudo dpkg -i cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb\nsudo apt-get update\nsudo apt-get install cuda\n```\n\n添加环境变量：\n\n打开 .bashrc\n\n sudo vim ~/.bashrc\n\n```\nexport CUDA_HOME=/usr/local/cuda \nexport PATH=$PATH:$CUDA_HOME/bin \nexport LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n```\n\nsource ~/.bashrc\n\nnvcc -V\n\n\n\n## runfile安装cuda\n\n下载runfile\n\n![](https://i.loli.net/2021/04/26/7G8c26kofdjJQBh.png)\n\n![](https://i.loli.net/2021/04/26/LI4shCiMqcKaNQB.png)\n\n一定要取消掉driver 此处！！！，因为已经装了驱动了\n\n![](https://i.loli.net/2021/04/26/5CmNy6BrOIiDlkp.png)\n\n\n\n```python3\nsudo vim ~/.bashrc\n```\n\n\n\n我们在文件最后一行添加：\n\n```\n$ export PATH=/usr/local/cuda-10.1/bin:/usr/local/cuda-10.1/NsightCompute-2019.1${PATH:+:${PATH}}\n$ export LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64\\\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n```\n\n\n\n```text\nsource ~/.bashrc\n```\n\n\n\n![](https://i.loli.net/2021/04/26/VDZRTx76oi8MeuK.png)\n\n\n\n## 安装TensorFlow2.1.0_gpu \n\n这上面虽然没写2.1.0_gpu 可是还得得装gpu版\n\n![](https://i.loli.net/2021/04/26/LNxBI3jmDrcGUVT.png)\n\n\n\n完成后 \n\nconda install cudatoolkit=10.1\n\n![](https://img2020.cnblogs.com/blog/1225390/202010/1225390-20201031135739329-731523260.png)\n\n\n\n## 安装cuDNN\n\nhttps://developer.nvidia.com/cudnn\n\n去下载对应版本，但是要登录一下\n\n解压后\n\n```\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include\nsudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\nsudo chmod a+r /usr/local/cuda/include/cudnn.h \nsudo chmod a+r /usr/local/cuda/lib64/libcudnn*\n```\n\n以配置cuDNN环境。\n\n通过\n\n```\ncat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\n```\n\n\n\n查看cuDNN版本\n\nover\n\n\n\n\n\n\n\n","tags":["配置记录"]},{"title":"快速排序模板","url":"/2021/04/26/快速排序模板/","content":"\n快速排序模板\n\n\n\n![](https://i.loli.net/2021/04/26/voAgqKV12i9Rxuz.png)\n\n1. 先确定分界点：$q[l] 、 q[(l+r)/2]、 q[r]$ 或随机\n2. 调整区间：小于等于x的在左半边，大于等于x的在右半边 (如何去调整)\n3. 递归处理左右两段\n\n## 由数据反推算法复杂度和算法内容\n\n![](https://i.loli.net/2021/05/14/i9EQsUqAYdW1rcB.png)\n\n\n\n## 实现\n\n\n\n暴力：\n\n- 声明两个数组 a[] 、b[]\n- 将$q[l~r]$ 遍历 \n- 如果 $q[i] \\le x$ 放到a[]中   \n- 如果 $q[i] \\ge x$ 放到b[]中   \n- 再将a、b数组放回q中\n\n优美：\n\n用两个指针，swap\n\n\n\n[关于JAVA中IO流类：BufferredReader的简单用法](https://blog.csdn.net/qq_42369555/article/details/82745923)\n\nbufferreader要比scanner快\n\n```java\npackage code;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.util.Collections;\n\npublic class 快排模板 {\n\n    public static void main(String[] args) throws IOException {\n        BufferedReader input = new BufferedReader(new InputStreamReader(System.in));\n        int n = Integer.parseInt(input.readLine());\n        int[] q = new int[n];\n        String[] linelist = input.readLine().split(\" \");\n        for (int i = 0; i < linelist.length; i++) {\n            q[i] = Integer.parseInt(linelist[i]);\n        }\n\n        quick_sort(q, 0, q.length - 1);\n\n        for (int i = 0; i < q.length; i++) {\n            System.out.print(q[i]);\n            System.out.print(\" \");\n        }\n\n    }\n\n    public static void quick_sort(int[] q, int l, int r) {\n        if (l >= r) return;\n        int x = q[l];\n        int i = l - 1;\n        int j = r + 1;\n        while (i < j) {\n            do i++; while (q[i] < x);\n            do j--; while (q[j] > x);\n            if (i < j) {\n                int t = q[i];\n                q[i] = q[j];\n                q[j] = t;\n            }\n        }\n        quick_sort(q, l, j);\n        quick_sort(q, j + 1, r);\n    }\n\n\n}\n\n```\n\n\n\n\n\n\n\n","tags":["刷题"]},{"title":"DP分析——石子合并","url":"/2021/04/24/DP分析——石子合并/","content":"\n# DP分析——石子合并\n\n设有 NN 堆石子排成一排，其编号为 1，2，3，…，N。\n\n每堆石子有一定的质量，可以用一个整数来描述，现在要将这 N 堆石子合并成为一堆。\n\n每次只能合并相邻的两堆，合并的代价为这两堆石子的质量之和，合并后与这两堆石子相邻的石子将和新堆相邻，合并时由于选择的顺序不同，合并的总代价也不相同。\n\n例如有 4 堆石子分别为 `1 3 5 2`， 我们可以先合并 1、2堆，代价为 4，得到 `4 5 2`， 又合并 1，2 堆，代价为 9，得到 `9 2` ，再合并得到 11，总代价为 4+9+11=244+9+11=24；\n\n如果第二步是先合并 2，3 堆，则代价为 7，得到 `4 7`，最后一次合并代价为 11，总代价为 4+7+11=22。\n\n问题是：找出一种合理的方法，使总的代价最小，输出最小代价。\n\n#### 输入格式\n\n第一行一个数 N 表示石子的堆数 N。\n\n第二行 N 个数，表示每堆石子的质量(均不超过 1000)。\n\n#### 输出格式\n\n输出一个整数，表示最小代价。\n\n#### 数据范围\n\n1≤N≤300     1≤N≤300\n\n#### 输入样例：\n\n```\n4\n1 3 5 2\n```\n\n#### 输出样例：\n\n```\n22\n```\n\n## 解\n\n![](https://i.loli.net/2021/04/24/CqE9QcaxYBzZKRw.png)\n\n![](https://i.loli.net/2021/04/24/gPlOsK5oXFcWutE.png)\n\n\n\n```java\npublic class DP_石子合并 {\n\n    public static void main(String[] args) {\n        Scanner scanner = new Scanner(System.in);\n        int N = scanner.nextInt();\n        int[] s = new int[N + 1];  //前缀和\n        for (int i = 1; i <= N; i++) {\n            s[i] = scanner.nextInt();\n            s[i] += s[i - 1];\n        }\n        int[][] dp = new int[N + 1][N + 1];\n\n        for (int len = 2; len <= N; len++) {//先枚举区间长度\n            for (int i = 1; i + len - 1 <= N; i++) {//再枚举区间左端点\n                int j = i + len - 1; //右端点\n                dp[i][j] = 100000000;\n                for (int k = i; k < j; k++) {\n                    dp[i][j] = Math.min(dp[i][j], dp[i][k] + dp[k + 1][j] + s[j] - s[i - 1]);\n                }\n            }\n        }\n        System.out.println(dp[1][N]);\n    }\n}\n```\n\n$O(n^3)$ \n\n---\n\n# 最长公共子序列\n\n给定两个长度分别为 N 和 M 的字符串 A 和 B，求既是 A 的子序列又是 B 的子序列的字符串长度最长是多少。\n\n#### 输入格式\n\n第一行包含两个整数 N 和 M。\n\n第二行包含一个长度为 N 的字符串，表示字符串 A。\n\n第三行包含一个长度为 M 的字符串，表示字符串 B。\n\n字符串均由小写字母构成。\n\n#### 输出格式\n\n输出一个整数，表示最大长度。\n\n#### 数据范围\n\n1≤N,M≤1000       1≤N,M≤1000\n\n#### 输入样例：\n\n```\n4 5\nacbd\nabedc\n```\n\n#### 输出样例：\n\n```\n3\n```\n\n\n\n## 解\n\n最坏情况下 aaaa,aaaaa，A中所有都是由 $2^n$ 个不同子序列。\n\n![](https://i.loli.net/2021/04/24/oxH4yliYPD23LeQ.png)\n\n![](https://i.loli.net/2021/04/24/dVY9UmoHTticMPC.png)\n\n\n\n```java\npublic static void main(String[] args) {\n        Scanner scanner = new Scanner(System.in);\n        int N = scanner.nextInt();\n        int M = scanner.nextInt();\n        String strA = \" \" + scanner.next();\n        String strB = \" \" + scanner.next();\n//        char[] A = strA.toCharArray();\n//        char[] B = strB.toCharArray();\n\n        int[][] dp = new int[N + 1][M + 1];\n\n        for (int i = 1; i <= N; i++) {\n            for (int j = 1; j <= M; j++) {\n                dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]);\n                if (strA.charAt(i) == strB.charAt(j)) {\n                    dp[i][j] = Math.max(dp[i][j], dp[i - 1][j - 1] + 1);\n                }\n            }\n        }\n        System.out.println(dp[N][M]);\n    }\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["DP"]},{"title":"Select, Answer and Explain-Interpretable Multi-hop Reading Comprehension over Multiple Documents","url":"/2021/04/22/Select-Answer-and-Explain-Interpretable-Multi-hop-Reading-Comprehension-over-Multiple-Documents/","content":"\n# Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents\n\n\n\n## 摘要\n\n选择、回答和解释(SAE)系统解决多文档RC问题。\n\n首先主要创新，用文档分类器过滤掉与答案无关的文档，从而减少分散注意力的信息量。\n\n然后将所选择的答案相关文档输入到模型以联合预测答案和支持句子。\n\n该模型在答案预测的表征层和支持句子预测的句子层都设置了多任务学习目标，\n\n并在这两个任务之间进行了基于注意力的交互，对模型进行了优化。\n\n关键词：过滤无关文档、多任务学习、混合注意力交互\n\n![](https://i.loli.net/2021/04/22/yArGlY6zuT7eMsW.png)\n\n## 在HotpotQA中什么是gold doc\n\nHotpotQA通过为答案提供支持句来鼓励可解释的QA模型，这些支持句通常来自多个文档，如果文档包含答案或包含对答案的支持句，则称为“黄金文档”。\n\n\n\n应答文本，它可以是一段文本或“是/否”。\n\n作者从答案和支持句标签导出GOLD文档标签。我们使用 $D_i$ 表示文档 i：如果Di是黄金文档，则标记为1，否则标记为0。还将答案类型标记为以下注释之一：“Span”、“Yes”和“No”。\n\n\n\n\n\n ## 选择gold doc(过滤文档)\n\n答案预测和支持句预测的上游任务。将分类排名最靠前的文档作为预测的黄金文档 gold doc。\n\n![](https://i.loli.net/2021/04/22/EapiyjuwNbPt49l.png)\n\n做文档过滤最直接做法就是采用bert的CLS摘要向量，做交叉熵分类\n$$\n\\begin{equation}\\begin{split} \n L = -\\sum_{i=0}^n t_ilogP(D_i) + (1-t_i) log(1-P(D_i))\n    \\end{split}\\end{equation}\n$$\n$t_i$ 是 $D_i$ 的标签，n是文档数，$P(D_i)$ 是文档i在标签 $t_i$ 中的概率。\n\n这种简单的方法的缺点：单独处理每个文档，而不考虑下游多跳推理任务所必需的文档间交互和关系。\n\n为解决此问题，作者提出了一个新的模型，如图上图CLS后，加了一层多头注意力层。\n\n意在：增加对从不同文档生成的“CLS”标记的关注的动机是鼓励文档间的交互。文档间交互对于文档间的多跳推理至关重要。\n\n优化：采用了新的成对学习排序损失。还将问题从分类问题描述为两两学习排序问题，\n\n通过将文档与所有其他文档进行比较，该模型能够更好地将一小部分黄金文档与睡觉分散注意力的文档区分开来。\n\n给每个文档一共分数 $S(.)$\n\n如果 $D_i$ 是gold doc $S(D_i) = 1 $, 否则 $S(D_i) = 0$\n\n然后，标记每对输入文档：给定一对输入文档 $(D_i，D_j)$，标签 $l$设置为：\n$$\n\\begin{equation}\\begin{split} \n l_{i,j} = \n\\begin{cases}\n 1, &if\\ S(D_i) \\gt S(D_j) \\\\\n 0 , &if\\  S(D_i) \\le S(D_j)\n\\end{cases}\n    \\end{split}\\end{equation}\n$$\n还认为包含答案范围的文档对于下游任务更重要。因此，如果$D_i$是包含答案跨度的黄金文献，$S(D_i)=2$。\n\n再将MHSA输出传递给双线性层来输出每对文档的概率，双线性层基于二元交叉熵进行训练，如下所示：\n$$\n\\begin{equation}\\begin{split} \n L = - \\sum_{i=0,j=0}^n \\sum_{j\\neq i} l_{i,j} logP(D_i,D_j) + (1-l_{i,j}) log(1-P(D_i,D_j))\n    \\end{split}\\end{equation}\n$$\n相关性定义为$ R_i=􏰅\\sum_j^n(P(D_i，D_j)>0.5)$。将来自该相关性排序的前k个排序的文档作为的过滤文档。\n\n\n\n\n\n## 答案和解释\n\n模型采用多任务学习的方式进行训练，以联合预测答案和对黄金文档的支持意义。\n\n基于GNN构建多跳推理图，将上下文语句嵌入作为节点，而不是像以往的工作那样以实体作为节点，直接输出带有答案预测的支持语句。\n\n为什么不用NER因为作者认为：\n\n目前GNN在QA任务中的应用通常需要实体作为图形节点，并且通过在具有上下文信息的节点上进行消息传递来实现推理。这仅在预定义的一组目标实体可用时才有可能。否则，需要使用命名实体识别(NER)工具来提取实体，这可能会导致图推理中的冗余实体和噪声实体。如果答案不是命名实体，则需要进一步处理以定位最终答案。\n\n token-level and sentence-level 多任务学习\n\n\n\n基于一种新的混合注意池机制\n\n将GNN中使用的上下文语句嵌入归结到令牌表示上。注意力权重是根据令牌表示上的答案广度日志和自我注意输出来计算的。这种基于注意力的交互能够利用“回答”和“解释”任务之间的互补信息。\n\n\n\n### 答案预测\n\n针对bert输出的每一个$H_i$ 用两层MLP做答案起始位置预测 $L$ 为长度\n$$\n\\begin{equation}\\begin{split} \n \\hat Y &= f_{span} (H^i) \\in R^{L\\times2}\\\\\n L ^{span} &= \\frac{1}{2}(CE(\\hat Y[:,0], y^{start}) + CE(\\hat Y[:,1], y^{end}))\n    \\end{split}\\end{equation}\n$$\n其中$\\hat Y$的第一行是起始位置的逻辑，第二行是结束位置的逻辑。$y^{star}t$和 $y^{end}$ 是范围 [0，L-1] 中的开始位置和结束位置的标签。CE表示交叉熵损失函数。\n\n\n\n### 支持句预测\n\n预测输入上下文中的句子是否为答案预测的支持证据。为了实现句子级预测，我们首先获得$H_i$中每个句子的序列表示。$H_i$ 是bert的token输出。\n$$\n\\begin{equation}\\begin{split} \n S^j - H[j^s:j^e,:] \\in R^{L^j\\times d}\n    \\end{split}\\end{equation}\n$$\n$S^j$是表示语句 j 内的标记嵌入的矩阵( 这里省略了样本索引i)。 $j^s$ 和 $j^e$ 定义了开始和结束位置，$L_j$ 是语句$j$ 的长度。\n\n\n\n### 多任务\n\n答案预测任务和支持句预测任务可以相辅相成。\n\n据观察，答案预测任务总是可以帮助支持句子预测任务，因为有答案的句子总是一条证据；\n\n但反过来情况不是一样的，因为可能有多个支持句子，概率最高的句子可能不包含答案\n\n所以答案预测任务总 可以帮助支持句子预测任务，因为有答案的句子总是一个证据；\n\n反之亦然，因为可能有多个支持句子，概率最高的句子可能不包含答案。\n\n因此，为了揭示这两个互补任务之间的相互作用，提出了一种基于注意力的总结句子表示法，以引入来自回答预测的互补信息。\n\n注意力权重的计算方法如下：在Sj上用自我注意计算一部分注意力，另一部分来自答案预测任务的起始位置日志和结束位置日志的总和。\n$$\n\\begin{equation}\\begin{split} \n \\alpha^j &= \\sigma(f_{att}(S^j) + \\hat Y[j^s:j^e,0] + \\hat Y[j^s:j^e,1])\\\\\n s^j &= \\sum^{L^j}_{k=0} \\alpha^j_k S^j[k,:] \\in R^{1\\times d}\n    \\end{split}\\end{equation}\n$$\nSj是表示语句j 的标记嵌入的矩阵\n\n$f_{att}$ 是一个两层MLP输出size为1，$\\sigma$是softmax\n\n$α_j ∈ R^{L^j×1}$表示句子j的每个token上的关注度权重。\n\n### 构建GNN\n\n接下来，在语句嵌入Sj上建立GNN模型，以显式地促进对预测gold doc中所有语句的多跳推理，从而更好地利用复杂的关系信息。使用语句嵌入Sj来初始化图的节点特征。采用基于多关系图卷积网络(GCN)的消息传递策略来更新图的节点特征，并将最终的节点特征输入到MLP中，得到每个句子的分类。\n\n![](https://i.loli.net/2021/04/22/kfrJdNaDZBqyAhV.png)\n\n根据问题和句子中出现的命名实体和名词短语设计了三种类型的边：\n\n- 如果两个节点最初来自同一文档，则在这两个节点之间添加一条边(上图中的实节点)\n- 如果表示两个节点的句子在问题中都具有命名实体或名词短语(可以是不同的)，则在来自不同文档的两个节点之间添加边。(图中的虚线)\n- 如果表示两个节点的句子具有相同的命名实体或名词短语，则在来自不同文档的两个节点之间添加一条边。(图中的虚线)\n\n第一种类型的边的动机是希望GNN掌握每个文档中呈现的全局信息。\n\n第二类和第三类边，为了以更好地捕捉这种跨文档推理路径。跨文档推理是通过从问题中的实体跳到未知的桥梁实体或比较问题中两个实体的属性来实现的 。\n\n\n\n对于消息传递，使用具有门控机制的多关系GCN。\n\n假设 $h^0_j$ 表示从语句嵌入 $S_j$的初始节点嵌入，则一跳(或一层)之后的节点嵌入计算可表示为:\n$$\n\\begin{equation}\\begin{split} \n h_j^{k+1} &= act(u_j^k) \\odot g^k_j + h^k_j \\odot (1-g^k_j) \\\\\n u^k_j &= f_s(h^k_j) + \\sum_{r\\in R} \\frac{1}{|N_j^r|} \\sum_{n\\in N^r_j} f_r(h_n^k)\\\\\n g_j^k &= sigmoid (f_g([u_j^k; h^k_j])) \n    \\end{split}\\end{equation}\n$$\n\n\n其中R 是一些列边类型， $N^r_j$ 是边类型为r的 j 节点的邻居。\n\n$h^k_n$ 是节点n的第k层节点表示。\n\n$f_r、f_s、f_g$中的每一个都定义了输入节点表示上的变换，并且可以使用MLP来实现。\n\n门控$g_j^k$ 是由0和1之间的值组成的向量，用于控制来自计算的更新$u^k_j$ 或来自原始节点表示的信息。\n\n函数$act$表示非线性激活函数。\n\n最后得到每个节点的最终表示 $h_j$ 后用两层MLP 最终预测 。\n\n$\\hat y^{sp}_j = sigmoid(f_{sp}(h_j))$ \n\n\n\n除了支持句子预测任务之外，还在GNN输出之上添加了另一个任务，以说明“Yes/No”类型的问题。\n\n预测任务描述为3类(“Yes”、“No”和“span”)分类\n\n引入：\n\n$h = \\sum_j a_jh_j$\n\n$a = \\sigma(\\hat y^{sp})$\n\n$\\hat y^{ans} = f_{ans}(h)$\n\n最终loss表达为：\n$$\n\\begin{equation}\\begin{split} \n L = \\gamma L^{span} + BCE(\\hat y^{sp}, y^{sp}) + CE(\\hat y^{ans}, y^{ans})\n    \\end{split}\\end{equation}\n$$\n$BCE()$ 二元交叉熵函数\n\n为了考虑不同损失的尺度差异，在跨度损失中加入了一个权重γ。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"DP分析法--01背包问题","url":"/2021/04/20/DP分析法——01背包问题/","content":"\n# DP分析法--01背包问题\n\n从集合角度来分析DP问题，DP问题的题目一般都是从有限集中求得最值的问题。\n\n![](https://i.loli.net/2021/04/20/SlgJ96RzdyGp5fw.png)\n\n[01背包问题](https://www.acwing.com/problem/content/2/)\n\n有 N 件物品和一个容量是 V 的背包。每件物品只能使用一次。\n\n第 i 件物品的体积是 $v_i$，价值是 $w_i$。\n\n求解将哪些物品装入背包，可使这些物品的总体积不超过背包容量，且总价值最大。\n输出最大价值。\n\n#### 输入格式\n\n第一行两个整数，N，V，用空格隔开，分别表示物品数量和背包容积。\n\n接下来有 N 行，每行两个整数 $v_i,w_i$，用空格隔开，分别表示第 i 件物品的体积和价值。\n\n#### 输出格式\n\n输出一个整数，表示最大价值。\n\n#### 数据范围\n\n0<N,V≤10000<N,V≤1000\n0<vi,wi≤10000<vi,wi≤1000\n\n#### 输入样例\n\n```\n4 5\n1 2\n2 4\n3 4\n4 5\n```\n\n#### 输出样例：\n\n```\n8\n```\n\n\n\n\n\n## 解\n\n最多$2^N$, 从$2^N$ 个方案里找总价值最大的方案。——有限集合的最值问题\n\n状态表示：\n\n- 选择问题一般$f(i,j)$ 第一维表示前i个物品,第二维是限制 (经验)\n\n- 集合：所有只考虑前i个物品，且总体积不超过j的选法的集合。\n- 属性：集合中每一个方案的最大价值Max\n\n状态计算：\n\n- 所有不选第i个物品的方案 $f(i-1,j)$\n- 所有选择第i个物品的方案 $f(i-1,j-v_i) + w_i$\n- $Max(f(i-1,j), f(i-1,j-v_i)+w_i)$\n\n```java\nimport java.util.Scanner;\n\npublic class Main{\n    public static void main(String[] args) throws Exception {\n        // 读入数据的代码\n        Scanner reader = new Scanner(System.in);\n        // 物品的数量为N\n        int N = reader.nextInt();\n        // 背包的容量为V\n        int V = reader.nextInt();\n        // 一个长度为N的数组，第i个元素表示第i个物品的体积；\n        int[] v = new int[N + 1] ;\n        // 一个长度为N的数组，第i个元素表示第i个物品的价值；\n        int[] w = new int[N + 1] ;\n\n        for (int i=1 ; i <= N ; i++){\n            // 接下来有 N 行，每行有两个整数:v[i],w[i]，用空格隔开，分别表示第i件物品的体积和价值\n            v[i] = reader.nextInt();\n            w[i] = reader.nextInt();\n        }\n        reader.close() ;\n\n        // 正式工作的代码\n        /*\n        定义一个二阶矩阵dp[N+1][V+1],\n        这里之所以要N+1和V+1，是因为第0行表示只能选择第0个物品的时候，即没有物品的时候\n        第0列表示背包的体积为0的时候，即不能装任何东西的时候\n\n        dp[i][j]表示在 只能选择前i个物品，背包容量为j的情况下，背包中物品的最大价值\n        对于dp[i][j]有两种情况：\n        1. 不选择当前的第i件物品/第i件物品比背包容量要大，则dp[i][j] = dp[i-1][j]\n        2. 选择当前的第i件物品（潜在要求第i件物品体积小于等于背包总容量），则能装入的物品最大价值为：\n            当前物品的价值 加上 背包剩余容量在只能选前i-1件物品的情况下的最大价值\n            dp[i][j] = dp[i-1][j-v[i]] + w[i]\n        dp[i][j]在两种情况中选择比较大的情况作为当前的最优解；\n        即：\n        if(j >= v[i]):\n            dp[i][j] = max(dp[i-1][j], dp[i-1][j-v[i]] + w[i])\n        else:\n            dp[i][j] = dp[i-1][j]\n        */\n        int[][] dp = new int[N+1][V+1];\n        dp[0][0] = 0;\n        for(int i = 1; i <= N; i++){\n            for(int j = 0; j <= V; j++){\n                if(j >= v[i]){\n                    dp[i][j] = Math.max(dp[i-1][j], dp[i-1][j-v[i]] + w[i]);\n                }else{\n                    dp[i][j] = dp[i-1][j];\n                }\n            }\n        }\n        System.out.println(dp[N][V]);\n    }\n}\n\n\n```\n\n优化后\n\n```java\nimport java.util.Scanner;\n\npublic class Main{\n    public static void main(String[] args) throws Exception {\n        // 读入数据的代码\n        Scanner reader = new Scanner(System.in);\n        // 物品的数量为N\n        int N = reader.nextInt();\n        // 背包的容量为V\n        int V = reader.nextInt();\n        // 一个长度为N的数组，第i个元素表示第i个物品的体积；\n        int[] v = new int[N + 1] ;\n        // 一个长度为N的数组，第i个元素表示第i个物品的价值；\n        int[] w = new int[N + 1] ;\n\n        for (int i=1 ; i <= N ; i++){\n            // 接下来有 N 行，每行有两个整数:v[i],w[i]，用空格隔开，分别表示第i件物品的体积和价值\n            v[i] = reader.nextInt();\n            w[i] = reader.nextInt();\n        }\n        reader.close() ;\n\n        // 正式算法的代码\n        // 将dp优化为一维数组\n        /*\n        注意，这里第二层循环的时候，还是小到大循环的话，那么\n\n        dp[i][j] = Math.max(dp[i-1][j], dp[i-1][j-v[i]] + w[i])\n        实际上变成了\n        dp[i][j] = Math.max(dp[i][j], dp[i][j-v[i]] + w[i]);\n\n        因为i-1的值已经在前面被更新过了，覆盖了\n        为了避免这个问题，所以要逆序更新，即先更新第i个，然后更新第i-1个，从而保证第i-1个不被覆盖\n\n        如果不逆序的话，输出结果为10，dp数组实际为：\n        0 0 0 0 0 0 \n        0 2 4 6 8 10\n        0 2 4 6 8 10\n        0 2 4 6 8 10\n        0 2 4 6 8 10\n        */\n        int[] dp = new int[V+1];\n        dp[0] = 0;\n        for(int i = 1; i <= N; i++){\n            for(int j = V; j >= v[i]; j--){\n                dp[j] = Math.max(dp[j], dp[j-v[i]] + w[i]);\n            }\n            // for(int j = 0; j <= V; j++){\n            //     System.out.print(dp[j]);\n            //     System.out.print(\" \");\n            // }\n            // System.out.print(\"\\n\");\n        }\n        System.out.println(dp[V]);\n    }\n}\n\n```\n\n\n\n\n\n\n\n```java\n    public static int o1bagSolutionOptimization(int[] weight, int[] value, int bagWeight) {\n        int num = weight.length;\n        int[] dp = new int[bagWeight + 1];\n        dp[0] = 0;\n        for (int i = 1; i <= num; i++) {\n            for (int j = bagWeight; j >= 1; j--) {\n                if (j >= weight[i - 1]) {\n                    dp[j] = Math.max(dp[j], dp[j - weight[i - 1]] + value[i - 1]);\n                }\n            }\n        }\n\n\n        return dp[bagWeight];\n    }\n    public static void main(String[] args) {\n        Scanner sc = new Scanner(System.in);\n        int itemsNumber = sc.nextInt();\n        int bagWeight = sc.nextInt();\n        int[][] arr = new int[itemsNumber][2];\n        int[] weight = new int[itemsNumber];\n        int[] value = new int[itemsNumber];\n        for(int i = 0; i < itemsNumber; i++) {\n            for(int j = 0; j < 2; j++) {\n                arr[i][j] = sc.nextInt();\n            }\n            weight[i] = arr[i][0];\n            value[i]=   arr[i][1];\n        }\n        System.out.println(o1bagSolutionOptimization(weight, value, bagWeight));\n    }\n\n\n```\n\n\n\n## 完全背包问题\n\n```java\npublic class 完全背包问题 {\n    // 完全背包和01背包的区别是完全背包中每个物品可以用无限次\n// 01背包：f[i][j] = max(f[i-1][j], f[i-1][j-v]+w)\n// 完全背包：f[i][j] = max(f[i-1][j], f[i][j-v]+w)\n    public static void main(String[] args) throws Exception {\n        Scanner reader = new Scanner(System.in);\n        int N = reader.nextInt();\n        int V = reader.nextInt();\n        int[] v = new int[N + 1];\n        int[] w = new int[N + 1];\n\n        for (int i = 1; i <= N; i++) {\n            v[i] = reader.nextInt();\n            w[i] = reader.nextInt();\n        }\n        reader.close();\n\n        int[] dp = new int[V + 1];\n        dp[0] = 0;\n        for (int i = 1; i <= N; i++) {\n            for (int j = 0; j <= V; j++) {\n                if (j >= v[i]) {\n                    dp[j] = Math.max(dp[j], dp[j - v[i]] + w[i]);\n                }\n            }\n        }\n        System.out.println(dp[V]);\n    }\n\n    // int nativeDp(int n,int m){\n    //     int[] f = new int[maxN];\n    //     for(int i=1;i<=n;i++){\n    //         for(int j=m;j>=v[i];j--){\n    //             for(int k=0;k*v[i]<=j;k++){\n    //                 f[j] = Math.max(f[j], f[j-k*v[i]]+k*w[i]);\n    //             }\n    //         }\n    //     }\n    // }\n}\n```\n\n\n\n\n\n","tags":["DP"]},{"title":"LongFormer:The Long-Document Transformer","url":"/2021/04/18/LongFormer-The-Long-Document-Transformer/","content":"\n# LongFormer:The Long-Document Transformer\n\n主要记录一些Longfromer的原理和使用时的细节。\n\n## 摘要\n\n针对的问题：\n\n- 基于Transformer的模型，由于self-attention的操作，导致不能处理很长的序列。\n- self-attention的处理规模和序列长度是成二次关系的。\n\n![](https://i.loli.net/2021/04/18/3YtkrO18p2dAvmV.png)\n\n因为self-attention对于每个token都要计算打分，也就是缩放点积中的$QK^T$ 矩阵运算。\n\n这相当于对每个token之间都照顾到了注意信息。\n\n每个token代表一个小格，自注意力机制的QK都是自己，所以是个正方形。\n\n为解决这个问题，作者引入了三种具有随序列长度线性缩放的注意机制，将规模缩减成线性。\n\n分别是局部窗口注意和任务激活的全局注意力。\n\n并且还提供了LongFormer的预训练模型。\n\n定义了生成结构为Long-Forward-Encoding-Decoder(LED) \n\n\n\n\n\n## 引入&相关工作\n\n熟知的Bert等预训练模型，最大长度为512，多的就要截断，这样可能会潜在地导致重要的跨分区信息丢失问题。\n\n然而当时已有的针对解决长文本的方法，都是基于自回归语言模型的。\n\n而LongFormer是可以应用于迁移学习环境中的文档级NLP任务的。\n\n![](https://i.loli.net/2021/04/18/KNJa3dZx6fCG8eH.png)\n\n之后可能会读几篇。ltr从左到右的模型，其受益于双向语境(自回归或从左到右的语言建模被粗略地定义为在给定输入序列中的先前符号/字符的情况下估计现有符号/字符的概率分布)。\n\nspare代表模型通过稀疏性来进行优化。\n\nGenerating long se-quences with sparse transformers.其使用由BlockSparse提供的大小为8x8的块的扩展滑动窗口的形式，但没有探索预训练设置。等等\n\n\n\n## LongFormer\n\n原始Transformer的自注意力机制有$O(n^2)$ 的时间和空间内存复杂度。\n\n为了解决这个问题，作者根据指定相互关注的输入位置对的“注意模式”来稀疏完整的自我注意矩阵\n\n与full self-attention不同的是，提出的注意力模式与输入序列成线性关系，这使得它对较长的序列是有效的。\n\n### 注意力模式\n\n#### 滑动窗口 (Sliding Window)\n\n设固定窗口大小为 w，transformer层数为$l$, token的每边 $\\frac{1}{2}w$  计算复杂度为$O(n\\times w)$\n\n![](https://i.loli.net/2021/04/18/XaDokntURBWdNSe.png)\n\n作者认为：根据应用程序的不同，为每个图层使用不同的w值可能有助于在效率和模型表达能力之间取得平衡。\n\n\n\n#### 空洞滑窗(Dilated Sliding Window)\n\n类似于CNN的空洞卷积\n\n空洞尺寸 $d$ 感受野是 $l\\times d\\times w$\n\n![](https://i.loli.net/2021/04/18/SxDhujGwCVIvt2g.png)\n\n在多头注意力中，每个注意力头部计算不同的注意力分数。\n\n作者发现，每个头具有不同扩张配置设置的话效果会好：\n\n允许一些没有空洞的头部专注于局部语境，而另一些带空洞的则专注于更长的语境，从而提高了性能。\n\n\n\n#### 全局注意力(Global Attention)\n\n![](https://i.loli.net/2021/04/18/tVGNpUa3o9gIluf.png)\n\n例如对于QA，问题和文档连接在一起，允许模型通过自我关注将问题与文档进行比较。\n\n有时需要使用特殊的全局CLS作为整体的表达，所以就需要再这某些个关键点地方计算全局注意力，关注每一个token。其他的还是滑窗的形式。\n\n我们在几个预先选择的输入位置添加了“全局关注”。\n\n由于这样的记号token的数量相对于n很小，并且与n无关，因此组合的局部和全局注意的复杂度仍然是O(N)。\n\n这时，计算打分函数就可以分为两组QKV，分别是全局的$Q_g,K_g,V_g$ 和 滑窗局部的 $Q_s,K_s,V_s$\n\n昂贵的运算是矩阵乘法 $QK^T$，因为Q和K都具有n(序列长度)投影。对于LongFormer，空洞滑动窗口注意只计算固定数量$QK^T$的对角线。\n\n在实现的时候主要用到了带状乘法。还定制了特别的CUDA内核。。\n\n\n\n### 对于自回归的语言模型\n\n可以使用空洞滑动窗口注意力，并且可以跨层使用不同尺寸的窗口，效果可能更佳。\n\n对较低层使用较小的窗口大小，并在移动到较高层时增加窗口大小\n\n这允许顶层了解整个序列的较高级别表示，同时使较低层捕获本地信息。此外，它还在效率和性能之间取得平衡。\n\n(窗口大小越小，非零值越少，计算开销越小)\n\n(窗口大小越大，表示能力更丰富，通常会带来性能提升)\n\n\n\n## 实验\n\n和训练长文本的模型进行对比 ，BPC值越小越好\n\n![](https://i.loli.net/2021/04/18/Oxo2A1SCsaIeDL8.png)\n\n\n\n## 在QA上的Finetuning\n\n分别采用了我比较关注的多文档数据集 WikiHop/HotpotQA(干扰榜)/TriviaQA\n\n将问题和文档连接成一个长序列放入Longformer，最后加一个预测层。\n\n![](https://i.loli.net/2021/04/18/lqTB5cSPrD8Z3w7.png)\n\n### WikiHop\n\n数据特点：\n\n- 候选答案个数由2个到79个不等。\n\n- 文章段落数量由3段到63段不等\n\n数据集不为多跳推理链提供任何中间注释，需要模型代之以从间接答案监督中推断它们。\n\n数据预处理：\n\n将问题和答案与特殊令牌连接在一起\n\n$ [q] question [/q] [ent] candidate1 [/ent] ... [ent] candidateN [/ent] $\n\n上下文也是使用文档分隔符进行间隔\n\n$</s> context1 </s> ... </s> contextM </s>$\n\n在准备好输入数据后，从每个模型的顶层开始计算活动。获取问题和答案候选并将它们连接到尽可能多的上下文直到模型序列长度(Roberta为512，LongFormer为4,096)，在模型中运行序列，收集输出激活，并重复，直到用尽所有上下文(除了LongFormor-Large之外的所有模型，由于存储器要求，我们只包括第一个4,096长度的序列)。然后，将所有块的所有激活连接成一个长序列。在Longformer的下，使用全局注意力来关注整个问答候选序列。\n\n最终预测，对每个[ent] 附加一个线性层，输出一个logit，最后平均所有候选答案的logits。 用softmax和交叉熵得出最终答案。\n\n优化策略：\n\nAdam、Linear warmup超过200梯度更新对于最大LR，然后linear decay剩余训练。\n\n使用梯度累积最终batch达到32\n\n其他超参Dropout weight decay 都和Roberta相同。\n\n对LR[2e-5，3e-5，5e-5]和epoch[5，10，15]进行网格搜索。\n\nLR=3e-5，15个epoch是最好的Longform-Base配置。\n\n\n\n### TriviaQA\n\nTriviaQA有超过10万个问题、答案、文档。\n\n文档是维基百科文章，答案是文章中提到的命名实体。\n\n回答问题的跨度没有注释，但可以使用简单的文本匹配找到它。\n\n数据预处理：\n\n$[s] question [/s]document [/s]$\n\n在所有问题符号上都使用全局注意力。\n\n\n\n## HotpotQA\n\n使用两阶段首先确定相关段落，然后确定最终答案范围和证据。\n\n这主要是因为首先删除分散注意力的段落，可以降低最终认识和范围检测的噪声，这一点也被发现非常重要此数据集中最新的最新方法。\n\n数据预处理：\n\n$[CLS] [q] question [/q] ⟨t⟩ title1 ⟨/t⟩ sent1,1 [s] sent1,2 [s] ...⟨t⟩ title2 ⟨/t⟩ sent2,1 [s] sent2,2 [s] ...$\n\n使用全局注意力来问句标记、段落计时开始标记以及句子标记。\n\n在段落标题顶部增加了前馈层，用于预测相关段落的开始标记，以及用于预测证据句子的句子标记。\n\n在对第一阶段模型进行训练后，预测了训练集和开发集的相关段落得分。然后，保留最多5个原始得分高于预先指定的阈值(-3.0)的段落，并从上下文中删除其他段落。然后，根据得到的缩短上下文训练第二阶段模型。\n\n将跨度、问题分类、句子和段落损失结合起来，使用线性损失组合对模型进行多任务训练。\n\n使用ADAM优化器对模型进行了训练，并进行了线性warmup(1000步)和线性衰减。我们使用最小超参数调整，使用3E-5和5E-5的LR和3到7的epoch，发现LR为3E-5和5个历元的模型效果最好。\n\n![](https://i.loli.net/2021/04/19/LuOCHUx1eMPwDW9.png)\n\n","tags":["nlp"]},{"title":"Kaggle上传dataset的方法","url":"/2021/04/15/Kaggle上传dataset的方法/","content":"\n# Kaggle快速上传dataset的方法\n\n\n\n## 原理\n\n从国内上传到有cdn的地方(如GitHub), 再在kaggle的kernel上下载下来，直接上传dataset。\n\n\n\n## 方法\n\n\n\n首先需要掌握kaggle-api的使用，kaggle-api是kaggle官方提供的命令行工具，可以从命理完成比赛数据的下载、dataset下载上传，获取榜单等操作。\n\nhttps://github.com/Kaggle/kaggle-api\n\n本地安装：pip install kaggle\n\nKaggle已经安装好了，不用再安装\n\n\n\n步骤1：下载账户API json\n\nhttps://www.kaggle.com/me/account\n\n步骤2：在页面创建一个dataset\n\nhttps://www.kaggle.com/datasets\n\n步骤3：下载dataset的metadata\n\n运行：kaggle datasets metadata shopee-models\n\n步骤4：下载数据集并上传到dataset\n\n完整代码：\n\n```sh\n# 将API json文件写到这里\n!mkdir /root/.kaggle\nlines = '''{\"username\":\"写你的用户名\",\"key\":\"写你的key\"}'''\nwith open('/root/.kaggle/kaggle.json', 'w') as up:    \n\t\tup.write(lines)\n# 创建文件夹，写入dataset的metadata\n!mkdir hubmapkidneysegmentation\nlines = '''{\n\t\"id\": \"finlay/shopee-models\",\n\t\"id_no\": 122348,\n\t\"title\": \"shopee_models\",\n\t\"subtitle\": \"\",\n\t\"description\": \"\",\n\t\"keywords\": [],\n\t\"resources\": []\n}'''\nwith open('hubmapkidneysegmentation/dataset-metadata.json', 'w') as up:\n\t\tup.write(lines)\n# 下载文件，这里用axel多线程下载，直接用wget也可以的。\n!apt-get install axel\n!axel -n 12 https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth -o hubmapkidneysegmentation/baseline_fold0_densenet_224_epoch50.pth\n# 上传文件，这里会覆盖上传\n!kaggle datasets version -p ./hubmapkidneysegmentation -m \"Updated data fcn\"\n```\n\n","tags":["DataGame"]},{"title":"DUMA: Reading Comprehension with Transposition Thinking","url":"/2021/04/14/DUMA-Reading-Comprehension-with-Transposition-Thinking/","content":"\n# DUMA: Reading Comprehension with Transposition Thinking\n\nDUMA：DUal Multi-head Co-Attention model\n\n这是一篇针对解决多项选择任务的MRC网络结构。题目中的Transposition Think，被作者赋义为分别从文章和问题的角度来考虑对方的关注点。\n\n主要特点：\n\n- 基于预训练语言模型(得到表示编码，替代复杂的匹配网络)\n- 衔接多层co-attention(从三元组中捕捉关系)\n\n多项选择任务可以抽象为(文章P，问题q，选项a) 三元组。\n\n针对多项选择的特点多项选择MRC尤其依赖于匹配网络的设计，它被认为是有效地捕捉文章、问题和答案三元组之间的关系。(不能只考虑推理如何做的更好，还要考虑答案出现的关键位置也就是匹配网络的作用)\n\n \n\n文中总结的人在做阅读理解题时的特点：\n\n- 快速通读文章的整体内容，问题和回答选项，以建立全局印象，然后进行换角度思考过程。\n- 根据问答选项的特有信息，重新考虑文章的细节，收集问答选项的支持证据。\n- 根据文章中的特有信息，重新考虑问题和答案选项，以确定正确的选项，排除错误的选项。\n\n当人们重读文章时，他们倾向于根据对问答选项的印象提取关键信息，重读问答选项时也是如此\n\n\n\n---\n\n## DUMA\n\n多项选择问题可以定义模型需要学习一个概率分布$F(A_1,A_2,...,A_t|P,Q)$\n\n![](https://i.loli.net/2021/04/14/IE9asGiRTlLJNV2.png)\n\nEncoder 接受文本输入生成一个全局序列表达，这个过程类似人类第一次阅读整个内容以获得总体印象。\n\nDecoder则收集所有信息的答案预测以选择正确答案选项。\n\nDUMA层位于encoder和decoder之间，意在模仿人类转换思考角度的过程，从问题文章和关键词中捕捉关系信息。\n\n\n\n\n\n### Encoder\n\n作者用的是PrLMs，其将文章、问题和所有不同的候选答案拼接作为输入。\n\n$P=[p_1,p_2,..,p_m]$    ， $Q=[q_1,q_2,...,q_n]$ ,   $A=[a_1,a_2,...,a_k]$\n$$\n\\begin{equation}\\begin{split} \n E = Enc(P \\oplus Q \\oplus A )\n \\end{split}\\end{equation}\n$$\n这个输入到预训练的方式可能会遇到点问题，一般预训练语言模型比如bert都会限制一个输入的大小，如果文章过长的话，模型看不到问题和选项可能会导致训练效果不佳。可以改为 Q、A、P的形式，因为一般Q和A都比较短。\n\n$E = [e_1,e_2,...,e_{m+n+k}]$  \n\n$e_i$ 为固定维度$d_{model}$ 的向量，是各自的token。\n\n### Dual Multi-head Co-Attention\n\n使用双多头共同注意模型来计算文章和问答的attention表征。(可堆叠k层)\n\n其实就是一个多头co-attention，定义一个Q、K、V (Q不是上面的问题Q)\n\n先从E中分离出$E^P = [e^P_1,e^P_2,...,E^P_{t_p}]$、$E^{QA} = [e^{qA},e^{qA},...,E^{qA}_{t_{q_a}}]$\n\n使用两种计算attention的方法：\n\n- $E^P$ 做Query ，$E^{QA}$ 做 Key和Value\n\n- $E^{QA}$ 做Query ，$E^{P}$ 做 Key和Value\n\n$$\n\\begin{equation}\\begin{split} \n Attention(E^P,E^{QA},E^{QA}) &= softmax(\\frac{E^P(E^{QA})^T}{\\sqrt{d_k}})E^{QA}\\\\\n head_i &= Attention(E^PW^Q_i,E^{QA}W^K_i)\\\\\n MIIA(E^P, E^{QA}, E^{QA}) &= Concat(head_1,head_2,...,head_h) W^O\\\\\n MHA_1 &= MHA(E^P, E^{QA}, E^{QA}) \\\\\n MHA_2 &= MHA(E^{QA}, E^{P}, E^P) \\\\\n DUMA (E^P, E^{QA}) &= Fuse(MHA_1,MHA_2)\\\\\n    \\end{split}\\end{equation}\n$$\n\n \n\n其中$W_i^Q \\in R^{d_{model} \\times d_q}$ 、 $W_i^K \\in R^{d_{model} \\times d_k}$、  $W_i^V \\in R^{d_{model} \\times d_q}$ 、$W_i^O \\in R^{hd_v \\times d_{model}}$  : h 头数\n\n$MHA$: 多头注意力\n\n$Fuse$ 函数先使用均值池化来汇集$MHA(·)$的序列输出，然后再聚合两个池化的输出。\n\n后文实验了三种聚合方法 元素乘法  元素相加  concat\n\n表示在决定哪个是最佳答案选项之前，对所有关键信息进行混合。\n\n### Decoder\n\n$$\n\\begin{equation}\\begin{split} \n O_i &= DUMA(E^P, E^{QA_i}) \\\\\n L(A_r|P,Q) &= -log\\frac{exp(W^TO_r)}{\\sum_{i=1}^s exp(W^TO_i)}\n \\end{split}\\end{equation}\n$$\n\ns 是选项数量\n\n\n\n## Multi-choice MRC数据集\n\nDREAM and RACE\n\n![](https://i.loli.net/2021/04/14/63cBOaFhfGIgd58.png)\n\n## 实验\n\n![](https://i.loli.net/2021/04/14/3tQ1BCvzHSo9bTU.png)\n\n![](https://i.loli.net/2021/04/14/NchfAZRWxCIuQeS.png)\n\n![](https://i.loli.net/2021/04/14/d6JDXcVaTEmZPLF.png)\n\n","tags":["nlp"]},{"title":"FREELB: ENHANCED ADVERSARIAL TRAINING FOR NATURAL LANGUAGE UNDERSTANDING","url":"/2021/04/09/FREELB-ENHANCED-ADVERSARIAL-TRAINING-FOR-NATURAL-LANGUAGE-UNDERSTANDING/","content":"\n# FreeLB: Enhanced Adversarial Training For Natural Language Understanding\n\n[nlp中的对抗训练](https://coding-zuo.github.io/2021/04/07/nlp%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83-%E4%B8%8Ebert%E7%BB%93%E5%90%88/)\n\n承接上文，上文主要讲对抗训练的原理与物理意义与发展，对抗性训练是创建健壮神经网络的一种方法。在对抗性训练期间，小批次的训练样本受到对抗性扰动的污染，然后用于更新网络参数，直到得到的模型学会抵抗此类攻击，并且对模型起到了正则化的效果，提高模型泛化能力并且防止过拟合。\n\n这篇论文结合现在流行的预训练模型或transformer模型只能结合到下游任务的embedding中。\n\n提出FreeLB算法在GLUE上结合Roberta达到了当时的SOTA，是基于Transformer的自然语言理解和常识推理任务模型来做对抗。\n\n\n\n## 摘要\n\n对抗性训练可以最小化标签保留输入扰动的最大风险，已被证明对提高语言模型的泛化能力是有效的。\n\nFreeLB (Free Large-Batch)，通过在单词嵌入中添加对抗性扰动，并最小化输入样本周围不同区域内的对抗性风险，从而提高了嵌入空间的不变性。\n\n在GLUE基准上的实验表明，当仅应用于精调阶段时，它能够将BERT-BASE模型的整体测试分数从78.3提高到79.4，将Roberta-Large模型的测试分数从88.5提高到88.8。\n\n\n\n## 创新点\n\n针对PGD算法的问题：当K较小时，基于PGD的对抗性训练仍然会导致高度卷积和非线性的损失面，在更强的对手下很容易被打破，当K大时计算开销又很大。\n\n利用最近提出的“Free”训练策略在不同范数约束下用多样化的对抗样本来丰富训练数据，\n\n“Free”的对抗性训练算法在一次反向传递中同时更新模型参数和对抗性扰动。\n\n还使用将大部分对抗性更新限制在第一层，有效减少的对抗过程正反向传播总量。\n\n比PGD计算成本小，能在大规模的预训练模型上进行对抗训练。\n\n\n\n## 文本的对抗样本(对手) \n\n1. 黑盒环境下对embedding进行扰动(对手不是从样本进行攻击)\n2. 在输入中添加分散注意力的句子(人工)\n3. 用GANs将输入投影到潜在空间，并搜索接近原始的文本对手\n\n![](https://i.loli.net/2021/04/09/daKoDQkv35hwmEP.png)\n\n第二三算是一种辅助模型，数据增强的一种形式。\n\n如何在没有人工评估的情况下通过单词/字符替换来构建保留标签的对抗性示例仍然不清楚，因为每个单词/字符的含义取决于上下文。\n\n所以主要还是采用第一种进行对抗训练。\n\n因为词的输入表达有很多种，像词embedding、句子embedding和位置embedding。作者和其他对抗训练一样只干扰词embedding和拼接词的embedding。\n\n注意，基于Embedding的对手严格来说比更传统的基于文本的对手更强大，因为对手可以在单词嵌入上进行在文本域中不可能进行的操作。因为CV都是从样本层面进行扰动，这个扰动从embedding上扰动，相当于在更高级的层面，所以更强大。\n\n## FreeLB\n\n此前预训练语言模型对于下游任务已被证实很有效。\n\n作者的目标是通过在下游语言理解任务的精调过程中增强它们在嵌入空间中的鲁棒性，进一步提高这些预先训练的语言模型在下游语言理解任务上的泛化能力。\n\n由于这篇论文只对对抗性训练的效果感兴趣，而不是产生实际的对抗性示例，因此使用基于梯度的方法在输入句子的嵌入中添加范数有界的对抗性扰动。\n\n定义模型的输入One-hot向量为 $ Z=[z_1,z_2,...,z_n]$\n\n嵌入矩阵为V\n\n语言模型看成是一个 $y=f_{\\theta}(X), X=VZ$ , y是模型输出 $\\theta$是可学习参数。\n\n定义对抗扰动为 $\\delta$ \n\n新的预测输出变为 $y'=f_{\\theta}(X+\\delta)$\n\n为了保持语义，我们将δ的范数限制为较小，并假设模型的预测在扰动后不会改变。\n\n上面的定义和其他人的研究基本都是相同的，FreeLB区别在于不要求X归一化。\n\nFreeLB吸取了FreeAT和YOPO加速方法, 几乎不需要任何开销就可以获得参数的梯度。实现了与标准PGD训练模型相当的健壮性和泛化能力，只使用与自然训练相同或略多的正反向传播。\n\n### FreeAT (Free Adversarial Training): NIPS2019\n\n从FGSM到PGD，主要是优化对抗扰动的计算，虽然取得了更好的效果，但计算量也一步步增加。对于每个样本，FGSM和FGM都只用计算两次，一次是计算x的前后向，一次是计算x+r的前后向。而PGD则计算了K+1次，消耗了更多的计算资源。因此FreeAT被提了出来，在PGD的基础上进行训练速度的优化。\n\nFreeAT的思想是在对每个样本x连续重复m次训练，计算r时复用上一步的梯度，为了保证速度，整体epoch会除以m。r的更新公式为：\n$$\n\\begin{equation}\\begin{split} \n r_{t+1} = r_t + \\epsilon \\cdot sign(g)\n    \\end{split}\\end{equation}\n$$\n伪代码：\n\n```text\n初始化r=0\n对于epoch=1...N/m:\n  对于每个x:\n    对于每步m:\n      1.利用上一步的r，计算x+r的前后向，得到梯度\n      2.根据梯度更新参数\n      3.根据梯度更新r\n```\n\n缺点：FreeLB指出，FreeAT的问题在于每次的r对于当前的参数都是次优的（无法最大化loss），因为当前r是由r(t-1)和theta(t-1)计算出来的，是对于theta(t-1)的最优。\n\n代码：[https://github.com/mahyarnajibi...](https://link.zhihu.com/?target=https%3A//github.com/mahyarnajibi/FreeAdversarialTraining/blob/d70774030871fa3207e09ce8528c1b84cd690603/main_free.py%23L160)\n\n### YOPO (You Only Propagate Once): NIPS2019\n\n代码：[https://github.com/a1600012888/YOPO-You-Only-Propagate-Once](https://link.zhihu.com/?target=https%3A//github.com/a1600012888/YOPO-You-Only-Propagate-Once)\n\n可以参考[加速对抗训练——YOPO算法浅析](https://zhuanlan.zhihu.com/p/95904001)\n\n极大值原理PMP(Pontryagin's maximum principle)是optimizer的一种，它将神经网络看作动力学系统。这个方法的优点是在优化网络参数时，层之间是解藕的。通过这个思想，我们可以想到，既然扰动是加在embedding层的，为什么每次还要计算完整的前后向传播呢？\n\n基于这个想法，作者想复用后几层的梯度，假设p为定值：\n\n![[公式]](https://www.zhihu.com/equation?tex=p+%3D+%5Cnabla_%7Bg_%7B%5Ctilde%5Ctheta%7D%7D%28l%28g_%7B%5Ctilde%5Ctheta%7D%28f_0%28x_i%2Br_i%5E%7Bj%2C0%7D%2C+%5Ctheta_0%29%29%2Cy_i%29%29%5Ccdot%5Cnabla_%7Bf_0%7D%28g_%7B%5Ctilde%5Ctheta%7D%28f_0%28x_i%2Br_i%5E%7Bj%2C0%7D%2C+%5Ctheta_0%29%29%29+%5C%5C)\n\n则对r的更新就可以变为\n\n![[公式]](https://www.zhihu.com/equation?tex=r_i%5E%7Bj%2Cs%2B1%7D+%3D+r_i%5E%7Bj%2Cs%7D%2B%5Calpha_1p%5Ccdot%5Cnabla_%7Br_i%7Df_0%28x_i%2Br_i%5E%7Bj%2Cs%7D%2C%5Ctheta_0%29+%5C%5C)\n\n我们可以先写出YOPO的梯度下降版本：\n\n```text\n对于每个样本x\n初始化r(1,0)\n对于j=1,2,...,m:\n  1.根据r(j,0),计算p\n  对于s=0,1,...,n-1:\n    2.计算r(j,s+1)\n  3.另r(j+1,0)=r(j,n)\n```\n\n作者又提出了PMP版本的YOPO，并证明SGD的YOPO是PMP版的一种特殊形式。这样每次迭代r就只用到embedding的梯度就可以了。\n\nYOPO还主张在每次反向传播后，应将第一隐层的梯度作为常数，利用该常数与网络第一层的雅可比的乘积对对手进行多次额外更新，以获得强对手。\n\n### 回到FreeLB\n\n与FreeAT不同的是，YOPO从每个上升步长开始累加参数的梯度，并且只在K个内上升步长之后更新一次参数。\n$$\n\\begin{equation}\\begin{split} \n  min_{\\theta}\\mathbb{E}_{(Z,y)∼ D ,{m∼M }} [\\frac {1}{K}\\sum_{t=0}^{K-1} max_{\\delta_t\\in \\Omega_t\t}L(f_{\\theta}(x+\\delta_t),y)] \n \\end{split}\\end{equation}\n$$\n\n对比 PGD:\n$$\n\\begin{equation}\\begin{split} \n  min_{\\theta}\\mathbb{E}_{(x,y)∼ D} [max_{\\Delta x\\in \\Omega\t}L(x+\\Delta x,y;\\Theta)] \n    \\end{split}\\end{equation}\n$$\n\nFreeLB和PGD主要有两点区别：\n\n1. PGD是迭代K次r后取最后一次扰动的梯度更新参数，FreeLB是取K次迭代中的平均梯度\n2. PGD的扰动范围都在epsilon内，因为伪代码第3步将梯度归0了，每次投影都会回到以第1步x为圆心，半径是epsilon的圆内，而FreeLB每次的x都会迭代，所以r的范围更加灵活，更可能接近局部最优：\n\n![](https://i.loli.net/2021/04/10/ZMxvfdq4FXRn69S.jpg)\n\n它执行多次PGD迭代来构造对抗性实例，并在每次迭代中同时累积“free”参数梯度∇θL。\n\n伪代码：\n\n```text\n对于每个x:\n  1.通过均匀分布初始化r，梯度g为0\n  对于每步t=1...K:\n    2.根据x+r计算前后向，累计梯度g\n    3.更新r\n  4.根据g/K更新梯度\n```\n\n论文中还指出了很重要的一点，就是**对抗训练和dropout不能同时使用**，加上dropout相当于改变了网络结构，会影响r的计算。如果要用的话需要在**K步中都使用同一个mask**。\n\n![](https://i.loli.net/2021/04/10/WTh7O4Ui5YenzNM.png)\n\n\n\n\n\n## 参考文献\n\n[一文搞懂NLP中的对抗训练FGSM/FGM/PGD/FreeAT/YOPO/FreeLB/SMART](https://zhuanlan.zhihu.com/p/103593948)\n\n[对抗训练的理解，以及FGM、PGD和FreeLB的详细介绍](https://blog.csdn.net/weixin_41712499/article/details/110878322)\n\n","tags":["nlp"]},{"title":"图神经网络的对抗攻击","url":"/2021/04/08/图神经网络的对抗攻击/","content":"\n# 图神经网络的对抗攻击\n\n最近要汇报一个关于安全方面的研究。本来打算讲一些和安全擦边的关于nlp对抗训练提升模型鲁棒性的内容，正好和最近学习的阅读理解比赛相关，可以作为一个提分trick。\n\n但老师强调要和安全相关少讲过程。而nlp中的对抗样本不可以加在原始样本中，只能在embedding中加入扰动，这样就没法攻击，多数用来提升模型鲁棒性。所以就拍马研究了一下图网络的对抗攻击。\n\n刚开始了解，希望可以从中找出可以和我研究方向结合的地方。\n\n如有不对的地方还希望联系我指点一下。\n\n[nlp中的对抗训练&与bert结合](https://coding-zuo.github.io/2021/04/07/nlp%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83-%E4%B8%8Ebert%E7%BB%93%E5%90%88/)\n\n在上一篇文章中主要介绍的是对抗训练，其实是一种防御的策略，对提高模型而言FGM相当于加了一个正则项。 \n\n## 图网络攻击难点\n\n- 离散的结构/特征，难以直接利用现有的基于梯度的方法。\n\n- 对于“无法感知”的扰动如何定义。\n\n- 节点分类往往属于直推式学习，训练数据和测试数据联合使用以学习模型，这就使得攻击方法注定是与poisoning/causative attack相关，而非仅是evasion attack。\n\n\n\n\n\n\n\n\n\n\n\n## 参考文献\n\n[图对抗攻击 Graph Adversarial Attack](https://zhuanlan.zhihu.com/p/88934914)\n\n","tags":["GNN"]},{"title":"nlp中的对抗训练&与bert结合","url":"/2021/04/07/nlp中的对抗训练-与bert结合/","content":"\n#  nlp中的对抗训练学习\n\nPPT : https://coding-zuo.github.io/adversary/index.html\n\n由于深度神经网络强大的表示学习能力，在许多领域都取得了很大的成功，包括计算机视觉、自然语言处理、语音识别等。然而，在其卓越性能的背后，深度神经网络作为一个黑箱，缺乏可解释性与鲁棒性，使得它易受到对抗攻击而对抗性攻击的存在可能是深度学习模型的一个固有弱点。\n\n深度学习的对抗一般有两种含义：\n\n- 一是生成对抗网络(Generative Adversarial Network,GAN) 代表一大类先进的生成模型。(这方面我不是很了解)\n- 另一个则是跟对抗攻击、对抗样本相关的领域。(主要关心模型在小扰动下的稳健性)\n\n\n\n## 方法介绍\n\n在CV领域，我们需要通过对模型的对抗攻击和防御来增强模型的稳健型，比如在自动驾驶系统中，要防止模型因为一些随机噪声就将红灯识别为绿灯。\n\n在NLP领域，类似的对抗训练也是存在的，不过NLP中的对抗训练更多是作为一种正则化手段来提高模型的泛化能力！\n\n这使得对抗训练成为了NLP刷榜的“神器”之一，前有微软通过RoBERTa+对抗训练在[GLUE](https://gluebenchmark.com/leaderboard)上超过了原生RoBERTa。\n\n\n\n## 对抗样本\n\n要认识对抗训练，首先要了解“对抗样本”，它首先出现在论文[《Intriguing properties of neural networks》](http://https//arxiv.org/abs/1312.6199)之中。简单来说，它是指对于人类来说“看起来”几乎一样、但对于模型来说预测结果却完全不一样的样本，比如下面的经典例子：\n\n![](https://i.loli.net/2021/04/07/EizJ85fCHyX2drj.png)\n\n“对抗攻击”，其实就是想办法造出更多的对抗样本。\n\n“对抗防御”，就是想办法让模型能正确识别更多的对抗样本。\n\n所谓对抗训练，则是属于对抗防御的一种，它构造了一些对抗样本加入到原数据集中，希望增强模型对对抗样本的鲁棒性；同时，如本文开篇所提到的，在NLP中它通常还能提高模型的表现。\n\n用对抗训练的思路来提升NLP模型，有两个实现角度：\n\n1. 因为nlp的输入通常是one-hot向量，两个one-hot向量其欧式距离恒为$\\sqrt 2$ ，理论上不存在微小的扰动，不想cv图像那样可以对连续实数向量来做。比如，$\\Delta x$ 是实数向量，$x+\\Delta x$还是一个有意义的图。所以很多研究都是在embedding层上做扰动的，因为embedding层是我们自己训练的，所以不太可能出现认为的恶意对抗攻击。\n\n   ![](https://i.loli.net/2021/04/09/daTOFDIU3EtfyGs.png)\n\n2. 这种角度不知道还算不算对抗，但可以说是一种数据增强手段。如上图中下面的问题，经过缩写，添加标点，或者同义词近义词替换等等。通过辅助模型提升鲁棒性。\n\n\n\n## Min-Max\n\n对抗训练可以统一写成如下格式：\n$$\n\\begin{equation}\\begin{split} \n min_{\\theta}\\mathbb{E}_{(x,y)∼ D} [max_{\\Delta x\\in \\Omega\t}L(x+\\Delta x,y;\\Theta)] \n    \\end{split}\\end{equation}\n$$\n其中$D$ 代表训练集，x代表输入，y代表标签，θ是可学习模型参数，L(x,y;θ)是单个样本的loss，Δx是对抗扰动，Ω是扰动空间。\n\n理解为：\n\n1. $max_{\\Delta x\\in \\Omega}L(x+\\Delta x,y;\\theta)$ ，往输入x中注入扰动$\\Delta x$， 目的是希望 $ L(x+\\Delta x,y;\\theta)$ 损失越大越好，也就是让现有模型的预测出错;\n\n2. 当然$\\Delta x$ 不能太大、无约束，否则达不到“看起来几乎一样”的效果，所以$Δx$要满足一定的约束，常规的约束是$‖Δx‖≤ϵ$，其中$ϵ$是一个常数；\n\n3. 构造好对抗样本后，用$x+\\Delta x,y$作为数据去最小化loss，来更新参数$\\theta$ (梯度下降)\n4. 重复执行1.2.3步。\n\n整个对抗训练优化过程是一个max和min交替执行的过程：通过注入max损失，在梯度下降让损失变min。\n\n\n\n## 如何计算$\\Delta x$——快速梯度FGM\n\n$\\Delta x$的目的是增大Loss，而我们知道让loss减少的方法是梯度下降，那反过来，让loss增大的方法自然就是梯度上升，因此可以简单地取\n$$\n\\begin{equation}\\begin{split} \n \\Delta x &= ϵ∇_xL(x,y;θ)\\\\\n ∇_xL(x,y;θ) &= (\\frac {\\partial L }{\\partial x})\n    \\end{split}\\end{equation}\n$$\n求loss对x的梯度，然后根据梯度给Δx赋值，来实现对输入的干扰，完成干扰之后再执行常规的梯度下降。\n\n为了防止$\\Delta x$过大，通常要对 $∇xL(x,y;θ)$ 标准化，常见方式为：\n$$\n\\begin{equation}\\begin{split} \n \t\\Delta x = ϵ \\frac {∇_xL(x,y;θ)}{||∇_xL(x,y;θ)||} \\text{或} \\Delta x=  ϵsign(∇_xL(x,y;θ))\n    \\end{split}\\end{equation}\n$$\n采用右边的取扰动值的算法叫FGSM(ICLR2015)，理解为扰动是沿着梯度方向向损失值的极大值走。\n\n采用左边取扰动值的算法叫FGM(ICLR2017)，理解为在每个方向上都走相同的一步找到更好的对抗样本。\n\n有了$\\Delta x$，得到：\n$$\n\\begin{equation}\\begin{split} \n min_{\\theta}\\mathbb{E}_{(x,y)∼ D} [L(x+\\Delta x,y;\\Theta)] \n    \\end{split}\\end{equation}\n$$\n这就构成了一种对抗训练方法，被称为**Fast Gradient Method（FGM）**，它由GAN之父Goodfellow在论文[《Explaining and Harnessing Adversarial Examples》](https://arxiv.org/abs/1412.6572)首先提出。\n\n此外，对抗训练还有一种方法，叫做**Projected Gradient Descent（PGD）**，其实就是通过多迭代几步来达到让$L(x+Δx,y;θ)$更大的$Δx$（如果迭代过程中模长超过了$ϵ$，[《Towards Deep Learning Models Resistant to Adversarial Attacks》](https://arxiv.org/abs/1706.06083)。在后文....\n\n### 梯度惩罚\n\n假设已经得到对抗扰动 $\\Delta x$ ,更新 $\\theta$ 时，对 L 进行泰勒展开：\n$$\n\\begin{equation}\\begin{split} \n min_{\\theta} \\mathbb{E}_{(x,y)∼ D} [L(x+\\Delta x,y;\\theta)] &\\approx min_{\\theta} \\mathbb{E}_{(x,y)∼ D}[L(x,y;\\theta) + <∇_xL(x,y;θ), \\Delta x>] \\\\\n &= min_{\\theta} \\mathbb{E}_{(x,y)∼ D}[L(x,y;\\theta) + ∇_xL(x,y;θ) \\cdot \\Delta  x ] \\\\\n &= min_{\\theta} \\mathbb{E}_{(x,y)∼ D}[L(x,y;\\theta) + ∇_xL(x,y;θ)^T  \\Delta  x ] \n    \\end{split}\\end{equation}\n$$\n对应的 $\\theta$ 的梯度为:\n$$\n\\begin{equation}\\begin{split} \n ∇_{\\theta} L(x,y;\\theta) + ∇_{\\theta} ∇_xL(x,y;θ)^T  \\Delta  x \n    \\end{split}\\end{equation}\n$$\n代入 $ \\Delta x = ϵ∇_xL(x,y;θ)$:\n$$\n\\begin{equation}\\begin{split} \n &∇_{\\theta} L(x,y;\\theta) + ϵ ∇_{\\theta} ∇_xL(x,y;θ)^T  ∇_xL(x,y;θ)\\\\ &= ∇_{\\theta}(L(x,y;θ) + \\frac{1}{2} ϵ ||∇_xL(x,y;θ)||^2)\n    \\end{split}\\end{equation}\n$$\n这个结果表示，对输入样本施加 $ϵ∇xL(x,y;θ)$ 的对抗扰动，一定程度上等价于往loss里边加入“梯度惩罚”\n$$\n\\begin{equation}\\begin{split} \n \\frac{1}{2} ϵ ||∇_xL(x,y;θ)||^2\n    \\end{split}\\end{equation}\n$$\n\n如果对抗扰动是$ϵ‖∇xL(x,y;θ)‖$ ，那么对应的梯度惩罚项则是$ϵ‖∇xL(x,y;θ)‖$（少了个1/2，也少了个2次方）。\n\n### 几何图像\n\n事实上，关于梯度惩罚，我们有一个非常直观的几何图像。以常规的分类问题为例，假设有n个类别，那么模型相当于挖了n个坑，然后让同类的样本放到同一个坑里边去：\n\n![](https://i.loli.net/2021/04/09/xeOucXmabjkrS4A.png)\n\n梯度惩罚则说“同类样本不仅要放在同一个坑内，还要放在坑底”，这就要求每个坑的内部要长这样：\n\n![](https://i.loli.net/2021/04/09/tHylhowkCpvP2IM.png)\n\n为什么要在坑底呢？因为物理学告诉我们，坑底最稳定呀，所以就越不容易受干扰呀，这不就是对抗训练的目的么？\n\n那坑底意味着什么呢？极小值点呀，导数（梯度）为零呀，所以不就是希望‖∇xL(x,y;θ)‖‖∇xL(x,y;θ)‖越小越好么？这便是梯度惩罚的几何意义了。\n\n![](https://kexue.fm/usr/uploads/2020/03/3963498733.gif)\n\n苏神代码基于keras的：\n\n> https://github.com/bojone/keras_adversarial_training\n\n## Projected Gradient Descent (PGD)\n\n内部max的过程，本质上是一个非凹的约束优化问题，FGM解决的思路其实就是梯度上升，**那么FGM简单粗暴的“一步到位”，是不是有可能并不能走到约束内的最优点呢？**当然是有可能的。于是，一个很intuitive的改进诞生了：Madry在18年的ICLR中，提出了用Projected Gradient Descent（PGD）的方法，简单的说，就是**“小步走，多走几步”**，如果走出了扰动半径为$\\epsilon$的空间，就映射回“球面”上，以保证扰动不要过大：\n\n其中$\\mathcal{S}=\\{r\\in\\mathbb{R}^d:||r||_2 \\leq \\epsilon\\}$ 为扰动的约束空间，$\\alpha$为小步的步长。\n\n作者将这一类通过一阶梯度得到的对抗样本称之为“一阶对抗”，在实验中，作者发现，经过PGD训练过的模型，对于所有的一阶对抗都能得到一个低且集中的损失值，如下图所示：\n\n![](https://i.loli.net/2021/04/09/SosrVAW9UGYNm6T.png)\n\n我们可以看到，面对约束空间 $\\mathcal{S}$ 内随机采样的十万个扰动，PGD模型能够得到一个**非常低且集中的loss分布**，因此，在论文中，作者称PGD为**“一阶最强对抗”**。也就是说，只要能搞定PGD对抗，别的一阶对抗就不在话下了。\n\n```\n对于每个x:\n  1.计算x的前向loss、反向传播得到梯度并备份\n  对于每步t:\n      2.根据embedding矩阵的梯度计算出r，并加到当前embedding上，相当于x+r(超出范围则投影回epsilon内)\n      3.t不是最后一步: 将梯度归0，根据1的x+r计算前后向并得到梯度\n      4.t是最后一步: 恢复(1)的梯度，计算最后的x+r并将梯度累加到(1)上\n  5.将embedding恢复为(1)时的值\n  6.根据(4)的梯度对参数进行更新\n```\n\n基于PGD的对抗性训练被广泛认为是最有效的，因为它在很大程度上避免了模糊的梯度问题。它将一类对抗性训练算法转化为求解交叉熵损失的极大极小问题，该问题可以通过多次投影梯度上升步骤和随后的SGD步骤可靠地实现。\n\n\n\n## Virtual Adversarial Training\n\n除了监督训练，对抗训练还可以用在半监督任务中，尤其对于NLP任务来说，很多时候输入的无监督文本多的很，但是很难大规模地进行标注，那么就可以参考[13]中提到的Virtual Adversarial Training进行半监督训练。\n\n首先，我们抽取一个随机标准正态扰动（$d\\sim \\mathcal{N}(0, I)\\in \\mathbb{R}^d$），加到embedding上，并用KL散度计算梯度：\n\n然后，用得到的梯度，计算对抗扰动，并进行对抗训练：\n\n![](https://i.loli.net/2021/04/09/KYX3zILWf4A1Htg.png)\n\n实现方法跟FGM差不多\n\n## FreeAT & YOPO & FreeLB\n\n**优化的主要方向有两点：得到更优的扰动 & 提升训练速度**\n\n其实PGD效果不错但是它迭代多步计算开销很大，所以出现了这些针对效率上的优化，并且结合预训练语言模型。\n\n具体的就搜这些论文来看吧。\n\nFGSM: Explaining and Harnessing Adversarial Examples\n\nFGM: Adversarial Training Methods for Semi-Supervised Text Classification\n\nFreeAT: Adversarial Training for Free!\n\nYOPO: You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle\n\nFreeLB: Enhanced Adversarial Training for Language Understanding\n\nSMART: Robust and Efficient Fine-Tuning for Pre-trained Natural\n\n## 参考文献\n\n[[对抗训练浅谈：意义、方法和思考（附Keras实现）](https://kexue.fm/archives/7234)]\n\n[【炼丹技巧】功守道：NLP中的对抗训练 + PyTorch实现](https://fyubang.com/2019/10/15/adversarial-train/)\n\n[NLP --- >对抗学习：从FGM, PGD到FreeLB](https://blog.csdn.net/chencas/article/details/103551852)\n\n[TAVAT: Token-Aware Virtual Adversarial Training for Language Understanding](https://arxiv.org/pdf/2004.14543v3.pdf)\n\n[Adversarial Training Methods for Semi-Supervised Text Classification](https://arxiv.org/abs/1605.07725)\n[Adversarial Text Classification原作实现](https://github.com/tensorflow/models/blob/e97e22dfcde0805379ffa25526a53835f887a860/research/adversarial_text/adversarial_losses.py)\n\n[NLP(文本)中的对抗训练](https://blog.csdn.net/ganxiwu9686/article/details/105931668)\n\n[对抗训练的理解，以及FGM、PGD和FreeLB的详细介绍](https://blog.csdn.net/weixin_41712499/article/details/110878322)\n\n","tags":["nlp"]},{"title":"数组范围内计数","url":"/2021/04/07/数组范围内计数/","content":"\n# 数组范围内计数\n\n数组为 3,2,2,3,1, 查询为（0,3,2)。\n\n意思是在数组里下标 0-3 这个范围上，有几个 2? \n\n假设给一个数组 arr，对这个数组的查询非常频繁请返回所有查询的结果。\n\n给出：arr[    ]\n\n要查多个范围：\n\n[[0,3,2],\n\n[1,4,0]]\n\n结果返回：[第一个数组结果，第二个数组结果]\n\n暴力解法直接遍历肯定不可以。\n\n## 解法\n\n一、做一个map映射，遍历一遍数组，将每个值和每个值出现的下标位置数组做成key-value\n\n在根据查询的V，在所在值的数组内做二分查找。\n\n```java\npublic static class QueryBox2 {\n        private HashMap<Integer, ArrayList<Integer>> map;\n\n        public QueryBox2(int[] arr) {\n            map = new HashMap<>();\n            for (int i = 0; i < arr.length; i++) {\n                if (!map.containsKey(arr[i])) {\n                    map.put(arr[i], new ArrayList<>());\n                }\n                map.get(arr[i]).add(i);\n            }\n        }\n\n        public int query(int L, int R, int value) {\n            if (!map.containsKey(value)) return 0;\n            ArrayList<Integer> indexArr = map.get(value);\n            // 查询<L的下标有几个\n            int a = countLess(indexArr, L);\n            // 查询<R+1的下标有几个\n            int b = countLess(indexArr, R + 1);\n            return b - a;\n        }\n\n        // 在有序数组中，用二分法数出<limit 的数有几个\n        // 也就是用二分法，找到<limit的数中最右的位置\n        private int countLess(ArrayList<Integer> arr, int limit) {\n            int L = 0;\n            int R = arr.size() - 1;\n            int mostRight = -1;\n            while (L <= R) {\n                int mid = L + ((R - L) >> 1);\n                if (arr.get(mid) < limit) {\n                    mostRight = mid;\n                    L = mid + 1;\n                } else {\n                    R = mid - 1;\n                }\n            }\n            return mostRight + 1;\n        }\n    }\n```\n\n$O(mlogn)$\n\n\n\n扩展：\n\n如果是求范围内累加和\n\n可以生成一个前缀和数组类似本题\n\n\n\n\n\n## 腾讯原题\n\n给定整数 power。给定一个数组 arr。给定一个数组 reverse。含义如下\n\narr 的长度一定是 2 的 power 次方，reverse1 每个值一定都在 0 ~ power范围。\n\n例如 power=2, ar={3,1,4,2}, reverse={0,1,0,2}\n\n针对reverse的数组中每一个值 \n\n如第一个值为0，就是对$2^0=1$ 每一个arr数组以1个数为单位逆序。\n\n如第二个值为1，就是对$2^1=2$ 对arr数组每两个数逆序\n\n任何一个在前的数字可以和任何一个在后的数组，构成一对数。可能是升序关系、相等关系或者降序关系。\n\n最后求调整完的arr数组有多少个降序对。\n\n比如 arr 开始时有如下的降序对：(3,1)、（3.2)、（4.2），一共 3 个。\n\n以2个数调整后arr由{3,1,4,2} 变成{1,3,2,4} 降序对有{3,2} ，共1个\n\n\n\n经典做法每次都reverse\n\n```java\n// originArr长度一定是2的power次方\n    // reverseArr中每一个值，都是0-power范围上的数\n    public static int[] reversePair1(int[] originArr, int[] reverseArr, int power) {\n        int[] ans = new int[reverseArr.length];\n        for (int i = 0; i < reverseArr.length; i++) {\n            // 1 << (reverseArr[i]) == r[i]的2次方\n            reverseArray(originArr, 1 << (reverseArr[i]));\n            ans[i] = countReversePair(originArr);\n        }\n        return ans;\n    }\n\n    public static void reverseArray(int[] originArr, int teamSize) {\n        if (teamSize < 2) return;\n        for (int i = 0; i < originArr.length; i += teamSize) {\n            reversePart(originArr, i, i + teamSize - 1);\n        }\n    }\n\n    public static void reversePart(int[] arr, int L, int R) {\n        while (L < R) {\n            int temp = arr[L];\n            arr[L++] = arr[R];\n            arr[R--] = temp;\n        }\n    }\n\n    public static int countReversePair(int[] originArr) {\n        int ans = 0;\n        for (int i = 0; i < originArr.length; i++) {\n            for (int j = i + 1; j < originArr.length; j++) {\n                if (originArr[i] > originArr[j]) {\n                    ans++;\n                }\n            }\n        }\n        return ans;\n    }\n```\n\n\n\n优化方案\n\n[3,2 4,5, 0,1 3,5]\n\n两个数一组：  1个逆序对，3个升序对\n\n四个数一组： 0个逆序对，8个升序对\n\n八个数一组：10个逆序对，4个升序对\n\n每个数组的逆序对数是2、4、9个一组的加和\n\n当数组进行几个数一组翻转时\n\n翻转后的逆序对和升序对数量，和翻转前的升序对和逆序对相等，数量调换了。\n\n小的数调整后数量不影响大数量调整后的数量。\n\n\n\n所以直接查2、4、8、16个 数量再交换相加。\n\n如何高效生成预处理记录\n\n输入数据状况\n\npower范围[0,20]\n\narr长度范围[1,10e7]\n\nreverse长度范围[1,10e6]\n\n```java\npublic static int[] reversePair2(int[] originArr, int[] reverseArr, int power) {\n        int[] originReverse = Arrays.copyOf(originArr, originArr.length);\n        reversePart(originReverse, 0, originReverse.length - 1);\n        int[] recordDown = new int[power + 1];\n        int[] recordUp = new int[power + 1];\n        process(originArr, 0, originArr.length - 1, power, recordDown);\n        process(originReverse, 0, originReverse.length - 1, power, recordUp);\n\n        // recordDown[i] 2的i次方个数一组的划分中，降序的数量\n        // recordUp[i] 2的i次方个数一组的划分中，升序的数量\n        int[] ans = new int[reverseArr.length];\n        for (int i = 0; i < reverseArr.length; i++) {\n            int curPower = reverseArr[i]; // =3  2的1次方、2次方、3次方 要调整\n            for (int p = 1; p <= curPower; p++) {\n                int tmp = recordDown[p];\n                recordDown[p] = recordUp[p];\n                recordUp[p] = tmp;\n            }\n            for (int p = 1; p <= power; p++) {\n                ans[i] += recordDown[p];\n            }\n        }\n        return ans;\n    }\n\n    public static void process(int[] originArr, int L, int R, int power, int[] record) {\n        if (L == R) {\n            return;\n        }\n        int mid = L + ((R - L) >> 1);\n        process(originArr, L, mid, power - 1, record);\n        process(originArr, mid + 1, R, power - 1, record);\n        record[power] += merge(originArr, L, mid, R);\n    }\n\n    public static int merge(int[] arr, int L, int m, int r) {\n        int[] help = new int[r - L + 1];\n        int i = 0;\n        int p1 = L;\n        int p2 = m + 1;\n        int ans = 0;\n        while (p1 <= m && p2 <= r) {\n            ans += arr[p1] <= arr[p2] ? arr[p1++] : arr[p2++];\n            help[i++] = arr[p1] <= arr[p2] ? arr[p1++] : arr[p2++];\n        }\n        while (p1 <= m) {\n            help[i++] = arr[p1++];\n        }\n        while (p2 <= r) {\n            help[i++] = arr[p2++];\n        }\n        for (i = 0; i < help.length; i++) {\n            arr[L + i] = help[i];\n        }\n        return ans;\n    }\n```\n\n","tags":["刷题"]},{"title":"海华中文阅读理解比赛梳理/多卡并行/transformers","url":"/2021/04/06/海华中文阅读理解比赛梳理-多卡并行-transformers/","content":"\n# 海华中文阅读理解比赛梳理\n\n文文言文古诗词现代诗词\n\n1 字词解释 2 标点符号作用 3 句子解释 4 填空 5 选择正读音 6 推理总结 7 态度情感 8 外部知识\n\n不需要先验知识的问题\n\n如一个问题能够在文档中进行匹配，回答起来就几乎不需要先验知识需要先验知识的问題\n\n1、关于语言的知识：需要词汇/语法知识,例如:习语、谚语、否定、反义词、同义词语法转换\n\n2、特定领域的知识：需要但不限于些事实上的知识，这些事实与特定领域的概念概念定义和属性，概念之间的关系\n\n3、一般世界的知识：需要有关世界如何运作的一般知识，或者被称为常识。比如百科全书中的知识\n\n这个赛题的难点是有些预训练语言模型没有学到的先验知识怎么学\n\n## 赛题概述\n\n- train 训练集提供了6313条数据数据格式是和中小学生做的阅读题一样，一篇文章有两到三个问题每个问题有两到四个答案选项。\n- validation 验证集提供了1000条数据。\n\n原始单条数据格式如下：\n\n```json\n{\n    \"ID\": \"0001\",\n    \"Content\": \"春之怀古张晓风春天必然曾经是这样的：从绿意内敛的山头，一把雪再也撑不住了，噗嗤的一声，将冷面笑成花面，一首澌澌然的歌便从云端唱到山麓，从山麓唱到低低的荒村。。。。。很多省略。\",\n    \"Questions\": [\n      {\n        \"Q_id\": \"000101\",\n        \"Question\": \"鸟又可以开始丈量天空了。”这句话的意思是   （   ）\",\n        \"Choices\": [\n          \"A．鸟又可以飞了。\",\n          \"B． 鸟又要远飞了。\",\n          \"C．鸟又可以筑巢了。\"\n        ],\n        \"Answer\": \"A\"\n      },\n      {\n        \"Q_id\": \"000102\",\n        \"Question\": \"本文写景非常含蓄，请读一读找一找哪些不在作者的笔下有所描述\",\n        \"Choices\": [\n          \"A．冰雪融化\",\n          \"B． 蝴蝶在花间飞舞\",\n          \"C．白云在空中飘\",\n          \"D．小鸟在空中自由地飞\"\n        ],\n        \"Answer\": \"C\"\n      }\n    ]\n}\n```\n\n## EDA 与预处理\n\n将原始数据每个问题抽出来以 [文章- 问题 -答案] 作为一条数据。\n\n```json\n{\n    \"Question\": \"下列对这首诗的理解和赏析，不正确的一项是\",\n    \"Choices\": [\n        \"A．作者写作此诗之时，皮日休正患病居家，闭门谢客，与外界不通音讯。\",\n        \"B．由于友人患病，原有的约会被暂时搁置，作者游春的诗篇也未能写出。\",\n        \"C．作者虽然身在书斋从事教学，但心中盼望能走进自然，领略美好春光。\",\n        \"D．尾联使用了关于沈约的典故，可以由此推测皮日休所患的疾病是目疾。\"\n    ],\n    \"Answer\": \"A\",\n    \"Q_id\": \"000101\",\n    \"Content\": \"奉和袭美抱疾杜门见寄次韵  陆龟蒙虽失春城醉上期，下帷裁遍未裁诗。因吟郢岸百亩蕙，欲采商崖三秀芝。栖野鹤笼宽使织，施山僧饭别教炊。但医沈约重瞳健，不怕江花不满枝。\"\n}\n```\n\n训练集从6313变为15421条数据，相当于有15421个问题\n\n验证集从1000变为2444条数据，相当于有2444个问题\n\n接下来看看文章的长度如何？\n\n![](https://i.loli.net/2021/04/06/UXPvh1c6CIRY54J.png)\n\n```\ncount    15421.000000\nmean      1039.781272\nstd        435.583878\nmin         38.000000\n25%        744.000000\n50%       1067.000000\n75%       1251.000000\nmax       3047.000000\nName: content_len, dtype: float64\ncount    2444.000000\nmean      927.508592\nstd       481.552693\nmin        40.000000\n25%       596.000000\n50%       938.000000\n75%      1179.500000\nmax      3047.000000\nName: content_len, dtype: float64\n```\n\n发现content文章都非常长，绝大多数都超过了512。\n\n使用预训练模型bert的话，如何训练很长的文章是是个提高的点。\n\n我的想法是bert模型一个这个提高的点、看看最近比较火的Longformer怎么做，再用几个和长度无关的模型像lstm等最后做集成。\n\n![](https://i.loli.net/2021/04/06/gBUFiwHtaK195rq.png)\n\n答案中选C的居多，点歌都选C。。。\n\n\n\n在提供的测试集中有一个特别的地方，赛方给出了文章的类型。\n\n00 现代文 11文言文 22 古诗词 33现代诗词\n\n![](https://i.loli.net/2021/04/06/ZoVXJGdhrf7wga5.png)\n\n测试集还给了难度，使用想法：\n\n可以训练一个模型预测文本的难度和类型，标注训练集，可能会有提升。\n\n\n\n接下来将标签从ABCD转成0123\n\n```python\ntrain_df['label'] = train_df['Answer'].apply(lambda x:['A','B','C','D'].index(x)) \n\ntest_df['label'] = 0\n```\n\n\n\n## Baseline\n\n### 分词器\n\n采用transformers提供的bert分词器\n\n```python\ntokenizer = BertTokenizer.from_pretrained('model') #加载bert的分词器\n```\n\n这里我试过如果要将bert改成roberta，分词器还是要采用BertTokenizer，如果用RobertaTokenizer会报错。\n\n参考[关于transformers库中不同模型的Tokenizer](https://zhuanlan.zhihu.com/p/121787628)\n\n**由于中文的特殊性不太适合采用byte级别的编码，所以大部分开源的中文Roberta预训练模型仍然采用的是单字词表，所以直接使用`BertTokenizer`读取即可，** 不需要使用`RobertaTokenizer`。\n\n\n\n### 模型部分\n\nBertForMultipleChoice https://huggingface.co/transformers/model_doc/bert.html#bertformultiplechoice\n\n把每个问题和文章的不同选项拆开拼成一个输入。如下图第一行\n\n![](https://i.loli.net/2021/04/08/sgbWnyIqdT9hcrE.png)\n\nbaseline采用transformers提供的调包，封装好的BertForMultipleChoice (多项选择任务)，它的源码：\n\n```python\nclass BertForMultipleChoice(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    \n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n    ):\n       \n        num_choices = input_ids.shape[1]\n\n        input_ids = input_ids.view(-1, input_ids.size(-1))\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n\t\t\t\t# 将bert三个输入展平 输入到bertmodel\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n\t\t\t\t# 隐层输出\n        # last_hidden_state: [32=4*batch, seq_len,768]\n        # pooler_ouput: [32=4*batch,768]\n        pooled_output = outputs[1] # CLS https://www.cnblogs.com/webbery/p/12167552.html\n        # bert_output = outputs[0] # last_hidden\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        outputs = (reshaped_logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)\n```\n\n做bert方面的模型扩展可以参考上面，其实就是BertModel加上了线性层。\n\n\n\n\n\n### 制造模型输入数据\n\n```python\nclass MyDataset(Dataset):\n    def __init__(self, dataframe):\n        self.df = dataframe\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx): \n      #将一条数据从(文章,问题,4个选项)转成(文章,问题,选项1)、(文章,问题,选项2)...\n        label = self.df.label.values[idx]\n        question = self.df.Question.values[idx]\n        content = self.df.Content.values[idx]\n        choice = self.df.Choices.values[idx][2:-2].split('\\', \\'')\n        if len(choice) < 4: #如果选项不满四个，就补“不知道”\n            for i in range(4-len(choice)):\n                choice.append('D．不知道')\n        \n        content = [content for i in range(len(choice))]\n        pair = [question + ' ' + i[2:] for i in choice]\n        \n        return content, pair, label\n```\n\n$61536 = 15325\\times 4 + 71 \\times3+ 25\\times2 $\n\n数据将变成61536条\n\n如果用五折交叉验证： 训练集 49228 验证集12307\n\n如果Using 8 dataloader workers every process\n\n每个batch 8条数据的话  约等于每个epoch 训练集运行772次，验证集193次\n\n(这个地方不知道算的对不对)\n\n将数据做成bert需要的三种编码：\n\n```python\ndef collate_fn(data): \n  # 将文章问题选项拼在一起后，得到分词后的数字id，输出的size是(batch, n_choices, max_len)\n    input_ids, attention_mask, token_type_ids = [], [], []\n    for x in data:\n        text = tokenizer(x[1],\n                         text_pair=x[0],\n                         padding='max_length',  # 填充到使用参数max_length指定的最大长度，或者填充到模型的最大可接受输入长度(如果未提供该参数)。\n                         truncation=True,\n                         # TRUE或‘LIMEST_FIRST’：截断到使用参数max_length指定的最大长度，或者截断到模型的最大可接受输入长度(如果没有提供该参数)。这将逐个令牌截断令牌，如果提供了一对序列(或一批对)，则从该对中最长的序列中删除一个令牌。\n                         max_length=Param['max_len'],\n                         return_tensors='pt')  # 返回pytorch tensor格式\n        input_ids.append(text['input_ids'].tolist())\n        attention_mask.append(text['attention_mask'].tolist())\n        token_type_ids.append(text['token_type_ids'].tolist())\n    input_ids = torch.tensor(input_ids)\n    attention_mask = torch.tensor(attention_mask)\n    token_type_ids = torch.tensor(token_type_ids)\n    label = torch.tensor([x[-1] for x in data])\n    return input_ids, attention_mask, token_type_ids, label\n```\n\nDataLoader\n\n```python\ntrain_set = utils.MyDataset(train)\nval_set = utils.MyDataset(val)\n\n\"\"\"单卡直接写\"\"\"\ntrain_loader = DataLoader(train_set, batch_size=CFG['train_bs'], collate_fn=collate_fn, shuffle=True, num_workers=CFG['num_workers'])\nval_loader = DataLoader(val_set, batch_size=CFG['valid_bs'], collate_fn=collate_fn, shuffle=False, num_workers=CFG['num_workers'])\n\n\"\"\"多卡写法\"\"\"\n # 给每个rank对应的进程分配训练的样本索引\ntrain_sampler = DistributedSampler(train_set)\nval_sampler = DistributedSampler(val_set)\n # 将样本索引每batch_size个元素组成一个list 验证集不用\ntrain_batch_sampler = torch.utils.data.BatchSampler(train_sampler, batch_size=args.batch_size, drop_last=True)\n\n\ntrain_loader = DataLoader(train_set, batch_sampler=train_batch_sampler, pin_memory=False,\n                                  collate_fn=collate_fn, num_workers=2)\nval_loader = DataLoader(val_set, batch_size=args.batch_size, sampler=val_sampler, pin_memory=False, collate_fn=collate_fn, num_workers=2)\n```\n\nDistributedSampler/BatchSampler:\n\n[四Sampler源码](https://blog.csdn.net/m0_37400316/article/details/107210970)\n\n[（TORCH.NN.PARALLEL.DISTRIBUTEDDATAPARALLEL）时，DISTRIBUTEDSAMPLER(DATASET)用法解释](https://www.freesion.com/article/6505681767/)\n\n\n\n\n\n\n\nDataloader 中的 num_workers:\n\n加快训练进程\n为了加快训练过程，使用DataLoader类的num workers可选属性。\nnum workers属性告诉数据加载器实例要使用多少子进程来加载数据。默认情况下，num  workers值设置为0，值为0告诉加载程序在主进程内加载数据。\n这意味着训练将在主进程中按顺序工作。在训练过程中使用了一个batch，并且需要另一个batch之后，从磁盘读取批数据。现在，如果我们有一个worker进程，我们可以利用机器多个核的。这意味着在主进程准备好进行另一批处理时，下一批处理已经可以加载并准备就绪。这就是加速的来源。批处理使用其他工作进程加载，并在内存中排队。\n\n\n\n\n\n### 训练过程\n\n\n\n\n\n#### 优化配置\n\n多层不同学习率\n\n```python\nfc_para = list(map(id, model.module.classifier.parameters()))\nlstm_para = list(map(id, model.module.lstm.parameters()))\ngru_para = list(map(id, model.module.gru.parameters()))\nbase_para = filter(lambda p: id(p) not in fc_para, model.module.parameters())\nparams = [{'params': base_para},\n{'params': model.module.lstm.parameters(), 'lr': args.other_lr},\n{'params': model.module.gru.parameters(), 'lr': args.other_lr},\n{'params': model.module.classifier.parameters(), 'lr': args.fc_lr}]\nscaler = GradScaler() # 有v100的话还可以开半精度\noptimizer = AdamW(model.module.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n# criterion = nn.CrossEntropyLoss().cuda(local_rank)\ncriterion = utils.LabelSmoothingCrossEntropy().cuda(local_rank) # 标签平滑\n```\n\n\n\n#### 梯度累积\n\n由于机器显存限制，不得不用梯度累积来达到目的batch数。\n\n用多次小的 mini-batch 来模拟一个较大的 mini-batch，即：global_batch_size = batch_size*iter_size\n\nbatch size 和 learning rate 要等比例放大。但需要注意：特别大的 batch size 还需要再加上其他 trick 如 warmup 才能保证训练顺利（因为太大的初始 lr 很容易 train 出 nan）。\n\n```python\nloss = criterion(output, y) / args.accum_iter\n\nif ((step + 1) % args.accum_iter == 0) or ((step + 1) == len(train_loader)):\n    scaler.step(optimizer)\n    scaler.update()\n    scheduler.step()\n    optimizer.zero_grad()\n```\n\n苏神:[ 用时间换取效果：Keras梯度累积优化器](https://kexue.fm/archives/6794)\n\n\n\n\n\n#### loss计算与warmup\n\nwarmup顾名思义就是热身，在刚刚开始训练时以很小的学习率进行训练，使得网络熟悉数据，随着训练的进行学习率慢慢变大，到了一定程度，以设置的初始学习率进行训练，接着过了一些inter后，学习率再慢慢变小；学习率变化：上升——平稳——下降；\n\nwarm up setp（一般等于epoch*inter_per_epoch），当step小于warm up setp时，学习率等于基础学习率×(当前step/warmup_step)，由于后者是一个小于1的数值，因此在整个warm up的过程中，学习率是一个递增的过程！当warm up结束后，学习率以基础学习率进行训练，再学习率开始递减\n\n1、当网络非常容易nan时候，采用warm up进行训练，可使得网络正常训练；\n\n2、如果训练集损失很低，准确率高，但测试集损失大，准确率低，可用warm up；具体可看：https://blog.csdn.net/u011995719/article/details/77884728\n\n[[LR Scheduler]warmup](https://blog.zhujian.life/posts/f311f0.html)\n\n\n\n#### 模型保存与加载\n\n这里有个小地方要注意，因为多卡并行时model用DistributedDataParallel包装了，所以在save时不时直接的model.state_dict()，而是model.module.state_dict()。 这个问题当时困扰了我好久，模型保存完的都是没经过学习的参数。\n\n```python\nif val_acc > best_acc:\n    best_acc = val_acc\n    print(\"best:\", best_acc)\n    if distribute_utils.is_main_process():\n        torch.save(model.module.state_dict(),\n                   'spawn_adv_pgd_{}_fold_{}.pt'.format(args.model.split('/')[-1], fold))\n```\n\n\n\n\n\n\n\n\n\n## 提升点\n\n更长的文本（512、sliding window、xinet、longformer） \n\n- 滑动窗口把文章截成很多段然后取平均softmax\n- xlnet 不限制长度，时间长\n- longformer 4096 transformers有提供\n\n更好的模型（roberta、large、DUMA）\n\n- DUMA bert上再加attention\n\n更多的数据（爬虫、C3）\n\n- 先训练C3中文的有提升 但新改了规则说不让用外部数据了。\n\n比赛复盘[海华阅读理解比赛复盘]()\n\n\n\n\n\n\n\n## 遇到的问题\n\n1. 五折交叉验证有的轮次收敛有的轮次不收敛\n\n数据shuffle过，要加warmup用cosine lr，学习率往小调从2e-5调到1e-5\n\n还有一种情况是因为label不均衡造成的，每折数据不一样\n\n2. 验证集loss和acc都上涨\n\n现象很常见，原因是过拟合或者训练验证数据分布不一致造成。就是在训练后期，预测的结果趋向于极端，使少数预测错的样本主导了loss，但同时少数样本不影响整体的验证acc情况。\n\n\n\n\n\n\n\n## 可能用到的外部数据\n\n1、RACE dataset\n2、SQuAD2.0 and CoQA dataset\n3、ARC dataset\n4、DREAM dataset\n5、ChineseSquad，https://github.com/zengjunjun/ChineseSquad\n6、cmrc2018，https://github.com/ymcui/cmrc2018\n7、c3 dataset\n8、dureader dataset\n\n1、 爬取中学语文阅读理解试题（全部选项、无标注） https://github.com/sz128/ext_data_for_haihua_ai_mrc （内含网盘下载链接）\n2、C3数据：https://github.com/nlpdata/c3\n3、 开源的中文预训练语言模型：\n\nMacBERT (https://github.com/ymcui/MacBERT)\nChinese-BERT-wwm（https://github.com/ymcui/Chinese-BERT-wwm）\nChinese-ELECTRA（https://github.com/ymcui/Chinese-ELECTRA)\nALBERT-zh (https://github.com/brightmart/albert_zh)\nguwenBERT (https://github.com/Ethan-yt/guwenbert);","tags":["DataGame"]},{"title":"bfprt算法","url":"/2021/04/06/bfprt算法/","content":"\n# bfprt算法 求TopK\n\n**中位数的中位数算法** 最坏时间复杂度 $O(n)$\n\n在做topk问题时，最容易想到的就是先对所有数据进行一次排序，然后取其前k个。但问题有二：\n\n- 快排时间复杂度 $O(nlogn)$ ，但最坏时间复杂度 $O(n^2)$\n- 我们只要前k大的，而对其余的数也进行了排序，浪费了大量排序时间。\n\n除了这种方法堆排序也是一个比较好的选择，可以维护一个大小为k的堆，时间复杂度为 $O(nlogk)$\n\n## 堆排序topk\n\n- Heap是一种数据结构具有以下的特点：\n  1）**完全二叉树**；\n  2）heap中存储的值是**偏序**；\n- **Min-heap**: 父节点的值小于或等于子节点的值；\n  **Max-heap**: 父节点的值大于或等于子节点的值；\n- 一般都用数组来表示堆，i结点的父结点下标就为(i–1)/2。它的左右子结点下标分别为2 * i + 1和2 * i + 2\n- 堆中每次都删除第0个数据。为了便于重建堆，实际的操作是将最后一个数据的值赋给根结点，然后再从根结点开始进行一次从上向下的调整。\n\n```java\n# 大根堆比较器 top小\npublic  static class MaxheapComparator implements Comparator<Integer>{\n  @Override\n  public int compare(Integer o1, Integer o2){\n    return o2-o1;\n  }\n}\npublic static PriorityQueue getMinKNumsByHeap(int[] arr,int k){\n  if(k<1 || k>arr.length){\n    return null;\n  }\n  PriorityQueue<Integer> kHeap = new PriorityQueue<Integer>(k, new MaxheapComparator());\n  for(int i=0;i!=k;i++){\n    kHeap.add(arr[i]);\n  }\n  for(int i=k;i!=arr.length;i++){\n    if(arr[i]<kHeap.peek()){ // 返回第一个元素，而不从此PriorityQueue中删除一个元素。\n      kHeap.poll(); // 返回第一个元素，并从此PriorityQueue中删除一个元素。\n      kHeap.add(arr[i]);\n    }\n  }\n  return kHeap;\n}\npublic static void main(String[] args) {\n         int[] arr = { 1, 3, 2, 5, 9 };\n         // 测试普通方法\n         System.out.println(getMinKNumsByHeap(arr, 1).peek());\n         System.out.println(getMinKNumsByHeap(arr, 2).peek());\n         System.out.println(getMinKNumsByHeap(arr, 3).peek());\n         System.out.println(getMinKNumsByHeap(arr, 4).peek());\n         System.out.println(getMinKNumsByHeap(arr, 5).peek());\n     }\n```\n\n\n\n## 原理过程\n\n在快排基础上，先通过判断主元位置与k的大小使递归的规模变小。\n\n再通过修改快速排序中主元的选取方法来降低快速排序在最坏情况下的时间复杂度。\n\n- 选取主元\n\n- 以选取的主元为分界点，把小于主元的放到左边，大于主元的放到右边\n\n- 分别对左边和右边进行递归，重复上述过程\n\n  \n\n  \n\n\n\n1. 数组被划分为了 N/5 个小部分，每个部分的5个数排序需要 O(1) ，所有部分排完需要 O(N/5)=O(N)\n\n2. 取出每个小部分的中位数，一共有 N/5 个，递归调用BFPRT算法得到这些数中第 (N/5)/2 小的数（即这些数 的中位数），记为 pivot\n\n3. 以 pivot 作为比较，将整个数组划分为 <pivot , =pivot , >pivot 三个区域\n\n4. 判断第K小的数在哪个区域，如果在 = 区域则直接返回 pivot ，如果在 < 或 > 区域，则将这个区域的数递 归调用BFPRT算法\n\n5. base case ：在某次递归调用BFPRT算法时发现这个区域只有一个数，那么这个数就是我们要找的数\n\n```java\npublic static int getMinKthNum(int[] arr, int k){\n  if(arr==null || k>arr.length){\n    return Integer.MIN_VALUE;\n  }\n  int[] copyArr = Arrays.copyOf(arr, arr.length);\n  return BFPRT(copyArr, 0, arr.length-1, k-1);\n}\n// 取出每个小部分的中位数，一共有 N/5 个，递归调用BFPRT算法得到这些数中第 (N/5)/2 小的数（即这些数 的中位数），记为 pivot. 以 pivot 作为比较，将整个数组划分为 <pivot , =pivot , >pivot 三个区域\nprivate static int BFPRT(int[] arr, int begin, int end, int i){\n  if(begin==end) return arr[begin];\n  int pivot = medianOfMedians(arr, begin, end);\n  int[] pivotRange = partition(arr, begin, end, pivot);\n  if(i>= pivotRange[0] && i<=pivotRange[1]){\n    return arr[i];\n  } else if(i<pivotRange[0]){\n    return BFPRT(arr, begin, pivotRange[0]-1, i);\n  } else{\n    return BFPRT(arr, pivotRange[1]+1, end, i);\n  }\n}\n\nprivate static int[] partition(int[] arr, int begin, int end, int pivot){\n  int L= begin-1;\n  int R= end + 1;\n  int cur = begin;\n  while(cur!=R){\n    if(arr[cur]>pivot){\n      swap(arr, cur, --R);\n    } else if(arr[cur]<pivot){\n      swap(arr, cur++, ++L)\n    } else{\n      cur++;\n    }\n  }\n  return new int[]{L+1, R-1};\n}\n\nprivate static int medianOfMedians(int[] arr, int begin, int end){\n  int num = end - begin +1;\n  int offset = num % 5 ==0 ? 0 : 1;\n  int[] medians = new int[num/5 + offset];\n  for(int i=0;i<medians.length; i++){\n    int beginI = begin+i*5;\n    int endI = beginI + 4;\n    medians[i] = getMedian(arr, beginI, Math.min(endI, end));\n  }\n  return BFPRT(medians, 0, medians.length-1, medians.length/2);\n}\n\nprivate static int get Median(int[] arr, int begin, int end){\n  insertionSort(arr, begin, end);\n  int sum = end+begin;\n  int mid = (sum/2) + (sum%2);\n  return arr[mid];\n}\nprivate static void insertionSort(int[] arr, int begin, int end){\n  if (begin>=end) return;\n  for(int i=begin+1; i<=end; i++){\n    for(int j=i; j>begin; j--){\n      if(arr[j] < arr[j-1]){\n        swap(arr, j, j-1);\n      }else{\n        break;\n      }\n    }\n  }\n}\n```\n\n\n\n## 时间复杂度\n\n最坏情况下是 $O(n)$\n\n令 $T(n)$ 为所求的时间复杂度，则：\n$$\n\\begin{equation}\\begin{split} \n T(n) \\le T(\\frac {n}{5}) + T(\\frac{7n}{10}) + c\\cdot n\n    \\end{split}\\end{equation}\n$$\n\n\n- $T(\\frac n 5)$ 来自 GetPivotIndex()，n 个元素，5 个一组，共有 $⌊\\frac n5⌋$ 个中位数；\n- $T(\\frac {7n}{10})$ 来自 BFPRT()，在 $⌊\\frac n5⌋$ 个中位数中，主元 x 大于其中 $\\frac 12⋅\\frac n5=\\frac n{10}$ 的中位数，而每个中位数在其本来的 5 个数的小组中又大于或等于其中的 3 个数，所以主元 x 至少大于所有数中的 $\\frac n{10}⋅3=\\frac {3n}{10}$ 个。即划分之后，任意一边的长度至少为 $\\frac 3{10}$，在最坏情况下，每次选择都选到了 $\\frac 7{10}$ 的那一部分。\n- $c⋅n$ 来自其它操作，比如 InsertSort()，以及 GetPivotIndex() 和 Partition() 里所需的一些额外操作。\n\n设 $T(n)=t⋅n$，其中 t 为未知，它可以是一个正常数，也可以是一个关于 n 的函数，代入上式：\n\n$$ \\begin{align} t⋅n&≤\\frac {t⋅n}5+\\frac{7t⋅n}{10}+c⋅n \\tag{两边消去 n}\\\\ t&≤\\frac t 5+\\frac {7t}{10}+c \\tag{再化简}\\\\ t&≤10c \\tag{c 为一个正常数} \\end{align} $$\n\n其中 c 为一个正常数，故t也是一个正常数，即 $T(n)≤10c⋅n$，因此 $T(n)=O(n)$，至此证明结束。\n\n接下来我们再来探讨下 BFPRT 算法为何选 5 作为分组主元，而不是 2, 3, 7, 9 呢？\n\n首先排除偶数，对于偶数我们很难取舍其中位数，而奇数很容易。再者对于 3 而言，会有 $T(n)≤T(\\frac n 3)+T(\\frac {2n}3)+c⋅n$，它本身还是操作了 n 个元素，与以 5 为主元的 $\\frac {9n}{10}$ 相比，其复杂度并没有减少。对于 7，9，... 而言，上式中的 10c，其整体都会增加，所以与 5 相比，5 更适合。\n\n\n\n\n\n\n\n\n\n","tags":["刷题"]},{"title":"RoBERTa & Albert","url":"/2021/04/05/RoBERTa-Albert/","content":"\n# RoBERTa & Albert\n\n2021年了，bert的改进体也越来越多，Roberta和Albert是比较出名的两个改进体。\n\nRoberta主要针对bert的预训练任务如NSP，mask进行改进。并且扩大了batchsize和使用更长的序列训练，这两点可能在长文本竞赛上有作用。\n\nAlbert主要针对bert参数量太大，训练慢来进行改进。引入了跨层参数共享，embedding解绑分解，取消dropout和添加SOP预训练任务。\n\n\n\n## RoBERTa\n\n1. 使用更大的batch在更大的数据集上对Bert进行深度训练\n2. 不再使用NSP(Next Sentence Prediction)任务\n3. 使用更长的序列进行训练\n4. 动态改变训练数据的MASK模式\n\n\n\n### 静态Masking vs 动态Masking\n\n- 静态Masking:在数据预处理期间Mask矩阵就已经生成好了，每个样本只会进行一次随机Mask，每个epoch都是相同的。\n- 修改版静态Masking: 在预处理时将数据拷贝10份，每一份拷贝都采用不同的Mask，也就是说，同样的一句话有十种不同的mask 方式，然后每份数据都训练N/10个epoch\n- 动态Masking:每次向模型输入一个序列时，都会生成一种新的Mask方式，即不在预处理的时候进行mask，而是在向模型提供输入时动态生成Mask。\n\n![](https://i.loli.net/2021/04/05/zFrQXsCIgc98pu3.png)\n\n### 取消NSP任务\n\nRoBERTa 实验了 4 种方法：\n\n- SEGMENT-PAIR + NSP：输入包含两部分，每个部分是来自同一文档或者不同文档的 segment （segment 是连续的多个句子），这两个 segment 的 token 总数少于 512 。预训练包含 MLM 任务和 NSP 任务。这是原始 BERT 的做法\n- SENTENCE-PAIR + NSP：输入也是包含两部分，每个部分是来自同一个文档或者不同文档的单个句子，这两个句子的 token 总数少于 512 。由于这些输入明显少于 512 个 tokens，因此增加 batch size 的大小，以使 tokens 总数保持与 SEGMENT-PAIR + NSP 相似。预训练包含 MLM 任务和 NSP 任务\n- FULL-SENTENCES：输入只有一部分（而不是两部分），来自同一个文档或者不同文档的连续多个句子，token 总数不超过 512 。输入可能跨越文档边界，如果跨文档，则在上一个文档末尾添加标志文档边界的 token 。预训练不包含 NSP 任务\n- DOC-SENTENCES：输入只有一部分（而不是两部分），输入的构造类似于 FULL-SENTENCES，只是不需要跨越文档边界，其输入来自同一个文档的连续句子，token 总数不超过 512 。在文档末尾附近采样的输入可以短于 512 个 tokens， 因此在这些情况下动态增加 batch size 大小以达到与 FULL-SENTENCES 相同的 tokens 总数。预训练不包含 NSP 任务\n\n![](https://i.loli.net/2021/04/05/TqvNDS2WIXHBnAl.png)\n\n\n\n### 扩大Batch Size\n\n公认的因素：降低batch size会显著降低实验效果，具体可参考BERT，XLNet目录的相关Issue。\n\nRoberta 作者也证实了这一点。\n\n![](https://i.loli.net/2021/04/05/jRdWv3gEVMlLk14.png)\n\n其中，bsz 是 Batch Size；steps 是训练步数（为了保证 bsz*steps 近似相同，所以大 bsz 必定对应小 steps）；lr 是学习率；ppl 是困惑度，越小越好；最后两项是不同任务的准确率。\n\n### 文本编码\n\n- 基于 char-level ：原始 BERT 的方式，它通过对输入文本进行启发式的词干化之后处理得到。\n- 基于 bytes-level：与 char-level 的区别在于bytes-level 使用 bytes 而不是 unicode 字符作为 sub-word 的基本单位，因此可以编码任何输入文本而不会引入 UNKOWN 标记。\n\n\n\n\n\n## Albert\n\n最近在 NLP 领域的研究趋势是使用越来越大的模型，以获得更好的性能。ALBERT 的研究表明，无脑堆叠模型参数可能导致效果降低\n\n在论文中，作者做了一个有趣的实验\n\n> 如果更大的模型可以带来更好的性能，为什么不将最大的 BERT 模型 (BERT-large) 的隐含层单元增加一倍，从 1024 个单元增加到 2048 个单元呢？\n\n他们称之为 \"BERT-xlarge\"。令人惊讶的是，无论是在语言建模任务还是阅读理解测试（RACE）中，这个更大的模型的表现都不如 BERT-large\n\n![](https://i.loli.net/2021/04/05/5kJfDmlt8xiUFCv.png)\n\n\n\n### 概述\n\nALBERT 利用了参数共享、矩阵分解等技术大大减少了模型参数，用 SOP（Sentence Order Prediction） Loss 取代 NSP（Next Sentence Prediction） Loss 提升了下游任务的表现。但是 ALBERT 的层数并未减少，因此**推理时间（Inference Time）还是没有得到改进**。不过参数减少的确使得训练变快，同时 ALBERT 可以扩展到比 BERT 更大的模型（ALBERT-xxlarge），因此能得到更好的表现\n\n具体的创新部分有三个：\n\n1. embedding 层参数因式分解\n2. 跨层参数共享\n3. 将 NSP 任务改为 SOP 任务\n\n#### Factorized Embedding Parameterization\n\n原始的 BERT 模型以及各种依据 Transformer 的预训连语言模型都有一个共同特点，即 E=H，其中 E 指的是 Embedding Dimension，H 指的是 Hidden Dimension。这就会导致一个问题，当提升 Hidden Dimension 时，Embedding Dimension 也需要提升，最终会导致参数量呈平方级的增加。\n\n所以 ALBERT 的作者将 **E 和 H 进行解绑**，具体的操作就是**在 Embedding 后面加入一个矩阵进行维度变换**。E 的维度是不变的，如果 H 增大了，我们只需要在 E 后面进行一个升维操作即可\n\n![](https://i.loli.net/2021/04/05/8FjUN5XrKWAqvsP.png)\n\n所以，ALBERT 不直接将原本的 one-hot 向量映射到 hidden space size of H，而是分解成两个矩阵，原本参数数量为 V∗H，V 表示的是 Vocab Size。分解成两步则减少为 V∗E+E∗H，当 H 的值很大时，这样的做法能够大幅降低参数数量\n\n> V∗H=30000∗768=23,040,000\n>\n> V∗E+E∗H=30000∗256+256∗768=7,876,608\n>\n> 举个例子，当 V 为 30000，H 为 768，E 为 256 时，参数量从 2300 万降低到 780 万\n\n通过因式分解 Embedding 的实验可以看出，对于参数不共享的版本，随着 E 的增大，效果是不断提升的。但是参数共享的版本似乎不是这样，E 最大并不是效果最好。同时也能发现参数共享对于效果可能带来 1-2 个点的下降\n\n![](https://i.loli.net/2021/04/05/5W9ytZiukCfdLmQ.png)\n\n```python\ndef __init__(self):\n  self.emb = nn.Embedding(vocab_size, 128)\n  self.fc = nn.Linear(128, 1024)\ndef forward(self, x):\n  x = self.emb(x)\n  x = self.fc(x) # [batch_size, seq_len, 1024]\n```\n\n\n\n#### Cross-Layer Parameter Sharing\n\n传统 Transformer 的每一层参数都是独立的，包括各层的 self-attention、全连接。这样就导致层数增加时，参数量也会明显上升。之前有工作试过单独将 self-attention 或者全连接层进行共享，都取得了一些效果。ALBERT 作者尝试将所有层的参数进行共享，相当于只学习第一层的参数，并在剩下的所有层中重用该层的参数，而不是每个层都学习不同的参数\n\n![](https://i.loli.net/2021/04/05/zOjWTLiGyaXMvnq.png)\n\n使用参数共享提升了模型 的稳定性，曲线更平滑了。\n\nBERT-base 和 ALBERT 使用相同的层数以及 768 个隐藏单元，结果 BERT-base 共有 1.1 亿个参数，而 ALBERT 只有 3100 万个参数。通过实验发现，feed-forward 层的参数共享会对精度产生比较大的影响；共享注意力参数的影响是最小的\n\n![](https://i.loli.net/2021/04/05/V3Tf6EhcAiuXder.png)\n\n```python\n# 参数共享例子\ndef __init__(self):\n    self.enc_layer = TransformerEncoder()\ndef forward(self, x):\n    for _ in range(12):\n        x = self.enc_layer(x)\n# 参数不共享例子        \ndef __init__(self):\n    self.enc_layer1 = TransformerEncoder()\n    self.enc_layer2 = TransformerEncoder()\n    ....\n    self.enc_layer12 = TransformerEncoder()\ndef forward(self, x):\n    x = self.enc_layer1(x)\n    x = self.enc_layer2(x)\n    ....\n    x = self.enc_layer12(x)\n# 分组参数共享\ndef __init__(self):\n    self.enc_layer1 = TransformerEncoder()\n    self.enc_layer2 = TransformerEncoder()\ndef forward(self, x):\n    for i in range(11):\n      enc_inputs = self.encoder_layer(enc_inputs)\n    x = self.enc_layer2(x)\n```\n\n#### Sentence-Order Prediciton (SOP)\n\n**BERT** 引入了一个叫做**下一个句子预测**的二分类问题。这是专门为提高使用句子对，如 \"自然语言推理\" 的下游任务的性能而创建的。但是像 RoBERTa 和 XLNet 这样的论文已经阐明了 NSP 的无效性，并且发现它对下游任务的影响是不可靠的\n\n因此，ALBERT 提出了另一个任务 —— **句子顺序预测**。关键思想是：\n\n- 从同一个文档中取两个连续的句子作为一个正样本\n- 交换这两个句子的顺序，并使用它作为一个负样本\n\n![](https://i.loli.net/2021/04/05/XHDghABQW6Y2Fdf.png)\n\n![](https://i.loli.net/2021/04/05/K7vIQg5C2GthUda.png)\n\n#### Adding Data & Remove Dropout\n\n以上 ALBERT 都是使用跟 BERT 相同的训练数据。但是增加训练数据或许可以提升模型的表现，于是 ALBERT 加上 STORIES Dataset 后总共训练了 157G 的数据。另外，训练到 1M 步的时候，模型还没有对训练集 Overfit，所以作者直接把 Dropout 移除，最终在 MLM 验证集上的效果得到了大幅提升\n\n![](https://i.loli.net/2021/04/05/8u3sZJXQ4EFcn2C.png)\n\n\n\n#### Conclusion\n\n刚开始看这篇文章是很惊喜的，因为它直接把同等量级的 BERT 缩小了 10 + 倍，让普通用户有了运行可能。但是仔细看了实验后才发现参数量的减小是需要付出代价的\n\n![](https://i.loli.net/2021/04/05/U37dpafWzxT4lqD.png)\n\n需要注意的是，Speedup 是训练时间而不是 Inference 时间。Inference 时间并未得到改善，因为即使是使用了共享参数机制，还是得跑完 12 层 Encoder，故 Inference 时间跟 BERT 是差不多的\n\n实验用的参数如下\n\n![](https://i.loli.net/2021/04/05/svcV1HRtMd7jxX8.png)\n\n可以得出的结论是：\n\n1. 在相同的训练时间下，ALBERT 得到的效果确实比 BERT 好\n2. 在相同的 Inference 时间下，ALBERT base 和 large 的效果都没有 BERT 好，而且差了 2-3 个点，作者在最后也提到了会继续寻找提高速度的方法（Sparse attention 和 Block attention）\n\n另外，结合 **Universal Transformer** 可以想到的是，在训练和 Inference 阶段可以动态地调整 Transformer 层数（告别 12、24、48 的配置）。同时可以想办法去避免纯参数共享带来的效果下降，毕竟 Transformer 中越深层学到的任务相关信息越多，可以改进 Transformer 模块，加入记忆单元、每层个性化的 Embedding\n\n\n\n\n\n\n\n","tags":["nlp"]},{"title":"Heterogeneous Graph Neural Network","url":"/2021/03/30/Heterogeneous-Graph-Neural-Network/","content":"\n# Heterogeneous Graph Neural Network\n\n\n\n## 摘要\n\n挑战：不仅是因为需要合并由多种类型的节点和边组成的异质结构(图)信息，而且还因为需要考虑与每个节点相关联的异质属性或内容(例如，文字或图像)。\n\n方法：C1引入了一种带重启的随机游走策略，对每个节点的固定大小的强相关异构邻居进行采样，并根据节点类型对它们进行分组。\n\n接下来，设计一个包含两个模块的神经网络体系结构，用来聚合那些采样的相邻节点的特征信息。\n\n第一个模块C2：对异构内容的“深度”特征交互进行编码，并为每个节点生成内容Embedding。\n\n第二个模块C3：聚合不同相邻组(类型)的内容(属性)嵌入，并通过考虑不同组的影响来进一步组合它们，以获得最终的节点嵌入。\n\nHetGNN用途：在边链接预测、推荐、节点分类和聚类以及归纳节点分类和聚类等各种图挖掘任务\n\n\n\n## 异质图\n\n![](https://i.loli.net/2021/03/30/BAJN9XrwndLE5zW.png)\n\n学术图中，\n\n关系：作者与论文(写作)、论文与论文(引文)、论文与期刊(出版)\n\n此外，该图中的节点携带属性 如作者有id属性、文本有论文摘要属性。\n\n## 挑战\n\n![](https://i.loli.net/2021/03/30/a74yHiAdxeXI1jn.png)\n\n- 1：现有的GNN大多只聚合直接（一阶）相邻节点的特征信息，特征传播过程可能会削弱远邻节点的影响。此外，“中心”节点的嵌入生成受到弱相关邻居（“噪声”邻居）的影响，“冷启动”节点的嵌入由于邻居信息有限而没有得到充分的表示。因此，挑战1是：如何为HetG中的每个节点采样与嵌入生成密切相关的异构邻居。如上图C1。(信息聚合考虑的信息不够多)\n\n  方法：基于重启策略的随机游走，采样固定大小强相关异构邻居，并根据节点类型进行分组。\n\n- 2：HETG中的一个节点可以携带非结构化的异构内容。如上图C2，type1有属性＋文本，type2有属性+图片。因此挑战2是：如何设计节点内容编码器来解决HetG中不同节点的内容异构性。(异构属性信息如何嵌入)\n\n  方法：聚合模块1，用RNN对异构内容的“深层”特征交互进行编码，得到每个节点的内容嵌入。\n\n- 3：不同类型的邻居对HetG中节点嵌入的贡献不同。目前的GNN主要集中在齐次图上，没有考虑节点类型的影响。因此，挑战3是：如何通过考虑不同节点类型的影响来聚合异构邻居的特征信息，如图上图C3。(不同类型节点如何聚合)\n\n  方法：聚合模块2，利用另一个RNN聚合不同邻域组的内容嵌入，并进一步通过注意力机制进行组合，以度量异构节点类型的不同影响，并获得最终的节点嵌入。\n\n\n\n和其他模型对比\n\n![](https://i.loli.net/2021/03/30/a5wPN1Uc8hnB9Tu.png)\n\n## C-HetG\n\n内容关联异质图\n\n定义为有多种类型的节点V和边E的图。$G=(V，E，O_V，R_E)$。\n\n$O_V$ 和 $R_E$ 分别表示对象类型的集合和关系类型的集合。\n\n此外，每个节点与不同种类的内容相关联(例如，属性、文本或图像)。\n\n\n\n## 基于重启策略的随机游走 C1\n\n和GAT/GCN采样一阶邻居不一样。他们不能聚合多种类型节点的信息，并且采样信息不完整。\n\nHetGNN用这个方法采样。\n\nStep1：采样固定大小长度的随机游走路径$RWR(v)$ 。以概率p决定是走到当前节点还是返回上一节点。 v是起始节点，$RWR(v)$ 中不同类型节点的数量受到限制，以确保可以采样所有节点类型。\n\nStep2:：对不同类型的邻居进行分组。对于每个节点类型t，根据频率从 $RWR(v)$ 中选择前kt个节点，并将它们作为节点v的t型相关邻居的集合。\n\n保证了手机每个节点都有所有类型邻居的信息，并且对相同类型的邻居进行分组，以便后续任务。\n\n\n\n## 异质内容编码 C2\n\n![](https://i.loli.net/2021/03/30/O34hFWBto5xnsYT.png)\n\n对于第二个挑战，设计此模块去提取图中的异质内容$C_v$ ,编码为固定大小的embedding。\n\n定义$C_v$ 中的第i个特征表达为 $x_i \\in R^{d_f\\times 1}$\n\n$x_i$ 可以根据不同类型的内容采用不同的技术进行预训练。例如，可以利用Par2Vec来预先训练文本内容，或者使用CNNs来预先训练图像内容。 下面的 $FC_{\\theta_x}$ 就代表不同的特征转换器，参数为$\\theta_x$。\n\n采用双向LSTM学习深度特征：\n$$\n\\begin{equation}\\begin{split} \n    f_1(v) =\\frac {\\sum_{i\\in C_v}[\\overrightarrow{LSTM}\\{FC_{\\theta_x}(x_i) \\} \\oplus \\overleftarrow{LSTM} \\{ FC_{\\theta_x}(x_i)  \\}]}{ |C_v|}\n    \\end{split}\\end{equation}\n$$\n\n\n$\\oplus$ 链接操作。LSTM公式：\n$$\n\\begin{equation}\\begin{split} \n z_i &= \\sigma (U_zFC_{\\theta_x}(x_i) + W_zh_{i-1} + b_z) \\\\\n  f_i &= \\sigma (U_fFC_{\\theta_x}(x_i) + W_fh_{i-1} + b_z) \\\\\n   o_i &= \\sigma (U_oFC_{\\theta_x}(x_i) + W_oh_{i-1} + b_z) \\\\\n   \\tilde c_i &= tanh(U_cFC_{\\theta_x}(x_i)+ W_ch_{i-1} + b_c)\\\\\n   c_i &= f_i \\circ c_{i-1} + z_i \\circ \\tilde c_i\\\\\n   h_i &= tanh(c_i) \\circ o_i\n \\end{split}\\end{equation}\n$$\n\n## 聚合异构邻居 C3\n\n包含两个步骤：\n\n- 同类邻居聚合\n- 类型邻居聚合\n\n### 同类型聚合\n\n![](https://i.loli.net/2021/03/30/uh5zVmLsHNKiMyf.png)\n\n使用随机游走对每个节点的不同节点类型的固定大小邻居进行采样后。\n\n将$v∈V$ 的t类型抽样邻居集表示为$N_t(v)$\n\n经过上面的内容嵌入后变为$ v' \\in N_t(v)$ \n\n然后使用神经网络$f_2^t$ 来聚合得到的内容嵌入。\n\n聚合嵌入公式为：\n$$\n\\begin{equation}\\begin{split} \n f_2^t(v) = AGG_{v'\\in N_t(v)}^t \\{ f_1(v')\\}\n    \\end{split}\\end{equation}\n$$\n还是使用的Bi-LSTM：\n$$\n\\begin{equation}\\begin{split} \n f_2^t(v) =\\frac {\\sum_{v'\\in N_{t(v)}}[\\overrightarrow{LSTM}\\{f_1(v') \\} \\oplus \\overleftarrow{LSTM} \\{ f_1(v')  \\}]}{ |N_{t(v)}|}\n    \\end{split}\\end{equation}\n$$\n使用Bi-LSTM来聚集所有t型邻居的内容嵌入，并使用所有隐藏状态的平均值来表示一般的聚集嵌入。我们使用不同的Bi-LSTM来区分邻居聚合的不同节点类型。Bi-LSTM对无序邻域集进行操作，该集合的灵感来自于GraphSAGE\n\n### 类型邻居聚合\n\n![](https://i.loli.net/2021/03/30/4yp3YDo8l7JhSAc.png)\n\n上一步为节点v生成$|O_v|$ 聚集嵌入 (图中的节点类型集)。\n\n为了将这些基于类型的邻居嵌入与v的内容嵌入相结合，采用了注意机制。\n\n其动机是不同类型的邻居将对v的最终表示做出不同的贡献。因此，输出嵌入被表示为：\n$$\n\\begin{equation}\\begin{split} \n \\epsilon_v = a^{v,v} f_1(v) + \\sum_{t\\in O_v} \\alpha^{v,t} f_2^t(v)\n    \\end{split}\\end{equation}\n$$\n$\\epsilon_v \\in \\Re^{d\\times 1}$  , d 是嵌入维度\n\n$f_1(v)$ 是获得v的内容嵌入\n\n$f_2^t(v)$ 是类型聚合嵌入\n\n$\\alpha^{v,*}$  表示不同嵌入的重要性\n\n定义：$F(v) =\\{ f_1(v) \\bigcup (f_2^t(v) ,t\\in O_v)\\}$\n$$\n\\begin{equation}\\begin{split} \n \\alpha^{v,i} = \\frac{ exp \\{LeakyReLU(u^T[f_i\\oplus f_1(v)])\\}}{\\sum_{f_j\\in F(v)} exp\\{ LeakyReLU(u^T[f_j\\oplus f_1(v)]) \\}}\n    \\end{split}\\end{equation}\n$$\n\n\n## \n\n\n\n\n\n\n\n\n\n## HetGNN\n\n![](https://i.loli.net/2021/03/30/FKu5qcMT1AahHlk.png)\n\n四个组成部分\n\n- 采样异质邻居\n- 编码节点的异质内容\n- 聚合异质邻居\n- 制定目标函数，设计训练过程。\n\n\n\n\n\n\n\n","tags":["GNN"]},{"title":"Cognitive Graph for Multi-Hop Reading Comprehension at Scale(ACL2019)","url":"/2021/03/27/Cognitive-Graph-for-Multi-Hop-Reading-Comprehension-at-Scale-ACL2019/","content":"\n# Cognitive Graph for Multi-Hop Reading Comprehension at Scale(ACL2019)\n\nppt : https://coding-zuo.github.io/CogQA_RevealJS/ \n\n## 认知图谱\n\n知识图谱+认知推理+逻辑表达。\n认知图谱依据人类认知的双加工理论，动态构建带有上下文信息的知识图谱并进行推理。\n认知图谱可以被解释为“基于原始文本数据，针对特定问题情境，使用强大的机器学习模型动态构建的，节点带有上下文语义信息的知识图谱”。\n\n**认知图谱和知识图谱的区别？**\n认知图谱是包含知识图谱的相关技术的。知识图谱的任务主要是包括知识图谱的表示、构建和存储。这些是构建知识库的过程。认知推理的底层是知识推理，而知识图谱目的是完善知识。面向知识图谱的认知推理可以基于已有的知识推理出新的知识，或者发现错误矛盾的知识。认知图谱是为了解决复杂理解问题或少样本知识图谱推理问题如歧义问题、链接困难、关系的冗余与组合爆炸等。认知推理其实更具有人脑特性，相对更动态一些，可以基于知识感知来调整推理，也可以基于推理来调整知识和感知。交叉了认知科学对人类知识的总结，有助于划分和处理知识图谱的相关问题。\n\n认知图谱主要有三方面创新，分别对应人类认知智能的三个方面：\n\n1.（长期记忆）直接存储带索引的文本数据，使用信息检索算法代替知识图谱的显式边来访问相关知识。\n\n2.（系统1推理）图谱依据查询动态、多步构建，实体节点通过相关实体识别模型产生。\n\n3.（系统2推理）图中节点产生的同时拥有上下文信息的隐表示，可通过图神经网络等模型进行可解释的关系推理。\n\n本质上，认知图谱的改进思路是减少图谱构建时的信息损失(两元一谓)，将信息处理压力转移给检索和自然语言理解算法，同时保留图结构进行可解释关系推理。\n\n![](https://i.loli.net/2021/03/27/2mdSVkFas3NlWhQ.png)\n\n\n\n\n\n## 摘要\n\n- 提出新的多跳QA框架CogQA\n- 基于双过程理论System1:隐式提取，System2:显式推理\n- 可以给出答案的解释路径\n- 基于Bert和GNN处理HotpotQA数据集\n- 评估指标F1 score\n\n## Introduction\n\n现在的单段阅读理解机器已经超过人了，像SQuAD。但要跨过机器阅读理解和人阅读理解的鸿沟还很难。\n\n主要有三个主要挑战：\n\n- 理解能力：如对抗性测试所揭示的那样，单段问答模型倾向于在与问题匹配的意义中寻找答案，这不涉及复杂的推理。因此多跳阅读是要克服的。\n- 可解释性：显式推理路径能够验证逻辑严格性，对质量保证系统的可靠性至关重要。数据集中给出的是无序的句子级别的解释，但我们人可以通过逻辑一步一步给出有序的、实体级别的解释。\n- 大规模的(时间成本)：任何QA系统都要处理大规模的知识。现在已有的DrQA是通过预检索来减少规模到几个段落的范围。这个框架是单段阅读和多段阅读的结合，但和人脑中大量记忆和知识而言是一种折中的做法。时间成本不会随着段落增加而增加。\n\n### 两个系统的工作\n\n**隐式提取System1**：模仿大脑通过隐式注意提取相关信息，是直觉和无意识的系统。\n从段落中提取与问题相关的实体和答案日期，并对其语义信息进行编码。\n\n![](https://i.loli.net/2021/03/27/jY967V5RuPvpgm8.jpg)\n\n如上图，系统一从段落和语义信息中提取问题相关的实体和答案候选。\n\n**显式推理System2**：在System1基础上进行有意识的的可控的推理。\n\n系统一根据提出的问题提供给系统二资源，系统二根据信息进行深度推理，挖掘相关信息。两个系统协作，迭代的给出快慢思考。\n\n在信息一提出出的信息图上，搜集线索。并且指导系统一更好的提取下一跳实体。\n\n迭代直到所有可能的答案都被找到，再由系统二推理出最终答案。\n\n![](https://i.loli.net/2021/03/27/lO9sy8hDARqk3uI.jpg)\n\n\n\n`系统1(system 1)负责经验性的直觉判断，这一黑盒过程提取重要信息，并动态构建认知图谱；系统2(system 2)则在图上进行关系推理，由于认知图谱保留了实体节点上语义信息的隐表示，所以在符号逻辑之外，比如图神经网络等深度学习模型也可以大显身手。`\n\n![](https://i.loli.net/2021/03/27/SPXlQ4nOkLcpARH.jpg)\n\n这块有一个缺点，GCN在有节点新加入的时候要重新训练图模型？这个要等我研究研究源码\n\n前沿节点(frontier node)有两种:\n\n- 新添加的节点\n- 图中新添加边的节点(需重新访问)\n\n\n系统1在线索和问题Q的指导下读取para[x]，提取跨度并生成语义向量sem[x，Q，clues]。同时，系统2更新隐藏表示X，并为任何后继节点y准备线索clues[y，G]。基于X进行最终预测。\n\n算法流程\n1.提取在问题Q中提到的实体作为认知图的初始化，并且标记为前沿节点。\n2.重复下面过程直到21(在每一步中，我们访问一个前沿节点x)\n3.从前沿节点中跳出一个节点x\n4.从前沿节点收集线索clues[x,G],例如线索可以是提及x的句子。(线索句可以是提到的x的句子)\n5.在词库W中找到包含x的段落para[x]\n6.system1生成语义向量sem[x,Q,clues],初始化X[x]\n7.如果x是一个hop节点\n8.system1在para[x]中找到hop和answer的局部\n9.遍历hop内容，每个内容为y\n10.如果内容y不在G内并且y在词库W内\n11.用y创建一个新的hop节点\n12.如果y属于G，并且x和y的边不在图内\n13.在图中添加x和y的连线\n14.让节点y作为一个前沿节点\n15.循环结束\n16.答案部分，每个答案是y\n17.添加一个新的答案节点y和edge(x,y)到G中\n18.循环结束\n19.x过程结束\n20.用System2更新隐含X的表示\n21.直到G中没有前沿节点或G足够大；\n22.返回答案节点中概率最大的节点作为最终答案。\n\n关于可解释性，认知图谱有显式的路径，除了简单的路径，认知图还可以清楚地显示联合或循环推理过程。\n在这些过程中，可能会带来关于答案的新线索。\n\n尺度可伸缩性，框架理论上是可伸缩的，因为引用所有段落的唯一操作是通过标题索引访问哪些段落。\n\n对于多跳问题，传统的检索-抽取框架可能会牺牲后续模型的潜力，因为距离问题多跳的段落可能共享的常用词很少，与问题的语义关系也很小，导致检索失败。然而，这些段落可以通过在我们的框架中使用线索迭代展开来发现。\n\n## 实施方案\n\n![](https://i.loli.net/2021/03/27/A7W8mvJ1z2sioSc.jpg)\n\n### System1\n系统1的作用，在线索clues和问题Q的指导下提取spans并生成语义向量sem[x,Q,clus]。\nclues是前置节点段落的句子，从文中直接提取原始句子，这样做方便BERT训练。\n\n![](https://i.loli.net/2021/03/27/A7W8mvJ1z2sioSc.jpg)\n\nBert的输入句子分为A和B两部分：\n$$\n    \\begin{equation}\\begin{split} \n    A:[CLE]Question[SEP]clues[x,G][SEP]B:Para[x]\n    \\end{split}\\end{equation}\n$$\n\n\\[CLE\\]:放在每个句子的第一位，classification用于下游分类任务。\n为什么用CLS？因为self-attention，[CLS]的output含有整句话的完整信息。在每个词的时候，对自己这个词的评分会很大。用无意义的CLS可以公平的反应整个句子的特征。\n\\[SEP\\]:separator分隔连接token序列的符号。\n\n![](https://i.loli.net/2021/03/27/7ROBPrVjkQfH91s.jpg)\n\n$$\n    \\begin{equation}\\begin{split} \n    P^{start}_{ans}[i]=\\frac{e^{S_{ans} \\cdot T_i}}{\\sum_je^{S_{ans}\\cdot T_j}}\n    \\end{split}\\end{equation}\n$$\n\n$S_{hop}$、$E_{hop}$、$S_{ans}$、$E_{ans}$ :可学习的参数，用来预测目标span。\n$T\\in R^{L\\times H}$:是bert的输出向量,L是输入序列长度，H是隐层维度。\n$P^{start}_{ans}[i]$:是第i个输入token到ans范围内开始位置的概率。\n\n\n\n\n\n我们只聚焦topK开始的概率${start_k}$,对于每个k的结束位置$end_k$:\n$$\n    \\begin{equation}\\begin{split} \n    end_k = argmax_{start_k\\leq j\\leq start_k + maxL }P^{end}_{ans}[j]\n    \\end{split}\\end{equation}\n$$\nmaxL：是最大概率的span长度。\n$P^{end}_{ans}[j]$:是第j个输入token到ans范围内结束位置的概率。\n\n文章说是这么区分下一跳和答案的：\n答案和下一跳协议具有不同的属性。答案提取在很大程度上依赖于问题所指示的字符。例如，“纽约市”比“2019”更有可能是WHERE问题的答案，而下一跳实体通常是其描述与问题中的语句相匹配的实体。\n\n为了识别不相关的段落，利用在§3.4.1中引入负抽样进行训练，系统1生成负阈值。在顶部\nk个跨度，起始概率小于负阈值将被丢弃。因为对第0个token[CLS]进行预训练以合成\n用于下一句预测的所有输入标记任务Pstart[0]充当ANS这是我们实施过程中的一个阈值。\n\n### System2\n系统2，更新隐藏表示X，并为任何后继节点y准备clues[y，G]。基于X输出最终预测结果。\n\n第一个功能是为前沿节点准备clues[x，G]，我们将其实现为收集提到x的x个前置节点的原始语句。\n第二个功能是更新隐含表示X，这是系统2的核心功能。隐含表示$X∈R^{n×H}$代表G中所有n个实体的理解。要完全理解实体x与问题q之间的关系，仅仅分析语义sem[x，Q，clues]是不够的。由于图结构的归纳偏差，GNN已被提出用于对图进行深入学习，特别是关系推理。\n\n$$\n    \\begin{equation}\\begin{split} \n    \\Delta = \\sigma((AD^{-1})^T\\sigma(XW_1))\n    \\end{split}\\end{equation}\n$$\n$$\n    \\begin{equation}\\begin{split} \n    X’=\\sigma(XW_2+\\Delta)\n    \\end{split}\\end{equation}\n$$\n$W_1,W_2 \\in R^{H\\times H}$:权重矩阵\n$\\sigma(XW_1)$ 左乘$(AD^{-1})^T$(列归一化A):可以解释为局部化光谱过滤.\n在访问边界节点x的迭代步骤中，其隐藏表示X[x]按照公式(4)(5)更新。\n在实验中发现“异步更新”在性能上没有明显的差别，在G最终确定后，将所有节点的X一起分多步更新，效率更高，在实践中被采用。\n\n\n\n### 训练细节\n\n模型采用负采样，在训练集的段落中预先提取下一跳hop和answer span。\n对于每个para[x]和问题Q有下面这种字典数据。\n$$\n    \\begin{equation}\\begin{split} \n    D[x,Q] = \\{(y_1,start_1,end_1),...,(y_n,start_n,end_n)\\}\n    \\end{split}\\end{equation}\n$$\n$start_i$和$end_i$ 是在para[x] 中根据一个实体或者答案$y_i$模糊匹配出来的。\n\n### 任务1：Span Extraction\n\n基于$D[x,Q]$，得到$P^{start}_{ans}$,$P^{end}_{ans}$,$P^{start}_{hop}$,$P^{end}_{hop}$\n在每个段落中最多出现一个$answer(y,start,end)$\n因此，定义一个one-hot向量$g_{ans}^{start}$ ，其中$g_{ans}^{start}[start]=1$。然而，一个段落中可能出现多个不同的ans next-hop spans，因此$g_{hop}^{start_i}[start]=1/k$ ，其中k是下一跳跨度的数量。\n\n为了能够区分不相关的段落，在G中预先增加了不相关的negative-hop节点。\n\n### 任务2：预测答案节点\n\n\n\n\n\n## 实验\n\n### 数据集\n\n使用HotpotQA的全维基设置来构建实验。基于维基百科文档中的第一段图，众包收集了112,779个问题，其中84%的问题需要多跳推理。数据被分成训练集(90,564个问题)、发展集(7,405个问题)和测试集(7,405个问题)。开发和测试集中的所有问题都是困难的多跳案例。\n\n \n\n## 总结&展望\n系统2的推理如何实现？现在的方法（如图神经网络）虽然使用关系边作为归纳偏置，却仍然无法执行可控、可解释、鲁棒的符号计算。系统1如何为现有的神经-符号计算方法提供可行前续工作？\n\n文本库应该如何预处理或预训练，才能有助于访问相关知识的检索？\n\n另辟蹊径？本文介绍的认知图谱是基于认知科学的双通道理论，是否还存在其他支撑理论？或者直接构建一个符号推理和深度学习相结合的新型学习架构？\n\n如何与人类记忆机理相结合？人类记忆机理包括长期记忆和短期记忆，但其工作模式和工作机理并不清楚。长期记忆可能存储的是一个记忆模型，记忆模型不再是一个概念的网络，而是一个计算模型的网络。\n\n认知图谱如何与外界反馈相结合是一个全新的问题。当然这里可以考虑通过反馈强化学习来实现，但具体方法和实现模式还需要深入探讨。\n\n## 参考文献\n[浅谈多跳阅读理解](https://zhuanlan.zhihu.com/p/133483274)\n[BERT的[CLS]有什么用](https://www.pianshen.com/article/5232700066/)\n[从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)\n[从知识图谱到认知图谱：历史、发展与展望](http://toutiao.secjia.com/knowledge-map-to-cognitive-map-0820/)\n[图神经网络及其在知识图谱中的应用](https://zhuanlan.zhihu.com/p/208697908?utm_source=wechat_timeline)\n[还在用[CLS]？从BERT得到最强句子Embedding的打开方式！](http://www.360doc.com/content/20/1227/10/7673502_953710193.shtml)\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs","url":"/2021/03/27/Multi-hop-Reading-Comprehension-across-Multiple-Documents-by-Reasoning-over-Heterogeneous-Graphs/","content":"\n# Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs\n\n\n\n京东出品19年采用异质图网络的一篇多跳阅读理解，这篇是在WikiHopQA数据集上，在没有使用预训练语言模型的情况下第一次超过人类表现。20年还有一篇在HotpotQA上的，过两天准备也读一读。前一阵子用京东在线客服和智能语音客服的时候，感觉效果不错，果然发现这两篇文章。\n\n先说一下WikiHopQA\n\n训练集43,738个样本，开发集5129个。候选答案个数2-79个之间，平均19.8个。文档数3-63个，平均13.7篇。每篇文档的字数4-2046字，平均100.4个字。该数据集的难度在于，要从多篇文章中找相关文档，还要依据多篇文章做推理。推理跳数的增加，与问题的相关性越来越小，统计距离较远，非参数化的检索方法如传统的TF-IDF，无法搜索到最终答案所在的上下文。如下图：\n\n![](https://i.loli.net/2021/03/27/IrqT6k5tdo9Az37.png)\n\n每个样本包含字段：dict_keys(['answer', 'id', 'candidates', 'query', 'supports'])\n\n目前主要做法有三种思路：构造推理链，一般用RNN网络进行链式推理；从网络模型角度改进；构造graph，利用GCN，GAT等方法在图上推理。\n\n和HotpotQA一样都是属于多跳阅读理解数据集。针对一个问题给出多个支撑文档，来做推理。可看之前的文章[HotpotQA数据集](https://coding-zuo.github.io/2021/03/23/HotpotQA%E6%95%B0%E6%8D%AE%E9%9B%86/)\n\n不同的是，WikiHopQA给出了候选答案，可以看成是多项选择题。\n\n\n\n\n\n## 创新点\n\n提出Heterogeneous Document-Entity  graph (HDE graph) ，可以从候选答案、文档和实体中，聚合不同级别的信息粒度。\n\n使得不同类型的节点之间能够进行丰富的信息交互，从而便于精确的推理。不同类型的节点与不同类型的边相连，以突出查询、文档和候选之间呈现的各种结构信息。\n\n\n\n## 如何设计\n\n![](https://i.loli.net/2021/03/28/disz9JgGEIf5u1C.png)\n\n数据集提供的是候选答案、问题和一系列文档。分布作为输入，分别表示为：$C_q, q(s,r,?), S_q $\n\n其中，$q(s,r,?)$  内分别表示为主体、关系和位置客体。任务是预测查询的正确答案$a^*$。\n\n首先将这些输入文字，映射成词向量，分别为 $X_q\\in R^{l_q\\times d}$ 、$X_s^i\\in R^{l_s^i\\times d}$ 其中i为第i个词、  $, X_c^j\\in R^{l_c^j\\times d}$  其中j为第j个词。\n\n$l$ 为每段文字长度 ，d为词向量维度。\n\n这就是上图中的最下面一层。\n\n--------\n\n再往上一层encoder采用的是双向GRU，分别对C/q/S 的上下文信息进行编码。\n\n编码后的维度是  $ H_q\\in R^{l_q\\times h}$  、 $H_s^i\\in R^{l_s^i\\times h}$ 、 $H_c^j\\in R^{l_c^j\\times h}$   其中h是RNN输出维度。\n\n再观察上图，发现在encoder上面一层，Documents上面和其他两个地方不同，中间加了一个实体提取。\n\n是作者发现有些通过查询query和候选答案C，在文档中提取被提及的实体。提取到每个mentions的开始位置和结束位置。\n\n每一次提及都被视为一个实体Entiry。第i个文档提出的实体表示为 $H_s^i, M\\in R^{l_m\\times h}$   , $l_m$ 是实体长度。\n\n------\n\n再向上一层，是co-attention协同注意力机制，特意读了下协同注意力DCN那篇文章[协同注意力和自注意力的区别(DCN+)](https://coding-zuo.github.io/2021/03/25/%E5%8D%8F%E5%90%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%92%8C%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E5%8C%BA%E5%88%AB/)\n\n用到这个任务上其实就是一个协同每两个特征来提取新特征。\n\n通俗上讲，Q和S 相互co-attention就是带着问题看文章，读完文章看问题。C和Q做带着选项看问题。\n\n拿RNN输出的问题序列$H_q \\in R^{l_q\\times h}$ 和文档$H_s^i\\in R^{l_s^i\\times h}$为例计算过程如下：\n$$\n\\begin{equation}\\begin{split} \n    A^i_{qs} = H_s^i(H_q)^T  \\in R^{l_s^i\\times l_q}\n    \\end{split}\\end{equation}\n$$\n得到查询和每个字的点积打分。\n\n为简化起见，在后面的上下文中，我们使用上标i，它表示对第i个文档的操作。\n$$\n\\begin{equation}\\begin{split} \n    C_q &= softmax(A_{qs}^T)H_s \\in R^{l_q\\times h}\\\\\n    C_s &= softmax(A_{qs})H_q \\in R^{l_s\\times h}\n    \\end{split}\\end{equation}\n$$\n进一步使用GRU ,f 对共同参与的文档上下文进行编码\n$$\n\\begin{equation}\\begin{split} \n    D_s = f(softmax(A_{qs})C_q)\\in R^{l_s\\times h}\n    \\end{split}\\end{equation}\n$$\n最终拼接\n$$\n\\begin{equation}\\begin{split} \n   S_{ca} =[C_s;D_s] \\in R^{l_s\\times 2h}\n    \\end{split}\\end{equation}\n$$\n同样的co-attention应用在query和候选，query和实体得到 $C_{CA},E_{ca}$ 。\n\n注意，作者不在查询和对应于查询主题的实体之间共同关注，因为查询主题已经是查询的一部分。为了保持维度的一致性，应用了一个具有tanh激活函数的单层多感知器(MLP)，将查询主题实体的维数增加到2h。\n\n这个查询主题query subject到底是什么我有点疑问。\n\n-----\n\n再往上是一个自注意力池化。\n\n当co-attention产生文档的查询感知上下文表示时，自关注集合被设计为通过选择重要的查询感知信息将顺序上下文表示转换为固定维度的非顺序特征向量。\n$$\n\\begin{equation}\\begin{split} \n    a_s&=softmax(MLP(S_ca))\\in R^{l_s\\times 1}\\\\\n    s_{sa}&= a_s^TS_{ca} \\in R^{1\\times 2h}\n    \\end{split}\\end{equation}\n$$\n\n\n同样，对每个实体和候选，可得到  $c_{sa}, e_{sa}$  。\n\n------\n\n再接下来就是构建异质图了。\n\n![](https://i.loli.net/2021/03/28/AtPhugd79czJ3k2.png)\n\n\n\n一共三种类型的节点，绿色是文档节点、蓝色是实体节点、黄色是候选答案节点。\n\n七种建边规则：\n\n- 文档节点和候选节点：如果候选节点在文档中出现时建立一条边。\n- 文档节点和实体节点：如果实体节点从这个文档中提取出来建立一条边。\n- 候选节点和实体节点：如果实体节点被候选答案提及建立一条边。\n- 两个实体节点之间：如果他们在同一个文档中被提取时建立一条边。\n- 两个实体节点之间: 如果他们被相同的候选或问题提及，且实体在不同的文档中时建立一条边。\n- 所有的候选节点相互连接\n- 不满足以上条件的不进行相连。\n\n## HGN传播聚合机制\n\n信息传递\n$$\n\\begin{equation}\\begin{split} \n    z_i^k = \\sum_{r\\in R} \\frac{1}{|N_i^r|} \\sum_{j\\in N_i^r} f_r(h_j^k)\n    \\end{split}\\end{equation}\n$$\n\n\n其中R 是所有边的类型\n\n$N_i^r$ 是 第i个节点边类型是r的邻居集合\n\n$h_j^k$ 是第j个邻居节点的第k层表达\n\n$h_j^0$ 是self-attention的输出\n\n$f_r$ 是MLP\n\n$z_i^k$ 是第i个节点第k层的局和信息\n\n可以与变换更新后的节点i：\n$$\n\\begin{equation}\\begin{split} \n    u_i^k = f_s(h_i^k) + z_i^k\n    \\end{split}\\end{equation}\n$$\n$f_s$ :MLP\n\n为了解决多层GNN的过平滑问题，作者采用了加门控的方式。\n$$\n\\begin{equation}\\begin{split} \n    g_i^k &= sigmoid(f_g([u_i^k;h_i^k]))\\\\\n    h_i^{h+1} &= tanh(u_i^k)\\odot g_i^k + h_i^k \\odot (1-g_i^k)\n    \\end{split}\\end{equation}\n$$\n$\\odot$ :element-wise product = element-wise multiplication = Hadamard product\n\n含义：两个矩阵对应位置元素进行乘积\n\n以上的$f$ 都是单层MLP 输出维度为2h\n\n## 最终预测\n\n使用候选节点和与候选提及相对应的实体节点的最终节点表示来计算分类分数：\n$$\n\\begin{equation}\\begin{split} \n    a = f_C(H^C)+ACC_{max}(f_E(H^E))\n    \\end{split}\\end{equation}\n$$\n$H^C \\in R^{C\\times 2h}$ , C是候选数量\n\n$H^M \\in R^{M\\times 2h}$ , M是节点数量\n\n$f_C,f_E$ 两层MLP\n\n$ACC_{max}$ :是对属于同一候选者的实体的分数取最大值的操作。\n\n隐含层大小为输入维的一半，输出维数为1，我们将可预测节点和实体节点的得分直接相加，作为多个候选者的最终得分。因此，输出分数向量$a∈r^{c×1}$给出了所有候选的分布。由于任务是多类分类，采用交叉熵损失作为训练目标，以a和标签作为输入。\n\n\n\n## 实验\n\n![](https://i.loli.net/2021/03/28/ZMIraibAGWv9VL8.png)\n\n\n\n![](https://i.loli.net/2021/03/28/3JDMT6lNtiWIBQf.png)\n\n\n\n\n\n\n\n\n\n","tags":["GNN&nlp"]},{"title":"Pytorch多GPU并行实例","url":"/2021/03/27/Pytorch多GPU并行实例/","content":"\n# Pytorch Train_Multi_GPU\n\nhttps://pytorch.org/tutorials/intermediate/ddp_tutorial.html\n\n两种方式：\n\n- DataParallel（DP）：Parameter Server模式，一张卡位reducer，实现也超级简单，一行代码。\n- DistributedDataParallel（DDP）：All-Reduce模式，本意是用来分布式训练，但是也可用于单机多卡。\n\n最后还有一个pycharm远程服务器的配置，还有如何在pycharm里配置run参数为：\n\n```\npython -m torch.distributed.launch --nproc_per_node=4 --use_env train_multi_gpu_using_launch.py\n```\n\n这个行运行命令是用DistributedDataParallel时的，指定的运行参数。具体介绍在后面写。\n\n\n\n### DataParallel vs DistributedDataParallel\n\n- 如果模型太大而无法容纳在单个GPU上，则必须使用 **model parallel** 将其拆分到多个GPU中。 DistributedDataParallel与模型并行工作； DataParallel目前不提供。\n\n- DataParallel是单进程、多线程的，只能在单机上工作，而DistributedDataParallel是多进程的，既可用于单机，也可用于多机。即使在一台机器上，DataParallel通常也比DistributedDataParallel慢，这是因为线程间的GIL争用、每次迭代复制模型以及分散输入和收集输出带来的额外开销。\n\n- DistributedDataParallel适用于模型并行；DataParallel目前不能。当DDP与模型并行相结合时，每个DDP进程使用模型并行，所有进程共同使用数据并行。\n\n\n\n## 单机多卡理论基础\n\n- 按照并行方式来分：模型并行 vs 数据并行\n- 按照更新方式来分：同步更新 vs 异步更新\n- 按照算法来分：Parameter Server算法 vs AllReduce算法\n\n### 常见的多GPU使用 \n\n![jz0tf9.png](https://i.loli.net/2021/03/27/stRfpYBavhDPwnu.png)\n\n模型并行，将网络不同模块放到不同GPU上去运行。训练速度无提升，但可让非常大的模型分布在多块gpu。\n\n![](https://i.loli.net/2021/03/27/syT1vPiIwEMUYXF.png)\n\n数据并行，将数据和模型同时放到多个GPU，同时进行正向传播和反向传播，并行输入样本进行训练， 相当于加大了batchsize，训练速度也加快了。\n\n\n\n### 数据如何在不同设备间进行分配\n\n\n\n\n\n### 误差梯度如何在不同设备间通信\n\n\n\n\n\n\n\n\n### 多GPU训练常用启动方式\n\n- [torch.distributed.lauch](https://pytorch.org/docs/stable/distributed.html?highlight=distributed#module-torch.distributed.launch) : 代码量少，启动速度快。如果开始训练后，手动强制终止程序，有小概率会出现进程没有杀掉的情况。\n- torch.multiprocessing: 拥有更好的控制和灵活性\n\n## DataParallel\n\nhttps://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html\n\n\n\n\n\n## DistributedDataParallel（DDP）\n\nDistributedDataParallel（DDP）在module级别实现数据并行性。它使用[torch.distributed](https://link.zhihu.com/?target=https%3A//pytorch.org/tutorials/intermediate/dist_tuto.html)包communication collectives来同步梯度，参数和缓冲区。并行性在单个进程内部和跨进程均有用。在一个进程中，DDP将input module 复制到device_ids指定的设备，相应地按batch维度分别扔进模型，并将输出收集到output_device，这与[DataParallel](https://link.zhihu.com/?target=https%3A//pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)相似。\n\n### 处理速度不同步时\n\n在DDP中，Model, forward method 和 differentiation of the outputs是分布式的同步点。期望不同的过程以相同的顺序到达同步点，并在大致相同的时间进入每个同步点。否则，快速流程可能会提早到达，并在等待时超时。因此，用户负责进程之间的工作负载分配。有时，由于例如网络延迟，资源争用，不可预测的工作量峰值，不可避免地会出现不同步的处理速度。为了避免在这些情况下超时，请确保在调用[init_process_group](https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/distributed.html%23torch.distributed.init_process_group)时传递足够大`timeout`value\n\n\n\n\n\n\n\n\n\n## 常见报错\n\nsubprocess.CalledProcessError: Command ‘[’/home/labpos/anaconda3/envs/idr/bin/python’, ‘-u’, ‘main_distribute.py’, ‘–local_rank=1’]’ returned non-zero exit status 1.\n\n这个错出现是前面有代码写的不对，可以先在DistributedDataParallel 中加入find_unused_parameters=True。试试，一般不是分布式部分的错，是前面哪里写的不对。很可能是data_loader哪里仔细检查一下。\n\n\n\n验证集loss和acc都上涨\n\n验证集loss上升，acc也上升这种现象很常见，原因是过拟合或者训练验证数据分布不一致导致，就是在训练后期，预测的结果趋向于极端，使少数预测错的样本主导了loss，但同时少数样本不影响整体的验证acc情况。\n\n问一下，这时如果设置早停，是不是以loss最小早停合理点？以前见过用准确率设置早停的。假设交叉熵损失，训练与验证集分布大概一致的条件下\n\n答：准确率比较好\n\n## 遇到问题\n\nloss下降但最终效果不好，得到的模型结果像是只在四分之一数据做训练后的效果。\n\n\n\n\n\n## 参考文献\n\n[PyTorch分布式训练简介](https://blog.csdn.net/baidu_19518247/article/details/89635181)\n\n[Pytorch多机多卡分布式训练](https://zhuanlan.zhihu.com/p/68717029)\n\n[【分布式训练】单机多卡的正确打开方式（三）：PyTorch](https://zhuanlan.zhihu.com/p/74792767)\n\n[PyTorch 24.单机多卡的理论基础](https://zhuanlan.zhihu.com/p/158886284)\n\n[【分布式训练】单机多卡的正确打开方式（三）：PyTorch](https://zhuanlan.zhihu.com/p/74792767)\n\n[PyTorch 20.GPU训练](https://zhuanlan.zhihu.com/p/158375254)\n\n[pytorch 分布式训练 distributed parallel 笔记](https://blog.csdn.net/m0_38008956/article/details/86559432)\n\n[Pytorch 分布式训练](https://zhuanlan.zhihu.com/p/76638962)\n\n[Pycharm：运行、调试pytorch分布式训练代码](https://blog.csdn.net/lxb206/article/details/114293060)\n\n[如何在pycharm中运行/调试torch分布式训练](https://zhuanlan.zhihu.com/p/144815822)\n\n[pytorch 分布式训练 distributed parallel 笔记](https://blog.csdn.net/m0_38008956/article/details/86559432?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&dist_request_id=&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control)\n\n","tags":["DataGame"]},{"title":"协同注意力和自注意力的区别(DCN+)","url":"/2021/03/25/协同注意力和自注意力的区别/","content":"\n# 协同注意力和自注意力的区别(DCN+)\n\n读阅读理解QA的论文发现co-attention没见过，self-attention和attention又忘得差不多了。\n\n就先读了一下DCN和DCN+的论文\n\nDYNAMIC COATTENTION NETWORKS FOR QUESTION ANSWERING\n\nDCN+: MIXED OBJECTIVE AND DEEP RESIDUAL COATTENTION FOR QUESTION ANSWERING+\n\n注意力机制有很多种变形，这里我只考虑最近接触可能会用的。\n\n- soft&hard attention\n- key-value pair attention\n- self-attention\n- Multi-head attention\n- co-attention\n\n## attention\n\n注意力机制就是计算机模仿人的注意力，对信息分配一个权重，对关注的信息分配较大的权重，不重要的信息反之。\n\n例如，我们的视觉系统倾向于关注图像中辅助判断的部分信息，并忽略掉不相关的信息。同样，在涉及语言或视觉的问题中，输入的某些部分可能会比其他部分对决策更有帮助。例如，在翻译和总结任务中，输入序列中只有某些单词可能与预测下一个单词相关。同样，在image-caption问题中，输入图像中只有某些区域可能与生成caption的下一个单词更相关.\n\n### soft和hard的区别\n\nSoft attention是一种全局的attention，其中权重被softly地放在源图像所有区域\n\nHard attention一次关注图像的一个区域，采用0-1编码，时间花费较少，但是不可微分，所以需要更复杂的技术来进行训练\n\n在机器学习中soft 常常表示可微分，比如sigmoid和softmax机制，而hard常常表示不可微分\n\nsoft hard attention机制是在图像生成标题任务中被提出的，其原始任务如下:\n\n![](https://i.loli.net/2021/03/25/4bCNYj5f6kQcIXt.png)\n\n上面是soft 下面是hard，我们可以看到，soft的权重是每次被放置在整张图像上，注重强调的部分（越白）的数值越接近1，越黑越接近0\n\n下面的一排非黑即白，白色区域为1，黑色区域为0。\n\n\n\n现在主流用soft比较多，其主要步骤有两个：\n\n针对输入$X=[x_1,x_2...x_3]$ (提取对象)\n\n- 1计算输入信息熵的注意力分布\n- 2根据注意力分布计算输入信息的加权平均\n\n### 计算注意力分布\n\n给定一个和任务相关的查询向量q，用注意力变量$z \\in [1,N]$ 表示被选择信息的索引位置，即 z=i，表示选择了第i个输入信息。\n\n其中查询向量q可以是动态生成的，也可以是可学习的参数。\n\n#### Soft-attention计算分布\n\n在给定输入信息x的查询变量q下，选择第i个输入信息的概率。\n$$\n\\begin{equation}\\begin{split} \n\\alpha_i &= p(z=i|x,q) \\\\\n&= softmax(s(x_i,q))\n\\end{split}\\end{equation}\n$$\n其中$\\alpha_i$ 为注意力分布，$s(x_i,q)$ 为打分函数。\n\n常用的打分函数:\n\n- 加性模型: $s(x_i,q) = v^Ttanh(Wx_i+Vq)$\n- 双线性：$s(x_i,q)=x_i^TWq$\n- 点积：$s(x_i,q)=x_i^Tq$\n- 缩放点积:$s(x_i,q)= \\frac{x_i^Tq}{d^{\\frac{1}{2}}}$\n\n#### 加权平均\n\n$$\n\\begin{equation}\\begin{split} \natt(X,q) &= \\sum_{i=1}^N \\alpha_i x_i \\\\\n&=E_{z\\sim p(z|X,q)}[X]\n\\end{split}\\end{equation}\n$$\n\n\n\n\n\n![](https://i.loli.net/2021/03/25/XrtYUMNH8J1KvLP.png)\n\n\n\n\n\n#### key-value pair attention\n\n其实就是输入信息是(k,v)键值对形式。$(K,V)=[(k_1,v_1),(k_2,v_2),...,(k_n,v_n)]$\n\n其中键用来计算注意力分布$\\alpha_i$，值用来计算聚合信息\n\n当K=V时，键值对注意力=柔性注意力\n\n![](https://i.loli.net/2021/03/25/hWkdyl3TGVjsRY4.png)\n\n如上图，计算注意力分布\n$$\n\\begin{equation}\\begin{split} \n\\alpha_i &= \\frac{exp(s(k_i,q))}{\\sum_{j=1}^Nexp(s(k_j,q))} \\\\\natt((K,V),q) &= \\sum_{i=1}^N \\alpha_iv_i \\\\\n&=\\frac{exp(s(k_i,q))}{\\sum_{j=1}^Nexp(s(k_j,q))} v_i\n\\end{split}\\end{equation}\n$$\n\n\n#### self-attention \n\n查询向量q、键向量k、值向量v ， 都等于输入向量序列。\n\n可参考下面的多头自注意力\n\n#### multi-head self attention\n\n查询向量Q、键向量K、值向量V ， 都等于输入向量序列的线性表示。\n\n假设输入序列 $X=[x_1,x_2,...,x_n]\\in R^{d_1\\times R}$，输出的是$H=[h_1,h_2,...,h_n]\\in R^{d_2\\times R}$\n\n$$Q = W_qX \\in d_3\\times N$$\n\n$$K = W_kX \\in d_3\\times N$$\n\n$$V = W_vX \\in d_2\\times N$$\n$$\n\\begin{equation}\\begin{split} \n\\hat h_i &= att((K,V),q_i) = \\sum_{j=1}^N \\alpha_{ij} v_j \\\\\n&= \\sum_{j=1}^N softmax(s(k_j,q_i))v_j\n\\end{split}\\end{equation}\n$$\n其中$i,j\\in[1,N]$ 为输出和输入的向量序列位置\n\nTransformer里用的是上面的缩放点积打分函数s\n\n\n\n![](https://i.loli.net/2021/03/25/OGKUbvgx9jY4PiS.png)\n\n如果在encoder-decoder架构中\n\nattention一般用在encoder和decoder之间做衔接的部分\n\nself-attention 一般在块内部\n\n比如在翻译任务中，Sourse和Target内部通常用self-attention提取特征，两者之间用attention\n\n\n\n\n\n\n\n## DCN\n\nCo-attention 共同注意力机制就从DCN讲起，DCN是一个QA模型，为了解决然而，问答场景中单次通过的性质，对于不正确答案的局部最大值恢复的问题。它首先融合了问题和文档的共同依赖表示，以便集中于两 者的相关部分。然后，动态指向解码器迭代潜在的答案跨度，该迭代过程使模型能够从对应于不正确答案的初始局部最大值中恢复。这个是论文的话，其实不是coattention来恢复的，是动态指向解码器。\n\n其实就是因为一段内容里可能多多个正确答案，但是我们在模型输出的时候选的是最大概率的开始和结尾，DCN用一种迭代的方式，以找到局部极值概率点来当做答案。\n\n![](https://i.loli.net/2021/03/25/jx5iEAS3KZlwpb9.png)\n\n所以，co-attention就是带着问题去阅读，融合问题和文档的特征调整的attention机制。\n\n\n\nDynamic Decoder \n\n![](https://i.loli.net/2021/03/25/Kkg8E5pAJLVbZyu.png)\n\nHMN\n\n![](https://i.loli.net/2021/03/25/thIAcEHed1ZWpwv.png)\n\n![](https://i.loli.net/2021/03/25/SsvJj6t3iaP4DRm.png)\n\n\n\nmax运算计算张量第一维上的最大值。第一个maxout层和最后一个maxout层的输出之间存在高速连接。\n\n### DCN+\n\n![](https://i.loli.net/2021/03/25/V7nA6PkYDpLh5Ka.png)\n\n","tags":["nlp"]},{"title":"字符串无序匹配","url":"/2021/03/25/字符串无序匹配/","content":"\n给定长度为 m 的字符串 aim，以及一个长度为 n 的字符串 str 问能否在 str 中找到一个长度为 m 的连续子串，使得这个子串刚好由 aim 的 m 个字符组成，顺序无所谓返回任意满足条件的一个子串的起始位置，未找到返回-1。\n\n思路：\n\n找到一个串和aim排序对比\n\n```java\npublic static void main(String[] args) {\n        Scanner input = new Scanner(System.in);\n        while (input.hasNext()) {\n            String aim = input.next();\n            String str = input.next();\n            int m = aim.length();\n            int i = 0;\n            int j = m - 1;\n            char[] charAim = aim.toCharArray();\n            Arrays.sort(charAim);\n            int flag = 0;\n            while (j != m) {\n                char[] sub = str.substring(i, j + 1).toCharArray();\n                Arrays.sort(sub);\n                flag = 0;\n                for (int k = 0; k < sub.length; k++) {\n                    if (sub[k] != charAim[k]) {\n                        flag = 1;\n                        break;\n                    }\n                }\n                if (flag == 1) {\n                    i++;\n                    j++;\n                    continue;\n                } else {\n                    System.out.println(\"true\");\n                    break;\n                }\n            }\n            if (flag == 1) System.out.println(\"false\");\n        }\n    }\n```\n\n面试上写这个暴力算法没有分 O(n^3*logN)\n\n优化O(N*M):\n\n```java\npublic boolean isTY(char[] str, int L, char[] aim) {\n        // 0-255 统计数组\n        // 'a' 97\n        // count[97] ++;\n        int[] count = new int[256];\n        for (int i = 0; i < aim.length; i++) {\n            count[aim[i]]++;\n        }\n        //count 中含有0-255\n        for (int i = 0; i < str.length; i++) {\n            if (count[str[i]] == 0) {\n                return false;\n            }\n            count[str[i]]--;\n        }\n        return true;\n    }\n\n    public int containExactly2(String a, String b) {\n        if (a == null || b == null || a.length() < b.length()) {\n            return -1;\n        }\n        char[] str = a.toCharArray();\n        char[] aim = b.toCharArray();\n        for (int L = 0; L <= str.length - aim.length; L++) {\n            if (isTY(str, L, aim)) {\n                return L;\n            }\n        }\n        return -1;\n    }\n\n```\n\n一共有N-M个要判断匹配的串，要遍历M个长度\n\n\n\n最后优化:O(n)\n\n先把目标字符串做成一个字典表。\n\n出现了的字符做成一个计数字典\n\n没有出现的就认为是0\n\n遍历输入字符串的时候\n\n让滑动窗口内的计数减减，多的可以记成负数\n\n每次从窗口右边进来的我都减\n\n从窗口左边画出的我都加一\n\n最终遍历完看无效的负数点有没有来判断有没有同源异构\n\n```java\npublic int containExactly(String a, String b) {\n        if (a == null || b == null || a.length() < b.length()) {\n            return -1;\n        }\n        char[] str = a.toCharArray();\n        char[] aim = b.toCharArray();\n        int[] count = new int[256];\n        for (int i = 0; i < aim.length; i++) {\n            count[aim[i]]++;\n        }\n        int M = aim.length;\n        int inValidTimes = 0;\n        int R = 0;\n        // 先让窗口用于M个字符\n        for (; R < M; R++) {\n            if (count[str[R]]-- <= 0) {\n                inValidTimes++;\n            }\n        }\n        // 如果第一个是同源异构词\n        if (inValidTimes == 0) {\n            return R - M;\n        }\n        // 窗口滑动\n        for (; R < str.length; R++) {\n            if (inValidTimes == 0) {\n                return R - M;\n            }\n            // 0[0..M-1]M\n            // [1..M]\n            // [2...M+1]\n            if (count[str[R]]-- <= 0) {\n                inValidTimes++;\n            }\n            if (count[str[R - M]]++ < 0) {\n                inValidTimes--;\n            }\n        }\n        return inValidTimes == 0 ? R - M : -1;\n    }\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["刷题"]},{"title":"已知后序序列构建二叉搜索树","url":"/2021/03/24/已知后序序列构建二叉搜索树/","content":"\n# 已知后序序列构建二叉搜索树\n\n思路：\n\n[2,4,3,6,8,7,5]\n\n最后一个是根节点，从后往前找，第一个比根小的数是左子树的根，根前面的数是右子树的根。\n\n就变成了子问题，已知他们的根，怎么构建左子树和右子树\n\n时间复杂度$O(N^2)$\n\n优化用二分法，找比根小的节点，一直找到无法二分。就可以找到有序的部分。遍历行为替换成二分。\n\n```java\npublic class PosArrayToBST {\n\n    public static class Node {\n        public int value;\n        public Node left;\n        public Node right;\n\n        public Node(int v) {\n            value = v;\n        }\n    }\n\n    public static Node posArrayToBST1(int[] posArr) {\n        return process1(posArr, 0, posArr.length - 1);\n    }\n\n    public static Node process1(int[] posArr, int L, int R) {\n        if (L > R) return null;\n        Node head = new Node(posArr[R]);\n        if (L == R) return head;\n        int M = L - 1; // 为了全部是全左或全右子树\n        for (int i = L; i < R; i++) {\n            if (posArr[i] < posArr[R]) {\n                M = i;\n            }\n        }\n        head.left = process1(posArr, L, M);\n        head.right = process1(posArr, M + 1, R - 1);\n        return head;\n    }\n\n    public static Node process2(int[] posArr, int L, int R) {\n        Node head = new Node(posArr[R]);\n        if (L == R) return head;\n        int M = -1;\n        for (int i = L; i < R; i++) {\n            if (posArr[i] < posArr[R]) {\n                M = i;\n            }\n        }\n\n        if (M == -1) {\n            head.right = process1(posArr, L, R - 1);\n        } else if (M == R - 1) {\n            head.left = process2(posArr, L, R - 1)\n        } else {\n            head.left = process1(posArr, L, M);\n            head.right = process1(posArr, M + 1, R - 1);\n        }\n\n        return head;\n    }\n\n    public static Node process3(int[] posArr, int L, int R) {\n        if (L > R) return null;\n        Node head = new Node(posArr[R]);\n        if (L == R) return head;\n        int M = L - 1;\n        int left = L;\n        int right = R - 1;\n        while (left <= right) {\n            int mid = left + ((right - left) >> 1);\n            if (posArr[mid] < posArr[R]) {\n                M = mid;\n                left = mid + 1;\n            } else {\n                right = mid - 1;\n            }\n        }\n        head.left = process2(posArr, L, M);\n        head.right = process2(posArr, M + 1, R - 1);\n        return head;\n    }\n\n}\n```\n\n","tags":["刷题"]},{"title":"HotpotQA数据集","url":"/2021/03/23/HotpotQA数据集/","content":"\n# HotpotQA数据集：A Dataset for Diverse, Explainable Multi-hop Question Answering\n\n\n\n官方地址：https://hotpotqa.github.io/index.html\n\n为解决当前QA数据集不能训练系统回答复杂问题和提供可解释的答案问题而提出。\n\n## 摘要\n\nHotpotQA基于113k(十一万三千)个维基百科问答对，有四个特点：\n\n- 问题需要对多个支持文档进行查找和推理才能回答\n- 问题是多样的，不受任何预先存在的知识库或知识模式的限制\n- 提供句子级别的推理所需的事实支持，允许QA系统在强有力的监督下进行推理和解释预测\n- 提供了一种新型的拟事实比较问题来测试QA系统提取相关事实和进行必要比较的能力。\n\n[![6HJiz6.png](https://z3.ax1x.com/2021/03/23/6HJiz6.png)](https://imgtu.com/i/6HJiz6)\n\n所构建的多跳问答数据集样式\n\n## 贡献\n\n\n\n### 构建维基百科超链接图\n\n使用整个英文维基百科做为语料库。\n\n- 维基百科文章中的超链接通常自然地涉及上下文中的两个(已经消除歧义的)实体之间的关系，这可能被用来促进多跳推理。\n- 每篇文章的第一段通常包含许多可以有意义地查询的信息。\n\n基于这些观察，我们从所有维基百科文章的第一段中提取了所有的超链接。用这些超链接，构建了一个有向图G，edge(a,b) 表示超链接从文章的第一段a 到 文章b。\n\n\n\n\n\n### 生成候选段落对\n\n为了生成用于与超连接图G的多跳问题回答的有意义的段落对，引入了一个桥梁实体“bridge entity”。\n\n比如问题是：when was the singer and songwriter of Radiohead born？\n\n为了回答这个问题，我们需要推理出 “singer and songwriter of Radiohead”是“Thom Yorke”\n\n在从文章中计算出他的生日。“Thom Yorke”就是桥梁实体。\n\n对于G中的edge(a,b)。桥梁实体通常是链接a和文档b的桥梁\n\n也就是说打开维基百科，第一段全是超链接的段就是a，超链接上面的名字通常是桥梁实体，链接过去的文章就是b。\n\n文章b通常确定a和b之间共享上下文的主题，但并不是所有文章b都适合收集多跳问题。例如，像国家这样的实体在维基百科中经常被提及，但与所有传入链接不一定有太多共同之处。\n\n\n\n### 比较问题\n\n收集了新一种类型的多跳问题----比较问题。\n\n其主要思想是，比较来自同一类别的两个实体通常会产生有趣的多跳问题\n\n例如：Who has played for more NBA teams, Michael Jordan or Kobe Bryant?\n\n还在比较问题中引入了是/否问题子集。\n\n回答这些问题通常需要算术比较，例如比较给定的出生日期的年龄。\n\n\n\n### 收集支持事实\n\n为了增强问答系统的可解释性，我们希望它们在生成答案时输出一组得出答案所需的支持事实。\n\n这些佐证事实可以作为关注哪些判决的有力监督。此外，现在可以通过将预测的支持事实与基础事实进行比较来检验模型的解释能力。\n\n\n\n\n\n## 处理和基准设置\n\n[![6HdD61.png](https://z3.ax1x.com/2021/03/23/6HdD61.png)](https://imgtu.com/i/6HdD61)\n\ntrain_easy: 单跳问题 18089 个。\n\ntrain_medium: 56,814个，占多跳示例的60% ,是baseline可以回答正确的问题。\n\n把难的问题分为四个子集：\n\ntrain_hard: 15661个\n\ndev : 7405个\n\ntest-distractor ：7405个\n\ntest-fullwiki : 7405个\n\nTest-distractor 和 Test-fullwiki 是两个基线，官网上的两个表单。\n\n### Distractor\n\n用tfidf检索8个段落作为干扰项，混合两个gold段落(用来收集问题和答案) shuffle构成干扰设置。()\n\n### Fullwiki\n\n要求模型回答所有维基百科文章的第一段(没有指定黄金段落)来充分测试模型定位相关事实以及对它们进行推理的能力。\n\n(真正的野外推理)\n\n两个test不能同时用，答案会泄露。\n\n\n\n## 数据集分析\n\n分析数据集中涵盖的问题类型、答案类型和多跳推理类型。\n\n### 问题类型\n\n![](https://i.loli.net/2021/03/24/LvZhegMF8RwVDUA.png)\n\n### 答案类型\n\n![](https://i.loli.net/2021/03/24/fliA8Zm65y9NdEB.png)\n\n### 推理类型\n\n![](https://i.loli.net/2021/03/24/yTm2pasAt6hJL75.png)\n\n","tags":["nlp"]},{"title":"Spatial-Temporal Graph Convolutional Network for Video-based Person Re-identification(CVPR2020)","url":"/2021/03/23/Spatial-Temporal-Graph-Convolutional-Network-for-Video-based-Person-Re-identification-CVPR2020/","content":"\nSpatial-Temporal Graph Convolutional Network for Video-based Person Re-identification(CVPR2020)\n===============================================================================================\n我做的ppt：https://coding-zuo.github.io/re-id-ppt/index.html\n\n行人重识别\n----------\n\n行人重识别（Person\nRe-identification），简称为ReID，是利用计算机视觉技术判断图像或者视频序列中是否存在特定行人的技术。广泛被认为是一个图像检索的子问题。给定一个监控行人图像，检索跨设备下的该行人图像。\n\n方法分为以下几类： \n- 基于表征学习的ReID方法 \n- 基于度量学习的ReID方法 \n- 基于局部特征的ReID方法 \n- 基于视频序列的ReID方法 \n- 基于GAN造图的ReID方法\n\n本文基于视频序列的ReID方法\n--------------------------\n\n通常单帧图像的信息是有限的，因此有很多工作集中在利用视频序列来进行行人重识别方法的研究（本篇论文就是）。基于视频序列的方法最主要的不同点就是这类方法不仅考虑了图像的内容信息，还考虑了帧与帧之间的运动信息等。\n\n### CNN&RNN\n\n利用CNN来提取图像的空间特征，而基于视频序列的方法主要思想是利用CNN\n来提取空间特征的同时利用RNN来提取时序特征。\n![](https://pic4.zhimg.com/v2-d01de4281c4079b3fd25e58a0351c27f_r.jpg \"Title\")\n\nRNN在Re-ID任务中对时间信息的建模效果有限，或者由于其复杂的结构而难以训练。\n\n### AMOC\n\n累计运动背景网络(Accumulative motion context network,\nAMOC)。AMOC输入的包括原始的图像序列和提取的光流序列。通常提取光流信息需要用到传统的光流提取算法，但是这些算法计算耗时，并且无法与深度学习网络兼容。\n![](https://pic3.zhimg.com/v2-9ed4efb7e67f29891e9ca0bda0729a6a_r.jpg \"Title\")\n\n光流法是非常耗时的，并且光流对于遮挡和噪声来说不够健壮。\n\n### temporal pooling & spatial-temporal attenttion\n\n论文：这些方法没有充分考虑人体各部分在不同帧之间的时间关系，效果有限。\n\n### 瓶颈问题\n\n方向的存在挑战： \n- 不同下摄像头造成行人外观的巨大变化； \n- 目标遮挡（Occlusion）导致部分特征丢失； \n- 不同的 View Illumination导致同一目标的特征差异； \n- 不同目标衣服颜色近似、特征近似导致区分度下降；\n\n论文解决挑战：\n1.仅使用外观特征不足以区分，但它们的身体结构信息是不同的。利用序列中各部分之间的时空关系可以缓解这些问题。\n\n2.当边界框不完美，或者存在噪声或遮挡时，基于外观的要素可能效果较差，并且基于图像的re-ID在这种情况下可能不能很好地工作。\n\n论文方法解决：显式地利用不同帧之间补丁的时间关系，以缓解遮挡和不对准问题。\n具体地说，通过连接不同帧的所有块来构建图来建模时间关系，目的是提供不同块之间的互补信息，从而缓解拥塞和错位问题。另一方面，我们还考虑了帧内的结构信息，通过为视频中的每一帧构造补丁图来提供互补的外观信息。\n\n3.基于图像的识别最具挑战性的难点之一是如何区分视觉上相似的身份，而大多数基于图像的方法只能依赖于提取细粒度的外观特征。\n\n在基于视频的人Re-ID中，相同身份的结构信息(例如形状信息)将更加完整和精确，因为每个视频具有许多帧，这些帧可能覆盖更多的视图和姿势。Structural\nGCN Module\n\n### Spatial-Temporal Graph Convolutional Network\n\n《Person re-identification with deep similarity-guided graph neural\nnetwork》2018 《Learning context graph for person search》2019 《Videos\nas space-time region graphs》2018 --- 视频分类\n\n论文：它们忽略了帧内或帧间不同身体部位之间的关系，是基于图像的，不考虑时间关系。\n\n《Adaptive graph representation learning for video person\nre-identification》2019---------引入图神经网络，利用姿态对齐连接和特征相似性连接实现相关区域特征之间的上下文交互。此外，该方法连接了所有帧的不同部分的特征，不对每帧身体部分的空间关系进行建模，忽略了帧内的结构信息。\n\n创新点、贡献\n------------\n\n（1）利用GCN来模拟人体不同部位在一帧内和帧间的潜在关系，为人们提供更具鉴别力和鲁棒性的信息\n（2）提出了时空GCN框架来联合建模视频层的整体斑块关系和帧级的单个帧的结构信息，该框架可以学习斑块之间的区分和鲁棒的时空关系，从而促进基于视频的Re-ID。\n\n模型设计\n--------\n\n设计了3个分支 \n- 上部分支是用于从相邻帧上的斑块中提取时间线索的时间分支 \n- 中间分支是通过对空间关系建模来提取人体结构信息的空间分支 \n- 底层分支是提取行人外观特征的全局分支。\n\n![-w1414](https://i.loli.net/2021/03/23/BC3YgrzAhbtFqHW.png)\n\n-   首先把每一帧放到CNN中，得到Fi ∈ Rh×w×c，F ={F1,F2,...,FT}，T为帧数。\n-   再把没个feature map Fi水平切分成P个patch，pi = 1,...,N。\n-   patch数量N为T\\*P，把每个P做平均池化后得到patch特征向量为xi ∈ Rc, i =\n    1,...,N.\n\n用GCN去学习patches之间的关系。 G(V,E)有N个节点，vi ∈ V,eij = (vi,vj) ∈\nE. 每个patches就是图中的节点，边e代表他们之间的关系。 A ∈ RN×N\n是这个图的邻接矩阵。 ![-w621](https://i.loli.net/2021/03/23/jKIY7PWqREQ8Dpm.png)\n\n![-w406](https://i.loli.net/2021/03/23/kdoSKRwTjizyW2A.png)\n这个式子，表示两个patch的关系，φ表示原始面要素的对称变换，φ =\nwx。w是可通过反向传播学习的d×d维权重。\n这个变换的意义是：它允许我们自适应地选择和学习帧内或跨不同帧的不同补丁的相关性，结合其他节点的信息。\n\n归一化 神经网络一般对输入数据的规模很敏感。\n对于亲和力矩阵的每一行，所有边值(即连接到面片i的边)的总和应为1。\n邻接矩阵的每个元素都应该是非负的，系数应该在(0，1)的范围内。\n![-w394](https://i.loli.net/2021/03/23/g96qEtu8sfFAjBy.png)\n接着给邻接矩阵加上单位矩阵\n![-w171](https://i.loli.net/2021/03/23/O7au8z49APyMwfb.png)\n这么做是因为将来GCN拿邻接矩阵A和权重相乘，意味着对于每个节点，我们把这个节点的所有邻接节点的feature向量加了起来，但没有加自己这个点。\n之后，使用re-normalization技巧来近似图-拉普拉斯：\n![-w210](https://i.loli.net/2021/03/23/xA5hYeUioD4M1G8.png)\n邻接矩阵乘以度矩阵减小数据规模，度矩阵也反应了一些节点信息。 \n### Temporal branch\n作用：使用不同帧的所有补丁来构建图，其目的是捕捉跨帧的补丁之间的互补信息。\n方法：使用GCN捕获pathches时域关系，构建M层GCN\n![-w289](https://i.loli.net/2021/03/23/AECcOzH2sY3fBFm.png)\nXm是第m层隐层特征，X0是通过CNN获得的特征patch。 Wm是被学习的参数矩阵。\n每层图卷积后跟一个nomalization层用和LeakyRelu\n![-w421](https://i.loli.net/2021/03/23/8zmZVbWfaovcIJO.png) 最后使用Max\npooling作用于Xm 最后得到 ft ∈ R1×dm 是时域GCN特征，dm设置为2048\n\n#### Structural GCN Module(spatial relations branch)\n\n作用：提供额外的辨别性信息，以加强重新识别系统。\n方法：使用GCN捕获不同patch的空间关系。然后，对视频中各帧的GCN特征进行融合，得到视频中的内在结构特征。\nGis (Vis , Eis ), Vis = {xi,1 , xi,2 , . . . , xi,P }\n下标i表示第i帧，并且每个帧被分成P个patch\n![-w511](https://i.loli.net/2021/03/23/HADw5CYgLnSaZsb.png)\n\n独立地利用每一帧的块之间的关系来捕获视频序列中的结构信息。我们将GCNs的所有输出特征聚合在一起，形成视频的结构特征。\n![-w318](https://i.loli.net/2021/03/23/maljbhcMGAX5fNJ.png)\n邻接矩阵公式和上面介绍的一样。k为第i帧上的第k层图卷积。 Wik ∈ Rdk ×dk\n输出的经过Max pooling降维后的特征矩阵为：XiK ∈ RP ×256\n最后，将视频的特征连接起来，最后的特征表示为fs。\n\n### Global branch\n全局分支提取每个视频的全局外观特征。\n\n### loss function \nbatch hard triplet和softmax cross-entropy Ltriplet\nand Lsoftmax \n### Triplet loss \nTriplet loss学到的是一个好的embedding，相似的图像在embedding空间里是相近的，可以判断是否是同一个人脸。\\[3\\]\n\nTriplet loos 需要三份数据(可以从一个batch中选择):Anchor、Positive、Positive\n- 其中Anchor表示当前数据，Positive是跟A相同人的数据，Positive是不同人的数据。\n- 当前向量、同一人不同向量、不同人不同向量\n- 将一个图像经过特征提取后是一个向量，让这个向量和postive更近更好，让这个向量和negative越远越好。\n![](media/16070719577662/16093032756806.jpg)\n\n目的：让A和P非常接近，A与N尽可能远离\n公式：$ \\lVert f(A)-f(P)\\rVert^2 \\le \\lVert f(A)-f(N)\\rVert^2 $ 其中f表示通过网络进行编码。\n是否会存在问题？如果f把所有的输入都编码成0，依然成立。\n\n那么，这个目标就改为: $\\lVert f(A)-f(P)\\rVert^2 - \\lVert f(A)-f(N)\\rVert^2 + a \\le 0$ \n其中a是margin间隔，表示d(A,P)和d(A,N)相差多少。\n\n\n\n同类之间的距离至少要比不同类距离要少多少。\nTriplet loss:\n$$\n\\begin{equation}\\begin{split} \n    L(A,P,N) = max(\\lVert f(A)-f(P)\\rVert^2-\\lVert f(A)-f(N)\\rVert^2 +a,0)\n    \\end{split}\\end{equation}\n$$\n但是对于约束条件:$d(A,P)+a \\le d(A,N)$ 理论上都是A与P很近，A与N较远。\n实际中用的最多的是hard negative方法，也就是在选择样本的时候，让$d(A,P)\\approx d(A,N)$ 这样给网络一些挑战，才能激励它学习。\n\n- 在同一个人P的特征中找最不像的距离最大的P。\n- 在不同人N的特征中找最像的距离最小的N。\n\n![](https://i.loli.net/2021/03/23/J4kSqvmjgdhYc5i.png) \n![](https://i.loli.net/2021/03/23/7gEZUoy6LNbaAXt.png)\n\n\n\n论文在三个分支上分别计算loss加在一起。\n![-w499](https://i.loli.net/2021/03/23/EoyTcBunLpFNRHM.png)\n\n三个分支输出的特征 fglobal , ft, fs。 fall = \\[fglobal, ft, fs\\]\n把fall放到softmax cross-entropy loss\n![-w373](https://i.loli.net/2021/03/23/1RiZHSQ4TF2pYUc.png)\n\n实验\n----\n\n### 数据集 \nDukeMTMC-VideoReID和MARS，是两个行人重识别的数据集。 \n\n- MARS有1261个身份id，的17503个tracklet和3,248个distractor序列。 \n- DukeMTMC-VideoReID 有1812个身份id，4832个tracklets\n\n评估协议：累积匹配特性曲线（CMC）和平均精度（map）来评价提出的模型的性能。\n\n### 细节(复现用)\nCNN模型用的是在imagenet预训练后的ResNet50，并且最后一层必输设为1。\n采用受限随机采样策略从每个视频中随机采样T=8帧。\n把图片resize为256x128并且随机水平翻转。 模型训练800轮。\n初始化学习率为0.0003，每200轮缩小十倍\nAdam优化器、16个身份为一个batch每个身份有四个追踪器tracklet,16 × 4 × 8 =\n512 images。\n\nTGCN有3层、SGCN有2层 把每个feature map水平切分的P为4\n\n为什么SGCN、TGCN选P=4 当Patch的数量太多时，Patch太小包含不了足够的信息。\n相反，当Patch的数量太少时，Patch可能会忽略细微但有区别的线索。\n\n### 代码\n![](https://i.loli.net/2021/03/23/Zv1cF3AXMJfzTsP.png)\n![](https://z3.ax1x.com/2021/03/23/6TfXSf.png)\n\n### 性能对比\n\n#### 纵向对比其他模型性能\n\n![-w712](https://i.loli.net/2021/03/23/dJQOKyHc9DUM7Tj.png)\n\n![-w694](https://i.loli.net/2021/03/23/C6UKZeWf3dBoYaS.png)\n现有的基于注意力的方法(包括STA、GLTR)独立地处理不同区域和帧，并且它们没有充分考虑补丁之间的内在关系。\nM3D,3D卷积运算计算量大，并且对空间不对准很敏感。 Wu et\nal. 对比这个图方法的模型，在两个数据集都优于他。\n\n#### 向内对比，三分支搭配性能\n![-w732](media/16070719577662/16072629848124.jpg)\n\n提出的方法综合考虑了人体不同部位在同一帧内和不同帧之间的潜在关系，可以提供更具区分性和鲁棒性的信息，并且能够进行端到端的训练。这些实验结果验证了该方法的优越性。\n\n主要创新点：考虑了人体不同部位在同一帧内和不同帧之间的潜在关系。\n\n#### 向内对比，替换GCN为全连接层性能\n![-w655](https://i.loli.net/2021/03/23/AlkZ4XIGOm8hioB.png) 证明了图卷积的必要性。\n\n结论\n----\n\n1.利用斑块间的时间关系缓解遮挡问题，利用斑块间的空间关系区分外观相似的歧义样本的有效性。\n2.提出STGCN模型\nSGCN:空间分支通过建模各帧面片之间的关系来学习人体的结构信息。\nTGCN:时态分支通过对不同帧之间的斑块的时态关系进行建模，可以缓解遮挡问题。\n\n未来方向，问题\n--------------\n\n加深层数会使模型效果不好，两层图卷积叫深度？浅层GCN不能有效地将节点信息传播到整个数据图。这个是GCN的过平滑的通病，还没有解决这个问题。\n\n如下图表现的一样。 ![-w645](https://i.loli.net/2021/03/23/so8cnV4RIBm3dMf.png)\n\n我个人觉得这篇论文，在GCN的结构上是可以改进的。是不是可以考虑残差思想来改进GCN网络结构，或者引入其他结构来优化？\n而且论文代码暂时未开放，模型的复杂度到底如何，还需进一步看看。\n\n## 参考文献\n\n[1][基于深度学习的行人重识别研究综述](https://zhuanlan.zhihu.com/p/31921944)\n[2][基于深度学习的person re-identification综述 Deep Learning for Person Re-identification: A Survey and Outlook](https://blog.csdn.net/rytyy/article/details/105232594)\n[3][Triplet-Loss原理及其实现、应用](https://blog.csdn.net/u013082989/article/details/83537370)\n[4][如何通俗的解释交叉熵与相对熵?](https://www.zhihu.com/question/41252833)\n[5][卷积神经网络系列之softmax，softmax loss和cross entropy的讲解](https://blog.csdn.net/u014380165/article/details/77284921)\n[6][卷积神经网络系列之softmax loss对输入的求导推导](https://blog.csdn.net/u014380165/article/details/79632950)\n[7][中山大学提出行人重识别新方法和史上最大数据集SYSU-30k，已开源！](https://zhuanlan.zhihu.com/p/329077441)\n[8][基于视频的行人再识别（1）：从认识Mars数据集开始](https://blog.csdn.net/qq_34132310/article/details/83869605)\n\n[CVPR 2020 | 旷视研究院提出新方法，优化解决遮挡行人重识别问题](https://mp.weixin.qq.com/s/EhAeaA68Ek27EptkTfZiBQ)\n[目标检测中mAP的计算方法](https://zhuanlan.zhihu.com/p/94597205)\n[视觉算法的工业部署及落地方面的技术知识，怎么学？](https://www.zhihu.com/question/428800593)","tags":["GNN&cv"]},{"title":"LeetCode470随机函数返回等概率的值","url":"/2021/03/23/随机函数返回等概率的值/","content":"\n# LeetCode470随机函数返回等概率的值\n\n\n\n![](https://i.loli.net/2021/03/23/jdgCOPBTVRs5W9U.png)\n\n问题描述：\n\n1. 给定一个随机函数f，等概率返回1~5中的一个数字，\n   这是唯一可以是使用的随机机制，如何实现等概率\n   返回1~7中的一个数字。\n2. 给定一个随机函数f，等概率返回a~b中的一个字母，\n   这是唯一可以是使用的随机机制，如何实现等概率\n   返回a~d中的一个数字。\n\n这种做法一般考虑用二进制的方法，等概率的返回0和1。\n\n如有一个等概率返回12345的函数f\n\nf':如果是1和2返回0，4或5返回1，如果是3重新调用f'。\n\n用0和1拼出一个数这样就可以实现等概率。\n\n只有两个二进制位，可以等概率返回0到3。00、01、10、11\n\n三个二进制位，可以等概率返回0到7。000，001，010，011，100，101，110，111\n\nLeetcode:\n\n```java\nclass Solution extends SolBase {\n    public int rand10() {\n        int ans = 0;\n        do {\n            ans = (rand01() << 3) + (rand01() << 2) + (rand01() << 1) + rand01();\n//        } while (ans == 15 || ans == 14 || ans == 13 || ans == 12 || ans == 11 || ans == 10);\n        } while (ans > 9);\n        return ans + 1;\n    }\n\n\n    public int rand01() {\n        int ans = 0;\n        do {\n            ans = rand7();\n        } while (ans == 4);\n        return ans < 4 ? 0 : 1;\n    }\n }\n```\n\n通用：\n\n```java\npublic class RandomBox {\n\n    //13 - 17\n    // 13+ [0,4]\n    public int random() {\n        return min + (int) (Math.random() * (max - min + 1));\n    }\n\n    public int rand01(int min, int max) {\n        int size = max - min + 1;\n        // size是奇数还是偶数\n        boolean odd = (size & 1) != 0;\n        int mid = size / 2;\n        int ans = 0;\n        do {\n            ans = random() - min;\n        } while (odd && ans == mid);\n        return ans < mid ? 0 : 1;\n    }\n\n    public int rand(int min, int max, int from, int to) {\n        if (from == to) {\n            return from;\n        }\n        // 3-9\n        // 0-6\n        int range = to - from;\n        int num = 1;\n        //求0-range需要几个2进制位\n        while ((1 << num) - 1 < range) {\n            num++;\n        }\n\n        int ans = 0;\n        do {\n            ans = 0;\n            for (int i = 0; i < num; i++) {\n                ans |= (rand01(min, max) << i);\n            }\n        } while (ans > range);\n        return ans + from;\n    }\n\n}\n\n```\n\n再一题：\n\n给一个随机函数f，以p概率返回0，以1-p概率返回1\n\n这是唯一可以用的随机机制，如何实现等概率返回0和1 \n\n思路：\n\n如果连续两次返回00或11 重新选取，如果返回01取为0，如果返回10取为1\n\n```java\n public int f() {\n \t\t\treturn Math.random() < 0.92 ? 0 : 1;\n }\n\n public int g() {\n     int first = 0;\n     do {\n         first = f();\n     } while (first == f());\n     return first;\n  }\n```\n\n","tags":["刷题"]},{"title":"HopRetriever:Retrieve Hops over Wikipedia to Answer Complex Questions(AAAI2020)","url":"/2021/03/22/HopRetriever-Retrieve-Hops-over-Wikipedia-to-Answer-Complex-Questions-AAAI2020/","content":"\n# HopRetriever:Retrieve Hops over Wikipedia to Answer Complex Questions(AAAI2020)\nhttps://arxiv.org/abs/2012.15534\n\n## 摘要\n\n- 大型文本语料库中收集支持证据对于开放领域问答(QA)是一个巨大的挑战。\n- 本文方法：\n  - 将hop定义为**超链接**和**出站链接文档**的组合\n  - 超链接编码成**提及嵌入**，相当于在上下文被提及的结构知识，表示出站链接实体建模。\n  - 出站链接文档编码成**文档嵌入**，相当于非结构化的知识。\n- 使用Hotpot数据集，该数据集文章我在这里写过[TODO]\n\nme:想要更好的检索，光用匹配一种检索方式不好，可以结合相关语义信息同时进行检索，会更准确。\n\n那么难点就是如何找到相关部分提取语义信息用于检索。\n\n\n\n## 介绍\n\n多跳问答任务需要从多个支持文档中搜集分散的证据，来提取答案。最近主流方法是将多跳证据收集视为迭代文档检索问题。\n\n在开放域下，多跳QA的一个关键部分是从整个知识源中检索证据路径，分解成几个单步文档检索。\n\n另一part是在基于知识库KB下，并尝试像虚拟结构化知识库(KB)那样遍历文本数据，专注于提到的实体。\n\n如这篇文章关于认知图谱的[TODO]我也写过。\n\n\n\n作者认为，线索收集可以分为两种\n\n- 实体介绍性文档内的信息丰富非结构的事实，关注非结构实体知识。\n- 实体本身之间的结构化和隐式关系，关注结构化实体知识。\n\n![](https://z3.ax1x.com/2021/03/22/6TmCef.png)\n\n\n\n结构性知识是指提及关系。可能是对应的Q1从一篇文章中提及的。Q1的左边好像是文章\n\n非结构性知识是指知识库之类外来知识。可能是对应Q2从知识库得来。Q2的左边好像是个知识库\n\n回答一个复杂的问题需要结合上面两种知识，我觉得对啊，就像问人一个问题，我已有的知识可能不够，我通过搜索引擎搜到一些知识来补充我回答这个问题的能力。\n\n本文作者要考虑的问题是，基于什么证据可以跳到第二个文档进行进一步检索。\n\nQ1是基于文档匹配\"directed by\"来找到下一个跳，从而可以有充足的证据。\n\nQ2问题更复杂一点，有三首歌被提及，只有其中一首是相关问题的，像上面那种关系不足以去在三者中做选择。这就需要通过和实体相关的无结构知识才能找到答案。\n\n---\n\n### 出发点\n\n所以作者认为，为了在Wikipedia中收集足够的支持证据，有必要同时考虑实体之间的关系结构和隐藏在介绍性文档中的非结构化知识。\n\n当应答过程遵循“顺藤摸瓜”的模式时，隐含的实体层次关系使得检索更加高效。但是，当关系链失败时，文档中的那些非结构化事实就会登台。\n\n本文研究如何将结构化知识和非结构化知识结合起来，共同为证据收集做出贡献。\n\n### 定义hop\n\n定义hop为超链接和相应出站链接文档的组合，后面要将两种embedding结合为hop。\n\n维基百科中的超链接暗示一个实体的介绍性文档如何提及其他一些内容\n\n而出站链接文档存储所有非结构化的事实和事件，这使得一跳包含关系证据和拟事实证据，以供将来检索。\n\n\n\n### HopRetriever思路\n\n对于维基百科文档中提到的每个实体，我们将其周围的文本上下文编码到提及嵌入中，以表示隐含的结构化知识。\n\n对于文档中非结构化知识的表示，与以往的工作一样，使用BERT对文档文本进行编码，条件是原始问题。\n\n对于每个步骤检索，从一个文档(实体)到另一个文档(实体)的跳跃可以从两个角度收集证据：\n\n- 当前文档是如何提到另一个文档。\n\n- 在另一实体的介绍性文件中隐藏了哪些事实\n\n\n\n## 相关工作\n\n### 文档级推理\n\n这种方法在不知道先前检索到的证据文档的情况下独立地查找证据文档，当证据文档中的一个与原始问题有少量语义关系时，可能会导致检索失败。\n\n为避免这个问题有些人提出引入多步检索器，实现对多个证据文件的重复检索。\n\n最近2020年一个PathRetriver，是沿着文本图的出站链接检索文档路径的。利用文档的图结构，减少了每一步检索过程中文档的搜索空间，这比以往的迭代检索器要小得多。(这个可以看看是不是用图网络来对出站非结构关系知识进行学习的) \n\nHopRetriever和他的最大不同是多考虑了，在文章之间的结构化和多值关系。\n\n\n\n### 以实体为中心的推理\n\n大多数QA都是以实体为中心进行推理的。通过实体提及来收集证据。\n\n代表一个是认知图谱那篇，一个是Differentiable Reasoning over a Virtual Knowledge Base(*ICLR 2020,*)(这个可以安排读一读)\n\n作者认为他们的问题是，当问题不是“顺藤摸瓜”时，提及本身不能为跳过哪个实体提供足够的推理证据。\n\n我感觉不太认可，认知图谱那篇同样是用bert来提取下一跳hop，可以是相同语境下的一些线索啊，不是一定提及本身啊。可能bert学的没那么强大？\n\n\n\n### 对问题分解\n\n建议将一个复杂的问题分解为几个更简单的子问题，并在每个步骤进行单跳QA。\n\n问题分解的挑战是确保每个子问题收集真正必要的证据。\n\n如果结构化关系建立失败，可能不会建立出一个可推理的子问题，用于进一步跳跃。\n\n\n\n## 方法细节\n\n这块公式参数细节比较多，一点一点梳理。\n\n### 任务定义\n\n开放域的多跳问答一般分为两个模型：\n\n- 检索模型 $D_q=Retriever(q,K)$ : 用来从大范围知识源K中收集很多证据。$D_q$ 应包含回答多跳问题所需的多个文档。\n- 阅读模型 $a=Reader(q,D_q)$ : 将$D_q$和q中的所有文本事实连接在一起，并馈送到答案提取模型阅读器中，以获得答案a\n\n每个维基百科页面对应一个实体 $e_i$\n附有介绍性的文档尾 $d_i$ ，如果在$D_i$中存在链接到$e_j$的锚点。就定义一个提及关系 $m_{i,j} = e_i \\rightharpoonup^{d_i} e_j$ 。\n\n知识源的定义为$K=\\{D,E,M\\}$\n\nM 是 提及关系$m_{i,j}$的集合\n\nE 是 实体$e_i$的集合\n\nD 是 附加文档$d_i$的集合\n\n论文的任务只是检索模型，且$D_q$ 是迭代得到的。在每个检索步骤，通过不仅检查包含在中的非结构化事实，而且还检查在最新选择的文档中对其的提及来获取文档。为了实现这一点，将非结构化的文本事实和提及分别编码，然后在一跳内将它们一起表示。当通过维基百科进行检索时，HopRetriever使用跃点hop作为匹配对象。\n\n![]( https://z3.ax1x.com/2021/03/23/6T4MDg.png)\n\n\n\n### Hop 如何Embedding\n\n$$\n\\begin{equation}\\begin{split} \n    hop_{i,j} = 提及关系的比重 \\cdot 提及Embedding的线性表示(结构化知识) + 外部文档关系的比重 \\cdot 外部文档Embedding的线性表示(非结构化知识)\n\\end{split}\\end{equation}\n$$\n\n\n\nMention Embedding提及嵌入(结构化实体关系)\n\n就是把包含entity的文档fed给bert，并且添加两个[MARKER] tokens选第一个被marker的作为mention embeding。\n\n如果文中没有直接提到entity，就用一个可学习的向量$m_p$表示\n$$\n\\begin{equation}\\begin{split} \n    m_{i,j}=\n    \\begin{cases}\n    BERT_{[M-j]}(q;d_i), &if\\ m_{i,j}\\in M\\\\\n    m_p, &if\\  otherwise\n    \\end{cases}\n\\end{split}\\end{equation}\n$$\n\n\n![](https://z3.ax1x.com/2021/03/23/6Tvkyd.png)\n\n\n\nDocument Embedding\n\n将$d_j$中的文本事实(与q拼接)送入BERT，将关于实体$e_j$的非结构化知识编码为文档嵌入$u_j$，并将[cls]的输出表示作为文档嵌入向量：\n$$\n\\begin{equation}\\begin{split} \n    u_j = BERT_{[CLS]}(q;d_j)\n\\end{split}\\end{equation}\n$$\n\n\nKnowledge fusion\n\n总体过程为，h为检索的历史向量。\n$$\n\\begin{equation}\\begin{split} \n    a_m &= hW_km{i,j}\\\\\n    a_u &= hW_ku_j \\\\\n    \\{w_m,w_u\\} &= softmax(\\{a_m, a_u\\})\\\\\n    hop_{i,j} &= w_m\\cdot W_vm_{i,j} + w_u\\cdot W_uu_j\n\\end{split}\\end{equation}\n$$\n\n\n![](https://z3.ax1x.com/2021/03/23/6TvonA.png)\n\n在第t步选择的$d_j$ 的概率计算为\n$$\n\\begin{equation}\\begin{split} \n    p(d_j) = sigmoid(h_t^Thop_{i,j})\n\\end{split}\\end{equation}\n$$\n\n\n### 细粒度句子级检索\n\n一个文档不可能所有的句子都是答案所必须的，指出必须的那些支持句子，对于明确推理线索是必须的。\n\n计算一个句子是不是留下采用以下公式：\n$$\n\\begin{equation}\\begin{split} \n    s_{i,l} &= BERT_{[SM-l]}(q;d_i) \\\\\n    p(s_i, l) &= sigmoid(h_tW_s,s_{i,l})\n\\end{split}\\end{equation}\n$$\n\n\n大于0.5定义为支持句。\n\n\n\n\n\n### 目标函数\n\n序列预测模型就是上面的RNN图，在t步下的目标函数为\n$$\nlogp(d_j) + \\sum_{\\hat d_j \\in D, \\hat d_j\\neq d_j}log(1-p(s_{s_i,l}))\n$$\n辅助支持句的预测任务，在第t步目标函数为\n$$\n\\sum_{l\\in L_i}logp(s_{i,l}) + \\sum_{l\\in L_i}log(1-p(s_{i,l}))\n$$\n\n\n## 实验\n\n数据集采用 HotpotQA [我要写TODO] 0564 个问答对。主要关注 fullwiki 部分。支持文档分散在 5M 的维基百科中。 \n\n实验包括三个部分：\n\n- 初步检索。基于 TF-IDF 选取前 500 个文档，作为初始文档。\n- 支持文档检索和支持句子预测。迭代检索初始文档。\n- 答案提取。通过 BERT 获取答案。\n\n作者还采用了一种基于 BERT 的神经排序器，获取更精确的前 500 个文档。同时，使用 ELECTRA （ELECTRA: Pre-training Text Encoders as Discrimi- nators Rather Than Generators. In *International Conference on Learning Representations*.）代替 BERT 进行答案获取。结果作为 HopRetriever-plus。这也体现了更好的初步检索的重要性。\n\n\n\n不同类型问题的embedding权重，结构和非结构\n\n![](https://z3.ax1x.com/2021/03/23/67PbrD.png)\n\n不同case的权重\n\n[![67iaQK.png](https://z3.ax1x.com/2021/03/23/67iaQK.png)](https://imgtu.com/i/67iaQK)\n\ncase3 当没有提及时，非结构化的文档embedding发挥主要作用\n\n\n\n\n\n## 结论\n\nHopRetriever 能够将结构性知识和非结构性知识进行结合，确定比较好的跃点hop。同时，跃点迭代模型能够一步步寻找下一个跃点，最终确定答案实体。除此之外，初步文档检索也是十分重要的内容，文章采用的神经排序器效果不错，值得后面继续研究。\n\n\n\n## 参考文献\n\n[HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions 论文阅读笔记]([https://www.bluestragglers.com/kgqa-%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0%ef%bc%88%e5%8d%81%e4%ba%8c%ef%bc%89/](https://www.bluestragglers.com/kgqa-论文阅读笔记（十二）/))\n\n\n\n","tags":["GNN&nlp"]},{"title":"LeetCode面试17_21直方图装水","url":"/2021/03/22/LeetCode面试17-21直方图装水/","content":"\n# 直方图装水\n\n![](https://i.loli.net/2021/03/22/UJAq2P9xl7D1VWa.png)\n\n\n\n## 思路\n\n- 如果用程序来描述直方图高度的话不好描述\n- 问题可以想成，每个数组下标下当前列中可以放多少水\n- 每个横坐标下的最大存水量=min(当前的左边最大高度，当前右边的最大高度)\n- 如果我当前高度比左右两边最大值都大，那么我横坐标上肯定没水\n- 当前i水量=min{max左，max右} - arr[i] > 0 ? 当前i水量 ： 0\n- 总的存水量=每个横坐标上能存水量的加和\n- 第0个和最后一个就肯定无水\n\n## 优化\n\n普通方法，像上面的思路，到每个i位置都会向左向右遍历一个最大值，复杂度有点高。\n\n技巧：预处理数组，为的是不用每次都遍历去求最大最小值,用的时候直接取\n\n比如原始数组为[3,1,6,7,2,4,3]\n\n从左到右，从右到左，正反遍历此数组，如果当前数比之前的数小就取之前的值作为当前值，当前数比钱已给数大就还选本来的值。\n\n从左到右遍历后为[3,3,6,7,7,7,7]\n\n从右到左遍历后为[7,7,7,7,4,4,3]\n\n以空间换时间，还不是最优\n\n技巧二：(最优)\n\n声明两个指针，L和R。因为数组两端点不会有水L=1,R=N-2。\n\n再声明两变量，LMax：L扫过的部分的最大值，RMax：R扫过的部分最大值。初值为LMax=arr[0],RMax=arr[N-1]\n\n此时就可以计算出L和R当前所能存水的最大值。因为R左边最大值肯定大于等于LMax，L右边最大值肯定大于等于RMax\n\n当LMax大于RMax时，R处的值可求 \n\n当RMax大于LMax事，L处的值可求\n\n相等事，LR可同时求算\n\nRMax和LMax随遍历进行更新。\n\n```java\nclass Solution {\n    public static int water1(int[] arr) {\n        if (arr == null && arr.length < 2) {\n            return 0;\n        }\n        int N = arr.length;\n        int water = 0;\n        for (int i = 0; i < N; i++) {\n            int leftMax = Integer.MIN_VALUE;\n            for (int j = 0; j < i; j++) {\n                leftMax = Math.max(leftMax, arr[j]);\n            }\n            int rightMax = Integer.MIN_VALUE;\n            for (int j = i + 1; j < N; j++) {\n                rightMax = Math.max(rightMax, arr[j]);\n            }\n            water += Math.max(Math.min(leftMax, rightMax) - arr[i], 0);\n        }\n        return water;\n    }\n\n    public static int water2(int[] arr) {\n        if (arr == null && arr.length < 2) {\n            return 0;\n        }\n        int N = arr.length;\n        int water = 0;\n        int left = Integer.MIN_VALUE;\n        int[] leftMaxs = new int[N];\n        for (int i = 0; i < N; i++) {\n            leftMaxs[i] = Math.max(leftMaxs[i - 1], arr[i]);\n        }\n        int[] rightMaxs = new int[N];\n        rightMaxs[N - 1] = Integer.MIN_VALUE;\n        for (int i = N - 1; i >= 9; i--) {\n            rightMaxs[i] = Math.max(rightMaxs[i + 1], arr[i]);\n        }\n        int warter = 0;\n        for (int i = 0; i < N; i++) {\n            warter += Math.max(Math.min(leftMaxs[i-1], rightMaxs[i+1]) - arr[i], 0);\n        }\n\n        return water;\n    }\n    public static int water3(int arr[]) {\n          if (arr == null && arr.length < 2) {\n              return 0;\n          }\n          int N = arr.length;\n          int L = 1;\n          int R = N - 2;\n          int leftMax = arr[0];\n          int rightMax = arr[N - 1];\n          int water = 0;\n          while (L <= R) {\n              if (leftMax <= rightMax) {\n                  water += Math.max(0, leftMax - arr[L]);\n                  leftMax = Math.max(leftMax, arr[L++]);\n              } else {\n                  water += Math.max(0, rightMax - arr[R]);\n                  rightMax = Math.max(rightMax, arr[R--]);\n              }\n          }\n     }\n}\n```\n\n\n\n","tags":["刷题"]},{"title":"阅读理解数据集综述","url":"/2021/03/22/阅读理解数据集综述/","content":"\n# 阅读理解数据集综述\n\n## 1. 阅读理解任务定义\n\n阅读理解任务可以被当作是一个有监督学习问题，具体来说，该任务可 以详细描述为:给定一个数据集 T，其中 T 的每一个样本都以下的三元组来表示:\n$$\nT = {(P_i, Q_i, A_i)}_{i=1}^n\n$$\n其中，$P_i$ 代表第 $i$ 个样本中的文章片段，$Q_i$ 代表第 $i$ 个样本中的问题，$A_i$  代表第 $i$  个样本中根据文章和问题所回答的答案。阅读理解的任务是通过学习得到一个预测函数 $f$ ，使得我们能够通过给定的 $P_i$  与 $Q_i$  来预测出 $A_i$ :\n$$\nf(P_i, Q_i) \\to A_i\n$$\n通俗来讲，阅读理解任务就是通过给定一个文章片段，给定一个问题，要求计算机能够通过文章片段与问题来获得答案。\n\n## 2. 阅读理解任务类型\n\n阅读理解有多种类型，其划分的一个主要依据是根据答案的类型进行划分，这么区分的主要原因在于答案的不同使得模型输出层，损失函数，评估方式等发生很大变化。\n\n目前来看，阅读理解任务根据具体答案形式的不同可以大致区分为以下四类:\n\n- **填空式阅读理解。**\n\n  填空式阅读理解有一个很明显的特点：答案往往是一个单词而非句子。填空式阅读理解任务可以描述为:给定一段文章片段与一个问题，要求机器根据文章片段与问题来推理出合理的答案， 且答案往往是文章片段的某个词。\n\n  填空式阅读理解在阅读理解发展的早 期起到了至关重要的作用，现在已经退出主流数据集了，具体典型的数据集 有:CNN&Daily Mail，Who did What等数据集。\n\n- **抽取式阅读理解。**\n\n  抽取式阅读理解任务可以描述为:给定一段文章片 段，给定一个问题，要求机器根据该问题从文章片段中找出一个连续的片段作为答案。\n\n  考虑到输出问题，此类问题又转化为预测答案的开始与结束的两 个位置 $pos_{start}$ 与 $pos_{end}$ 。此时，问题就转化成为一个分类问题，答案可以用篇章词片段表示为 $[ pos_{start} , pos_{end} ]$ 。\n\n  在过去两年中，此类数据集一直是学术界的主流数据集，极大的推动了阅读理解领域的发展，其中最典型的数据集包括 SQuAD，MS Marco，NewsQA，TriviaQA等数据集。\n\n- **多选式阅读理解。**\n\n  多选式阅读理解任务可以描述为：给定一段文章片段，给定一个问题，给定多个选项，要求机器根据文章片段与问题从答案选项中选择一个最合适的答案。\n\n  通过将阅读理解问题转化为分类问题可以更准 确的评估机器对语言的理解能力，这也是此类数据集强于抽取式数据集的一 大原因。\n\n  此类数据集是目前研究人员研究的热点之一，代表性的数据集有 RACE，CLOTH等。\n\n- **生成式阅读理解。**\n\n  生成式阅读理解任务可以描述为：给定一段文章片 段，给定一个问题，要求机器基于文章片段与问题生成一个合适的答案，该答案不局限于文章中存在的词语，而是自由生成的。\n\n  此类型的阅读理解任务 更适合实际生活场景，但是由于生成的句子无法做准确评估，因此一直无法 成为业界的主流数据集。代表性的数据集有 NARRATIVEQA，CoQA等。\n\n\n\n## 3. 阅读理解任务的评估方式\n\n| 任务类型       | 评估方法           |\n| -------------- | ------------------ |\n| 填空式阅读理解 | 准确率(Accuracy)   |\n| 抽取式阅读理解 | EM(完全匹配值)，F1 |\n| 多选式阅读理解 | 准确率(Accuracy)   |\n| 生成式阅读理解 | BLEU，ROUGE        |\n\n对于抽取式阅读理解任务，由于答案通常为一个片段，一般同时采用两\n种评估方式:\n\n- 完全匹配值(Exact Match，EM)。该指标用来判定预测的答案与给 定的答案是否完全相同，即预测的开始位置 $pos^{pred}_{start}$ 与终止位置 $pos^{pred}_{end}$ 是否与真实值相同，其计算公式下：\n  $$\n  EM = \\begin{cases} 1, & pos^{pred}_{start} == pos^{real}_{start}  \\, and \\, pos^{pred}_{end} == pos^{real}_{end} \\\\ 0, & otherwise \\end{cases}\n  $$\n\n- F1 值。该指标主要评估预测的答案片段与正确答案的重合率，其计 算公式如下所示：\n  $$\n  F1 = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}\n  $$\n\n## 4. 现有数据集分类\n\n本节汇集了当前大多数的阅读理解数据集，并对其进行简单描述\n\n### 1. 填空式阅读理解\n\n考虑到这部分其实已经几乎没人在搞了，因此就不做详细描述了。\n\n| 数据集            | 语言    | 状态             |\n| ----------------- | ------- | ---------------- |\n| MCTest [1]        | English | 过时，不推荐研究 |\n| CNN/Daily Mail[2] | English | 过时，不推荐研究 |\n| CBT[3]            | English | 过时，不推荐研究 |\n| Quasar-S[4]       | English | 过时，不推荐研究 |\n\n- CNN&Daily Mail： 最具代表的数据聚集，数据来源于CNN 和 Daily Mail。\n- CBT：数据来源于儿童读物。\n\n### 2. 抽取式阅读理解\n\nhttps://www.leiphone.com/news/201903/QcmBwrYSo8QyWXRb.html\n\n| 数据集                                                       | 语言        | 状态                                 |\n| ------------------------------------------------------------ | ----------- | ------------------------------------ |\n| [SQuAD 1.0](https://rajpurkar.github.io/SQuAD-explorer/) [5] | English     | 过时                                 |\n| [**SQuAD 2.0**](https://rajpurkar.github.io/SQuAD-explorer/) [6] | **English** | **热点**                             |\n| [**DuReader**](https://zhuanlan.zhihu.com/p/36415104)        | **Chinese** | **热点**                             |\n| [**MS MARCO**](https://zhuanlan.zhihu.com/p/53525750)        | **English** | **非研究热点，但跟搜索引擎紧密结合** |\n| [CoQA](https://zhuanlan.zhihu.com/p/43050014) [9]            | English     | 热点，接替SQuAD                      |\n| [TriviaQA](http://nlp.cs.washington.edu/triviaqa/) [10]      | English     | 热点                                 |\n| [HotpotQA](https://hotpotqa.github.io/) [11]                 | English     | 热点                                 |\n| Quasar-T [4]                                                 | English     | 非研究热点                           |\n| SearchQA[12]                                                 | English     | 非研究热点                           |\n| [CMRC 2018](https://hfl-rc.github.io/cmrc2018/open_challenge/) | Chinese     | 研究热点                             |\n| [CMRC 2019](https://hfl-rc.github.io/cmrc2019/)              | Chinese     | 热点                                 |\n| [NewsQA](https://www.microsoft.com/en-us/research/project/newsqa-dataset/) [13] | English     | 有点意思                             |\n| [QuAC](http://quac.ai/) [14]                                 | English     | 非热点                               |\n\n- SQuAD 1.0：来源于维基百科，给定 context 于 question， 从 context 中截取一个片段，该片段作为答案。 是一个典型的抽取式问题。\n- SQuAD 2.0：在 SQuAD 1.0 的基础上新增超过5万无法回答的问题。这要求模型不仅要在能够在问题可回答时给出答案，还要判断哪些问题是阅读文本中没有材料支持的，并拒绝回答这些问题。\n- DuReader： 中文阅读理解数据集，应该是国内最棒的阅读理解数据集。它的格式跟 下面的 MS MARCO 相似。DuReader中的问题和文档均来自百度搜索和百度知道。答案是人为产生的，而不是原始上下文中的片段。DuReader之所以与众不同，是因为它提供了新的问题类型，例如yes、no和opinion。与事实性问题相比，这些问题有时需要对文档的多个部分进行汇总。\n- MS MARCO：， 很工业化的数据集，来自Bing 用户查询，因此跟搜索引擎技术紧密相连，十分适合学习。为了克服以前的数据集的弱点，它具有四个主要功能。首先，所有问题都是从真实用户查询中收集的；其次，对于每个问题，使用Bing搜索引擎搜索10个相关文档作为上下文；第三，人为这些问题标注了答案，因此它们不仅限于上下文范围，还需要更多的推理和总结；第四，每个问题有多个答案，有时甚至冲突，这使得机器选择正确的答案更具挑战性。MS MARCO使MRC数据集更接近真实世界。\n- CoQA：， 对话式阅读理解数据集，这跟现实生活又近了一步，是现在研究的热点。CoQA包含约8000轮对话，问题的答案有五种类型，分别为Yes、No、Unknown，文章中的一个span和生成式答案。当根据文章和之前的对话信息无法回答当前问题时，答案为Unknown。该数据集不仅提供答案，而且给出了答案的依据，每一种类型的答案的依据都是文章中的一个span。\n\n- TriviaQA：。该数据集构造问答对，然后从维基百科等页面中寻找对应的论据。最终通过上述方式构造了约65,000个“问题-答案-论据”三元组，通过这种方式构造的数据集比SQuAD更接近实际使用场景。对比SQuAD数据集，其主要集中于是推理方面的问题，并且实验证明一些在SQuAD上表现良好的模型在TriviaQA上并不能获得理想的结果。\n- HotpotQA：研究基于多个信息内容的多步推理，然后回答问题。这意味着答案并不仅仅来源于单一文档。\n- Quasar-T：不建议深入研究。\n- SearchQA：作者构建该数据集的目的是构建能反映检索系统噪声的阅读理解数据集，作者通爬取 Jeopardy 上的问题，然后将问题作为query 在Google 上检索，获得 answer snippets。 该数据集是通过程序生成的，因此噪声不可避免的比较高，因此不建议深入研究。\n- NewsQA：该数据集是从CNN新闻网站上构造的，构造方法与SQuAD一致。\n\n- QuAC： 对话式阅读理解数据集。\n\n### 3. 多选式阅读理解\n\n| 数据集                                                       | 语言    | 状态             |\n| ------------------------------------------------------------ | ------- | ---------------- |\n| [RACE](http://www.qizhexie.com//data/RACE_leaderboard) [15]  | English | 热点，可研究     |\n| [CLOTH](http://www.qizhexie.com/data/CLOTH_leaderboard) [16] | English | 一般，已解决     |\n| [ARC](https://allenai.org/data/arc) [17]                     | English | 一般，不推荐     |\n| Who did What [18]                                            | English | 过时，不推荐研究 |\n| [OpenBookQA](https://leaderboard.allenai.org/open_book_qa/submissions/public) [19] | English | 一般，不推荐     |\n| [CommonsenseQA](https://www.tau-nlp.org/commonsenseqa)  [20] | English | 一般，不推荐     |\n| [COSMOS QA](https://wilburone.github.io/cosmos/) [21]        | English | 一般             |\n\n- RACE： RACE 取自于中国中高考阅读理解题型，我个人认为这是目前最能体现阅读理解能力的数据集之一，十分值得研究。\n- CLOTH：来自中文中高考完形填空题型，相较于RACE， CLOTH 天然的适合 BERT 这种 AE 模型来填词，因此 CLOTH 可以说是已经被解决了，准确率比人高。\n- ARC：ARC 取自中学生考试中的科学问题，并进一步分为ARC-Challenge 于 ARC-Easy 两个子集，共包含大约8000个问题，此外，该数据集中提供与该任务相关的包含14M科学事实的语料库用来回答这些问题。\n- OpenBookQA：包含大约6000个问题，每个问题包括四个选项，此外，与ARC数据集相似，该数据集也提供了参考语料库，包含1326个事实，每个问题期望结合语料库中的某一个事实来得到答案。此外，还需要结合一些常识知识。如何准确的利用参考语料库与常识知识成为了该数据集的主要问题之一。\n- CommonsenseQA：来自于ConceptNet，其包含大约12000个需要结合背景知识的问题。在该数据集中，标注者根据ConceptNet中的实体概念来自由构造问题，来使问题包含人类所具有的、但难以在网络资源中检索到的背景知识，故回答问题需要利用问题、候选答案，以及仅仅使用检索策略无法检索到的背景知识。\n- COSMOS QA：包含35600个需要常识阅读理解的问题，其专注于解决需要跨越上下文、而不是定位指定片段的推理问题。\n\n### 4. 生成式阅读理解\n\n生成式阅读理解目前还没有热起来的趋势，相关的数据集也没有进入主流视野，个人不建议做这方面的研究。 这一大原因在于文本生成作为单一的任务迟迟得不到突破，至少目前为止（2020年），看不到突破的影子，个人觉得还需要一些时间。\n\n### 5. 其他\n\n其他还有一些数据集，如bAbi，LAMBADA， SCT，MCScript，NarrativeQA，DuoRC，CliCR 等，水平有限，累了，就不做赘述了。\n\n## 最后\n\n本文总结了大多数的数据集，但是并没有对数据集进行详细描述，一来是因为工作量比较大，二来是觉得没有必要。 一般做阅读理解紧跟几个主流数据集就行，太多数据集反而会乱了自身阵脚。\n\n## Reference\n\n### 1. 博客参考\n\n[赛尔笔记 | 机器阅读理解简述](https://zhuanlan.zhihu.com/p/111410698)\n\n[RCPapers](https://github.com/thunlp/RCPapers)\n\n### 2. 填空式阅读理解\n\n[1] (MCTest) **MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text.** Matthew Richardson, Christopher J.C. Burges, and Erin Renshaw. EMNLP 2013. [paper](http://www.aclweb.org/anthology/D13-1020).\n\n[2] (CNN/Daily Mail) **Teaching Machines to Read and Comprehend.** Hermann, Karl Moritz, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. NIPS 2015. [paper](https://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf)\n\n[3] (CBT) **The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations.** Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. arXiv preprint arXiv:1511.02301 (2015). [paper](https://arxiv.org/pdf/1511.02301)\n\n[4] (Quasar) **Quasar: Datasets for Question Answering by Search and Reading.** Bhuwan Dhingra, Kathryn Mazaitis, and William W. Cohen. arXiv preprint arXiv:1707.03904 (2017). [paper](https://arxiv.org/pdf/1707.03904)\n\n### 3. 抽取式阅读理解\n\n[5 ]  (SQuAD 1.0) **SQuAD: 100,000+ Questions for Machine Comprehension of Text.** Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. EMNLP 2016. [paper](https://aclweb.org/anthology/D16-1264)\n\n[6] (SQuAD 2.0) **Know What You Don't Know: Unanswerable Questions for SQuAD.** Pranav Rajpurkar, Robin Jia, and Percy Liang. ACL 2018. [paper](http://aclweb.org/anthology/P18-2124)\n\n[7] (DuReader) **DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications.** Wei He, Kai Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang. ACL 2018 Workshop. [paper](https://arxiv.org/abs/1711.05073)\n\n[8]  (MS MARCO) **MS MARCO: A Human Generated MAchine Reading COmprehension Dataset.** Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng.  arXiv preprint arXiv:1611.09268 (2016). [paper](https://arxiv.org/pdf/1611.09268)\n\n[9] (CoQA) **CoQA: A Conversational Question Answering Challenge.** Siva Reddy, Danqi Chen, and Christopher D. Manning. arXiv preprint arXiv:1808.07042 (2018). [paper](https://arxiv.org/pdf/1808.07042)\n\n[10] (TriviaQA) **TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.** Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer. arXiv preprint arXiv:1705.03551 (2017). [paper](https://arxiv.org/pdf/1705.03551)\n\n[11] (HotpotQA) **HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering**. Yang Z , Qi P , Zhang S , et al. . 2018.[paper](https://arxiv.org/abs/1809.09600v1)\n\n[12] (SearchQA) **SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.** Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. arXiv preprint arXiv:1704.05179 (2017). [paper](https://arxiv.org/pdf/1704.05179)\n\n[13] (NewsQA) **NewsQA: A Machine Comprehension Dataset.** Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. arXiv preprint arXiv:1611.09830 (2016). [paper](https://arxiv.org/pdf/1611.09830)\n\n[14] (QuAC) **QuAC : Question Answering in Context.** Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and  Luke Zettlemoyer. arXiv preprint arXiv:1808.07036 (2018). [paper](https://arxiv.org/pdf/1808.07036)\n\n### 3.  多选式阅读理解\n\n[15] (RACE) **RACE: Large-scale ReAding Comprehension Dataset From Examinations.** Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. EMNLP 2017. [paper](http://aclweb.org/anthology/D17-1082)\n\n[16] (CLOTH) **Large-scale Cloze Test Dataset Created by Teachers.** Qizhe Xie, Guokun Lai, Zihang Dai, and Eduard Hovy. EMNLP 2018. [paper](https://arxiv.org/pdf/1711.03225)\n\n[17] (ARC) **Think you have Solved Question Answering?Try ARC, the AI2 Reasoning Challenge.** Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. arXiv preprint arXiv:1803.05457 (2018). [paper](https://arxiv.org/pdf/1803.05457)\n\n[18] (Who did What) **Who did What: A Large-Scale Person-Centered Cloze Dataset** Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, and David McAllester. EMNLP 2016. [paper](https://aclweb.org/anthology/D16-1241)\n\n[19] (OpenBookQA) Mihaylov T, Clark P, Khot T, et al. Can a suit of armor conduct electricity? a new dataset for open book question answering[J].  2018. [paper](https://arxiv.org/abs/1809.02789)\n\n[20] Talmor A, Herzig J, Lourie N, et al. Commonsenseqa: A question answering challenge targeting commonsense knowledge[J]. 2018. [paper](https://arxiv.org/abs/1811.00937)\n\n[21] Huang L, Bras R L, Bhagavatula C, et al. Cosmos QA: Machine reading comprehension with contextual commonsense reasoning[J]. arXiv, 2019. [paper](https://arxiv.org/abs/1909.00277)\n\n### 其他\n\n[22] (bAbi) **Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks.** Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. arXiv preprint arXiv:1502.05698 (2015). [paper](https://arxiv.org/pdf/1502.05698)\n\n[23] (LAMBADA) **The LAMBADA Dataset:Word Prediction Requiring a Broad Discourse Context.** Denis Paperno, Germ ́an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern ́andez. ACL 2016. [paper](https://www.aclweb.org/anthology/P16-1144)\n\n[24] (SCT) **LSDSem 2017 Shared Task: The Story Cloze Test.** Nasrin Mostafazadeh, Michael Roth, Annie Louis,Nathanael Chambers, and James F. Allen. ACL 2017 workshop. [paper](http://aclweb.org/anthology/W17-0906)\n\n[25] (MCScript) **MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge.** Simon Ostermann, Ashutosh Modi, Michael Roth, Stefan Thater, and Manfred Pinkal. arXiv preprint arXiv:1803.05223.  [paper](https://arxiv.org/pdf/1803.05223.pdf)\n\n[26] (NarrativeQA) **The NarrativeQA Reading Comprehension Challenge**.\nTomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. TACL 2018. [paper](http://aclweb.org/anthology/Q18-1023)\n\n[27] (DuoRC) **DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension.** Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and Karthik Sankaranarayanan. ACL 2018. [paper](http://aclweb.org/anthology/P18-1156)\n\n[28] (CliCR) **CliCR: a Dataset of Clinical Case Reports for Machine Reading Comprehension.** Simon Suster and Walter Daelemans. NAACL 2018. [paper](http://aclweb.org/anthology/N18-1140) ","tags":["nlp"]},{"title":"天池赛题:天猫重复购学习笔记(我的EDA模板)","url":"/2021/01/29/16080066587381/","content":"highlight_shrink:\n---\n\n# 天池赛题:天猫重复购学习笔记(我的EDA模板)\n\n字段解释都在:[这里](https://tianchi.aliyun.com/competition/entrance/231576/information)\n代码在GitHub:[这里](https://github.com/Coding-Zuo/DataGame/tree/main/tianchi/%E5%A4%A9%E7%8C%AB%E7%94%A8%E6%88%B7%E9%87%8D%E5%A4%8D%E8%B4%AD)\n- 复购率 = 重复购买用户数量/用户样本数量\n- 复购率 = 重复购买行为次数(或 交易次数)/用户样本数量\n\n[TOC]\n\n## EDA步骤\n\n### 1.看数据类型、数量、样例\n无疑是一些pd.read_csv(_)和data.info()、data.head()。大致看一看\n查看是否有单一值变量\n\n```python\n#查看训练集测试集中特征属性只有一值的特征\ntrain_one_value = [col for col in train.columns if train[col].nunique() <= 1]\ntest_one_value = [col for col in test.columns if test[col].nunique() <= 1]\nprint('one value featrues in train:',train_one_value)\nprint('one value featrues in test: ',test_one_value)\n```\n\n### 2.区分类别变量和连续变量\n一般类别变量的处理和连续型变量的处理不一样\n类别型可能会encode，看一些数量分布。\n连续型可能会看一看核分布。\n\n```python\n#区分类别特征与连续特征\ndef split_features(df,colnums,nums=30):\n    label_features={}\n    continue_features={}\n    for key in colnums:\n        nunique=df[key].nunique()\n        if np.issubdtype(df[key][0],np.int) and nunique<=nums:\n            label_features.update({key:nunique})\n        elif np.issubdtype(df[key][0],np.float) and nunique<=nums:\n            label_features.update({key:nunique})\n        else:\n            continue_features.update({key:nunique})\n    print(label_features)\n    #return label_features,continue_features\n\nuser_info_colnums=user_info.columns.values\nuser_log_colnums=user_log.columns.values\ntrain_colnums=train.columns.values\nlable_nunique_maxnums=20\nprint('user_info:')\nsplit_features(user_info,colnums=user_info_colnums,nums=lable_nunique_maxnums)\nprint('user_log :')\nsplit_features(user_log,colnums=user_log_colnums,nums=lable_nunique_maxnums)\nprint('train :')\nsplit_features(train,colnums=train_colnums,nums=lable_nunique_maxnums)\n```\n可知 \nuser_info:\n{'age_range': 9, 'gender': 3}\nuser_log :\n{'action_type': 4}\ntrain :\n{'label': 2}\n\n### 3.看是否有缺失值\n\n| 插补方法      | 说明                                                         | 优点                             | 缺点                                   | 使用环境         |\n| ------------- | :----------------------------------------------------------- | -------------------------------- | -------------------------------------- | ---------------- |\n| 类均值插补    | 数值型：均值。<br/>非数值型：众数（出现频率最高的值）值比较稳定性；低估资料变异 | 简单易行：被插补的值比较稳定     | 不能反映缺失值的变异性；低估资料变异   | 低缺失率首选     |\n| 类随机插补    | 聚类填充；使用所有可能的值填充；组合完整化方法               | 能体现数据变异性                 | 依赖于观测值                           | 低缺失率         |\n| 回归插补      | 基于完整的数据集，建立回归方程（模型）                       | 方差估计好                       | 稳定性依赖于辅助变量，抽样误差不易控制 | 变量间的相关性强 |\n| Em 插补       | 通过观测数据的边际分布可以对未知参数进行极大似然估计         | 利用充分，考虑了缺失值的不确定性 | 计算复杂                               | 高缺失率         |\n| 多重插补 MCMC | 估计出持插补的值，然后加上不同的噪声，形成多组可选插补值     | 利用充分，考虑了缺失值的不确定性 | 计算复杂                               | 高缺失率首选     |\n\n\n\n```python\n# 有时0代表缺失值，对于缺失值严重的一般删除处理\n# 缺失值较少一般三种：\n#统计量填充：连续值推荐用中位数，离散值：用众数，不能用均值和中位数\n#特殊值填充：正常范围内取值\n#不处理：xgb、lgb对缺失值不太敏感，算法本身有一套缺失值处理方法\ndef missing_value_rate(data,col_name):\n    rate_list = {}\n    for col in col_name:\n        rate = (data.shape[0]-data[col].count())/data.shape[0]\n#         na_zero_num = data[data[col].isna() | (data[data[col]==0])].count()\n        lable_foamt = 'rate:{}'.format(rate)\n        rate_list[col]=lable_foamt\n    return rate_list\n```\n\n```python\nprint('age_range:',user_info[user_info['age_range'].isna() | (user_info['age_range']==0)].count())\nprint()\nprint('gender:',user_info[user_info['gender'].isna() | (user_info['gender']==0)].count())\nmissing_value_rate(user_info,user_info.columns)\n```\n\n### 4.观察数据分布、不均衡样本\n不均衡样本，可采用\n\n- 随机欠采样\n- 随机过采样\n- 基于聚类的过采样\n- SMOTE算法\n- 基于数据清洗的SMOTE\n\n首先describe()看一看。\n#### 正负样本分布\n```python\nlabel_gp = train.groupby('label')['user_id'].count()\nprint('正负样本数量:',label_gp)\n```\n\n```python\nfig = plt.figure()\n# 样本分布不均匀 可采用负样本过采样技术，训练多个模型后求平均或者调整模型的损失函数样本比例的权重\nax = plt.subplot(1,2,1)\nlabels = [0,1]\nsizes = [label_gp[0],label_gp[1]]\nexplode = (0,0)\nplt.pie(sizes,explode=explode,labels=labels,autopct='%1.1f%%',shadow=False,startangle=150)\n\nplt.subplot(1,2,2)\nsns.countplot(train['label'])\nplt.show()  \n```\n![-w355](https://i.loli.net/2021/03/22/VuwAyesYqEUKkib.png)\n\n\n\n#### 对店铺的分析\n\n```python\n#top5销量店铺\ntrain.merchant_id.value_counts().head()\ntrain_data_merchant = train.copy()\ntrain_data_merchant['TOP5']=train_data_merchant['merchant_id'].map(lambda x:1 if x in [4044,3828,4173,1102,4976] else 0)\ntrain_data_merchant = train_data_merchant[train_data_merchant['TOP5']==1]\nplt.figure(figsize=(8,6))\nplt.title('Merchant vs Label')\nsax = sns.countplot('merchant_id',hue='label',data=train_data_merchant)\n```\n对比一下top5店铺回购的比例，可看出不同店铺复购率不同，可能与店铺售卖商品和运营有关。\n![-w507](https://i.loli.net/2021/03/22/6brYGId4tyvh2js.png)\n\n```python\n# 查看店铺的复购分布\nmerchant_repeat_buy = [rate for rate in train.groupby(['merchant_id'])['label'].mean() if rate<=1 and rate>0]\n\nplt.figure(figsize=(8,4))\n\nimport scipy.stats as stats\nax = plt.subplot(1,2,1)\nsns.distplot(merchant_repeat_buy,fit=stats.norm)\nax = plt.subplot(1,2,2)\nres = stats.probplot(merchant_repeat_buy,plot=plt)\n```\n不同店铺有不同的复购率，在0到0.3之间。\n![-w495](https://i.loli.net/2021/03/22/YfWSRQaX3x8wdGh.png)\n\n#### 对用户方面的分析\n通过user_id/age_range/gender等方面\n\n```python\n# 对用户分析\nuser_repeat_buy = [\n    rate for rate in train.groupby(['user_id'])['label'].mean()\n    if rate <=1 and rate>0\n]\n\nplt.figure(figsize=(8,6))\nax = plt.subplot(1,2,1)\nsns.distplot(user_repeat_buy,fit=stats.norm)\nax = plt.subplot(1,2,2)\nres = stats.probplot(user_repeat_buy,plot=plt)\n```\n看出进六个月用户复购概率很小，基本为一次买主\n![-w509](https://i.loli.net/2021/03/22/8ve2dRhlVkCXaxI.png)\n\n```python\n# 用户性别分析\ntrain_user_info =train.merge(user_info,on=['user_id'],how='left')\n\nplt.figure(figsize=(8,8))\nplt.title('Gender vs label')\nax = sns.countplot('gender',hue='label',data=train_user_info)\nfor p in ax.patches:\n    height = p.get_height()\n```\n![-w519](https://i.loli.net/2021/03/22/OARdpIU5tBqMGjy.png)\n```python\n# 不同性别对用户的复购率不同\nrepeat_buy = [rate for rate in train_user_info.groupby(['gender'])['label'].mean()]\n\nplt.figure(figsize=(8,4))\nax = plt.subplot(1,2,1)\nsns.distplot(repeat_buy,fit=stats.norm)\n\nax = plt.subplot(1,2,2)\nres = stats.probplot(repeat_buy,plot=plt) \n```\nprobplot主要是直观的表示观测与预测值之间的差异。一般我们所取得数量性状数据都为正态分布数据。预测的线是一条从原点出发的45度角的虚线，事件观测值是实心点。\n偏离线越大，则两个数据集来自具有不同分布的群体的结论的证据就越大。\n![-w494](https://i.loli.net/2021/03/22/Y2xQn74AGUZTHsl.png)\n\n```python\n# 对用户年龄分析\nplt.figure(figsize=(8,8))\nplt.title('Age vs label')\nax = sns.countplot('age_range',hue='label',data = train_user_info)\n```\n![-w514](https://i.loli.net/2021/03/22/cwrPBGtfKVZbJRC.png)\n```python\n# 用户年龄复购的分布\n# 不同年龄段对用户的复购率不同\nrepeat_buy = [rate for rate in train_user_info.groupby(['age_range'])['label'].mean()]\n\nplt.figure(figsize=(8,4))\n\nax = plt.subplot(1,2,1)\nsns.distplot(repeat_buy,fit=stats.norm)\nax = plt.subplot(1,2,2)\nres = stats.probplot(repeat_buy,plot=plt)\n```\n![-w499](https://i.loli.net/2021/03/22/jkeTh1aUYZq9ELH.png)\n\n### 5.对比训练集和测试集分布\n\n```python\n\n```\n### 6.探查重要影响因素\n\n```python\ncolormap = plt.cm.viridis\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correaltion of Feature',y=1.05,size=15)\nsns.heatmap(train_user_info.astype(float).corr(),linewidths=0.1,vmax=1.0,square=True,cmap=colormap,linecolor='white',annot=True)\n```\n\n## 特征工程\n类别型特征的转换：决策树等少数模型能直接处理字符串形式的输入。\n逻辑回归、svm等需类别型特征处理成数值型特征后才能工作。\n现在比赛基本上都是lightgbm和xgb这些GDBT比较有优势，所以也不用怎么做。了解一些处理方法。\n\n- 序号编码\n- 独热编码\n- 二进制编码\n\n\n### 特征组合\n1、离散特征可两两组合成高阶组合特征，高维组合特征处理的目的是提高复杂关系的拟合能力。\n如： 语言：中文、英文\n    剧集类型：电影电视剧\n    可组合成：中文电影、中文电视剧、英文电影、英文电视剧\n当引入ID特征时，通常要降维。以推荐为例。通常有：\n\n| 是否点击 | uid=1,item=1 | uid=2,item=2 | .... | uid=n,item=n |\n| -------- | ------------ | ------------ | ---- | ------------ |\n| 0        | 1            | 0            | ...  | 0            |\n| 1        | 0            | 1            | ...  | 0            |\n\n\n\n当uid有10000个，item有10000个时有100000000一般可采用SVD分解降低参数，还可以增加参数的迭代拟合数量，防止过拟合。\n\n\n\n2、决策树组合特征：\n![](https://i.loli.net/2021/03/22/RB9qlWDKb3try6c.png)\n\n\n​    \n## 模型训练\n## 模型验证\n## 特征优化\n\n## EDA代码技巧罗列(方便快速拷贝)\n\n### 画字段测试集和训练集数量对比饼图\n\n```python\n#画字段测试集和训练集数量对比饼图\ndef pie_category(train,test):\n    plt.figure(figsize=[9,7])\n    train.value_counts().plot.pie()\n    print(\"train:\",Counter(train))  \n    print(\"test:\",Counter(test))  \n\npie_category(train.XINGBIE,test.XINGBIE)\n```\n### 选择Dataframe数据集中的某几列\n\n```python\n#选择Dataframe数据集中的某几列\nfrom sklearn.base import BaseEstimator,TransformerMixin\n\nclass DataFrameSelector(BaseEstimator,TransformerMixin):\n    def __init__(self,attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        return X[self.attribute_names]\n        \nlianxu_train = DataFrameSelector(column_lianxu).transform(train)\nlianxu_test =  DataFrameSelector(column_lianxu).transform(test)\n```\n### 多列KDE分布\n\n```python\n# 多列KDE分布\ndist_cols = 3\ndist_rows = len(column_lianxu)\nplt.figure(figsize=(4 * dist_cols, 4 * dist_rows))\n\nlianxu_train = DataFrameSelector(column_lianxu).transform(train)\nlianxu_test =  DataFrameSelector(column_lianxu).transform(test)\n\ni = 1\nfor col in column_lianxu:\n    ax = plt.subplot(dist_rows, dist_cols, i)\n    ax = sns.kdeplot(lianxu_train[col], color=\"Red\", shade=True)\n    ax = sns.kdeplot(lianxu_test[col], color=\"Blue\", shade=True)\n    ax.set_xlabel(col)\n    ax.set_ylabel(\"Frequency\")\n    ax = ax.legend([\"train\", \"test\"])\n\n    i += 1\nplt.show()\n```\n### 包库常用设置拷贝\n```python\n# 包库常用设置拷贝\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom scipy import stats\nimport matplotlib\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport gc\nimport warnings\nfrom collections import Counter\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.max_open_warning': 0})\npd.set_option('expand_frame_repr', False) #数据超过总宽度后，是否折叠显示\npd.set_option('display.width', 100) #数据显示总宽度\npd.set_option('max_rows', 100) #显示最多行数，超出该数以省略号表示\npd.set_option('max_columns', 100) #显示最多列数，超出该数以省略号表示\npd.set_option('max_colwidth', 16) #设置单列的宽度，用字符个数表示，单个数据长度超出该数时以省略号表示\npd.set_option('large_repr', 'truncate') #数据超过设置显示最大行列数时，带省略号显示/若是info则是统计信息显示\npd.set_option('show_dimensions', True) #当数据带省略号显示时，是否在最后显示数据的维度\n\nsns.set_style(\"whitegrid\")\nmatplotlib.rcParams['font.sans-serif'] = ['SimHei']\nmatplotlib.rcParams['font.family']='sans-serif' \nmatplotlib.rcParams['axes.unicode_minus'] = False\nmatplotlib.fontsize='20'\n```\n### 查看类别特征是否测试集类别有不在训练集的\n\n```python\n#查看类别特征是否测试集类别有不在训练集的\ntest_outof_train=[]\nfor key in label_features:\n    test_unique=test_df[key].unique().tolist()\n    train_unique=train_df[key].unique().tolist()\n    for index in test_unique:\n        if index not in train_unique:\n            test_outof_train.append(key)\n            break\ntest_outof_train\n```\n### 类别特征分布\n```python\n# 类别特征分布\ndef show_label_features_distribution(df1,df2,Y=None):\n    df1=df1.value_counts().sort_index()\n    df2=df2.value_counts().sort_index()\n    df=pd.concat([df1,df2],axis=1)\n    feature_name=df.columns[0]\n    df.columns=['train','test'] \n    df.plot.bar(title=feature_name)\n    print(feature_name,'\\n',df)\n\nfor key in label_features:q\n    show_label_features_distribution(train_df[key],test_df[key])\n```\n### 连续特征分布\n```python\n# 连续特征分布\ndef show_continue_features_distribution(df1,df2):\n    feature_name=df1.name\n    g = sns.kdeplot(df1.values, color=\"Red\", shade = True)\n    g = sns.kdeplot(df2.values, ax =g, color=\"Green\", shade= True)\n    g.set_xlabel(feature_name)\n    g.set_ylabel(\"Frequency\")\n    g = g.legend([\"train\",\"test\"])\n    plt.show()\n    describe=pd.concat([df1.describe(),df2.describe()],axis=1)\n    describe.columns=[f'train {feature_name}',f'test {feature_name}']\n    print(describe)\n\nfor key in continue_featues:\n    show_continue_features_distribution(train_df[key],test_df[key])\n```\n### 清理缓存\n```python\ndef clear_mem():\n    %reset -f out\n    %reset -f in\n    gc.collect()\n```\n## 参考文献\n[matplotlib 知识点11：绘制饼图（pie 函数精讲）](https://www.cnblogs.com/biyoulin/p/9565350.html)\n[决策树中的类别特征问题（关于label encode还是one-hot的讨论）](https://blog.csdn.net/m0_37870649/article/details/104551969)\n[kaggle编码categorical feature总结](https://zhuanlan.zhihu.com/p/40231966)\n[TF-IDF算法介绍及实现](https://blog.csdn.net/asialee_bird/article/details/81486700)\n[Python中的TfidfVectorizer参数解析](https://blog.csdn.net/laobai1015/article/details/80451371)\n[关于target encoding与count encoding](https://blog.csdn.net/ssswill/article/details/90271293)","tags":["DataGame"]},{"title":"High-Order Information Matters Learning Relation and Topology for Occluded Person Re-Identification(CVPR2020)(泛读)","url":"/2021/01/29/16088026276866/","content":"# High-Order Information Matters: Learning Relation and Topology for Occluded Person Re-Identification(CVPR2020)(泛读)\n\nhttps://arxiv.org/abs/2003.08177\n## 解决方案\n主要解决遮蔽现象。\n整体思路可以再各种跟验证相关的任务中去套用。\n\n\n针对遮蔽数据集Occluded-DukeMTMC\n提出三阶段模型：\n- 关键点局部特征提取 (关键点数据集或者关键点识别模型)\n- 图卷积融合关键点特征\n- 基于图匹配的方式来计算相似度并训练模型\n完成特征提取，重点解决遮蔽问题。\n\n![](https://i.loli.net/2021/03/22/pGFgesWTlOBR7fH.png)\n关键点可以学习人体走路运动的先验知识。\n\n1、关键点局部特征提取 \n关键点数据集或者关键点识别模型\n\n2、图卷积融合贯机电特征\n传统算法是点与点进行匹配，但当两个图像遮蔽位置不一致时，就没法进行对比关键点。\n图卷积就会好好利用未被遮挡的区域。\n\n3、基于图匹配的方式来计算相似度并训练模型\n匹配图像中哪些能用哪些不能用，能用的该怎么用，不能用的该怎么减少。\n计算U匹配矩阵(13x13)关键点。\n\n![](https://i.loli.net/2021/03/22/JxhiZMtfjGT2w1c.png)\n\n### 第一阶段S(关键点局部特征提取)\n根据CNN提取到feature map和十三个关键点的信息。\nlocal是十三个关键点信息。\nglobal是feature map(global average pooling得到)\n![-w417](https://i.loli.net/2021/03/22/82pZIRDka5mWf6B.png)\n热度图是关键点乘以feature map\n选择一个pose estimation模型即可，得到的是各个关键点的热度图信息，通过热度图得到原始特征图的局部信息。\n![](https://i.loli.net/2021/03/22/RexpiASI2hQcVBK.png)\n用feature map和关键点信息相乘，相当于在特征图中把信息画出来。\n得到一共十四个特征+global\n把local和global都进行训练，添加多个损失。\n\n### 第二阶段R(如何利用各个点的特征，用图卷积局部特征关系整合)\n![](https://i.loli.net/2021/03/22/3axhY2EUwpvDzSW.png)\n做特征的整合，得到的还是local和global，是一个有了相互关系的拓扑结构，相当于一个attention机制。\n将得到的邻接矩阵A来指导每个关键点特征如何跟其他关键点特征进行计算，并且A矩阵也要进行学习。\n\n如何利用好局部特征？加入关系\n先初始化邻接矩阵来进行图卷积，邻接矩阵在学习过程中更新，综合利用各关键点直接的信息。\n\n如何才能更好的针对每个输入利用不同的局部特征？没有边的就不用\n\n和整体差异越大的，越离群。\n利用差异特征来学习邻居矩阵A\n有了A就能开始图卷积了，用他来指导如何利用不同关键点的特征进行组合，最终再与输入的局部特征进行整合。\n\n![](https://i.loli.net/2021/03/22/ILAd2y5hYOR6vbt.png)\n$K=13$ 13个关键点\n$V_l^{in}$ 关键点特征向量 $batch*13*2048$\n$V_g^{in}$ 全局特征向量   $1*13*2048$\nrepeat调整向量大小把$1*13*2048$变成$batch*13*2048$后做减法\n然后经过abs绝对值、bn层、fc层后得到一个$K*K$的矩阵$A^{adp}$。\n用学习到的$A^{adp}$和邻接矩阵A做乘法。$A'=A^{adp}*A$\n之后在用$V_l^{in}$和上面相乘得到的结果当成$V_l^{in}*A'$相当于图卷积过程。\n再将本身特征$V_l^{in}$和图卷积后的关系特征相加。\n再concat全局特征向量$V_g^{in}$得到输出\n\n\n\n\n\n\n\n\n\n\n\n### 第三阶段T(图匹配，相似度计算)\n\n输入两张图像(经过了前两阶段后的结果)\n\nGraph Matching 计算一个14X14的相似度矩阵U，对一下关键点。表示两个图之间的关系。\n![-w454](https://i.loli.net/2021/03/22/WkqcwQo9avTXibR.png)\n进入了新的验证损失函数。\n就是sigmoid(emb1,emb2)的结果。\n\n![](https://i.loli.net/2021/03/22/s2ZIxedCYqESvk7.png)\n输入两个编码后的特征向量。\n先经过fc+relu提取下特征，再进行图匹配得出相似度矩阵U。\n然后是一个交叉cross的过程，分别交叉来得到个子匹配的特征结果。知道了哪里该匹配哪里不该，再进过fc+relu得到最终特征。\n\n## 参考文献\n[这篇的旷世推文](https://mp.weixin.qq.com/s/EhAeaA68Ek27EptkTfZiBQ)\n[【CVPR2020】：High-Order Information Matters: Learning Relation and Topology for Occluded Person Re-Ide](https://blog.csdn.net/baidu_41617231/article/details/107421943)\n[【唐宇迪】CVPR2020最新行人重识别论文解读](https://www.bilibili.com/video/BV1764y1c7jZ?p=3)\n[图解行人重识别论文系列](https://www.bilibili.com/video/BV13W411K7jM?from=search&seid=5052861388779194545)","tags":["GNN&cv"]}]