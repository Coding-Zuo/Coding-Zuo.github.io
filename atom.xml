<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Coding-Zuo</title>
  
  <subtitle>Coding And Studying</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-08-06T07:43:47.313Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Coding-Zuo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Docker学习(三)</title>
    <link href="http://example.com/2021/08/05/Docker%E5%AD%A6%E4%B9%A0-%E4%B8%89/"/>
    <id>http://example.com/2021/08/05/Docker%E5%AD%A6%E4%B9%A0-%E4%B8%89/</id>
    <published>2021-08-05T11:33:59.000Z</published>
    <updated>2021-08-06T07:43:47.313Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker学习-三"><a href="#Docker学习-三" class="headerlink" title="Docker学习(三)"></a>Docker学习(三)</h1><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><h3 id="portainer-不常用"><a href="#portainer-不常用" class="headerlink" title="portainer (不常用)"></a>portainer (不常用)</h3><p>Docker的图形化界面管理工具</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 外部8088 内部9000 </span><br><span class="line"># -v 挂载</span><br><span class="line">docker run -d -p 8088:9000\</span><br><span class="line">--restart&#x3D;always -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock --privileged&#x3D;true portainer&#x2F;portainer</span><br></pre></td></tr></table></figure><h3 id="Rancher-CI-CD再用"><a href="#Rancher-CI-CD再用" class="headerlink" title="Rancher(CI/CD再用)"></a>Rancher(CI/CD再用)</h3><hr><h2 id="镜像是什么"><a href="#镜像是什么" class="headerlink" title="镜像是什么"></a>镜像是什么</h2><p>是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件。它包含某个软件所需的所有内容，包括代码、运行时、库、环境变量和配置文件。</p><p>如何得到：</p><ul><li>远程仓库下载</li><li>拷贝</li><li>自己制作一个镜像DockerFile</li></ul><h3 id="镜像加载原理"><a href="#镜像加载原理" class="headerlink" title="镜像加载原理"></a>镜像加载原理</h3><blockquote><p>UnionFS(联合文件系统)</p></blockquote><p>我们下载的时候看到的一层层就是这个</p><p>UnionFS是一种分层、轻量级且高性能的文件系统，它支持文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下.</p><p>特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。</p><p>docker的镜像实际上由一层一层的文件系统组成，这种层级文件系统就是UnionFS</p><p>bootfs(boot file system)，在Docker镜像的最底层是bootfs，这一层与典型的Linux、Unix系统是一样的，它主要包含bootloader和kernel，bootloader主要是引导加载kernel，Linux刚启动时会加载bootfs文件系统。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载掉bootfs</p><p>rootfs(root file system)，在bootfs之上，包含的是Linux系统中的/dev /proc /bin /etc 等标准目录和文件，rootfs就是各种不同的操作系统发行版，如ubantu，centos</p><p>对于一个精简的OS，rootfs可以很小，只包含最基本的命令，因为底层直接用host的kernel。</p><h2 id="分层理解"><a href="#分层理解" class="headerlink" title="分层理解"></a>分层理解</h2><blockquote><p>分层的镜像</p></blockquote><p>下载的日志输出，可以看到是一层一层的在下载</p><p><img src="https://i.loli.net/2021/08/05/SmgOoLaUYiwG6sr.png" alt=""></p><p>为什么采用这种分层结构 ？</p><p>最大的好处就是资源共享，比如有多个镜像都从相同的Base镜像构建而来，那么宿主机只需在磁盘上保留一份base镜像，同时内存中也只需要加载一份base镜像，这样就可以为所有的容器服务了，而且镜像的每一层都可以被共享。</p><p>查看镜像分层方式可以通过docker image inspect命令</p><blockquote><p>特点</p></blockquote><p>Docker镜像都是只读的，当容器启动时，一个新的可写层被加载到镜像的顶部。</p><p>这一层就是通常说的容器层，容器之下的都叫镜像层。</p><p><img src="https://i.loli.net/2021/08/05/Bo3v1qXJnadp5Kh.png" alt=""></p><h2 id="如何commit镜像"><a href="#如何commit镜像" class="headerlink" title="如何commit镜像"></a>如何commit镜像</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker commit 提交容器成为一个新的副本</span><br><span class="line"># 和git类似</span><br><span class="line">docker commit -m&#x3D;&quot;提交的描述信息&quot; -a&#x3D;&quot;作者&quot; 容器id 容器镜像名:[TAG]</span><br></pre></td></tr></table></figure><h2 id="容器数据卷"><a href="#容器数据卷" class="headerlink" title="容器数据卷"></a>容器数据卷</h2><p>数据？如果数据都在容器中，那么容器删除，数据就会丢失！需求：数据可以持久</p><p>容器之间可以有一个数据共享的技术，docker容器中产生的数据，同步到本地</p><p>目录挂载，将容器内的目录挂载到linux上。</p><p><img src="https://i.loli.net/2021/08/06/cCQPLRghqlwpUZr.png" alt=""></p><p>总结一句话，容器的持久化和同步操作，容器间也是可以数据共享的。</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><blockquote><p>方式1：直接使用命令挂载 -v</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-v 主机目录:容器内目录</span><br><span class="line">-p 主机端口:容器内端口</span><br><span class="line"></span><br><span class="line">docker run -it -v &#x2F;home&#x2F;ceshi:&#x2F;home centos &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/06/TpZcrXgRi3SV9hB.png" alt=""></p><p>是双向的同步，哪怕容器已经停止。</p><p>好处：以后修改只需在本地修改即可。</p><h3 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h3><p>思考：mysql的数据持久化问题 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss# docker pull mysql:5.7</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行，需要数据挂载</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装启动mysql时，需要配置密码的！</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -d 后台运行</span></span><br><span class="line">(base) root@linux:/home/cpss# docker run -d -p 3310:3306 -v /data2/mysql/conf:/etc/mysql/conf.d -v /data2/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root --name mysql01 mysql:5.7</span><br><span class="line">72180bd20207e871aebdc0a06fddfe10e30d39561620c78558328b9ac0a30b9c</span><br></pre></td></tr></table></figure><h3 id="具名和匿名挂载"><a href="#具名和匿名挂载" class="headerlink" title="具名和匿名挂载"></a>具名和匿名挂载</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 匿名挂载</span></span><br><span class="line">-v 容器内路径</span><br><span class="line">docker run -d -P --name nginx01 -v /etc/nginx nginx</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看所有的volume情况，匿名卷挂载</span></span><br><span class="line">(base) root@linux:/data2/mysql# docker volume ls</span><br><span class="line"><span class="meta">#</span><span class="bash"> 这种就是匿名挂载，在-v只写了容器内的路径，没有写容器外的路径！</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 具名挂载</span></span><br><span class="line">docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx nginx</span><br><span class="line">(base) root@linux:/data2/mysql# docker volume ls</span><br><span class="line">DRIVER    VOLUME NAME</span><br><span class="line">local     juming-nginx</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过-v 卷名：容器内路径</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看一下这个卷</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/06/HStUJkLaIY9sXm7.png" alt=""></p><p>所有的docker容器内的卷，没有指定目录的情况下都是在 /var/lib/docker/volumes/xxxxx/_data</p><p>通过具名挂载可以方便的找到一个卷，大多数情况在使用的是具名挂载。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如何确定是具名挂载还是匿名挂载， 还是指定路径挂载</span></span><br><span class="line">-v 容器内路径 <span class="comment"># 匿名挂载</span></span><br><span class="line">-v 卷名：容器内路径 <span class="comment"># 具名挂载</span></span><br><span class="line">-v /宿主机路径:容器内路径 <span class="comment"># 指定路径挂载</span></span><br></pre></td></tr></table></figure><p>扩展</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:ro nginx</span><br><span class="line">docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:rw nginx</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过 -v 容器内路径:ro rw改变读写权限</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  容器对我们挂载出来的内容就有限定了</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ro <span class="built_in">readonly</span> 只要看到ro就说明这个路径只能通过宿主机来操作，容器内部是无法操作的。</span></span><br></pre></td></tr></table></figure><blockquote><p>方式二、dockerfile 创建镜像时就挂载出来</p></blockquote><p>Dockerfile 就是用来构建docker镜像的构建文件</p><p>通过脚本可以生成镜像，镜像是一层一层的，脚本一个个的命令，每个命令都是一层</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/data2/docker-volume# pwd</span><br><span class="line">/data2/docker-volume</span><br><span class="line">(base) root@linux:/data2/docker-volume# vim dockerfile1</span><br><span class="line">(base) root@linux:/data2/docker-volume# cat dockerfile1 </span><br><span class="line">FROM centos</span><br><span class="line"></span><br><span class="line">VOLUME [&quot;volume01&quot;,&quot;volume02&quot;]</span><br><span class="line"></span><br><span class="line">CMD echo &quot;---end---&quot;</span><br><span class="line"></span><br><span class="line">CMD /bin/bash</span><br><span class="line">(base) root@linux:/data2/docker-volume# docker build -f dockerfile1 -t zuo/centos:1.0 .</span><br><span class="line">Sending build context to Docker daemon  2.048kB</span><br><span class="line">Step 1/4 : FROM centos</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 300e315adb2f</span></span><br><span class="line">Step 2/4 : VOLUME [&quot;volume01&quot;,&quot;volume02&quot;]</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> 26da05b75834</span></span><br><span class="line">Removing intermediate container 26da05b75834</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 5ae4812f35a4</span></span><br><span class="line">Step 3/4 : CMD echo &quot;---end---&quot;</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> 29c52fec2f47</span></span><br><span class="line">Removing intermediate container 29c52fec2f47</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> cb1793533f3d</span></span><br><span class="line">Step 4/4 : CMD /bin/bash</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> c4bc1543fe44</span></span><br><span class="line">Removing intermediate container c4bc1543fe44</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> c635584bb2a8</span></span><br><span class="line">Successfully built c635584bb2a8</span><br><span class="line">Successfully tagged zuo/centos:1.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(base) root@linux:/data2/docker-volume# docker images</span><br><span class="line">REPOSITORY      TAG       IMAGE ID       CREATED          SIZE</span><br><span class="line">zuo/centos      1.0       c635584bb2a8   59 seconds ago   209MB</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建dockerfile文件，名字可以随机</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 文件中的内容 指令(大写) 参数</span></span><br><span class="line">FROM centos</span><br><span class="line">每个命令就是镜像的一层</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/06/9LG7RVIYzOsK4Sc.png" alt=""></p><p>这个卷是匿名挂载，一定有外部的目录</p><p><img src="https://i.loli.net/2021/08/06/g9H4moazDAIJUTj.png" alt=""></p><h3 id="数据卷容器"><a href="#数据卷容器" class="headerlink" title="数据卷容器"></a>数据卷容器</h3><p>两个mysql同步数据 —volumes-from</p><p><img src="https://i.loli.net/2021/08/06/nZx8FVqjUbBS1I2.png" alt=""></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动3个容器，通过我们刚才自己的写镜像启动</span></span><br><span class="line">(base) root@linux:/data2/docker-volume# docker run -it --name docker01 zuo/centos:1.0</span><br><span class="line"></span><br><span class="line">(base) root@linux:/data2/docker-volume# docker run -it --name docker02 --volumes-from docker01 zuo/centos:1.0</span><br></pre></td></tr></table></figure><p>容器之间配置信息的传递，数据卷容器的生命周期一直持续到没有容器为止</p><p>一旦持久化到了本地，本地的数据是不会删除的。</p><h2 id="DockerFile"><a href="#DockerFile" class="headerlink" title="DockerFile"></a>DockerFile</h2><p>dockerfile 是用来构建docker镜像的文件，命令参数脚本</p><p>构建步骤：</p><ul><li>编写一个dockerfile文件</li><li>docker build构建成为一个镜像</li><li>docker run 运行镜像</li><li>docker push 发布镜像(docker hub，阿里云镜像仓库)</li></ul><h3 id="构建过程"><a href="#构建过程" class="headerlink" title="构建过程"></a>构建过程</h3><p>基础知识</p><ul><li>每个保留关键字指令都是大写字母</li><li>执行从上到下顺序执行</li><li>每个指令都会创建提交一个新的镜像层</li></ul><p><img src="https://i.loli.net/2021/08/06/3JdmShcx4r6znqw.png" alt=""></p><p>dockerfile是面向开发的，我们以后要发布项目做镜像，要写。</p><h3 id="指令"><a href="#指令" class="headerlink" title="指令"></a>指令</h3><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span>          <span class="comment"># 基础镜像，一切从这里开始构建</span></span><br><span class="line"><span class="keyword">MAINTAINER</span>    <span class="comment"># 镜像是谁写的，姓名+邮箱</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash">           <span class="comment"># 镜像构建时要运行的命令</span></span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash">           <span class="comment"># 步骤，添加内容</span></span></span><br><span class="line">WORKERDIR     <span class="comment"># 镜像的工作目录</span></span><br><span class="line"><span class="keyword">VOLUME</span><span class="bash">         <span class="comment"># 挂载的目录</span></span></span><br><span class="line"><span class="keyword">EXPOSE</span>         <span class="comment"># 暴露端口</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash">            <span class="comment"># 指定容器启动时需要运行的命令,只有最后一个会生效，可被替代</span></span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash">     <span class="comment"># 指定容器启动时需要运行的命令，可以追加命令</span></span></span><br><span class="line"><span class="keyword">ONBUILD</span>        <span class="comment"># 当构建一个被继承 Dockerfile </span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash">           <span class="comment">#  类似ADD 将文件拷贝到镜像中</span></span></span><br><span class="line"><span class="keyword">ENV</span>            <span class="comment"># 构建的时候设置环境变量</span></span><br></pre></td></tr></table></figure><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ubuntu:<span class="number">18.04</span>  <span class="comment"># 指定基础镜像 如果为scratch代表从下一行开始是镜像的第一层</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&#x27;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&#x27;</span> &gt; /usr/share/nginx/html/index.html <span class="comment"># RUN指令用来执行命令，每一行代表新建docker的一个layer</span></span></span><br><span class="line"><span class="comment">#能在一个layer内执行的指令就通过&amp;&amp; 进行联接，并可应用shell中的换行符\</span></span><br><span class="line"><span class="comment">#在dockerfile每层都要检查，下载，展开的多余文件，以及缓存等能删除的尽量都去掉</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> <span class="comment">#COPY 指令将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置。</span></span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> package.json /usr/src/app/ <span class="comment"># 将当前上下文路径的json文件复制到image的指定路径下</span></span></span><br><span class="line"></span><br><span class="line">AND <span class="comment">#丰富了COPY的功能，但是会降低构件image速度，如果不需要自动解压缩，则不推荐使用该指令</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> <span class="comment"># ？？？？？？？？？ 还没理解</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> <span class="comment"># 当存在 ENTRYPOINT 后，CMD 的内容将会作为参数传给ENTRYPOINT，从而达到了我们预期的效果。</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> <span class="comment">#用来设置环境变量  ENV &lt;key&gt; &lt;value&gt; 或 ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;...</span></span><br><span class="line"><span class="keyword">ENV</span> VERSION=<span class="number">1.0</span> DEBUG=on \</span><br><span class="line">    NAME=<span class="string">&quot;Happy ONE&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> LD_LIBRARY_PATH=\</span><br><span class="line">$LD_LIBRARY_PATH:\</span><br><span class="line">$NAME/alpha</span><br><span class="line"></span><br><span class="line"><span class="keyword">ARG</span> <span class="comment"># ARG &lt;参数名&gt;[=&lt;默认值&gt;] Dockerfile 中的 ARG 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令 docker build 中用 --build-arg &lt;参数名&gt;=&lt;值&gt; 来覆盖</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ARG</span> DOCKER_USERNAME=library <span class="comment"># 注意：在FROM之前定义的ARG参数，会消失，在FROM后需要重新定义</span></span><br><span class="line"><span class="comment"># ARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。但是不要因此就使用 ARG 保存密码之类的信息，因为 docker history 还是可以看到所有值的。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">VOLUME</span><span class="bash"> <span class="comment"># 用于指定image启动时挂载到容器中的默认卷，而不是写入容器存储层</span></span></span><br><span class="line"><span class="keyword">VOLUME</span><span class="bash"> /data <span class="comment"># VOLUME [&quot;&lt;路径1&gt;&quot;, &quot;&lt;路径2&gt;&quot;...] 或 VOLUME &lt;路径&gt;</span></span></span><br><span class="line">在image启动时可替换</span><br><span class="line">docker <span class="keyword">run</span><span class="bash"> -d -v mydata:/data xxxx <span class="comment">#其中的 -v mydata:/data 就是挂载宿主机的卷到容器内</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="comment"># EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...] EXPOSE 指令是声明容器运行时提供服务的端口，这只是一个声明，在容器运行时并不会因为这个声明应用就会开启这个端口的服务</span></span><br><span class="line"><span class="comment"># 在 Dockerfile 中写入这样的声明有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> <span class="comment"># WORKDIR &lt;工作目录路径&gt; 使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">USER</span>  <span class="comment"># USER &lt;用户名&gt;[:&lt;用户组&gt;] 指定当前用户</span></span><br><span class="line"><span class="keyword">HEALTHCHECK</span></span><br><span class="line"><span class="bash">ONBUILD</span></span><br><span class="line">LEBEL</span><br><span class="line"><span class="keyword">SHELL</span><span class="bash"> <span class="comment">#SHELL 指令可以指定 RUN ENTRYPOINT CMD 指令的 shell，Linux 中默认为 [&quot;/bin/sh&quot;, &quot;-c&quot;]   </span></span></span><br><span class="line">Dockerfile 多阶段构建</span><br></pre></td></tr></table></figure><blockquote><p>创建一个字节的centos</p></blockquote><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> centos</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Docker网络"><a href="#Docker网络" class="headerlink" title="Docker网络"></a>Docker网络</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Docker学习-三&quot;&gt;&lt;a href=&quot;#Docker学习-三&quot; class=&quot;headerlink&quot; title=&quot;Docker学习(三)&quot;&gt;&lt;/a&gt;Docker学习(三)&lt;/h1&gt;&lt;h2 id=&quot;可视化&quot;&gt;&lt;a href=&quot;#可视化&quot; class=&quot;head</summary>
      
    
    
    
    
    <category term="Docker" scheme="http://example.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker学习(二)例子练习</title>
    <link href="http://example.com/2021/08/01/Docker%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E4%BE%8B%E5%AD%90%E7%BB%83%E4%B9%A0/"/>
    <id>http://example.com/2021/08/01/Docker%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E4%BE%8B%E5%AD%90%E7%BB%83%E4%B9%A0/</id>
    <published>2021-08-01T10:37:05.000Z</published>
    <updated>2021-08-05T11:34:55.748Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker学习-二-例子练习"><a href="#Docker学习-二-例子练习" class="headerlink" title="Docker学习(二)例子练习"></a>Docker学习(二)例子练习</h1><h2 id="部署Nginx"><a href="#部署Nginx" class="headerlink" title="部署Nginx"></a>部署Nginx</h2><ul><li>搜索镜像去docker hub上</li><li>下载镜像 docker pull nginx</li><li>docker run -d 后台运行 —name nginx01 -p 10024:80</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;home&#x2F;cpss# docker run -d --name nginx01 -p 10024:80 nginx</span><br><span class="line">84960293d8409dc9f7e70be88027c2149ece57d7cf02dc4d71eb81fe1651fc96</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;home&#x2F;cpss# docker ps</span><br><span class="line">CONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS                                     NAMES</span><br><span class="line">84960293d840   nginx     &quot;&#x2F;docker-entrypoint.…&quot;   21 seconds ago   Up 20 seconds   0.0.0.0:10024-&gt;80&#x2F;tcp, :::10024-&gt;80&#x2F;tcp   nginx01</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;home&#x2F;cpss# curl localhost:10024</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;&#x2F;title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;&#x2F;style&gt;</span><br><span class="line">&lt;&#x2F;head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;&#x2F;h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;&#x2F;p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;nginx.org&#x2F;&quot;&gt;nginx.org&lt;&#x2F;a&gt;.&lt;br&#x2F;&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;nginx.com&#x2F;&quot;&gt;nginx.com&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;</span><br><span class="line">&lt;&#x2F;body&gt;</span><br><span class="line">&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure><p>-p 暴露端口的概念</p><p><img src="https://i.loli.net/2021/08/04/4Bx8PzGlrD1T6vA.png" alt=""></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it nginx01 /bin/bash 进入容器</span><br><span class="line">root@84960293d840:/# whereis nginx</span><br><span class="line">nginx: /usr/sbin/nginx /usr/lib/nginx /etc/nginx /usr/share/nginx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(base) root@localhost:/home/cpss# docker ps</span><br><span class="line">CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS                                     NAMES</span><br><span class="line">84960293d840   nginx     &quot;/docker-entrypoint.…&quot;   8 minutes ago   Up 8 minutes   0.0.0.0:10024-&gt;80/tcp, :::10024-&gt;80/tcp   nginx01</span><br><span class="line">(base) root@localhost:/home/cpss# docker stop 84960293d840</span><br><span class="line">84960293d840</span><br></pre></td></tr></table></figure><p>思考：每次改动nginx配置文件，都需要进入容器内部，十分麻烦</p><p>可以在容器外部提供一个映射路径，达到在容器修改文件名，内部容器就可以自动修改。</p><p>这个技术是  -v 数据卷技术  </p><h2 id="部署-ES-Kibana"><a href="#部署-ES-Kibana" class="headerlink" title="部署 ES+Kibana"></a>部署 ES+Kibana</h2><p>ES暴露端口很多，也耗内存，数据一般需要放到安全目录，挂载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> --net somenetwork 网络配置</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --rm 用完就删掉</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 elasticsearch 比较耗内存</span></span><br><span class="line">docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; elasticsearch:7.14.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CONTAINER ID   NAME            CPU %     MEM USAGE / LIMIT     MEM %     NET I/O           BLOCK I/O       PIDS</span><br><span class="line">a9057d9c6e50   elasticsearch   2.54%     32.49GiB / 125.8GiB   25.83%    11.1kB / 1.94kB   100MB / 292MB   98</span><br><span class="line"></span><br><span class="line">(base) root@linux:/home/cpss# curl localhost:9200</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot; : &quot;a9057d9c6e50&quot;,</span><br><span class="line">  &quot;cluster_name&quot; : &quot;docker-cluster&quot;,</span><br><span class="line">  &quot;cluster_uuid&quot; : &quot;lkLPT_ssQ2CV30B55gn4bg&quot;,</span><br><span class="line">  &quot;version&quot; : &#123;</span><br><span class="line">    &quot;number&quot; : &quot;7.14.0&quot;,</span><br><span class="line">    &quot;build_flavor&quot; : &quot;default&quot;,</span><br><span class="line">    &quot;build_type&quot; : &quot;docker&quot;,</span><br><span class="line">    &quot;build_hash&quot; : &quot;dd5a0a2acaa2045ff9624f3729fc8a6f40835aa1&quot;,</span><br><span class="line">    &quot;build_date&quot; : &quot;2021-07-29T20:49:32.864135063Z&quot;,</span><br><span class="line">    &quot;build_snapshot&quot; : false,</span><br><span class="line">    &quot;lucene_version&quot; : &quot;8.9.0&quot;,</span><br><span class="line">    &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;,</span><br><span class="line">    &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;tagline&quot; : &quot;You Know, for Search&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 增加内存限制，修改卑职文件 -e 环境修改</span></span><br><span class="line">docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; -e ES_JAVA_OPTS=&quot;-Xms64m -Xmx512m&quot;   elasticsearch:7.14.0</span><br><span class="line"></span><br><span class="line">CONTAINER ID   NAME             CPU %     MEM USAGE / LIMIT     MEM %     NET I/O       BLOCK I/O         PIDS</span><br><span class="line">e40105c39e81   elasticsearch1   281.21%   676.5MiB / 125.8GiB   0.53%     2.84kB / 0B   26.9MB / 1.22MB   103</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/05/PiIaTA4C1DsMZz3.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Docker学习-二-例子练习&quot;&gt;&lt;a href=&quot;#Docker学习-二-例子练习&quot; class=&quot;headerlink&quot; title=&quot;Docker学习(二)例子练习&quot;&gt;&lt;/a&gt;Docker学习(二)例子练习&lt;/h1&gt;&lt;h2 id=&quot;部署Nginx&quot;&gt;&lt;a h</summary>
      
    
    
    
    
    <category term="Docker" scheme="http://example.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker学习(一)</title>
    <link href="http://example.com/2021/07/30/Docker%E5%AD%A6%E4%B9%A0-%E4%B8%80/"/>
    <id>http://example.com/2021/07/30/Docker%E5%AD%A6%E4%B9%A0-%E4%B8%80/</id>
    <published>2021-07-30T01:27:56.000Z</published>
    <updated>2021-08-01T10:36:24.367Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker学习-一"><a href="#Docker学习-一" class="headerlink" title="Docker学习(一)"></a>Docker学习(一)</h1><p>文档：<a href="https://docs.docker.com/">https://docs.docker.com/</a></p><p>Hub : <a href="https://hub.docker.com/">https://hub.docker.com/</a></p><h2 id="路线"><a href="#路线" class="headerlink" title="路线"></a>路线</h2><ul><li>Docker概述</li><li>Docker安装</li><li>Docker命令</li><li><ul><li>镜像命令</li><li>容器命令</li><li>操作命令</li><li>……</li></ul></li><li>Docker镜像</li><li>容器数据卷</li><li>DockerFile</li><li>Docker网络原理</li><li>Docker Compose</li><li>Docker Swarm</li><li>CI\CD Jenkins</li></ul><h2 id="Docker概述"><a href="#Docker概述" class="headerlink" title="Docker概述"></a>Docker概述</h2><p>Docker为什么会出现？</p><blockquote><p>环境配置十分麻烦，每个机器都要部署环境，很难跨平台，集群环境更浪费时间。项目能不能带上环境打包(镜像)。</p></blockquote><p>能干嘛？</p><blockquote><p>之前的虚拟机技术,浪费资源比较多</p></blockquote><p><img src="https://i.loli.net/2021/07/30/cG1EzWbCX5ArsTf.png" alt=""></p><p>缺点：</p><ul><li>资源占用多</li><li>冗余步骤多</li><li>启动很慢</li></ul><blockquote><p>容器化技术</p></blockquote><p>不是模拟一个完整的操作系统</p><p><img src="https://i.loli.net/2021/07/30/MojI6CL2lqRapYW.png" alt=""></p><p>不同之处：</p><ul><li>传统虚拟机，虚拟出一套硬件，运行一个完整的操作系统，然后在这个系统上运行安装软件</li><li>容器的应用直接运行在宿主机上的内核中，容器是没有自己的内核的，没有虚拟硬件，比较轻便</li><li>每个容器间是互相隔离的，每个容器内都有一共自己的文件系统，互不影响。</li></ul><blockquote><p>DevOps（开发 运维）</p></blockquote><p>更快速的交付和部署</p><p>传统：一堆帮助文档，安装程序</p><p>Docker: 打包镜像发布测试，一键运行</p><p>更便捷的升级和扩容缩容，更高效的计算资源利用，测试环境都高度一致。</p><p>Docker是内核级别的虚拟化，可以再一个物理机上运行很多的容器实例。</p><h2 id="Docker-基本组成"><a href="#Docker-基本组成" class="headerlink" title="Docker 基本组成"></a>Docker 基本组成</h2><p><img src="https://i.loli.net/2021/07/30/XMPGFCdjvi96tey.png" alt=""></p><p>从左到右，依次是客户端、服务器和仓库。</p><ul><li><p>镜像（Image）：docker镜像就好比是一个模板，可以通过这个模板来创建容器服务。如：tomcat镜像—-&gt;run—-&gt;tomcat01容器。通过这个镜像可以创建多个容器，最终服务运行或者项目运行就是在容器中的</p></li><li><p>容器（Containers）：Docker利用容器技术，可以独立运行一个或者一组应用，通过镜像来创建。启动，</p><p>停止，删除基本命令。目前可把这个容器简单理解为就是一个简易的linux系统。</p></li><li><p>仓库（Repository）：存放镜像的地方。分为公有仓库和私有仓库，和GitHub差不多。Docker hub默认是国外的，可以配阿里云镜像加速。</p></li></ul><h2 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h2><p><img src="https://i.loli.net/2021/07/30/oyDtNArhC1qmlXV.png" alt=""></p><p>如何查看hello world镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;home&#x2F;cpss# docker images</span><br><span class="line">REPOSITORY      TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">hello-world     latest    d1165f221234   4 months ago    13.3kB</span><br><span class="line">studyfang&#x2F;hgn   latest    37553493935b   10 months ago   8.88GB</span><br></pre></td></tr></table></figure><p>docker默认工作路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;home&#x2F;cpss# ls &#x2F;var&#x2F;lib&#x2F;docker&#x2F;</span><br><span class="line">buildkit  containers  image  network  overlay2  plugins  runtimes  swarm  tmp  trust  volumes</span><br></pre></td></tr></table></figure><h2 id="镜像加速"><a href="#镜像加速" class="headerlink" title="镜像加速"></a>镜像加速</h2><p>创建或修改 /etc/docker/daemon.json 文件，修改为如下形式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [</span><br><span class="line">    &quot;https:&#x2F;&#x2F;registry.docker-cn.com&quot;,</span><br><span class="line">    &quot;http:&#x2F;&#x2F;hub-mirror.c.163.com&quot;,</span><br><span class="line">    &quot;https:&#x2F;&#x2F;docker.mirrors.ustc.edu.cn&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;etc&#x2F;docker# vim daemon.json</span><br><span class="line">(base) root@localhost:&#x2F;etc&#x2F;docker# systemctl daemon-reload</span><br><span class="line">(base) root@localhost:&#x2F;etc&#x2F;docker# systemctl restart docker</span><br><span class="line">(base) root@localhost:&#x2F;etc&#x2F;docker# systemctl status docker</span><br></pre></td></tr></table></figure><p>使用docker info 查看镜像改变。</p><p><img src="https://i.loli.net/2021/07/30/AJiXGsvMfug9jc5.png" alt=""></p><h2 id="run流程原理"><a href="#run流程原理" class="headerlink" title="run流程原理"></a>run流程原理</h2><p> <img src="https://i.loli.net/2021/07/30/6jbSgOziPhAYNkd.png" alt=""></p><p>docker是怎么工作的？</p><blockquote><p>docker是一个client-server结构的系统，docker的守护进程运行在主机上，通过socket从客户端访问。</p><p>docker server 接收到docker client的指令就会执行这个命令</p></blockquote><p><img src="https://i.loli.net/2021/07/30/vFZk7yGAHVgfad1.png" alt=""></p><p>docker为什么比VM快？</p><blockquote><p>docker有比虚拟机更少的抽象层。</p><p>docker利用的是宿主机的内核，vm需要Guest OS</p></blockquote><p><img src="https://i.loli.net/2021/07/30/wormWUzExPMNBsh.png" style="zoom:150%;" /></p><p>所以新建一个容器的时候，docker不需要向虚拟机一样重新加载一个操作系统内核。避免引导操作，虚拟机是加载GuestOS，docker是利用宿主机的操作系统，省略了这个复杂的过程。</p><p><img src="https://i.loli.net/2021/07/30/ode6RsjDH1Y5yF3.png" style="zoom:150%;" /></p><h2 id="Docker的常用命令"><a href="#Docker的常用命令" class="headerlink" title="Docker的常用命令"></a>Docker的常用命令</h2><h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker version     <span class="comment"># docker的版本信息</span></span><br><span class="line">docker info <span class="comment"># 显示docker的系统信息，包括镜像和容器的数量</span></span><br><span class="line">docker 命令 --<span class="built_in">help</span>  <span class="comment"># 帮助命令</span></span><br></pre></td></tr></table></figure></h2><h3 id="镜像命令"><a href="#镜像命令" class="headerlink" title="镜像命令"></a>镜像命令</h3><p>docker images</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:/home/cpss<span class="comment"># docker images</span></span><br><span class="line">REPOSITORY      TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">hello-world     latest    d1165f221234   4 months ago    13.3kB</span><br><span class="line">studyfang/hgn   latest    37553493935b   10 months ago   8.88GB</span><br></pre></td></tr></table></figure><ul><li><p>REPOSITORY 镜像的仓库源</p></li><li><p>TAG 镜像的标签</p></li><li><p>IMAGE ID 镜像的id</p></li><li><p>CREATED 镜像的创建时间</p></li><li><p>SIZE 镜像大小</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Options:</span><br><span class="line">  -a, --all             Show all images (default hides intermediate images)</span><br><span class="line">  -q, --quiet           Only show image IDs</span><br></pre></td></tr></table></figure><p>docker search 搜索镜像</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:/home/cpss# docker search hotpotqa</span><br><span class="line">NAME                                  DESCRIPTION                     STARS     OFFICIAL   AUTOMATED</span><br><span class="line">qipeng/hotpotqa-eval                                                  0                    </span><br><span class="line">studyfang/hotpotqa                                                    0                    </span><br><span class="line">qipeng/hotpotqa-base                                                  0                    </span><br><span class="line">tuming1990/hotpotqa-docker                                            0                    </span><br><span class="line">hamishivi/hotpotqa-base               Hotpotqa with extra packages.   0                    </span><br><span class="line">qipeng/hotpotqa_submission_cuda10.2                                   0                    </span><br><span class="line">tswings2018/hotpotqa                  by deng                         0        </span><br></pre></td></tr></table></figure><p>docker pull</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># 下载镜像 docker pull 镜像名[:tag]</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/07/30/r9qWyoDSOTk7QKh.png" alt=""></p><p>docker rmi 删除镜像</p><p>可通过id 或者 名称来删</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi -f 镜像id</span><br></pre></td></tr></table></figure><h3 id="容器命令"><a href="#容器命令" class="headerlink" title="容器命令"></a>容器命令</h3><p>有了镜像才可以创建容器</p><p>这里下载一个centos镜像来测试学习</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull centos</span><br></pre></td></tr></table></figure><p>新建容器并启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">docker run [可选参数] image</span><br><span class="line"><span class="meta">#</span><span class="bash"> 参数说明</span></span><br><span class="line">--name=&quot;Name&quot; 容器名字  tomcat01 tomcat02 用来区分容器</span><br><span class="line">-d            后台方式运行 nohup</span><br><span class="line">-it           使用交互方式运行，进入容器查看内容</span><br><span class="line">-p            指定容器的端口  ip:主机端口:容器端口 主机端口:容器端口(常用)   容器端口</span><br><span class="line">-P随机指定端口</span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试 启动并进入容器</span></span><br><span class="line">(base) root@linux:/home/cpss# docker run -it centos /bin/bash</span><br><span class="line">[root@ef41db25d696 /]# 容器内就是自己的服务器环境</span><br><span class="line"></span><br><span class="line">docker ps # 查看正在运行的容器</span><br><span class="line">docker ps -a # 查看曾经运行过的容器</span><br><span class="line">docker ps -a -n=1 # 显示个数</span><br><span class="line">docker ps -aq # 只显示编号</span><br></pre></td></tr></table></figure><p>退出容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">exit # 直接退出容器并停止</span><br><span class="line">ctrl +p +q # 容器不停止退出</span><br></pre></td></tr></table></figure><p>删除容器</p><p>删除容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker rm 容器id                # 删除指定的容器 不能删除正在运行的容器 -f强制删除</span><br><span class="line">docker rm -f $(docker ps -aq)  # 删除所有的容器</span><br><span class="line"></span><br><span class="line">docker ps -a -q|xargs docker rm # 删除所有的容器</span><br></pre></td></tr></table></figure><p>启动和停止容器的操作</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker start 容器id</span><br><span class="line">docker restart 容器id</span><br><span class="line">docker stop 容器id</span><br><span class="line">docker <span class="built_in">kill</span> 容器</span><br></pre></td></tr></table></figure><h3 id="常用其他命令"><a href="#常用其他命令" class="headerlink" title="常用其他命令"></a>常用其他命令</h3><p>后台启动容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -d centos</span><br><span class="line"><span class="meta">#</span><span class="bash"> 问题 docker ps时发现centos停止了</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 常见的坑，docker 容器使用后台运行，就必须要有一个前台进程。docker发现没有应用就会自动停止。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 容器启动后，发现自己没有提供服务，就会立即停止</span></span><br></pre></td></tr></table></figure><p>查看日志</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss# docker logs --help</span><br><span class="line">Options:</span><br><span class="line">      --details        Show extra details provided to logs</span><br><span class="line">  -f, --follow         Follow log output</span><br><span class="line">      --since string   Show logs since timestamp (e.g. 2013-01-02T13:23:37Z) or relative (e.g. 42m for 42 minutes)</span><br><span class="line">  -n, --tail string    Number of lines to show from the end of the logs (default &quot;all&quot;)</span><br><span class="line">  -t, --timestamps     Show timestamps</span><br><span class="line">      --until string   Show logs before a timestamp (e.g. 2013-01-02T13:23:37Z) or relative (e.g. 42m for 42 minutes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> -tf 显示日志</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --tail number 要显示日志条数</span></span><br><span class="line">docker logs -tf --tail 10 f3c59b35b738</span><br><span class="line"><span class="meta">#</span><span class="bash"> 容器没有日志</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 自己写一段shell</span></span><br><span class="line">(base) root@linux:/home/cpss# docker run -d centos /bin/sh -c &quot;while true;do echo 111;sleep 1;done&quot;</span><br><span class="line">ba0ae87cb0949d44e179f03e2bb3e25a38b394bb98b7aa0f4a1a2b9ad68ca86d</span><br><span class="line">(base) root@linux:/home/cpss# docker ps</span><br><span class="line">CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS        PORTS     NAMES</span><br><span class="line">ba0ae87cb094   centos    &quot;/bin/sh -c &#x27;while t…&quot;   3 seconds ago   Up 1 second             determined_bouman</span><br><span class="line">(base) root@linux:/home/cpss# docker logs -tf --tail 10 ba0ae87cb094</span><br></pre></td></tr></table></figure><p>查看容器中的进程信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker top 容器id</span><br></pre></td></tr></table></figure><p>查看镜像的元数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">docker inspect 容器id</span><br><span class="line"></span><br><span class="line">(base) root@linux:/home/cpss# docker inspect ba0ae87cb094</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;Id&quot;: &quot;ba0ae87cb0949d44e179f03e2bb3e25a38b394bb98b7aa0f4a1a2b9ad68ca86d&quot;,</span><br><span class="line">        &quot;Created&quot;: &quot;2021-08-01T03:10:14.298411164Z&quot;,</span><br><span class="line">        &quot;Path&quot;: &quot;/bin/sh&quot;,</span><br><span class="line">        &quot;Args&quot;: [</span><br><span class="line">            &quot;-c&quot;,</span><br><span class="line">            &quot;while true;do echo 111;sleep 1;done&quot;</span><br><span class="line">        ],</span><br><span class="line">        &quot;State&quot;: &#123;</span><br><span class="line">            &quot;Status&quot;: &quot;exited&quot;,</span><br><span class="line">            &quot;Running&quot;: false,</span><br><span class="line">            &quot;Paused&quot;: false,</span><br><span class="line">            &quot;Restarting&quot;: false,</span><br><span class="line">            &quot;OOMKilled&quot;: false,</span><br><span class="line">            &quot;Dead&quot;: false,</span><br><span class="line">            &quot;Pid&quot;: 0,</span><br><span class="line">            &quot;ExitCode&quot;: 137,</span><br><span class="line">            &quot;Error&quot;: &quot;&quot;,</span><br><span class="line">            &quot;StartedAt&quot;: &quot;2021-08-01T03:10:15.270494437Z&quot;,</span><br><span class="line">            &quot;FinishedAt&quot;: &quot;2021-08-01T03:12:01.287526932Z&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;Image&quot;: &quot;sha256:300e315adb2f96afe5f0b2780b87f28ae95231fe3bdd1e16b9ba606307728f55&quot;,</span><br><span class="line">        .....</span><br><span class="line">]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>进入当前正在运行的容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 我们通常容器都是使用后台方式运行的，需要进入容器，修改一些配置</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 方式1</span></span><br><span class="line">docker exec -it 容器id bashShell</span><br><span class="line"><span class="meta">#</span><span class="bash"> 方式2</span></span><br><span class="line">docker attach 容器id </span><br><span class="line"><span class="meta">#</span><span class="bash"> 区别</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> attach 正在执行的代码 进入正在执行的终端，不会启动新的进程</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">exec</span> 进入容器后开启一个新的终端，可以在里面操作</span></span><br></pre></td></tr></table></figure><p>从容器内拷贝文件到主机上</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 容器停止也可以拷贝，容器在数据就在</span></span><br><span class="line">docker cp 容器id:容器内路径 目的的主机路径</span><br><span class="line"><span class="comment"># 拷贝是一个手动过程，以后可以使用 -v 卷的技术 可以实现自动同步</span></span><br></pre></td></tr></table></figure><h3 id="命令小结"><a href="#命令小结" class="headerlink" title="命令小结"></a>命令小结</h3><p><img src="https://i.loli.net/2021/08/01/ha7fdJZE2jnNIOU.png" alt=""></p><p>现在学的是Images 和 Cotainer里的命令，其他的还没学</p><p><img src="https://i.loli.net/2021/08/01/FH1ZyqXoS3wx2Dg.png" alt=""></p><p><img src="https://i.loli.net/2021/08/01/YTrjW8M1c3I5sQ9.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Docker学习-一&quot;&gt;&lt;a href=&quot;#Docker学习-一&quot; class=&quot;headerlink&quot; title=&quot;Docker学习(一)&quot;&gt;&lt;/a&gt;Docker学习(一)&lt;/h1&gt;&lt;p&gt;文档：&lt;a href=&quot;https://docs.docker.com/</summary>
      
    
    
    
    
    <category term="Docker" scheme="http://example.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>HotpotQA Submission Guide</title>
    <link href="http://example.com/2021/07/28/HotpotQA-Submission-Guide/"/>
    <id>http://example.com/2021/07/28/HotpotQA-Submission-Guide/</id>
    <published>2021-07-28T09:12:56.000Z</published>
    <updated>2021-08-06T07:43:46.495Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HotpotQA-Submission-Guide"><a href="#HotpotQA-Submission-Guide" class="headerlink" title="HotpotQA Submission Guide"></a>HotpotQA Submission Guide</h1><p>记录如何提交模型在HotpotQA test</p><h2 id="codalab安装与注册"><a href="#codalab安装与注册" class="headerlink" title="codalab安装与注册"></a>codalab安装与注册</h2><p>先去注册 <a href="https://worksheets.codalab.org/">https://worksheets.codalab.org/</a></p><p>首先安装codalab</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install codalab -U </span><br></pre></td></tr></table></figure><p>如果ERROR: Cannot uninstall ‘PyYAML’. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.</p><p>使用 pip install codalab -U —ignore-installed PyYAML</p><p>Codalab wiki ： <a href="https://github.com/codalab/codalab-worksheets/wiki">https://github.com/codalab/codalab-worksheets/wiki</a></p><p>注册安装完成后可以再命令行登录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cl work</span><br><span class="line">Requesting access at https://worksheets.codalab.org</span><br><span class="line">Username: guest1</span><br><span class="line">Password:</span><br><span class="line">Currently on worksheet https://worksheets.codalab.org::home-guest1(0x39729afdca6140869a11e055e4cc0649).</span><br></pre></td></tr></table></figure><p><code>cl work</code>命令的意思就是切换工作表（worksheet），默认的工作表指向主页工作表 （<code>home-&lt;username&gt;</code>）。</p><h2 id="先一个例子提交Hotpot-QA的baseline"><a href="#先一个例子提交Hotpot-QA的baseline" class="headerlink" title="先一个例子提交Hotpot QA的baseline"></a>先一个例子提交Hotpot QA的baseline</h2><p>distractor setting 是需要提交代码的，full wiki不需要。先主要攻克 distractor setting吧</p><p>尝试完baseline再上传我自己的模型。</p><blockquote><p>你的分数想要在排行榜上出现，需要预留最多一个月的时间。</p></blockquote><p>在干扰项设置中，要求您将代码提交给Codalab，并根据隐藏的测试集对其进行评估。您应该首先确保您的代码能够正确生成dev的输出和评估结果，以便可以更容易地将您的设置转移到测试集。下面，提供一个提交baseline模型的示例。</p><h3 id="Step-1-Preparing-code-and-data"><a href="#Step-1-Preparing-code-and-data" class="headerlink" title="Step 1: Preparing code and data"></a>Step 1: Preparing code and data</h3><p>首先将基线模型的GitHub存储库克隆到Codalab包中（在codalab上代码是公开的，想要不公开。。）</p><p>在命令行中运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cl run --request-network <span class="string">&#x27;git clone https://github.com/hotpotqa/hotpot.git&#x27;</span> -n repo</span><br></pre></td></tr></table></figure><p>—request-network：需要网络环境， -n是添加别名为repo，以后可以更容易地引用它(而不是每次都使用长UUID)。</p><p>注意这里git克隆的要是https的ssh的路径会失败。</p><p>成功后刷新网页控制台：</p><p><img src="https://i.loli.net/2021/07/28/mo35I4yMLEDawFH.png" alt=""></p><p>然后，我们上传对训练集进行预处理后生成的词汇映射文件。</p><p>创建包含所有必要预处理文件的mappings.zip文件，即idx2char.json、idx2word.json、char2idx.json、word2idx.json、char_emb.json和word_emb.json。这个是baseline运行所需要的。</p><p>作者提供了下载mappings.zip的下载地址：<a href="http://curtis.ml.cmu.edu/datasets/hotpot/mappings.zip">http://curtis.ml.cmu.edu/datasets/hotpot/mappings.zip</a></p><p>要上传数据到Codalab CLI，只需运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cl upload mappings.zip -n mappings</span><br></pre></td></tr></table></figure><p>完全上传后，Codalab会为您解压zip文件。</p><p>当然，还需要上传预先训练好的模型文件。我们已经准备好了预先训练好的文件model.pt。</p><p>下载地址：<a href="http://curtis.ml.cmu.edu/datasets/hotpot/model.pt">http://curtis.ml.cmu.edu/datasets/hotpot/model.pt</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cl upload model.pt -n model</span><br></pre></td></tr></table></figure><p>如果超时就重新执行</p><p><img src="https://i.loli.net/2021/07/28/eDSWJn71hkAIlGr.png" alt=""></p><h3 id="Step-2-Preparing-the-environment"><a href="#Step-2-Preparing-the-environment" class="headerlink" title="Step 2: Preparing the environment"></a>Step 2: Preparing the environment</h3><p>现在基本已经准备好对新的输入进行预测。我们只需要设置代码需要在其中运行的适当环境。</p><blockquote><p>要做到这一点，最简单的方法是使用Docker镜像，我们在<a href="https://hub.docker.com/r/qipeng/hotpotqa-base">qipeng/hotpotqa-base</a>上提供了一个镜像，其中预装了nvidia GPU相关库和Anaconda 3。我们还安装了运行此docker映像中的基线模型所需的所有软件包，这样我们就不必在Codalab包中安装所有东西。</p></blockquote><p>如果确实忘记了环境中的某些内容，也可以在Codalab中轻松设置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cl run -n download_spacy_model --request-docker-image qipeng/hotpotqa-base:gpu --request-network :repo <span class="string">&#x27;cp -r repo/hotpot .; python -m spacy download en&#x27;</span></span><br></pre></td></tr></table></figure><p>注意：由于评估期间禁止使用网络，此捆绑包仅用于演示目的。对于需要下载的软件依赖项，强烈建议下载到您准备的Docker镜像中。</p><p><img src="https://i.loli.net/2021/07/28/JzasC3nmrFNPI1U.png" alt=""></p><h3 id="Step-3-Running-evaluation"><a href="#Step-3-Running-evaluation" class="headerlink" title="Step 3: Running evaluation"></a>Step 3: Running evaluation</h3><p>现在，继续根据刚刚上传的模型进行预测，并评估输出。</p><p>要在dev集上运行上传的基线模型的预测，我们运行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cl run -n predict --request-docker-image qipeng/hotpotqa-base:gpu --request-gpus 1 --request-cpus 4 --request-memory 32g repo:download_spacy_model :mappings input.json:0xbdd8f3 :model <span class="string">&#x27;cp -r repo/hotpot .; cp mappings/* hotpot; mkdir hotpot/model; cp model hotpot/model/model.pt; cp input.json hotpot; cd hotpot; python main.py --mode prepro --data_file input.json --para_limit 2250 --data_split dev; python main.py --mode test --data_split dev --para_limit 2250 --batch_size 24 --init_lr 0.1 --keep_prob 1.0 --sp_lambda 1.0 --save model --prediction_file pred.json; cp pred.json ../;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>让我们看看上面的命令中发生了什么。在第一部分中，我们使用-n predic命名包，并使用指定所需的资源</p><p>—request-docker-image qipeng/hotpotqa-base:gpu</p><p>—request-gpus 1</p><p>—request-cpus 4</p><p>—request-memory 32g</p><p>请注意，您不能在此捆绑包中使用—request-network</p><p>然后指定对其他包的依赖关系。repo:download_spacy_model表示将包download_spacy_model别名为repo</p><p>:mappings指定对捆绑包mapping的依赖关系，而不使用别名。</p><p>input.json:hotpotqa-data//dev_distractor_input_v1.0将输入json文件重命名为input.json，其中包id指向dev json文件。0xbdd8f3</p><p>请不要上传您自己版本的开发集文件并使用它，因为我们依赖官方的开发文件UUID来确定在评估期间用测试集替换什么(如果您使用自己的开发集文件，评估将失败)。</p></blockquote><p>然后，使用一系列cp命令从不同的包复制文件，并以我们的预测脚本可以处理的方式组织它们。</p><p>请注意，可以将每个引用的捆绑包视为当前捆绑包中的一个目录。例如，通过cp -r repo/hotpot。我们将repo捆绑包中的hotot子目录复制到当前捆绑包的“根”目录(开始运行捆绑包中的代码时所在的目录，而不是/root！)。</p><p>然后，我们调用main.py两次，第一次使用—mode prepro预处理dev集，第二次使用—mode test进行预测。如果您使用代码，则可以相应地更改此设置。</p><p>请注意，如果您的代码涉及预训练的特征提取(例如，Elmo或BERT)，则应该将其合并为此处命令的一部分，而不是作为包上传，因为您事先没有访问测试集的权限。（也就是说要预处理测试集的话要在一个捆绑包里进行多次运行python文件吧）</p><p>还要注意，您的模型不应该依赖于键类型和级别来进行预测，因为这些键没有出现在测试集中。</p><p>之后，我们将文件pred.json复制到当前包的“根”目录。请注意，文件名必须命名为pred.json，并且该文件必须放在包的“根”目录下，评估命令才能正常工作。</p><p><img src="https://i.loli.net/2021/07/28/cvhExjYFiz8bka9.png" alt=""></p><p>使用以下命令（将predict替换为您自己的prediction bundle的名称），确保您能够在dev set上评估您的模型而不会遇到任何问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cl macro hotpotqa-utils//dev-eval-distractor-v1.0 predict -n evaluate</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/07/28/EJ1SVlTsnHdZ9rq.png" alt=""></p><p><img src="https://i.loli.net/2021/07/28/jkswd5B198ZbqDi.png" alt=""></p><h3 id="Step-4-Describe-and-tag-your-submission-描述并标记您的提交"><a href="#Step-4-Describe-and-tag-your-submission-描述并标记您的提交" class="headerlink" title="Step 4: Describe and tag your submission 描述并标记您的提交"></a>Step 4: Describe and tag your submission 描述并标记您的提交</h3><p>准备好后，编辑预测捆绑包的说明，以反映在排行榜上显示所需的信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Model name (Affiliation) (single model &#x2F; ensemble) [paper name](paper link) (code link)</span><br></pre></td></tr></table></figure><p>如果您在使用深渊翻滚时愿意，可以使用匿名Anonymous作为您的从属关系，之后可以通过编辑您的深渊翻滚捆绑包的描述来修改它。</p><p>[paper名称] 和(代码链接)部分是可选的 </p><p>请注意，虽然[paper名称]和(paper链接)之间没有空格，因为这会造成代码链接的歧义。以下是一些示例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Baseline Model (Carnegie Mellon University, Stanford University, &amp; Universite de Montreal) (single model) [(Yang, Qi, Zhang, et al. 2018)](https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1809.09600.pdf) (https:&#x2F;&#x2F;github.com&#x2F;hotpotqa&#x2F;hotpot)</span><br><span class="line"></span><br><span class="line">My Awesome Model (Awesome Institute) (ensemble) [(Awesome et al., 2018)](https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1812.12345.pdf)</span><br><span class="line"></span><br><span class="line">PotLuck (Culinary University) (single model) [](https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1901.12345.pdf) (https:&#x2F;&#x2F;github.com&#x2F;potluck&#x2F;potluck)</span><br></pre></td></tr></table></figure><p>第一个示例正是我们用来描述基线模型的。第二个没有代码链接，第三个没有指定论文名称。</p><p>请注意，Codalab在捆绑描述中使用非ASCII字符有问题，因此请避免使用它们。</p><p>要提交您的捆绑包，请使用hotpotqa-diditor-test-submit标记您的预测捆绑包(这可以在Web UI上通过选择捆绑包并修改右侧面板上的标签来完成)，然后向彭琪(pengqi@cs.stanford.edu)发送一封简短的电子邮件，其中包含您的捆绑包UUID(0x后跟32个字符)或指向您的捆绑包的链接(不是您的工作表或工作表UUID！)。</p><p>请确保您的预测所依赖的所有捆绑包都是公开可读的(这是Codalab中的默认可见性)。</p><blockquote><p>重要信息：</p><p>1.请仅在dev集合上执行改善模型性能所需的任何模型选择或消融。不能在测试集上支持同一模型的多个提交。</p><p>2.请避免删除您的Submission捆绑包，即使在填写排行榜条目之后也是如此。这是您更新与您的Submission相关的信息的最佳方式，包括但不限于其名称、隶属关系、纸质链接、代码链接等。</p><p>3.如果你提交了多份报告(单一模型和集成模型)，请确保你的预测捆绑包有不同的名称。例如，predict-single和<code>predict-ensemble</code>。这是唯一一种我们在30天内容纳的多次提交。</p></blockquote><h2 id="1总结"><a href="#1总结" class="headerlink" title="1总结"></a>1总结</h2><ul><li>从github下载代码</li><li>上传模型和需要的数据</li><li>设置环境用docker</li><li>run</li></ul><p><img src="https://i.loli.net/2021/08/01/S6ik5vh4ImFxqCt.png" alt=""></p><p><a href="https://zhuanlan.zhihu.com/p/196343938">https://zhuanlan.zhihu.com/p/196343938</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;HotpotQA-Submission-Guide&quot;&gt;&lt;a href=&quot;#HotpotQA-Submission-Guide&quot; class=&quot;headerlink&quot; title=&quot;HotpotQA Submission Guide&quot;&gt;&lt;/a&gt;HotpotQA Su</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Heterogeneous Graph Transformer for Graph-to-Sequence Learning</title>
    <link href="http://example.com/2021/07/23/Heterogeneous-Graph-Transformer-for-Graph-to-Sequence-Learning/"/>
    <id>http://example.com/2021/07/23/Heterogeneous-Graph-Transformer-for-Graph-to-Sequence-Learning/</id>
    <published>2021-07-23T10:44:21.000Z</published>
    <updated>2021-07-23T13:48:47.720Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Heterogeneous-Graph-Transformer-for-Graph-to-Sequence-Learning"><a href="#Heterogeneous-Graph-Transformer-for-Graph-to-Sequence-Learning" class="headerlink" title="Heterogeneous Graph Transformer for Graph-to-Sequence Learning"></a>Heterogeneous Graph Transformer for Graph-to-Sequence Learning</h1><p>Graph2Seq学习的目的是将图结构的表示转换为单词序列，以便生成文本。</p><p>AMR-to-text是从抽象意义表示(AMR)图中生成文本的任务，其中节点表示语义概念，边表示概念之间的关系。</p><p>传统GNN只考虑了直接相连节点之间的关系，而忽略了远距离节点之间的间接关系。</p><p>Graph2Seq的其他两个和Graph Transformer的论文</p><ul><li><p>Graph transformer for graph-to-sequence learning AAAI 2020</p></li><li><p>Modeling graph structure in transformer for better AMR-to-text gen- eration  EMNLP 2019</p></li></ul><p>使用节点之间的最短关系路径来编码语义关系。但是，它们忽略了关系路径中节点的信息，对直接关系和间接关系没有区别地进行编码。当从直接邻居那里聚集信息时，可能会干扰信息的传播过程。</p><p>作者使用Heterogeneous Graph Transformer来独立地建模原始图的各个子图中的不同关系，包括节点之间的直接关系、间接关系和多种可能的关系。</p><h2 id="Input-Graph-Transformer"><a href="#Input-Graph-Transformer" class="headerlink" title="Input Graph Transformer"></a>Input Graph Transformer</h2><p>为了缓解语料库中的数据稀疏问题，作者将进一步将字节对编码(BPE)引入Levi图。</p><p>将原始节点拆分成多个子词节点。除了添加缺省连接外，我们还在子词之间添加了反向边和自循环边。</p><p>如下图：</p><p><img src="https://i.loli.net/2021/07/23/fswuO1n4J7bptHG.png" alt=""></p><p>例如，图中的单词Country被分割为co@@、un@@、try  它们之间有三种类型的边。</p><p><img src="https://i.loli.net/2021/07/23/LUmRx6udzNeJyAr.png" style="zoom:67%;" /></p><p>该任务一般先将抽象概念图(上图a)，转换成Levi图(上图b)。将AMR图转换为扩展的Levi图，该图可以看作是一个异构图，因为它具有不同类型的边。</p><h2 id="Heterogeneous-Graph-Transformer"><a href="#Heterogeneous-Graph-Transformer" class="headerlink" title="Heterogeneous Graph Transformer"></a>Heterogeneous Graph Transformer</h2><p><img src="https://i.loli.net/2021/07/23/MvH7kQdb2KFwert.png" alt=""></p><p>给定一个经过预处理的扩展Levi图，根据其异构性将扩展Levi图分成多个子图。</p><p>在每个Graph Encoder中，基于其在当前子图中的相邻节点来更新不同子图中的节点表示。然后，将该节点在不同子图中的所有表示组合在一起，以获得其最终表示。</p><h3 id="Graph-Encoder"><a href="#Graph-Encoder" class="headerlink" title="Graph Encoder"></a>Graph Encoder</h3><p>与其他Graph Transformer不同的是仅使用相对位置编码来隐藏结构信息。</p><p>在更新每个节点的表示时，直接屏蔽了非相邻节点的注意力。mask attention $\alpha_{ij}\notin N_i$  ，此外这个作者还尝试用了加性注意力这就和GAT几乎很像了。</p><p>因此，给定输入序列 $x=(x_1,…,x_n)$，每个关注头中表示为 $z_i$ 的节点i的输出表示如下计算：</p><script type="math/tex; mode=display">z_i = \sum_{j\in N_i} \alpha_{ij}(x_j W^V)</script><h3 id="Heterogeneous-Mechanism"><a href="#Heterogeneous-Mechanism" class="headerlink" title="Heterogeneous Mechanism"></a>Heterogeneous Mechanism</h3><p>在多头机制成功的激励下，提出了异质机制。考虑到一个句子，多头注意允许模型隐含地注意到来自不同位置的不同表示子空间的信息。相应地，异构机制使得模型显式地关注不同子图中的信息，对应于图的不同表示子空间，从而增强了模型的编码能力。</p><p>首先将所有的边类型组合成一个单一的边类型，从而得到一个同质连通子图。该连通子图实际上是一个包含原始图中完全连通信息的无向图。除了学习直连关系，还引入了一个完全连通子图来学习间接连接节点之间的隐含关系。</p><p>每个编码层中的输出z计算如下：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  z &= FFN(concat(z^{G^{sub}_1},...,z^{G_M^{sub}})W^O)\\ z_i^{G_m^sub} &= \sum_{j\in N_i^{G^{sub}_m} }\alpha_{ij}(x_jW^V), m\in[1,M]     \end{split}\end{equation}</script><p>$W^O\in R^{Md_z\times d_z}$参数矩阵 </p><p>作者还采用了子层之间的残差连接、FFN以及层归一化。</p><h3 id="Layer-Aggregation"><a href="#Layer-Aggregation" class="headerlink" title="Layer Aggregation"></a>Layer Aggregation</h3><p>编码层之间更好的信息传播可能带来更好的性能。</p><p>因此，我们研究了三种不同的Layer Aggregation方法，如图3所示。</p><p><img src="https://i.loli.net/2021/07/23/gfL3GKQDxzCamqX.png" alt=""></p><p>当更新第 $l$ 层节点的表示时，最近的方法是先聚合邻居，然后将聚合结果与来自 $(l−1)$ 层的节点表示相结合。此策略可视为不同图层之间跳过连接的一种形式。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  z_{N_i}^{(l)} &= AGGREGATE(\{z_j^{(l-1)}, \forall j \in N_i\})\\ z_i^{(l)} &= COMBINE(z_{N_i}^{(l)}, z_i^{(l-1)})    \end{split}\end{equation}</script><p>残差连接是另一种著名的跳跃连接，它使用identity mapping作为组合函数来帮助信号传播，但这些跳跃连接不能独立自适应地调整最后一层表示的邻域大小。</p><p>如果我们为$z_i^{(l)}$ skip一个层，则所有后续的单元例（如使用此表示的$z_i^{(l+j)}$) 都将隐式的使用此skip</p><p>因此，为了有选择地聚合前几层的输出，我们在模型中引入了跳跃体系。</p><p>在编码器的最后一层L，通过concat的方式组合前几个编码层的所有输出，以帮助模型有选择地聚合所有这些中间表示。</p><script type="math/tex; mode=display">z_i^{final} = Concat(z_i^{(L)},...,z_i^{(1)},x_i) W_{jump}</script><p>$W_{jump}\in R^{(Ld_z+d_x)\times d_z}$</p><p>此外，为了更好地改善信息传播，还可以引入稠密连通性。通过密集连接，l层中的节点不仅从第(l−1)层获取输入，而且还从所有前面的层提取信息： </p><script type="math/tex; mode=display">z_i^{(l)} = Concat(z_i^{(l-1)},..,z_i^{(1)},x_i) W^{(l)}_{dense}</script><p>$W^{(l)}_{dense} \in R^{d^{(l)}\times d_z}, d^{(l)}=d_x+d_z\times(l-1)$</p><p><img src="https://i.loli.net/2021/07/23/kqOvxdBz6YrnXj8.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Heterogeneous-Graph-Transformer-for-Graph-to-Sequence-Learning&quot;&gt;&lt;a href=&quot;#Heterogeneous-Graph-Transformer-for-Graph-to-Sequence-Lear</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>Graph Transformer for Graph-to-Sequence Learning</title>
    <link href="http://example.com/2021/07/23/Graph-Transformer-for-Graph-to-Sequence-Learning/"/>
    <id>http://example.com/2021/07/23/Graph-Transformer-for-Graph-to-Sequence-Learning/</id>
    <published>2021-07-23T02:11:03.000Z</published>
    <updated>2021-07-23T05:41:14.752Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Graph-Transformer-for-Graph-to-Sequence-Learning"><a href="#Graph-Transformer-for-Graph-to-Sequence-Learning" class="headerlink" title="Graph Transformer for Graph-to-Sequence Learning"></a>Graph Transformer for Graph-to-Sequence Learning</h1><p>这篇论文应用于在基于抽象语义表示(AMR)的文本生成和基于句法的神经机器翻译，句法机器翻译并入源端语法信息可以提高翻译质量。如图给出了AMR到文本生成的示例。</p><p><img src="https://i.loli.net/2021/07/23/QDi75OPI64YbHgA.png" alt=""></p><p>论文应用Graph Transformer，其与限制近邻之间信息交换的图神经网络不同，Graph Transformer使用显式关系编码，允许两个远距离节点之间的直接通信。它为全局图结构建模提供了一种更有效的方法。</p><p>这篇论文想解决的是打破传统GNN的局部邻接特性，使用高效的全局信息。</p><h2 id="Methed"><a href="#Methed" class="headerlink" title="Methed"></a>Methed</h2><p>对于n个节点的图，以前的图神经网络将节点表示$v_i$计算为输入节点 $i$ 及其所有一阶邻域 $N(i)$的函数。图结构由每个节点表示的感受野隐式反映。然而，这种本地通信设计对于远程信息交换可能是低效的。</p><p>所以引入Graph Transformer，它提供了一种截然不同的范例，可以实现关系感知的全球通信。</p><p><img src="https://i.loli.net/2021/07/23/FtimYg4NB5MTkoS.png" alt=""></p><p>作者提出的是关系增强的全局注意力机制，和Graphromer一样任何节点对之间的关系被描述为它们之间的最短关系路径。</p><h3 id="Graph-Encoder"><a href="#Graph-Encoder" class="headerlink" title="Graph Encoder"></a>Graph Encoder</h3><p>责将输入图形转换为一组相应的节点嵌入。核心问题是如何在允许全连通通信的同时保持图的拓扑结构。</p><p> 作者的想法是将两个节点之间的显式关系表示融入到它们的表示学习中。在标准的多头注意中，元素 $x_i$ 和元素 $x_j$之间的注意分数简单地分别是它们的查询向量和键向量的点积：</p><script type="math/tex; mode=display">s_{ij} = f(x_i,x_j) =x_iW^T_qW_kx_j</script><p>假设我们已经学习了节点i和节点j之间的关系 $r_{ij}$ 的矢量表示，我们将其称为关系编码。</p><script type="math/tex; mode=display">[r_{i\to j};r_{j\to i}] = W_r r_{ij}</script><p>$r<em>{i\to j};r</em>{j\to i}$ 为正向和反向关系编码。</p><p>如果把正反两个关系编码加到节点embedding中，注意力分数计算可以为：</p><script type="math/tex; mode=display">\begin{equation}\begin{split} s_{ij} &= g(x_i, x_j, r_{ij})\\            & = (x_i + r_{i\to j})W_q^TW_k(x_j + r_{j\to i})\\            &= \underbrace{x_iW_q^TW_kx_j}_{(a)} +  \underbrace{x_iW_q^TW_kr_{j\to i}}_{(b)} \\            &+ \underbrace{r_{i\to j}W_q^TW_kx_j}_{(c)} + \underbrace{r_{i\to j}W_q^TW_kr_{j\to i}}_{(d)}    \end{split}\end{equation}</script><p>直观上，等式项意义:</p><ul><li>(a) 捕获纯粹基于内容的content-based addressing,，这是普通注意力机制中的原始term。</li><li>(b) 依赖于源节点的关系偏置。</li><li>(c) 依赖于目标节点的关系偏置。</li><li>(d) 对通用的关系偏差进行编码。</li></ul><p>在这里作者使用的是节点间的最短路径来表示关系。</p><h4 id="Relation-Encoder"><a href="#Relation-Encoder" class="headerlink" title="Relation Encoder"></a>Relation Encoder</h4><p>从概念上讲，关系编码为模型提供了关于应该如何收集和分发信息的全局指导，即在哪里关注。</p><p>对于NLP中的大多数图形结构，边标签传达了相邻节点之间的直接关系(例如，概念到概念所扮演的语义角色，以及两个单词之间的依存关系)。</p><p>作者将这种单跳关系定义扩展到多跳关系推理中，以刻画任意两个节点之间的关系。</p><p>例如第一个图中 want-01 到 girl的最短路径概念为，$\text{want-01} \to^{ARG1} \text{believe-01}\to^{ARG0} girl$ 传达girl是wanted的目标。</p><p>直观地说，两个节点之间的最短路径给出了它们之间最密切且可以说是最重要的关系</p><p>作者使用GRU将关系序列转换为分布表示。$i$ 到 $j$ 的最短路径关系为: $sp_{i\to j}=  [e(i,k_1), e(k1,k2),…,e(k_n,j)]$</p><p>其中$e(,)$是边标签，$k_{1:n}$ 是中继节点。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \overrightarrow s_t &= GRU_f(\overrightarrow s_{t-1}, sp_t)\\ \overleftarrow s_t &= GRU_f(\overleftarrow s_{t+1},sp_t)    \end{split}\end{equation}</script><p>concat最终关系表达为$r_{ij} = [\overrightarrow s_n; \overleftarrow s_0]$</p><h4 id="Bidirectionality"><a href="#Bidirectionality" class="headerlink" title="Bidirectionality"></a>Bidirectionality</h4><p>因为应用任务常是DAG，作者给做成理论双向交互的。反转边连接与原始边相同的两个节点，但方向不同，并使用反转标签。</p><p>此外作者还在每个图中引入一个额外的全局节点和自环边，该节点具有特殊标签GLOBAL与其他所有节点都有一条直接边。全局节点的最终表示$x_{global}$用作整个图表示。</p><h4 id="Absolute-Position"><a href="#Absolute-Position" class="headerlink" title="Absolute Position"></a>Absolute Position</h4><p>除了成对关系之外，一些绝对位置信息也是有益的。例如，AMR图的根作为整体焦点的粗略表示，使得到根节点的最小距离部分地反映了相应概念在整句语义中的重要性。 </p><p>位置嵌入添加到编码器堆栈底部的输入embedding中。例如，第一个中的Want-01是AMR图的根节点，因此其索引应该为0。也将全局节点的索引表示为0。</p><h3 id="Sequence-Decoder"><a href="#Sequence-Decoder" class="headerlink" title="Sequence Decoder"></a>Sequence Decoder</h3><p>和普通Transformer Decoder没什么大的区别</p><p>特殊的一点，使用全局图形表示$x_{global}$来初始化每个时间步的隐藏状态。</p><p>然后，通过在编码器的输出上交错多轮关注来更新每个时间步骤t处的隐藏状态 $h_t$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Graph-Transformer-for-Graph-to-Sequence-Learning&quot;&gt;&lt;a href=&quot;#Graph-Transformer-for-Graph-to-Sequence-Learning&quot; class=&quot;headerlink&quot; tit</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>Rethinking Graph Transformers with Spectral Attention</title>
    <link href="http://example.com/2021/07/21/Rethinking-Graph-Transformers-with-Spectral-Attention/"/>
    <id>http://example.com/2021/07/21/Rethinking-Graph-Transformers-with-Spectral-Attention/</id>
    <published>2021-07-21T03:56:04.000Z</published>
    <updated>2021-07-27T02:13:11.344Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Rethinking-Graph-Transformers-with-Spectral-Attention"><a href="#Rethinking-Graph-Transformers-with-Spectral-Attention" class="headerlink" title="Rethinking Graph Transformers with Spectral Attention"></a>Rethinking Graph Transformers with Spectral Attention</h1><p>提出了<em>Spectral Attention Network</em>(SAN)，它使用学习的位置编码(LPE)，可以利用全拉普拉斯频谱来学习给定图中每个节点的位置。通过利用拉普拉斯的全谱，模型在理论上具有强大的区分图形的能力，并且可以更好地从它们的共振中检测出相似的子结构。</p><p>在这项工作中，作者还是研究如何将Transformer体系结构应用于图形表示学习。开发了强大的可学习的位置编码方法，这些方法植根于谱图理论。 谱注意力网络(SAN)架构解决了先前图形转换器工作中的关键理论限制，并且明显超过了标准消息传递GNN的表达能力。</p><p>SAN方法的优势对比：</p><p><img src="https://i.loli.net/2021/07/26/x7U849s2RSCdQau.png" alt=""></p><ul><li>保持注意中的局部结构</li><li>使用边特征</li><li>连接非相邻节点</li><li>使用基于特征向量的PE进行注意</li><li>使用具有结构信息的PE</li><li>考虑特征值的排序</li><li>特征向量的范数不变量</li><li>考虑特征值的谱               (SAN独有)</li><li>考虑特征向量的变量#       (SAN独有)</li><li>意识到特征值的多重性    (SAN独有)</li><li>对特征向量的符号不变</li></ul><p>也就是说SAN结合了稀疏和稠密GT的特性，并且还考虑了特征值的谱、征向量的变量#、意识到特征值的多重性。</p><h2 id="基于特征函数的绝对和相对位置编码"><a href="#基于特征函数的绝对和相对位置编码" class="headerlink" title="基于特征函数的绝对和相对位置编码"></a>基于特征函数的绝对和相对位置编码</h2><p>因为不存在对节点进行排序或定义轴的规范方法。在本节中，作者将研究如何使用拉普拉斯的特征函数来定义图形中的绝对和相对PE，测量节点之间的物理相互作用，并使特定的子结构能够“听到”-类似于鼓的声音，揭示其结构。</p><h3 id="特征向量等价于图上的正弦函数"><a href="#特征向量等价于图上的正弦函数" class="headerlink" title="特征向量等价于图上的正弦函数"></a>特征向量等价于图上的正弦函数</h3><p>在Transformer架构中，一个基本方面是使用正弦和余弦函数作为序列的PE。然而，对于任意图形，sinusoids正弦不能被清楚地定义，因为沿轴的位置没有清晰的概念。取而代之的是，它们的等价性由图Laplacian L的特征向量 $\Phi$ 给出。</p><p>事实上，在欧几里得空间中，拉普拉斯算子对应于梯度的散度，其特征函数是正弦/余弦函数，平方频率对应于特征值(我们有时从这里起将这两个概念互换)。因此，在图域中，图的Laplacian的特征向量与正弦函数自然等价，并且这一直觉被用于最近的多项工作中，这些工作将特征向量用作GNN(Benchmarking graph neural networks)、定向流(Directional graph networks. ICML2021)和GT的PE。</p><p>在与正弦函数等价的情况下，我们很自然地发现，$\mathcal{F}[f]$的傅里叶变换函数应用于图$\mathcal{F}<a href="\lambda_i">f</a> = \langle f, \phi_i \rangle$，其中特征值被认为是该图的傅立叶域中的一个位置。因此，最好将特征向量视为位于特征值轴上的向量，而不是矩阵的组成部分，如图所示。</p><p><img src="https://i.loli.net/2021/07/26/Bg3QbcITZuMRLjs.png" alt=""></p><h3 id="关于相对位置，特征函数告诉我们什么？-物理应用"><a href="#关于相对位置，特征函数告诉我们什么？-物理应用" class="headerlink" title="关于相对位置，特征函数告诉我们什么？(物理应用)"></a>关于相对位置，特征函数告诉我们什么？(物理应用)</h3><p>除了模拟正弦函数外，拉普拉斯函数的特征向量还包含有关系统物理的重要信息，可以揭示距离度量。因为拉普拉斯运算符是物理学中的一个基本运算符，在麦克斯韦方程和热扩散中都有显著的应用。</p><p>在电磁理论中，拉普拉斯的(伪)逆，在数学上称为拉普拉斯的格林函数，表示电荷的静电势。</p><p>在图中，相同的概念使用拉普拉斯G的伪逆，并且可以通过其特征函数来计算。</p><p>如下公式，$G(j_1,j_2)$ 是节点$j_1,j_2$ 之间的电势。 $\hat \phi_i,\hat \lambda_i$ 为对称Laplacian$D^{\frac{-1}{2}}LD^{\frac{-1}{2}}$第 $i$个特征值和特征向量。</p><script type="math/tex; mode=display">G(j_1,j_2) = d_{j_1}^{\frac{1}{2}}d_{j_2}^{\frac{-1}{2}}\sum_{i>0}\frac{(\hat \phi_{i,j_1},\hat \phi_{i,j_2})^2}{\hat \lambda_i}</script><p>此外，傅立叶给出的热方程的原始解依赖于被称为傅立叶级数的正弦/余弦的和。由于拉普拉斯函数的特征向量是这些函数在图中的近似，我们找到了近似的解。热核与随机游走相关，我们利用两个热核之间的相互作用在下面方程中定义节点$j_1,j_2$之间的扩散距离$d_D$。类似的二次谐波距离$d_B$是一种不同的距离测量方法。这里我们使用正则拉普拉斯L的特征函数：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  d^2_D(j_1,j_2) &= \sum_{k>0} e^{-2t\lambda_i}(\phi_{i,j_1} - \phi_{i,j_2})^2\\ d_B^2(j_1,j_2)&=\sum_{i>0}\frac{(\phi_{i,j_1} - \phi_{i,j_2})^2}{\lambda_i^2}    \end{split}\end{equation}</script><p>这个方程，首先强调了在提供有关图中相对位置的信息时将特征向量与其对应的特征值配对的重要性。其次，我们注意到特征向量的乘积与静电相互作用成正比，而减法与扩散距离和重谐距离成正比。最后，所有3个方程都有一个一致的模式：在确定节点之间的距离时，频率/特征值越小，权重越大。</p><h3 id="听图的形状及其子结构"><a href="#听图的形状及其子结构" class="headerlink" title="听图的形状及其子结构"></a>听图的形状及其子结构</h3><p>特征值的另一个众所周知的性质是它们如何用于区分不同的图结构和子结构，因为它们可以解释为图的共振频率。</p><p>这就引出了一个著名的问题，即我们是否能从鼓的特征值中听到鼓的形状，同样的问题也适用于几何物体和3D分子。</p><p>通过将特征函数用于部分功能对应、算法理解几何和样式对应。分子图的特征向量的例子如图所示。</p><p><img src="https://i.loli.net/2021/07/26/o4M9SwJnLWTKCsj.png" alt=""></p><h2 id="Laplace-Eigenfunctions的规范"><a href="#Laplace-Eigenfunctions的规范" class="headerlink" title="Laplace Eigenfunctions的规范"></a>Laplace Eigenfunctions的规范</h2><p>在欧几里德空间和序列中，使用正弦波作为PE是很简单的：我们可以简单地选择一组频率，计算正弦波，并将它们添加或拼接到输入嵌入，就像在原始变压器中所做的那样。然而，在任意图中，复制这些步骤并不那么简单，因为每个图都有一组唯一的特征函数。</p><p>在接下来的部分中，将介绍谱图理论中的关键原则，在为图构造PE时要考虑这些原则，这些原则大部分被以前的方法忽略了。包括正则化，特征值及其多样性的重要性，特征向量的数量是可变的，以及符号模糊性。作者的LPE架构旨在解决这些问题。</p><p><strong>Normalization</strong> 给定拉普拉斯的特征值，就有一个维数大于1的相关特征空间。为了在模型中利用这些信息，必须选择一个单一的特征向量。在我们的工作中，我们使用L2正则化，因为它与格林公式也就是上面的第一个公式的定义是兼容的。因此，我们将始终选择特征向量$\phi$，使$⟨\phi，\phi⟩=1$。</p><p><strong>Eigenvalues</strong> 另一个基本方面是与每个特征向量相关联的特征值提供了有价值的信息。基于特征向量的特征值的排序在序列中起作用，因为频率是预先确定的。然而，这一假设在图中不起作用，因为它们的谱中的特征值可以改变。例如，在上图中，我们观察到排序如何忽略两个分子在 $λ = 1$ 以不同方式共振的事实。</p><p><strong>Multiplicities</strong> 选择特征函数的另一个重要问题是特征值高度多样的可能性，即当一个特征值多次作为特征多项式的根出现时。在这种情况下，相关联的特征空间可以具有2维或更多维，因为我们可以从具有相同特征值的任何特征向量的线性组合中生成有效的特征向量。这进一步复杂化了选择用于算法计算的特征向量的问题，并突出了拥有能够处理这种歧义的模型的重要性。</p><p><strong>Variable number of eigenvectors</strong> 图 $G_i$ 至多可以有 $N_i$ 个线性独立的特征向量，其中 $N_i$ 是它的节点数。最重要的是，$N_i$ 可以在数据集中的所有的 $G_i$ 都有所不同。GT选择了固定数目的k个特征向量给每个图，其中 $k≤N_i$，$∀i$。当数据集中最小的图的节点比最大的图少得多时，这就产生了一个主要的瓶颈，因为很小比例的特征向量将用于大型图。这不可避免地造成信息丢失，并激发了对构建k维固定PE的模型的需求，其中k不依赖于图中的特征向量的数目。</p><p><strong>Sign invariance</strong> 如前所述，特征向量存在符号歧义。由于φ的符号与它的正则化无关，在选择图的k个特征向量时，我们只剩下2k个可能的符号组合。以前的工作已经提出通过随机反转特征向量的符号来进行数据增强，虽然当k较小时可以工作，但是对于较大的k会变得困难。</p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>我们提出了一种体系结构，它可以使用特征函数作为PE，同时解决上述规范中提出的问题。我们的 <em>Spectral Attention Network</em> (SAN)模型输入图的特征函数，并将其投影到固定大小的学习位置编码(LPE)中。LPE允许网络使用每个图的整个拉普拉斯频谱，学习频率如何交互，并决定哪些频率对给定任务最重要。</p><p><img src="https://i.loli.net/2021/07/26/8uAUjDZ5EdNQpFs.png" alt=""></p><p>如图分为两步学习过程。</p><p>图中的(c-d-e)描述了第一步，在每个节点的特征函数上应用一个Transformer，为每个图生成一个LPE矩阵。</p><p>然后将LPE连接到节点嵌入图中(g-h)，然后将其传递给Graph Trabsformer (i)。如果任务涉及图分类或回归，则最终节点嵌入随后将传递到最终池化层。</p><h3 id="LPE-Transformer-Over-Nodes"><a href="#LPE-Transformer-Over-Nodes" class="headerlink" title="LPE Transformer Over Nodes"></a>LPE Transformer Over Nodes</h3><p>使用拉普拉斯编码作为节点特征在有关该主题的文献中是普遍存在的。LPE的想法受到上面第二个图的启发，其中特征向量 $\phi$ 被表示为一个非均匀序列，特征值λ是频率轴上的位置。使用此表示法，Transformers是处理它们并生成固定大小PE的自然选择。</p><p>LPE结构如图所示：</p><p><img src="https://i.loli.net/2021/07/26/yxhrKLAwXcvBTgD.png" alt=""></p><p>学习位置编码(LPE)结构，模型通过考虑m个特征值和特征向量来学习图的拉普拉斯谱，其中允许 $m≤N$，其中N表示节点数。</p><p>首先，我们通过将m个最低特征值与其关联的特征向量连接起来，为每个节点$j$ 创建一个大小为 $2×m$ 的嵌入矩阵。这里，m是要计算的特征向量的最大数目的超参数，并且类似于标准变压器的可变长度序列。对于 $m&gt;N$ 的图，只需添加掩码填充。注意，要捕获所有图的整个谱，只需选择m，使其等于图在数据集中具有的最大节点数。然后在大小为2的维度上应用线性层以生成大小为k的新嵌入。然后，Transformer编码器对长度为m且隐藏维数为k的序列计算self-attention。最后，sum pooling将该序列简化为固定的k维节点嵌入。</p><p>通过将特征值与归一化特征向量连接起来，该模型直接处理前三个规范。即将特征向量归一化，将特征向量与其特征值配对，并将特征向量的个数作为变量。此外，该模型意识到了多重性，并且有可能线性组合或忽略一些重复的特征值。</p><p>然而，这种方法仍然没有解决预先计算的特征向量的符号是任意的限制。为了解决这个问题，我们像以前的工作[13，12]所采用的那样，在训练过程中随机反转预先计算的特征向量的符号，以促进符号歧义的不变性。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Rethinking-Graph-Transformers-with-Spectral-Attention&quot;&gt;&lt;a href=&quot;#Rethinking-Graph-Transformers-with-Spectral-Attention&quot; class=&quot;heade</summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Heterogeneous Graph Transformer</title>
    <link href="http://example.com/2021/07/09/Heterogeneous-Graph-Transformer/"/>
    <id>http://example.com/2021/07/09/Heterogeneous-Graph-Transformer/</id>
    <published>2021-07-09T09:18:10.000Z</published>
    <updated>2021-07-09T13:42:02.369Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Heterogeneous-Graph-Transformer"><a href="#Heterogeneous-Graph-Transformer" class="headerlink" title="Heterogeneous Graph Transformer"></a>Heterogeneous Graph Transformer</h1><p>提出了一种用于Web规模异构图建模的异构图Transformer(HGT)体系结构。</p><p>其一是设计了节点和边类型相关的参数来表征对每条边的异构attention，使得HGT能够维护不同类型的节点和边的专用表示。</p><p>其二为了处理Web规模的图形数据，我们设计了异构小批量图形采样算法HG Samples，以实现高效和可扩展的训练</p><p>作者使用的是OAG学术图，其存在的异构关系如下图：</p><p><img src="https://z3.ax1x.com/2021/07/09/RxMqUA.png" alt=""></p><h2 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h2><p>GNN以前可以处理异质图是基于元路径的方法有PathSim, methpath2vec等。GNN火起来以后也出现了好多处理异质图的工作。</p><p>作者认为面临着几个问题：首先，它们大多涉及为每种类型的异构图设计元路径，需要特定的领域知识；其次，它们要么简单地假设不同类型的节点/边共享相同的特征和表示空间，要么只针对节点类型或边类型保持不同的非共享权重，使得它们不足以捕捉异构图的属性；最后，其固有的设计和实现使得它们无法对Web规模的异构图进行建模。</p><p>作者的目标是：保持节点和边类型的依赖表示，避免定制的元路径，并且能够扩展到Web规模的异构图。</p><h3 id="做法："><a href="#做法：" class="headerlink" title="做法："></a>做法：</h3><h4 id="异质处理"><a href="#异质处理" class="headerlink" title="异质处理"></a>异质处理</h4><p>为了处理图的异构性，引入了节点和边型依赖的注意机制。HGT中的异构相互关注度不是参数化的，而是通过基于其元关系三元组分解每条边e=(s，t)来定义的，即 <s的节点类型、s&t之间的e的边类型、t的节点类型>。上图说明了异质学术图的元关系。使用这些元关系来参数化权重矩阵，以计算每条边上的关注度。因此，允许不同类型的节点和边保持其特定的表示空间。</p><p>同时，不同类型的连接节点仍然可以交互、传递和汇聚消息，而不受其分布差距的限制。由于其体系结构的本质，HGT可以通过跨层的消息传递来融合来自不同类型的高阶邻居的信息，这可以被认为是“软”元路径。也就是说，即使HGT只将其一跳边作为输入，而不需要人工设计元路径，所提出的注意机制也可以自动和隐式地学习和提取对不同下游任务重要的“元路径”。</p><h4 id="异质子图采样法"><a href="#异质子图采样法" class="headerlink" title="异质子图采样法"></a>异质子图采样法</h4><p>为了对Web规模的异构图进行建模，设计了第一个用于小批量GNN训练的异构子图采样算法HG Samples。它的主要思想是对不同类型节点比例相近的异构子图进行采样。此外，它还被设计成保持采样子图的稠密性，以最大限度地减少信息损失。有了HG-sample，所有的GNN模型都可以在任意大小的异构图上进行训练和推断。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>思想：利用异构图的元关系来参数化异构相互关注、消息传递和传播步骤的权重矩阵。</p><p>有向图 $G = (V,E,A,R)$ ,  节点 $v \in V$，每个边$e \in E$ 。他们的类型映射函数为 $\tau(v):V \to A$ 、$\phi(e):E\to R$ </p><h3 id="元关系"><a href="#元关系" class="headerlink" title="元关系"></a>元关系</h3><p>对于一个边 $e = (s,t)$ ，元关系定义为 $&lt;\tau(s),\phi(e),\tau(t)&gt;$ 。$\phi(e)^{-1}$ 是关系的反向表达。</p><h3 id="HGT架构"><a href="#HGT架构" class="headerlink" title="HGT架构"></a>HGT架构</h3><p><img src="https://i.loli.net/2021/07/09/8CepfwW4dgEzsjc.png" alt=""></p><p>主要的三个组件：Heterogeneous Mutual Attention、Heterogeneous Message Passin和特定于Target-Specific Aggregation。</p><p>定义第$l$ 层的输出为 $H^l$, 也是第$l+1$层的输入。</p><h4 id="Heterogeneous-Mutual-Attention"><a href="#Heterogeneous-Mutual-Attention" class="headerlink" title="Heterogeneous Mutual Attention"></a>Heterogeneous Mutual Attention</h4><p>首先计算源节点 s 到目标节点 t 之间的 Mutual Attention。</p><p>针对问题是：通过使用一个权重矩阵W来假设s和t具有相同的特征分布。这种假设对于异构图通常是不正确的，因为在异构图中，每种类型的节点都可以有自己的特征分布。</p><p>给出目标节点 t ，以及它的邻居节点 $s \in N(t)$ 它们可能属于不同的分布。通过元关系三元组 $&lt;\tau(s),\phi(e),\tau(t)&gt;$, 计算mutual attention。</p><p>将目标节点t映射为query向量，将源节点s映射为key向量，并计算它们的点积作为关注度。</p><p>与Vanilla Transformer相比关键区别在于，Vanilla Transformer对所有单词使用一组投影映射，HGT的每个元关系都应该有一组不同的投影权重。</p><p>为了在保持不同关系特性的同时最大限度地实现参数共享，提出将交互算子的权重矩阵参数化为源节点投影、边投影和目标节点投影。</p><p>对每个边$e=(s,t)$进行h heads attention :</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  Attention_{HGT}(s,e,t) &= Softmax_{\forall s\in N(t)}(  \|_{i\in[1,h]} \text{ATT-head}^i(s,e,t) )\\\text {ATT-head}^i(s,e,t) &=(K^i(s) W^{ATT}_{\phi(e)}Q^i(t)^T) \cdot \frac{\mu<\tau(s),\phi(e),\tau(t)>}{\sqrt d} \\K^i(s) &= \text{K-Linear}_{\tau(s)}^i(H^{(l-1)}[s])\\Q^i(t) &= Q-Linear_{\tau(t)}^i (H^{(l-1)}[t])    \end{split}\end{equation}</script><p>$\text{ATT-head^i(s,e,t)}$ 是第 $i$ 个注意力头。$\text{K-Linear}^i_{\tau{(s)}}:R^d \to R^{\frac{d}{h}}$ 编码了源接地那s类型$\tau(s)$ 意味着每每个类型节点有独一无二的线性映射最大限度地对分布差异进行建模。</p><p>然后计算Query和Key的相似度， 异构图的一个独特特征是在节点类型对之间可能存在不同的边类型(关系)，例如 $τ(S)$和$τ(T)$ 。因此，与直接计算查询和键向量之间的点积的Vanilla Transformer不同，我们为每个边类型$\phi(e)$保留了一个不同的基于边的矩阵 $W^{ATT}_{\phi(e)}\in R^{\frac{d}{h}\times \frac{d}{h}}$ 。这样，即使在相同的节点类型对之间，该模型也可以捕获不同的语义关系。</p><p>此外，由于不是所有的关系对目标节点的贡献相等，我们增加了一个先验张量 $\mu\in R^{|A|\times |R|\times |A|}$ 表示每个元关系三元组的一般意义，作为对注意力的自适应缩放。</p><p>最后，我们将注意力集中在一起，以获得每个节点对的attention向量。</p><h4 id="Heterogeneous-Message-Passing"><a href="#Heterogeneous-Message-Passing" class="headerlink" title="Heterogeneous Message Passing"></a>Heterogeneous Message Passing</h4><p>在计算Mutual Attention的同时，将信息从源节点传递到目标节点。</p><p>与attention过程类似，希望在消息传递过程中加入边的元关系，以缓解不同类型节点和边的分布差异。</p><p>对于一对节点 $e=(s,t)$，我们通过以下公式计算其多头 Message:</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  Message_{HGT(s,e,t)} &= \|_{i\in [1,h]} \text{MSG-head}^i(s,e,t)\\ \text{MSG-head}^i(s,e,t) &= \text{M-Linear}_{\tau(s)}^i(H^{(l-1)}[s])W^{MSG}_{\phi(e)}    \end{split}\end{equation}</script><h4 id="Target-Specific-Aggregation"><a href="#Target-Specific-Aggregation" class="headerlink" title="Target-Specific Aggregation"></a>Target-Specific Aggregation</h4><script type="math/tex; mode=display">\hat H^{(l)}[t] =  \oplus_{\forall s\in N(t)} (Attention_{HGT}(s,e,t) \cdot Message_{HGT}(s,e,t))</script><p>这将来自不同特征分布的所有近邻(源节点)的信息聚集到目标节点 t。</p><p>最后一步是将目标节点t的向量映射回按其节点类型τ(T)索引的特定于类型的分布。为此，我们将线性投影A-线性τ(T)应用于更新后的向量H􏰅(L)[t]，随后是非线性激活和剩余连接[5]，如下所示：</p><h3 id="HGSampling"><a href="#HGSampling" class="headerlink" title="HGSampling"></a>HGSampling</h3><p><img src="https://i.loli.net/2021/07/09/5yz2s4vhCZPINX9.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Heterogeneous-Graph-Transformer&quot;&gt;&lt;a href=&quot;#Heterogeneous-Graph-Transformer&quot; class=&quot;headerlink&quot; title=&quot;Heterogeneous Graph Transforme</summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Transformer的辅助</title>
    <link href="http://example.com/2021/07/04/Transformer%E7%9A%84%E8%BE%85%E5%8A%A9/"/>
    <id>http://example.com/2021/07/04/Transformer%E7%9A%84%E8%BE%85%E5%8A%A9/</id>
    <published>2021-07-04T15:14:01.000Z</published>
    <updated>2021-07-05T02:07:18.843Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transformer的辅助"><a href="#Transformer的辅助" class="headerlink" title="Transformer的辅助"></a>Transformer的辅助</h1><p>转载：<a href="https://zhuanlan.zhihu.com/p/149634836">https://zhuanlan.zhihu.com/p/149634836</a></p><h2 id="为什么Transformer需要进行Multi-head-Attention"><a href="#为什么Transformer需要进行Multi-head-Attention" class="headerlink" title="为什么Transformer需要进行Multi-head Attention"></a><a href="https://www.zhihu.com/question/341222779/answer/814111138">为什么Transformer需要进行Multi-head Attention</a></h2><p>Attention is all you need论文中讲模型分为多个头，形成多个子空间，每个头关注不同方面的信息。</p><p>如果Multi-Head作用是关注句子的不同方面，那么不同的head就应该关注不同的Token；当然也有可能是关注的pattern相同，但是关注的内容不同，即V不同。</p><p>但是大量的paper表明，transformer或Bert的特定层有独特的功能，底层更偏向于关注语法；顶层更偏向于关注语义。</p><p>所以对Multi-head而言，同一层Transformer_block关注的方面应该整体是一致的。不同的head关注点也是一样。但是可视化同一层的head后，发现总有那么一两个头独一无二的，和其他头的关注不一样。</p><p>众多研究表明Multi-Head其实不是必须的，去掉一些头效果依然有不错的效果（而且效果下降可能是因为参数量下降），这是因为在头足够的情况下，这些头已经能够有关注位置信息、关注语法信息、关注罕见词的能力了，再多一些头，无非是一种enhance或noise而已。</p><h3 id="相关paper"><a href="#相关paper" class="headerlink" title="相关paper"></a>相关paper</h3><ul><li>A Multiscale Visualization of Attention in the Transformer Model <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1906.05714.pdf">https://arxiv.org/pdf/1906.05714.pdf</a></li><li>What Does BERT Look At? An Analysis of BERT’s Attention <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1906.04341v1.pdf">https://arxiv.org/pdf/1906.04341v1.pdf</a></li><li>Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1908.11365.pdf">https://arxiv.org/pdf/1908.11365.pdf</a></li><li>Adaptively Sparse Transformers<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1909.00015.pdf">https://arxiv.org/pdf/1909.00015.pdf</a></li><li>Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1905.09418.pdf">https://arxiv.org/pdf/1905.0941</a></li></ul><h2 id="Transformer为什么Q和K使用不同的权重矩阵生成，为什么不能使用同一个值进行自身的点乘？（注意和第一个问题的区别）"><a href="#Transformer为什么Q和K使用不同的权重矩阵生成，为什么不能使用同一个值进行自身的点乘？（注意和第一个问题的区别）" class="headerlink" title="Transformer为什么Q和K使用不同的权重矩阵生成，为什么不能使用同一个值进行自身的点乘？（注意和第一个问题的区别）"></a><a href="https://www.zhihu.com/question/319339652">Transformer为什么Q和K使用不同的权重矩阵生成，为什么不能使用同一个值进行自身的点乘？（注意和第一个问题的区别）</a></h2><p>既然K和Q差不多（唯一区别是W_k和W_Q权值不同），直接拿K自己点乘就行了，何必再创建一个Q？创建了还要花内存去保存，浪费资源，还得更新参数。</p><p><strong>为什么要计算Q和K的点乘？</strong></p><p>我们知道K和Q的点乘是为了得到一个attention score 矩阵，用来对V进行提纯。K和Q使用了不同的W_k, W_Q来计算，可以理解为是在不同空间上的投影。正因为有了这种不同空间的投影，增加了表达能力，这样计算得到的attention score矩阵的泛化能力更高。这里解释下我理解的泛化能力，因为K和Q使用了不同的W_k, W_Q来计算，得到的也是两个完全不同的矩阵，所以表达能力更强。</p><p> 但是如果不用Q，直接拿K和K点乘的话，你会发现attention score 矩阵是一个对称矩阵。因为是同样一个矩阵，都投影到了同样一个空间，所以泛化能力很差。这样的矩阵导致对V进行提纯的时候，效果也不会好。</p><h2 id="为什么在进行softmax之前需要对注意进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解"><a href="#为什么在进行softmax之前需要对注意进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解" class="headerlink" title="为什么在进行softmax之前需要对注意进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解"></a><a href="https://www.zhihu.com/question/339723385/">为什么在进行softmax之前需要对注意进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解</a></h2><p><strong>（</strong>论文中解释是：向量的点积结果会很大，将softmax函数push到梯度很小的区域，scaled会缓解这种现象。怎么理解将sotfmax函数push到梯度很小区域？还有为什么scaled是维度的根号，不是其他的数？<strong>）</strong></p><p>以数组为例，2个长度是len，均值是0，方差是1的数组点积会生成长度是len，均值是0，方差是len的数组。而方差变大会导致softmax的输入推向正无穷或负无穷，这时的梯度会无限趋近于0，不利于训练的收敛。因此除以len的开方，可以是数组的方差重新回归到1，有利于训练的收敛。</p><p>@LinT成功人士（） 以下感谢分享</p><p><strong>1. 为什么比较大的输入会使得softmax的梯度变得很小？</strong></p><p>对于一个输入向量$x\in R^d$, softmax函数将其映射/归一化到一个分布$\hat y\in R^d$。在这个过程中softmax先用一个自然底数$e$ 将输入中的元素检举先“拉大”，然后归一化为一个分布。假设某个输入x 中最大的元素下班是k，如果输入的数量级变大(每个元素都很大)，那么$\hat y_k$会非常接近1。</p><p>举个例子$x$ 的数量级对输入最大元素对应的预测概率$\hat y_k$的影响。假定输入 $x = [a,a,2a]^T$, 我们看看不同量级的$a$ 产生的$\hat y_3$有什么区别。</p><ul><li><img src="https://www.zhihu.com/equation?tex=a%3D1+" alt="[公式]"> 时， <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_3%3D0.5761168847658291" alt="[公式]"> ;</li><li><img src="https://www.zhihu.com/equation?tex=a%3D10++" alt="[公式]"> 时， <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_3%3D0.999909208384341" alt="[公式]">;</li><li><img src="https://www.zhihu.com/equation?tex=a%3D100++" alt="[公式]"> 时， <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_3%5Capprox+1.0" alt="[公式]"> (计算机精度限制)。</li></ul><p>可以看出，第三个元素里的值随着数量级的增加而接近于1。而我们知道softmax层后的每个元素之后为1。也就是说，向量里最大值索引的元素基本上占据所有的概率了。为了理解方便，可视化如下。可以看出，随着向量里最大元素的数量级的增大，它就越近于1，相当于整个输出变成了one-hot编码了<a href="#">y = [0,0,1]</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x  = np.linspace(<span class="number">0</span>,<span class="number">100</span>,<span class="number">100</span>)</span><br><span class="line">f = <span class="keyword">lambda</span> x: np.exp(x * <span class="number">2</span>) / (np.exp(x) + np.exp(x) + np.exp(<span class="number">2</span> * x))</span><br><span class="line">y = [f(temp) <span class="keyword">for</span> temp <span class="keyword">in</span> x]</span><br><span class="line">plt.plot(x,y)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/07/05/25qZGxyD17w9UgM.png" alt=""></p><p><img src="https://i.loli.net/2021/07/05/NZpiYSf8y7vVWEL.png" alt=""></p><p><strong>2. 维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩？</strong></p><p><img src="https://pic1.zhimg.com/80/v2-493286fbea075e160bf3bac214d2ac60_720w.jpg" alt="v2-493286fbea075e160bf3bac214d2ac60_720w"></p><p><img src="https://i.loli.net/2021/07/05/5BnlfzVdOjh14EA.png" alt=""></p><hr><h2 id="在计算注意力分数的时候如何对padding做mask操作？"><a href="#在计算注意力分数的时候如何对padding做mask操作？" class="headerlink" title="在计算注意力分数的时候如何对padding做mask操作？"></a><strong>在计算注意力分数的时候如何对padding做mask操作？</strong></h2><p>mask是将一些不要用的值掩盖掉，使其不产生作用。有两种mask，第一种是<strong>padding mask</strong>，在所有scaled dot-product attention都用到；第二种是<strong>sequence mask，</strong>在decoder的self-attention里面用到。</p><p><strong>padding mask：</strong>因为一个批量输入中，所有序列的长度使不同的。为了符合模型的输入方式，会用padding的方式来填充（比如填0），使所有序列的长度一致。但填充部分是没有意义的，所以在计算注意力的时候，不需要也不应该有注意力分配到这些填充的值上面。所以解决方式就是在填充的位置赋予一个<strong>很小的负值/负无穷（-np.inf）</strong>的值，<strong>经过softmax后的得分为0</strong>，即没有注意力分配到这个上面。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">padding_mask</span>(<span class="params">seq_k, seq_q</span>):</span></span><br><span class="line">  <span class="comment"># shape(seq_k)=(B,L_k) , shape(seq_q) = (B, L_q)</span></span><br><span class="line">  <span class="comment"># 因为要计算seq_k和seq_q的相似程度，来表示注意力的得分</span></span><br><span class="line">  <span class="comment"># padding mask要作用在 QK^T上，所以padding mask是跟seq_k和seq_q序列长度相关的矩阵</span></span><br><span class="line">  <span class="comment"># shape(padding mask) = (B, L_q, L_k)</span></span><br><span class="line">  len_q = seq_q.size(<span class="number">1</span>)</span><br><span class="line">  <span class="comment"># PAD is 0 这里要计算seq_k序列中，padding为0的地方，并将相应位置变为True，方便后续处理</span></span><br><span class="line">  pad_mask = seq_k.eq(<span class="number">0</span>)</span><br><span class="line">  <span class="comment"># 将每个seq_k序列扩展len_q次，shape[B, L_q, L_k]</span></span><br><span class="line">  pad_mask = pad_mask.unsqueeze(<span class="number">1</span>).expand(-<span class="number">1</span>, len_q, -<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">return</span> pad_mask</span><br><span class="line">  </span><br></pre></td></tr></table></figure><p>以上方法为大部分padding mask的计算形式，但实际上，这里做了seq_q全部有效的假设（没有padding），并不够精确 。自己的看法：上述代码expand操作，只是将seq_k中padding的部分重复了L_q次，并没有注意到，seq_q也有padding的部分。即在一个(L_q,L_k)矩阵中，只有最后几列需要掩码，实际矩阵的最后几行也需要掩码。（以后上图更形象）</p><p><strong>sequence mask：</strong>在decoder部分，因为不能见到下文信息（防止泄漏），所以用mask的方式掩盖掉当前时刻t及之后的下文信息。具体，可产生一个对角线为0的上三角矩阵，将其作用到每个decoder的输入列上。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sequence_mask</span>(<span class="params">seq</span>):</span></span><br><span class="line">    batch_size, seq_len = seq.size()</span><br><span class="line">    mask = torch.triu(torch.ones((seq_len, seq_len), dtype=torch.uint8),</span><br><span class="line">                    diagonal=<span class="number">1</span>)</span><br><span class="line">    mask = mask.unsqueeze(<span class="number">0</span>).expand(batch_size, -<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># [B, L, L]</span></span><br><span class="line">    <span class="comment"># 三角矩阵中，为1的部分是需要被掩码掉的</span></span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure><p>decoder-block有两个multi-head attention，下面的multi-head attention是目标输入的self-attention，需要用到1.padding mask：去除padding位置的影响；2.sequence mask：去掉下文穿越的影响。上面的multi-head attention只需要padding mask，因为下面的多头注意力已经磨平了下文信息。当<strong>encoder和decoder的输入序列长度一样时</strong>，可以通过padding mask+sequence mask作为scaled dot-product attention的attn_mask来实现。</p><p>其他情况的attn_mask（代码中的表达）等于padding mask</p><h2 id="为什么在进行多头关注的时候需要对每个head进行切割？"><a href="#为什么在进行多头关注的时候需要对每个head进行切割？" class="headerlink" title="为什么在进行多头关注的时候需要对每个head进行切割？"></a><a href="https://www.zhihu.com/question/350369171">为什么在进行多头关注的时候需要对每个head进行切割？</a></h2><p>@何妨吟啸且徐行 感谢回答</p><p>Transformer的多头注意力看上去是借鉴了CNN中同一卷积层内使用多个卷积核的思想，原文中使用了 8 个“scaled dot-product attention”，在同一“multi-head attention”层中，输入均为“KQV”，<strong>同时</strong>进行注意力的计算，彼此之前<strong>参数不共享</strong>，最终将结果<strong>拼接</strong>起来，这样可以允许模型在<strong>不同的表示子空间里学习到相关的信息</strong>，在此之前的 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1703.03130">A Structured Self-attentive Sentence Embedding</a> 也有着类似的思想。简而言之，就是希望每个注意力头，只关注最终输出序列中一个子空间，互相<strong>独立</strong>。其核心思想在于，抽取到更加丰富的<strong>特征信息</strong>。<br>回到题主的问题上来，如果只使用 one head 并且维度为 <img src="https://www.zhihu.com/equation?tex=d_%7Bmodel%7D" alt="[公式]"> ，相较于 8 head 并且维度为 <img src="https://www.zhihu.com/equation?tex=d_%7Bmodel%7D+%2F+8" alt="[公式]">。首先存在计算量极大的问题，并且高维空间下的学习难度也会相应提升，这就难免文中实验出现的参数量大且效果不佳的情况，于是将原有的高维空间转化为多个低维空间并再最后进行拼接，形成同样维度的输出，借此丰富特性信息，降低了计算量，而且取得了更好的效果，十分巧妙。</p><h2 id="为何在获取输入词向量之后需要对矩阵乘以embeddding-size的开方？意义是什么？"><a href="#为何在获取输入词向量之后需要对矩阵乘以embeddding-size的开方？意义是什么？" class="headerlink" title="为何在获取输入词向量之后需要对矩阵乘以embeddding size的开方？意义是什么？"></a><strong>为何在获取输入词向量之后需要对矩阵乘以embeddding size的开方？意义是什么？</strong></h2><p>embedding matrix的初始化方式是xavier init，这种方式的方差是1/embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。</p><h2 id="你还了解些关于位置编码的技术，各自的优缺点是什么？"><a href="#你还了解些关于位置编码的技术，各自的优缺点是什么？" class="headerlink" title="你还了解些关于位置编码的技术，各自的优缺点是什么？"></a><strong>你还了解些关于位置编码的技术，各自的优缺点是什么？</strong></h2><p><a href="https://zhuanlan.zhihu.com/p/105001610">https://zhuanlan.zhihu.com/p/105001610</a></p><h2 id="Transformer-为什么使用-layer-normalization，而不是其他的归一化方法？"><a href="#Transformer-为什么使用-layer-normalization，而不是其他的归一化方法？" class="headerlink" title="Transformer 为什么使用 layer normalization，而不是其他的归一化方法？"></a><a href="https://www.zhihu.com/question/395811291">Transformer 为什么使用 layer normalization，而不是其他的归一化方法？</a></h2><p><a href="https://www.zhihu.com/question/395811291/answer/1251829041">https://www.zhihu.com/question/395811291/answer/1251829041</a></p><h2 id="Transformer如何并行化的？解码器端可以做并行化吗？"><a href="#Transformer如何并行化的？解码器端可以做并行化吗？" class="headerlink" title="Transformer如何并行化的？解码器端可以做并行化吗？"></a><strong>Transformer如何并行化的？</strong>解码器端可以做并行化吗？</h2><p>Transformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，在self-attention模块，对于某个序列<img src="https://www.zhihu.com/equation?tex=x_%7B1%7D%2C+x_%7B2%7D%2C+%5Cdots%2C+x_%7Bn%7D" alt="[公式]">，self-attention模块可以直接计算<img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D%2C+x_%7Bj%7D" alt="[公式]">的点乘结果，而RNN系列的模型就必须按照顺序从<img src="https://www.zhihu.com/equation?tex=x_%7B1%7D" alt="[公式]">计算到<img src="https://www.zhihu.com/equation?tex=x_%7Bn%7D" alt="[公式]">。</p><h2 id="简单描述一下wordpiece-model和字节对编码？"><a href="#简单描述一下wordpiece-model和字节对编码？" class="headerlink" title="简单描述一下wordpiece model和字节对编码？"></a><strong>简单描述一下wordpiece model和字节对编码？</strong></h2><p><a href="https://zhuanlan.zhihu.com/p/86965595">https://zhuanlan.zhihu.com/p/86965595</a></p><h2 id="Transformer训练的时候学习率是如何设定的？"><a href="#Transformer训练的时候学习率是如何设定的？" class="headerlink" title="Transformer训练的时候学习率是如何设定的？"></a><strong>Transformer训练的时候学习率是如何设定的？</strong></h2><p><img src="https://i.loli.net/2021/07/05/FDESyq27uWr1lvJ.png" alt=""></p><h2 id="Transformer中multi-head机制是如何实现每个head提取的信息空间互斥的？"><a href="#Transformer中multi-head机制是如何实现每个head提取的信息空间互斥的？" class="headerlink" title="Transformer中multi-head机制是如何实现每个head提取的信息空间互斥的？"></a><a href="https://www.zhihu.com/question/357565475">Transformer中multi-head机制是如何实现每个head提取的信息空间互斥的？</a></h2><p>Transformer的多头注意力看上去是借鉴了CNN中同一卷积层内使用多个卷积核的思想，原文中使用了 8 个“scaled dot-product attention”，在同一“multi-head attention”层中，输入均为“KQV”，<strong>同时</strong>进行注意力的计算，彼此之前<strong>参数不共享</strong>，最终将结果<strong>拼接</strong>起来，这样可以允许模型在<strong>不同的表示子空间里学习到相关的信息</strong>。简而言之，就是希望每个注意力头，只关注最终输出序列中一个子空间，互相<strong>独立</strong>。其核心思想在于，抽取到更加丰富的<strong>特征信息</strong>。</p><h2 id="Transformer的细节到底是怎么样的？"><a href="#Transformer的细节到底是怎么样的？" class="headerlink" title="Transformer的细节到底是怎么样的？"></a><strong>Transformer的细节到底是怎么样的？</strong></h2><p><a href="https://www.zhihu.com/question/362131975/answer/945357471">https://www.zhihu.com/question/362131975/answer/945357471</a></p><h2 id="为什么Bert的三个Embedding可以进行相加？"><a href="#为什么Bert的三个Embedding可以进行相加？" class="headerlink" title="为什么Bert的三个Embedding可以进行相加？"></a><strong><a href="https://www.zhihu.com/question/374835153/answer/1040767499">为什么Bert的三个Embedding可以进行相加？</a></strong></h2><p>(Token Embedding、Segment Embedding、Position Embedding三个向量为什么可以相加呢？相加后向量的大小和方向就变了，语义不就变了吗？) 深度神经网络里变得非常复杂，本质上神经网络中每个神经元收到的信号也是“权重”相加得来。这三个向量为什么可以相加呢？因为三个embedding相加等价于三个原始one-hot的拼接再经过一个全连接网络。 相加后向量的大小和方向就变了，语义不就变了吗？这里不是语义变了，而是在训练的时候就是这几个向量相加进行训练的，训练完之后，将lookup后的向量进行相加，就能得到比较好的表示了。 从梯度的角度解释：</p><script type="math/tex; mode=display">(f+g+h)' = f'+g'+h'</script><h2 id="为什么BERT输入的最大长度要限制为512？"><a href="#为什么BERT输入的最大长度要限制为512？" class="headerlink" title="为什么BERT输入的最大长度要限制为512？"></a><strong><a href="https://www.zhihu.com/question/395903256">为什么BERT输入的最大长度要限制为512？</a></strong></h2><p>个人推断是考虑了计算与运行效率综合做出的限制。</p><p>BERT输入的最大长度限制为512, 其中还需要包括[CLS]和[SEP]. 那么实际可用的长度仅为510.<strong>但是别忘了,</strong>每个单词tokenizer之后也有可能被分成好几部分. 所以实际可输入的句子长度远不足510.</p><p>BERT由于position-embedding的限制只能处理最长512个词的句子。如果文本长度超过512，有以下几种方式进行处理：</p><p><strong>a）直接截断：</strong>从长文本中截取一部分，具体截取哪些片段需要观察数据，如新闻数据一般第一段比较重要就可以截取前边部分；</p><p><strong>b）抽取重要片段：</strong>抽取长文本的关键句子作为摘要，然后进入BERT；</p><p><strong>c）分段：</strong>把长文本分成几段，每段经过BERT之后再进行拼接或求平均或者接入其他网络如lstm。</p><p>另外transformer-xl 、<a href="https://zhuanlan.zhihu.com/p/133491514">LongFormer：用稀疏自注意力拓展模型文本容纳量</a>等优秀设计也可以解决长文本。</p><h2 id="为什么BERT选择mask掉15-这个比例的词，可以是其他的比例吗？"><a href="#为什么BERT选择mask掉15-这个比例的词，可以是其他的比例吗？" class="headerlink" title="为什么BERT选择mask掉15%这个比例的词，可以是其他的比例吗？"></a><strong>为什么BERT选择mask掉15%这个比例的词，可以是其他的比例吗？</strong></h2><p>来自@海晨威的算法屋</p><p>BERT采用的Masked LM，会选取语料中所有词的15%进行随机mask，论文中表示是受到完形填空任务的启发，但其实<strong>与CBOW也有异曲同工之妙</strong>。</p><p>从CBOW的角度，这里 <img src="https://www.zhihu.com/equation?tex=p%3D15%5C%25" alt="[公式]"> 有一个比较好的解释是：在一个大小为 <img src="https://www.zhihu.com/equation?tex=1%2Fp%3D100%2F15%5Capprox7" alt="[公式]"> 的窗口中随机选一个词，类似CBOW中滑动窗口的中心词，区别是这里的滑动窗口是非重叠的。</p><p>那从CBOW的滑动窗口角度，10%~20%都是还ok的比例。</p><p>上述非官方解释，是来自我的一位朋友提供的一个理解切入的角度，供参考。</p><p>来自@Serendipity</p><p>15%的概率是通过实验得到的最好的概率，xlnet也是在这个概率附近，说明在这个概率下，既能有充分的mask样本可以学习，又不至于让segment的信息损失太多，以至于影响mask样本上下文信息的表达。然而因为在下游任务中不会出现token“<mask>”，所以预训练和fine-tune出现了不一致，为了减弱不一致性给模型带来的影响，被mask的token有80%的概率用“<mask>”表示，有10%的概率随机替换成某一个token，有10%的概率保留原来的token，这3个百分比也是多次实验得到的最佳组合，在这3个百分比的情况下，下游任务的fine-tune可以达到最佳的实验结果。</p><h2 id="为什么BERT在第一句前会加一个-CLS-标志"><a href="#为什么BERT在第一句前会加一个-CLS-标志" class="headerlink" title="为什么BERT在第一句前会加一个[CLS]标志?"></a><strong>为什么BERT在第一句前会加一个[CLS]标志?</strong></h2><p>bert在token序列之前加了一个特定的token“[cls]”，这个token对应的向量后续会用在分类任务上；如果是句子对的任务，那么两个句子间使用特定的token“[seq]”来分割。</p><p>为什么选它呢，因为与文本中已有的其它词相比，这个无明显语义信息的符号会<strong>更“公平”地融合文本中各个词的语义信息</strong>，从而更好的表示整句话的语义。</p><p>这里补充一下bert的输出，有两种：</p><p>一种是get_pooled_out()，就是上述[CLS]的表示，输出shape是[batch size,hidden size]。</p><p>一种是get_sequence_out()，获取的是整个句子每一个token的向量表示，输出shape是[batch_size, seq_length, hidden_size]，这里也包括[CLS]，因此在做token级别的任务时要注意它。</p><h2 id="Bert和Transformer在loss上的差异"><a href="#Bert和Transformer在loss上的差异" class="headerlink" title="Bert和Transformer在loss上的差异"></a><strong>Bert和Transformer在loss上的差异</strong></h2><p>transformer的loss是在decoder阶段计算的。bert预训练的loss由2部分构成，一部分是NSP的loss，就是token“[cls]”经过1层Dense，然后接一个二分类的loss，其中0表示segment B是segment A的下一句，1表示segment A和segment B来自2篇不同的文本；另一部分是MLM的loss，segment中每个token都有15%的概率被mask，而被mask的token有80%的概率用“<mask>”表示，有10%的概率随机替换成某一个token，有10%的概率保留原来的token，被mask的token经过encoder后乘以embedding matrix的转置会生成在vocab上的分布，然后计算分布和真实的token的one-hot形式的cross entropy，最后sum起来当作loss。这两部分loss相加起来当作total loss，利用adam进行训练。bert fine-tune的loss会根据任务性质来设计，例如分类任务中就是token“[cls]”经过1层Dense，然后接了一个二分类的loss；例如问题回答任务中会在paragraph上的token中预测一个起始位置，一个终止位置，然后以起始位置和终止位置的预测分布和真实分布为基础设计loss；例如序列标注，预测每一个token的词性，然后以每一个token在词性的预测分布和真实分布为基础设计loss。</p><p>bert在encoder之后，在计算NSP和MLM的loss之前，分别对NSP和MLM的输入加了一个Dense操作，这部分参数只对预训练有用，对fine-tune没用。而transformer在decoder之后就直接计算loss了，中间没有Dense操作。</p><h2 id="为什么bert需要额外的segment-embedding"><a href="#为什么bert需要额外的segment-embedding" class="headerlink" title="为什么bert需要额外的segment embedding?"></a><strong>为什么bert需要额外的segment embedding?</strong></h2><p>因为bert预训练的其中一个任务是判断segment A和segment B之间的关系，这就需要embedding中能包含当前token属于哪个segment的信息，然而无论是token embedding，还是position embedding都无法表示出这种信息，因此额外创建一个segment embedding matrix用来表示当前token属于哪个segment的信息，segment vocab size就是2，其中index=0表示token属于segment A，index=1表示token属于segment B。</p><h2 id="为什么transformer的embedding后面接了一个dropout，而bert是先接了一个layer-normalization，再接dropout"><a href="#为什么transformer的embedding后面接了一个dropout，而bert是先接了一个layer-normalization，再接dropout" class="headerlink" title="为什么transformer的embedding后面接了一个dropout，而bert是先接了一个layer normalization，再接dropout?"></a><strong>为什么transformer的embedding后面接了一个dropout，而bert是先接了一个layer normalization，再接dropout?</strong></h2><p>LN是为了解决梯度消失的问题，dropout是为了解决过拟合的问题。在embedding后面加LN有利于embedding matrix的收敛。</p><h2 id="BERT模型有什么调参技巧"><a href="#BERT模型有什么调参技巧" class="headerlink" title="BERT模型有什么调参技巧?"></a><strong>BERT模型有什么调参技巧?</strong></h2><p><a href="https://www.zhihu.com/question/373856698/answer/1034691809">https://www.zhihu.com/question/373856698/answer/1034691809</a></p><h2 id="Transformer中warm-up和LayerNorm的重要性？"><a href="#Transformer中warm-up和LayerNorm的重要性？" class="headerlink" title="Transformer中warm-up和LayerNorm的重要性？"></a><strong>Transformer中warm-up和LayerNorm的重要性？</strong></h2><p><a href="https://zhuanlan.zhihu.com/p/84614490">https://zhuanlan.zhihu.com/p/84614490</a></p><h2 id="Bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？"><a href="#Bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？" class="headerlink" title="Bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？"></a><a href="https://www.zhihu.com/question/318355038">Bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</a></h2><p><a href="https://www.zhihu.com/question/318355038">https://www.zhihu.com/question/318355038</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Transformer的辅助&quot;&gt;&lt;a href=&quot;#Transformer的辅助&quot; class=&quot;headerlink&quot; title=&quot;Transformer的辅助&quot;&gt;&lt;/a&gt;Transformer的辅助&lt;/h1&gt;&lt;p&gt;转载：&lt;a href=&quot;https://zh</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>A Generalization of Transformer Networks to Graphs</title>
    <link href="http://example.com/2021/06/29/A-Generalization-of-Transformer-Networks-to-Graphs/"/>
    <id>http://example.com/2021/06/29/A-Generalization-of-Transformer-Networks-to-Graphs/</id>
    <published>2021-06-29T11:50:43.000Z</published>
    <updated>2021-07-22T07:23:48.197Z</updated>
    
    <content type="html"><![CDATA[<h1 id="A-Generalization-of-Transformer-Networks-to-Graphs"><a href="#A-Generalization-of-Transformer-Networks-to-Graphs" class="headerlink" title="A Generalization of Transformer Networks to Graphs"></a>A Generalization of Transformer Networks to Graphs</h1><p>作者提出了一种适用于任意图的Transformer结构的。 </p><h2 id="针对问题"><a href="#针对问题" class="headerlink" title="针对问题:"></a>针对问题:</h2><p>最初的Transformer相当于一个在所有单词之间都有连接的全连通图上操作，但这样的体系结构没有利用图的连通感应偏差，并且当图拓扑结构重要时，没有被编码到节点特征汇总，性能不好。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>作者提出四个新特性：</p><ul><li>首先，注意机制是图中每个节点的邻域连通性的函数。</li><li>其次，位置编码由Laplacian特征向量来表示，用在了原始Transformer在NLP中常用的正弦位置编码。</li><li>第三，用batch normalization层代替layer normalization，提供了更快的训练速度和更好的泛化性能。</li><li>最后，将对任务至关重要的边缘特征表示，加入到该graph-transformer结构中。</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="Graph-Transformer-2019"><a href="#Graph-Transformer-2019" class="headerlink" title="Graph Transformer-2019"></a>Graph Transformer-2019</h3><p>首先作者提出在2019年Graph Transformer那篇文章，为了捕捉全局信息将attention应用在全图节点上替代了局部邻居。</p><p>但作者认为这样限制了稀疏性的有效利用，这是在图数据上学习比较重要的ductive bias。 为了获取全局信息的目的，作者认为还有其他方法可以合并相同的信息，而不是放弃稀疏性和局部上下文。</p><blockquote><p>例如，使用特定于图形的位置特征(Zhang et al.2020)，或节点拉普拉斯位置特征向量(Belkin和Niyoi 2003；Dwivedi等人)。2020)或相对可学习的位置信息(You、Ying和Leskovec 2019)、虚拟节点 (Li et al. 2015)等。</p></blockquote><h3 id="Graph-BERT-2020"><a href="#Graph-BERT-2020" class="headerlink" title="Graph-BERT-2020"></a>Graph-BERT-2020</h3><p>其次作者在相关工作中评价了Graph-Bert那篇文章，其强调预先训练和并行化学习，使用一种子图批处理方案，创建固定大小的无链接子图，将其传递给模型，而不是原始图。</p><p>Graph-BERT采用几种位置编码方案的组合来捕获绝对节点结构和相对节点位置信息。由于原始图不直接用于Graph-BERT，并且子图在节点之间没有边(即，无链接)，所提出的位置编码的组合试图在节点中保留原始图的结构信息。</p><h3 id="HGT-2020、GTN-2019"><a href="#HGT-2020、GTN-2019" class="headerlink" title="HGT-2020、GTN-2019"></a>HGT-2020、GTN-2019</h3><p>Graph Transformer Networks(GTN)学习异构图，目标是将给定的异构图转换为基于元路径的图，然后执行卷积。</p><p>他们使用attention背后的重点是为了预置生成的meta-paths，除了能够处理任意数目的节点和边类型外，HGT还以基于中心节点和消息传递节点的时间戳差异的相对时间位置编码的形式捕获了异质图中信息流的动态变化。</p><h3 id="本文贡献"><a href="#本文贡献" class="headerlink" title="本文贡献"></a>本文贡献</h3><p>提出稀疏性和位置编码是图变形器开发中的两个关键方面。与为特定的图形任务设计一个性能最佳的模型相反，这篇的工作是尝试一个通用的graph-Transformer模型。</p><ul><li>提出了一种将Transformer网络可以用于任意结构同质图的方法，即Graph Transformer，并提出了一种具有边特征的扩展Graph Transformer，它允许使用显式的域信息作为边特征。</li><li>利用图数据集的拉普拉斯特征向量来融合节点位置特征， 与文献的比较表明，对于任意同质图，拉普拉斯特征向量比任何现有的编码节点位置信息的方法都要好。</li><li>实验证明其好于传统GNN</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="图的稀疏性"><a href="#图的稀疏性" class="headerlink" title="图的稀疏性"></a>图的稀疏性</h3><p>在NLP Transformer中，句子被视为完全连通的图形，这种选择有两个原因：</p><ul><li>很难在句子中的单词之间找到有意义的稀疏交互或联系。例如，句子中的一个词对另一个词的依赖性可以随上下文、用户的视角和特定应用而变化。一个句子中的词之间可能存在许多似是而非的基本事实连接，因此，句子的文本数据集没有显式的词交互可用。因此，让一个句子中的每个单词相互关注其他单词是有意义的，就像Transformer架构所遵循的那样。</li><li>在NLP Transformer中考虑的所谓的图通常具有少于数十或数百个节点。这在计算上是可行的，大型变压器模型可以在这种完全连通的文字图上进行训练。</li></ul><p>在高达数百万或数十亿的节点大小。可用的结构为我们提供了丰富的信息源，可以作为神经网络中的归纳偏差加以利用，而节点大小实际上使得这样的数据集不可能有一个完全连通的图。</p><h3 id="图的位置编码"><a href="#图的位置编码" class="headerlink" title="图的位置编码"></a>图的位置编码</h3><p>在NLP中，大多数情况下，基于Transformer的模型由每个字的位置编码补充。这对于确保每个单词的唯一表示以及甚至保留距离信息是至关重要的。</p><blockquote><p>对于图，唯一节点位置的设计是具有挑战性的，<strong>因为存在防止规范节点位置信息的对称</strong> 。事实上，大多数GNN学习的是节点位置不变的structural node信息。</p><p>这就是为什么简单的基于注意力的模型，如GAT，其中attention是局部邻域连通性的函数，而不是全图连通性。</p></blockquote><p>为了更好地对距离感知信息进行编码，(附近节点具有相似的位置特征，而较远的节点具有不同的位置特征)使用拉普拉斯特征向量作为图变换中的PE。</p><p> 作者在训练过程中随机反转特征向量的符号，遵循 Benchmarking graph neural networks 的做法 。预先计算了数据集中所有图的拉普拉斯特征向量。通过对图的拉普拉斯矩阵进行因式分解来定义特征向量；</p><script type="math/tex; mode=display">\Delta = I - D^{-1/2}AD^{-1/2}=U^T\Lambda U</script><p>使用节点的k个最小特征向量作为其位置编码，并对节点 $i$用 $λ_i$表示。</p><h3 id="Graph-Transformer-Architecture"><a href="#Graph-Transformer-Architecture" class="headerlink" title="Graph Transformer Architecture"></a>Graph Transformer Architecture</h3><p><img src="https://z3.ax1x.com/2021/06/29/RwVhq0.png" alt=""></p><p>左边的模型是为没有明确边属性的图设计的，右边的模型维护一个指定的边特征，以结合可用的边信息并在每一层维护它们的抽象表示。</p><h4 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h4><p>图 $G$ 的节点特征 $\alpha<em>i \in R^{d_n \times 1}$ ，节点$i,j$ 对应的边特征$\beta</em>{ij}\in R^{d_e \times 1}$</p><p>$\alpha<em>i, \beta</em>{ij}$  通过线性映射成为$d$ 维隐层特征 $h<em>i^0 , e</em>{ij}^0$ :</p><script type="math/tex; mode=display">\hat h_i^0 = A^0\alpha_i + a^0 ; e_{ij}^0 = B^0\beta_{ij}+b^0</script><p>其中，$A^0\in R^{d\times d_n}, B^0\in R^{d\times d_e}, a,b\in R^d$</p><p>将位置编码线性映射后加入节点特征:</p><script type="math/tex; mode=display">\lambda_i^0 = C^0\lambda_i +c^0; h_i^0 = \hat h_i^0 + \lambda_i^0</script><p>其中，$C^0\in R^{d\times k}, c^0 \in R^d$ . 请注意，拉普拉斯位置编码仅添加到输入层的节点特征，而不是在中间层。</p><h4 id="Graph-Transformer-Layer"><a href="#Graph-Transformer-Layer" class="headerlink" title="Graph Transformer Layer"></a>Graph Transformer Layer</h4><p>第 $l$ 层节点更新:</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \hat h_i^{l+1} &= O^l_h \parallel_{k=1}^H \left ( \sum_{j\in N_i} w_{ij}^{k,l} V^{k,l} h^l_j \right )\\ where, w_{i,j}^{k,l} &= softmax_j(\frac{Q^{k,l} \cdot K^{k,l}h_j^l}{\sqrt{d_k}})    \end{split}\end{equation}</script><p>其中，$Q^{k,l}, K^{k,l}, V^{k,l} \in R^{d_k \times d}, O^{l}_h\in R^{d\times d}$ ，$k$ 注意力haed数。</p><p>为了数值稳定性，在取softmax 的输出被限制介于−5到+5之间。</p><p>然后，将attention输出$h^{l+1}$传递给FFN，然后是残差和归一化层，如下所示：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \hat{\hat {h}}_i ^{l+1} &= Norm(h_i^l + \hat h^{l+1}_i) \\ \hat{\hat{\hat{h_i}}}^{l+1} &= W_2^l ReLU(W_1^l \hat{\hat {h}}_i ^{l+1}) \\ h^{l+1}_i &= Norm(\hat{\hat {h}}_i ^{l+1}+\hat{\hat{\hat{h_i}}}^{l+1})    \end{split}\end{equation}</script><p>其中 $W_1^l\in R^{2d \times d}, W_2^l \in R^{d\times 2d}$ 为了说明清楚，省略了偏执项。</p><h4 id="Graph-Transformer-Layer-with-edge-features"><a href="#Graph-Transformer-Layer-with-edge-features" class="headerlink" title="Graph Transformer Layer with edge features"></a>Graph Transformer Layer with edge features</h4><p>旨在更好地利用图数据集中以边属性形式提供的丰富特征信息。</p><p><img src="https://z3.ax1x.com/2021/06/30/RwzFXD.png" alt=""></p><p>这些边特征是对应于节点对的相关分数，所以将这些可用的边特征与通过 pairwise attention计算的隐含边分数联系起来。换言之，假设在query 和 key 特征投影相乘之后，当节点 $i$ 关注节点 $j$ 时，计算在softmax注意分数 $\hat w<em>{ij}$， 将该分数  $\hat w</em>{ij}$ 视为关于边 $<i，j>$的隐含信息。</p><p>利用边特征改进已经计算的隐式注意分数 $\hat w<em>{ij}$ 。通过简单地将两个值 $\hat w</em>{ij}$ 和 $e_{ij}$ 相乘来实现的</p><p>指定的节点对称的边特征表示管道，用于将边属性从一层传播到另一层：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}   \hat h_i^{l+1} &= O^l_h \parallel_{k=1}^H \left ( \sum_{j\in N_i} w_{ij}^{k,l} V^{k,l} h^l_j \right )\\ \\  \hat e^{l+1}_{ij} &= O^l_e \parallel_{k=1}^H(\hat w_{ij}^{k,l}),\\  \\  where, w_{ij}^{k,l} &= softmax_j(\hat w_{ij}^{k,l}) ,\\ \\  \hat w_{ij}^{k,l} &= \left(\frac{Q^{k,l} \cdot K^{k,l}h_j^l}{\sqrt{d_k}} \cdot E^{k,l}e^{l}_{ij} \right)    \end{split}\end{equation}</script><p>其中 $Q^{k,l}, K^{k,l}, V^{k,l} ,E^{k,l} \in R^{d_k\times d} , O^l_h,O^l_e\in R^{d\times d}$</p><p>其余d 也是要经过Transformer架构中的其他成分。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://z3.ax1x.com/2021/06/30/R0NJln.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;A-Generalization-of-Transformer-Networks-to-Graphs&quot;&gt;&lt;a href=&quot;#A-Generalization-of-Transformer-Networks-to-Graphs&quot; class=&quot;headerlink&quot;</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>Do Transformers Really Perform Bad for Graph Representation?</title>
    <link href="http://example.com/2021/06/23/Do-Transformers-Really-Perform-Bad-for-Graph-Representation/"/>
    <id>http://example.com/2021/06/23/Do-Transformers-Really-Perform-Bad-for-Graph-Representation/</id>
    <published>2021-06-23T05:22:15.000Z</published>
    <updated>2021-06-24T16:18:44.642Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Do-Transformers-Really-Perform-Bad-for-Graph-Representation"><a href="#Do-Transformers-Really-Perform-Bad-for-Graph-Representation" class="headerlink" title="Do Transformers Really Perform Bad for Graph Representation?"></a>Do Transformers Really Perform Bad for Graph Representation?</h1><p>作者提出了一个建立在Transformer体系结构上可以对图进行出色表征的模型——Graphormer。</p><p>近来Transformer和Graph进行结合尝试的文章有：</p><ul><li>Graph transformer for graph-to-sequence learning. AAAI 2020.</li><li>A generalization of transformer networks to graphs. AAAI 2021. (GT)</li><li>Heterogeneous graph transformer.  2020.</li><li>Re-thinking graph transformers with spectral attention.   2021.  (&gt;GT)</li><li>Graph transformer.  2019.</li><li>Direct multi-hop attention based graph neural network. 2020</li><li>Graph transformer networks.   2019.</li><li>Transformers are Graph Neaurl Networks. 2020</li><li>Graph-bert: Only attention is needed for learning graph representations. 2020</li></ul><p>虽然有很多利用Transformer进入图形领域的尝试，但唯一有效的方法是用Softmax attention 取代经典GNN变体中的一些关键模块(例如，特征聚合)。因此，Transformer体系结构是否适合于对图进行建模，以及如何使其在图表示学习中发挥作用，仍然是一个悬而未决的问题。</p><h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><p>How Transformers could perform well for graph representation learning?</p><p>作者解决办法：关键是要正确地将图的结构信息融入到模型中。</p><ol><li><p><em>Centrality Encoding</em>：捕捉节点在图中的重要性。在图中，不同的节点可以具有不同的重要性，例如，名人被认为比社交网络中的大多数网络用户更有影响力。然而，这种信息没有反映在self-attention模块中，因为它主要使用节点语义特征来计算相似度。</p><p>1.1. <em>degree centrality</em> ：利用度中心性进行Centrality Encoding，其中根据每个节点的度将可学习向量分配给每个节点，并将其添加到输入层中的节点特征。</p></li><li><p><em>Spatial Encoding</em> ：提出了一种新的空间编码来捕捉节点之间的结构关系。将图形结构化数据与其他结构化数据(例如语言、图像)区分开来的一个值得注意的几何属性是不存在用于嵌入图形的规范网格，结点只能位于非欧几里德空间中，并且由边连接。作者使用任意两个节点之间的<strong>最短路径距离</strong>作为示例，将其编码为Softmax关注度中的偏置项，以帮助模型准确地捕捉图中的空间相关性。</p><p>2.1. 此外，有时edge特征中还包含额外的空间信息，例如分子图中两个原子之间的键类型。设计了一种新的Edge Encoding方法，将这种信号进一步带入Transformer layers。</p></li></ol><h2 id="Graphormer"><a href="#Graphormer" class="headerlink" title="Graphormer"></a>Graphormer</h2><h3 id="Structural-Encodings-in-Graphormer"><a href="#Structural-Encodings-in-Graphormer" class="headerlink" title="Structural Encodings in Graphormer"></a>Structural Encodings in Graphormer</h3><p>三种有效的Graphormer编码：Centrality Encoding、Spatial Encoding、Edge Encoding in the Attention</p><p><img src="https://i.loli.net/2021/06/24/CoMykaSRbVxvtUc.png" alt=""></p><h4 id="特征层面-Centrality-Encoding"><a href="#特征层面-Centrality-Encoding" class="headerlink" title="特征层面 Centrality Encoding"></a>特征层面 Centrality Encoding</h4><p>普通的attention机制是基于节点之间的语义相关性来计算注意力分布的，节点的中心性可以衡量节点在图中的重要性，但这种信息在目前的attention计算中被忽略了。(attention不是也可以计算出什么比较重要吗，这里有点疑问)</p><p>在Graphormer中，作者使用了degree中心性，作为节点中心性的度量。根据每个节点的入度和出度为每个节点分配两个实值embedding向量。对每个节点都应用中心性编码，因此需将其作为输入添加到节点特征中。</p><script type="math/tex; mode=display">h_i^{(0)} = x_i + z_{deg^{-}(v_i)}^- + z^+_{deg^+(v_i)}</script><p>通过在输入中使用中心性编码，Softmax注意力既可以捕捉到查询和关键字中的节点重要性信号，又能捕捉到节点的重要性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.in_degree_encoder = nn.Embedding(<span class="number">512</span>, hidden_dim, padding_idx=<span class="number">0</span>)</span><br><span class="line">self.out_degree_encoder = nn.Embedding(<span class="number">512</span>, hidden_dim, padding_idx=<span class="number">0</span>)</span><br><span class="line">node_feature = node_feature + self.in_degree_encoder(in_degree) + self.out_degree_encoder(out_degree)</span><br></pre></td></tr></table></figure><h4 id="注意力层面-Spatial-Encoding"><a href="#注意力层面-Spatial-Encoding" class="headerlink" title="注意力层面 Spatial Encoding"></a>注意力层面 Spatial Encoding</h4><p>original的transformer的注意力机制使得它具有全局的感受野，前提是对于每一个token需要指定一个位置，比如一个句子中不同词的位置可以用 $1,2,3,4……$ 来代表序列中不同位置的编码。对于图来说并不存在序列这样的位置特性，那么应该如何考虑不同节点的位置信息呢？</p><p>图结构通常是在非欧空间下的，两个节点的位置关系可以用节点之间的最短路径 $\phi(v<em>i,v_j):V \times V \to R$ 来表示(无关联的两个节点之间的最短路径为-1)。 函数 $\phi$ 可以由图中节点之间的连通性来定义。在这篇文章中作者选用的是最短路径长度SPD。并进行可学习参数映射为 $b</em>{\phi(v_i,v_j)}$  ，在所有层之间共享。</p><p>将这一特征融入注意力矩阵，使得注意力系数也包含了图中节点的相对位置（连接关系）信息：</p><script type="math/tex; mode=display">A_{ij} = \frac{(h_iW_Q)(h_jW_K)^T}{\sqrt{d}} + b_{\phi}(v_i,v_j)</script><p>这样做的好处：</p><ul><li>首先，与传统GNN相比，其中接受场仅限于邻居，可以在公式中看到这一点。transformer层提供了全局信息，每个节点都可以关注图中的所有其他节点。</li><li>其次，通过使用 $b_{\phi(v_i,v_j)}$，单个transformer层中的每个节点可以根据图的结构信息自适应地关注所有其他节点。比如学到的b是个单调减函数，那就表达了<strong>最短路径越大的节点之间的关系越小</strong>，这也是图神经网络中的基本思想：<strong>越近的邻居的信息越重要</strong>。</li></ul><h4 id="注意力层面-Edge-Encoding-in-the-Attention"><a href="#注意力层面-Edge-Encoding-in-the-Attention" class="headerlink" title="注意力层面 Edge Encoding in the Attention"></a>注意力层面 Edge Encoding in the Attention</h4><p>此外，graph相关的任务中通常还有边的信息，这些特征对于图形表示很重要。</p><p>以前工作中的有两种编码方式：</p><ul><li>在第一种方法，边缘特征被添加到关联节点的特征。</li><li>在第二种方法，对于每个节点，其关联边的特征将与节点特征一起aggregation使用。</li></ul><p>但这种使用边特征的方法只将边信息传播到其关联的节点，这可能不是利用边信息来表示整个图的有效方式。</p><p>作者提出了一个新的edge编码方式，</p><p>注意机制需要估计每个节点对 $(v<em>i，v_j)$ 的相关性，作者认为应该在相关性中考虑连接它们的边 (像多跳图网络那样)。对于每个有序节点对 $(v_i，v_j)$，从 $v_i$ 到 $v_j$ 的最短路径为 $SP</em>{ij}=(e_1，e_2，…，e_n)$，则可以用路径的加权平均得到两个节点之间边的相关信息：</p><script type="math/tex; mode=display">e_{ij} = \frac{1}{N} \sum_{n=1}^N x_{e_n} (w_n^E)^T</script><p>其中 $x_{e_n}$ 是第n个边 $e_n$ 的特征，$w_n^E \in R^{d_E}$ 是第n个权重embedding，$d_E$ 是边特征的维度。</p><p>将这个信息也一起融入到注意力机制中：</p><script type="math/tex; mode=display">A_{ij} = \frac{(h_iW_Q)(h_jW_K)^T}{\sqrt{d}} + b_{\phi}(v_i,v_j) + e_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scaled Dot-Product Attention.</span></span><br><span class="line"><span class="comment"># Attention(Q, K, V) = softmax((QK^T)/sqrt(d_k))V</span></span><br><span class="line">q = q * self.scale</span><br><span class="line">x = torch.matmul(q, k)  <span class="comment"># [b, h, q_len, k_len]</span></span><br><span class="line">x = x + attn_bias</span><br><span class="line">x = torch.softmax(x, dim=<span class="number">3</span>)</span><br><span class="line">x = self.att_dropout(x)</span><br><span class="line">x = x.matmul(v)  <span class="comment"># [b, h, q_len, attn]</span></span><br></pre></td></tr></table></figure><blockquote><p>把 $A_{ij}$  的三部分进行拆分，三个分支的网络分别学习不同层面的信息，然后再后面再对三个分支的网络输出进行concat或者attention是否有帮助？</p></blockquote><h3 id="图池化的一个细节"><a href="#图池化的一个细节" class="headerlink" title="图池化的一个细节"></a>图池化的一个细节</h3><p>想要表示图级别的embedding，Graphormer采用了一个额外写的特殊节点 $[VNode]$ , 让它和图中每个节点都链接。</p><p>有点像bert中的CLS一样，整个图$h_G$的表示将是最终层中的 $[VNode]$ 的节点特征</p><h2 id="Do-these-modifications-make-Graphormer-more-powerful-than-other-GNN-variants"><a href="#Do-these-modifications-make-Graphormer-more-powerful-than-other-GNN-variants" class="headerlink" title="Do these modifications make Graphormer more powerful than other GNN variants?"></a><em>Do these modifications make Graphormer more powerful than other GNN variants?</em></h2><h3 id="Fact-1"><a href="#Fact-1" class="headerlink" title="Fact 1."></a>Fact 1.</h3><p>通过选择合适的权值和距离函数 $\phi$，Graphormer layer可以表示GIN、GCN、GraphSAGE等流行的广义网络模型的AGGREGATE和 COMBINE 步骤。</p><ol><li>Spatial encoding 使得 self-attention 模块能够区分节点 $v_i$ 的邻居集合 $N(v_i)$，从而SoftMax函数可以计算 $N(v_i)$ 上的平均统计量</li><li>通过了解节点的度，平均邻域可以转化为邻域之和</li><li>利用多头注意力和FFN，可以分别处理 $v_i$ 和 $N(v_i)$ 的表示，并且将其组合在一起。 </li></ol><h3 id="证明1"><a href="#证明1" class="headerlink" title="证明1"></a>证明1</h3><h4 id="采用空间编码的自我注意模块可以表示均值聚合"><a href="#采用空间编码的自我注意模块可以表示均值聚合" class="headerlink" title="采用空间编码的自我注意模块可以表示均值聚合"></a>采用空间编码的自我注意模块可以表示均值聚合</h4><script type="math/tex; mode=display">A_{ij} = \frac{(h_iW_Q)(h_jW_K)^T}{\sqrt{d}} + b_{\phi}(v_i,v_j)</script><p>如果 $\phi=1$ ，则设置 $b<em>{\phi}=0$ ，否则设置 $b</em>{\phi}=−∞$，其中 $\phi$ 是 SPD。意思是如果直接相连就考虑其原本的注意力分数，如果不是直接相连注意力分数就为负无穷，就和普通的GNN一样的了。</p><p>设置 $W_Q=W_K=0$ 且 $W_V$ 为单位矩阵，$softmax(A)V$ 表示邻居表示的平均值。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p> OGB-LSC quantum chemistry regression (i.e., PCQM4M-LSC) challenge</p><p>这是目前最大的图形级预测数据集，总共包含超过380万个图表</p><p>还有其他三个任务：ogbg- molhiv, ogbg-molpcba and ZINC</p><p>主要有两种模型大小：</p><p>Graphormer (L = 12, d = 768) ， $Graphormer_{SMALL}$ (L = 6,d = 512).</p><p><img src="https://i.loli.net/2021/06/25/QJ9CO7jATVIXzxE.png" alt=""></p><p><img src="https://i.loli.net/2021/06/25/h3dgUJ65qlzEAuX.png" alt=""></p><h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>将以前使用的位置编码(PE)与作者提出的空间编码进行了比较，这两种编码的目的都是对变压器的不同节点关系信息进行编码。</p><p>以前的基于变压器的GNN采用了各种PE，例如Weisfeiler-Lehman-PE(WL-PE)和Laplacian PE。采用的是Laplacian ，有文章证明一系列PE相比，它的性能很好。</p><p>采用空间编码的transformer体系结构的性能优于基于位置编码的transformer体系结构，说明了利用空间编码捕获节点空间信息的有效性。</p><p><img src="https://i.loli.net/2021/06/25/5INfs8M9XHSmqDK.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Do-Transformers-Really-Perform-Bad-for-Graph-Representation&quot;&gt;&lt;a href=&quot;#Do-Transformers-Really-Perform-Bad-for-Graph-Representation&quot; </summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>Avoiding Reasoning Shortcuts- Adversarial Evaluation, Training, and Model Development for Multi-Hop QA</title>
    <link href="http://example.com/2021/06/13/Avoiding-Reasoning-Shortcuts-Adversarial-Evaluation-Training-and-Model-Development-for-Multi-Hop-QA/"/>
    <id>http://example.com/2021/06/13/Avoiding-Reasoning-Shortcuts-Adversarial-Evaluation-Training-and-Model-Development-for-Multi-Hop-QA/</id>
    <published>2021-06-13T13:59:08.000Z</published>
    <updated>2021-06-23T02:33:27.490Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Avoiding-Reasoning-Shortcuts-Adversarial-Evaluation-Training-and-Model-Development-for-Multi-Hop-QA"><a href="#Avoiding-Reasoning-Shortcuts-Adversarial-Evaluation-Training-and-Model-Development-for-Multi-Hop-QA" class="headerlink" title="Avoiding Reasoning Shortcuts- Adversarial Evaluation, Training, and Model Development for Multi-Hop QA"></a>Avoiding Reasoning Shortcuts- Adversarial Evaluation, Training, and Model Development for Multi-Hop QA</h1><p>这篇文章作者发现在HotpotQA中经常包含Reasoning Shortcuts。也就是说模型没有真正的理解文章并进行推理，而是通过将问题与上下文中的句子进行词匹配来直接定位答案。</p><p>作者主要做了两件事：</p><ul><li>构建干扰文档数据，证明了存在推理shortcut现象。</li><li>设计一个新模型来缓解这个问题</li></ul><h2 id="对抗验证"><a href="#对抗验证" class="headerlink" title="对抗验证"></a>对抗验证</h2><p><img src="https://z3.ax1x.com/2021/06/14/27n5O1.png" alt=""></p><p>问题是What was the father of Kasper Schmeichel voted to be by the IFFHS in 1992? （卡斯珀·舒梅切尔的父亲1992年被IFFHS投票选为什么？）</p><p>模型需要从两个文档中考虑信息，找出隐含的推理链条。</p><p>Kasper Schmeichel $\rightarrow^{son}$  Peter Schemeichel $\rightarrow^{voted}$ World’s Best Goalkeeper</p><p>Kasper Schmeichel是Peter Schemeicher, Peter Schemeicher 被投票为 World’s Best Goalkeeper 世界最佳守门员。</p><p>在该示例中，也可以通过将问题中的几个关键字(“voted to be by the IFFHS in 1992——1992年投票的IFFHS”)与上下文中的相应事实相匹配来得到正确的回答，而无需通过第一跳推理来找到“Kasper Schmeichel的父亲”，因为两个distractor文档都不包含足够分散注意力的信息。 </p><p>因此，一个在现有评估上表现良好的模型并不一定表明它具有很强的复合推理能力。</p><p>随着对抗扰动文本被添加到上下文中，使用单跳shotcut方式不再可能找到正确的答案，这现在导致了两个可能的答案(“世界最佳守门员”和“世界最佳防守人”)。</p><p>添加干扰后模型预测答案为 IFFHS World’s Best Defender 最佳防守人。</p><h2 id="如何构建对抗数据"><a href="#如何构建对抗数据" class="headerlink" title="如何构建对抗数据"></a>如何构建对抗数据</h2><p><strong>要构建这种对抗的需求</strong></p><blockquote><p>作者指出HotpotQA从维基百科中选择距离目标问题最短的bigram TF-IDF的前8个文档作为干扰项，形成总共10个文档的上下文。由于在生成问题时没有向群组工作人员提供导向文档，因此不能保证在给定整个上下文的情况下，两个支持文档都是必要的来推断答案。</p><p>多跳假设可以通过两种方式由不完整的分心文档来实现。</p><ol><li><p>其中一个选定的干扰项可能包含推断答案所需的所有证据，从经验上讲，在HotpotQA中没有发现这样的情况，因为关于一个主题的Wiki文章很少讨论另一个主题的细节</p></li><li><p>整个分散注意力的文档池可能不包含真正分散读者/模型注意力的信息。</p></li></ol></blockquote><p>作者把绕过推理回答问题这种方式，叫做shortcut 。其经常出现在HotpotQA中的桥接问题中，比较问题一般不能匹配而得出。作者采样了50个桥接问题，发现其中26个有这种问题。</p><hr><p>设原内容、问题答案为 $(C,q,a)$ 是可能包含shortcut问题的数据，作者是想将其变为$(C’,q,a)$ </p><p>$q,a$ 不变，$C’$ 变成接近 $C$ 的文章。在HotpotQA中是提供两个支持文档$P$的， 其中$P\subset C$ 。</p><p> 在构建对抗文本时也就是ADDDoc。就是利用新的 $P’$ ，混合$(C,P’)$ 构成新的数据集。</p><p><strong>那么 $P’$ 如何来的？</strong></p><p>假设 $p2\in P$ 是包含答案的支持文档，$p1\in P$ 是包含线索的文档。</p><p>ADDDoc是利用词或短语级别的干扰，将 $p2$ 替换成$p’2$ ，其包含满足推理快捷方式但不与整个问题的答案相矛盾的假答案。</p><p><strong>那么词语是如何替换的？</strong></p><p>首先，对于答案中的每个非停用词，都会在GloVe100维向量空间中找到最接近的10个替代词，它的子串与原始答案的重叠子串长度不超过3个。如(Mumbai → Delhi, Goalkeeper → Defender)。如果这个过程失败，就从HotpotQA dev集合的整个答案池中随机抽样一个候选者 。 </p><p>如果原始答案有多个单词，我们将答案中的一个非停用词与相应的抽样答案单词替换，以创建假答案(“World’s Best Goalkeeper → World’s Best Defender”)。</p><p><img src="https://z3.ax1x.com/2021/06/14/27EFfA.png" alt="27EFfA.png"></p><p>问：Sachin Wamer作为软件工程师所在的公司的总部在哪里？</p><p>由此产生的段落 $p’2$ 提供了一个满足推理捷径的答案，但也与整个问题的真实答案相矛盾，因为它形成了另一个有效的推理链，将问题与假答案连接起来 (Sachin Warrier $\rightarrow^{work}$ TCS $\rightarrow^{at}$ Delhi)。</p><p>为了打破这个矛盾的推理链，我们需要用另一个实体替换连接两个证据的桥梁实体（在这种情况下为“Tata Consultancy Services”），这样生成的答案就不再作为有效答案 。</p><p>用从 HotpotQA 开发集中所有文档标题中随机采样的候选者替换 $p’2$ 的标题。 如果 $p1$ 的标题出现在$p’2$ 中，我们也将其替换为另一个采样标题，以彻底消除 $p’2$ 和 $p1$ 之间的联系。</p><p>如上图将 Tata Consultancy Services  替换为  Valencia Street Circuit </p><h2 id="模型方法"><a href="#模型方法" class="headerlink" title="模型方法"></a>模型方法</h2><h3 id="Encoding"><a href="#Encoding" class="headerlink" title="Encoding"></a>Encoding</h3><p>对于cotnext 和question 使用v 维Highway Network 合并 字符嵌入和GloVe词嵌入。</p><p>得到 $x\in R^{J\times v}$ 和 $q\in R^{S\times v}$ 其中 J 是文章长度 S 为问题长度</p><p>整体结构和BiDAF那个文章相似。</p><p><a href="https://imgtu.com/i/27xDFP"><img src="https://z3.ax1x.com/2021/06/14/27xDFP.png" alt="27xDFP.png"></a></p><h3 id="Single-Hop-Baseline"><a href="#Single-Hop-Baseline" class="headerlink" title="Single-Hop Baseline"></a>Single-Hop Baseline</h3><p>使用bi-attention + self-attention ，给定上下文和问题encoding  $h,u$ 经过 context-to-query $BiAttn(h,u)$ 计算得到一个相似矩阵 $M^{S\times J}$ :</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  M_{s,j} &= W_1u_s +W_2h_j + W_3(u_s\odot h_j) \\ p_{s,j} &= \frac {exp(M_{s,j})}{\sum_{s=1}^S exp(M_{s,j})}\\ c_{q_j} &= \sum_{s=1}^S p_{s,j} u_s    \end{split}\end{equation}</script><p>$\odot$ 对应元素相乘。</p><p>然后query-to-context 注意力：</p><script type="math/tex; mode=display">\begin{equation}\begin{split} m_j &= max_{1\le s\le S} M_{s,j} \\ p_{s,j} &= \frac {exp(m_{j})}{\sum_{j=1}^J exp(m_{j})}\\ q_c &= \sum_{j=1}^J p_{j} h_j\\ h'_j &= [h_j ;c_{q_j}; h_j\odot c_{q_j}; c_{q_j} \odot q_c]\\ h^1 &= BiLSTM(h')    \end{split}\end{equation}</script><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://i.loli.net/2021/06/23/mPiX5Rkz1o9tCSl.png" alt=""></p><p><img src="https://i.loli.net/2021/06/23/yQODEbgUcaeZApK.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Avoiding-Reasoning-Shortcuts-Adversarial-Evaluation-Training-and-Model-Development-for-Multi-Hop-QA&quot;&gt;&lt;a href=&quot;#Avoiding-Reasoning-Sh</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>TRANSFORMER-XH: MULTI-EVIDENCE REASONING WITH EXTRA HOP ATTENTION</title>
    <link href="http://example.com/2021/06/10/TRANSFORMER-XH-MULTI-EVIDENCE-REASONING-WITH-EXTRA-HOP-ATTENTION/"/>
    <id>http://example.com/2021/06/10/TRANSFORMER-XH-MULTI-EVIDENCE-REASONING-WITH-EXTRA-HOP-ATTENTION/</id>
    <published>2021-06-10T09:58:11.000Z</published>
    <updated>2021-06-25T10:10:14.751Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TRANSFORMER-XH-MULTI-EVIDENCE-REASONING-WITH-EXTRA-HOP-ATTENTION"><a href="#TRANSFORMER-XH-MULTI-EVIDENCE-REASONING-WITH-EXTRA-HOP-ATTENTION" class="headerlink" title="TRANSFORMER-XH: MULTI-EVIDENCE REASONING WITH EXTRA HOP ATTENTION"></a>TRANSFORMER-XH: MULTI-EVIDENCE REASONING WITH EXTRA HOP ATTENTION</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>提出Transformer-XH配有eXtra Hop attention以完全数据驱动的方式实现了结构化文本的内在建模。</p><p>完全数据驱动应该是指不仅可以处理序列结构的数据，还可以处理图结构。</p><p>eXtra Hop attention 除了关注每个序列内的记号外，还可以连接的文本序列之间跳跃</p><p>因此文档之间传播信息和构建全局上下文表示来更好地进行联合多证据推理。</p><p>eXtra Hop attention的作用：</p><ul><li>当每段文本与其他证据相关时能够更全局地表示每段文本所贡献的证据</li><li>以一种自然的方式，通过必要的边信息传递对证据图联合推理</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>面临的第一个挑战就是由于transformer的softmax计算在所有token对上，很难处理长文本。</p><p> Transform-XL(eXtra Long) 通过将更长的文本(多段落文档) 分解成文本段序列  $\lbrace X<em>1,…,X_r,…,X</em>{\zeta} \rbrace$ 来解决。</p><p>使用如下的公式计算在相邻文本段之间传播信息：</p><script type="math/tex; mode=display">\hat H_r^{l-1} = [cat(Freeze(H_{r-1}^{l-1}) ,H_r^{l-1})]</script><p>其中 $H_r^{l-1}$ 表示第 r 个文本段的第 $l-1$ 层Transformer表达。</p><p>新注意力机制中的 $Q,K,V$ 就表达为：</p><script type="math/tex; mode=display">\hat Q^T; \hat K^T; \hat V^T =  W^q\cdot \hat H^{l-1}_r; W^k\cdot \hat H^{l-1}_r; W^v \cdot \hat H^{l-1}_r</script><p>之后就还是送到缩放点积中。这一点处理长文本可能是参考了TransformerXL。</p><blockquote><p>Transformer-XL 的重要组件之一，<strong>Segment Recurrence Mechanism（段循环机制）</strong>想做的就是，能不能在前一段计算完后，将它计算出的隐状态都保存下来，存到一个 Memeory 中，之后在计算当前段的时候，<strong>将之前存下来的隐状态和当前段的隐状态拼起来，作为 Attention 机制的 K 和 V，从而获得更长的上下文信息</strong></p></blockquote><p><img src="https://i.loli.net/2021/06/10/5zH2Avr7IiXOl6d.png" alt=""></p><p>然而，在许多情况下，文本段被组织在线性序列之外的结构中。例如，文档由图形结构中的超链接连接，这种图形结构不容易简化为形成线性序列，从而禁止了Transformer-XL的递归方法。</p><p>下面将引出eXtra Hop attention 如下图</p><p><img src="https://i.loli.net/2021/06/10/lZicWgnPv4L1dz9.png" alt=""></p><p>图a 表示 三个链接的文档 $d_2,d_1,d_3$ . Transform-XH使用eXtra Hop attention沿图形边缘传播信息，从而在连接的文本序列之间实现信息共享。</p><p>结构化文本包含一些列节点 $ X =\lbrace X<em>1,…,X_r,…X</em>{\zeta} \rbrace$ , 对应于一个文本序列。 </p><p>目标是生产表达 $X =\lbrace \hat H<em>1,…,\hat H_r,…\hat H</em>{\zeta} \rbrace$  ，其不仅合并了每个序列 $X$ 中的本地信息，而且还合并了关于整个结构化文本 ${X，E}$的全局上下文。</p><p>Transform-XH通过两种注意机制来实现这一点：in-sequence attention 和 eXtra Hop attention。</p><p>in-sequence attention 和 Transformer一样， 在 $l$ 层，第 $i$ 个token 收集从同一文本段τ内的其他 token 的信息：</p><script type="math/tex; mode=display">h_{r,i}^l = \sum_j softmax_j (\frac {q_{r,i}^T \cdot k_{r,j}}{ \sqrt d_k}) \cdot v_{r,j}</script><p>eXtra Hop attention, 使用第 CLS token 作为 attention hub ，在 $l$ 层，第 $\tau$ 个文本序列，如果 $τ$ 文本序列与另一个文本序列η之间存在边( $e_{τη}=1 $) :</p><script type="math/tex; mode=display">\hat h_{r,0}^l = \sum_{\eta ; e_{r\eta}=1} softmax_{\eta} (\frac {\hat q_{r,0}^T \cdot \hat k_{\eta,0}}{\sqrt d_k}) \cdot \hat v_{\eta,0}</script><p>节点 $τ$  使用hop query $\hat q<em>{r,0}$ 和 key $\hat k</em>{\eta, 0}$ 计算其邻居 $η$ 上的关注度权重，然后乘以邻居的value $\hat v_{\eta,0}$ ，最后将两个注意力机制聚合起来得到新的 $l$ 层的表达:</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \hat h_{r,0}^l &= Linear(cat[h_{r,0}^l, \hat h_{r,0}^l ])\\ \hat h_{r,i}^l &= h^l_{r,i} ; \forall i \ne 0    \end{split}\end{equation}</script><p>$ i \ne 0$ 是 non-hub tokens </p><p>一层 eXtra Hop attention 可视为沿着边 E 信息传递的 single-step </p><p>例如，在图a中，文档节点 $d_3$ 通过使用hop attention ,  $d_1→d_3$ 从其邻居d1收集信息来更新其表示。当多个Transformer-xh层被堆叠时，$d_1$ 中的该信息包括来自其in-sequence attention 的 $d_1$的本地上下文，以及来自 $ l-1$ 层的 hop attention ，$d_2−d_1$ 的交叉序列信息。因此，L 层 Transformer-XH可以处理最多 L 跳以外的信息。</p><p>总之 Transformer-XH共有三个主要属性，可对原始结构化文本数据进行有效建模：</p><ul><li>信息沿边的传播</li><li>信息的重要性 (hop attention)</li><li>序列内和跨序列信息的平衡 (attention combination)</li></ul><p>在 $H$ 中学习的表示可以天生地表达结构化文本中的细微差别，这些细微差别是复杂的推理任务(如多跳QA和自然语言推理)所需的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;TRANSFORMER-XH-MULTI-EVIDENCE-REASONING-WITH-EXTRA-HOP-ATTENTION&quot;&gt;&lt;a href=&quot;#TRANSFORMER-XH-MULTI-EVIDENCE-REASONING-WITH-EXTRA-HOP-A</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>旋转数组的最小数字&amp;搜索旋转排序数组</title>
    <link href="http://example.com/2021/06/07/%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97-%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/"/>
    <id>http://example.com/2021/06/07/%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97-%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/</id>
    <published>2021-06-07T07:25:23.000Z</published>
    <updated>2021-06-07T08:44:45.385Z</updated>
    
    <content type="html"><![CDATA[<h1 id="旋转数组的最小数字-amp-搜索旋转排序数组"><a href="#旋转数组的最小数字-amp-搜索旋转排序数组" class="headerlink" title="旋转数组的最小数字&amp;搜索旋转排序数组"></a>旋转数组的最小数字&amp;搜索旋转排序数组</h1><h2 id="旋转数组最小数字"><a href="#旋转数组最小数字" class="headerlink" title="旋转数组最小数字"></a>旋转数组最小数字</h2><h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。</p><p>输入一个升序的数组的一个旋转，输出旋转数组的最小元素。</p><p>例如数组 {3,4,5,1,2}{3,4,5,1,2} 为 {1,2,3,4,5}{1,2,3,4,5} 的一个旋转，该数组的最小值为 11。</p><p>数组可能包含重复项。</p><p><strong>注意</strong>：数组内所含元素非负，若数组大小为 00，请返回 −1−1。</p><p>样例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入：nums &#x3D; [2, 2, 2, 0, 1]</span><br><span class="line"></span><br><span class="line">输出：0</span><br></pre></td></tr></table></figure><h3 id="解法：二分查找"><a href="#解法：二分查找" class="headerlink" title="解法：二分查找"></a>解法：二分查找</h3><p>题目中有排序两字，自然较优的解是涉及到二分法的</p><p>假设我们用下图表示数组，水平线代表数字相同，横坐标代表数字下标</p><p><img src="https://i.loli.net/2021/06/07/iFZJBWC4teHIxKR.png" alt=""></p><p>我们发现除了最后水平的一段（黑色水平那段）之外，其余部分满足二分性质：</p><p>竖直虚线左边的数满足 $numbers[i] ≥ numbers[0]$ ；</p><p>而竖直虚线右边的数不满足这个条件。</p><p>我们要找的便是不满足上诉性质的那段中的最小值。</p><p>所以我们先将最后水平的一段删除 , 使得右半段不满足 $numbers[i] ≥ numbers[0]$ ，而是严格满足 $numbers[i] &lt; numbers[0]$。</p><p>另外，如果处理数组完全单调的情况：</p><p>当删除最后一段后，如果剩下的最后一个大于等一第一个数，说明数组完全单调。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findMin</span><span class="params">(<span class="keyword">int</span>[] numbers)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(numbers == <span class="keyword">null</span> || numbers.length==<span class="number">0</span>) <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> left=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = numbers.length-<span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 去除第二段有序数组中最后的和第一段第一个数相同的数</span></span><br><span class="line">        <span class="comment">// 使得第二段有序数组严格满足numbers[i]&lt;numbers[0]</span></span><br><span class="line">        <span class="keyword">while</span>(right&gt;<span class="number">0</span> &amp;&amp; numbers[right]==numbers[<span class="number">0</span>])&#123;</span><br><span class="line">            right--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 如果此时整个数组都有序了，那么numbers[0]就是最小值</span></span><br><span class="line">        <span class="keyword">if</span>(numbers[<span class="number">0</span>] &lt; numbers[right])&#123;</span><br><span class="line">            <span class="keyword">return</span> numbers[<span class="number">0</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span>(left&lt;right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left+right&gt;&gt;<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span>(numbers[mid] &lt; numbers[<span class="number">0</span>])&#123; <span class="comment">// 说明mid落在了右半段，最小值在[left,mid]里</span></span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                left = mid +<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> numbers[left];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="搜索旋转数组"><a href="#搜索旋转数组" class="headerlink" title="搜索旋转数组"></a>搜索旋转数组</h2><p><img src="https://i.loli.net/2021/06/07/wMJ9hGK68eZspRy.png" alt=""></p><p>数组旋转后可画出如下图：</p><p><img src="https://i.loli.net/2021/06/07/zudQlKM7UBmpbhq.png" alt=""></p><p>橙色线表示的就是旋转后数组的左右两个部分。不难发现，如果下标落在右半部分，则一定有 $nums[mid] &lt;= nums[nums.length-1]$</p><p>判断 $nums[mid] &lt;= nums[nums.length-1]$ 是否成立</p><ul><li>成立：说明当前mid落在了数组的右半部分，而我们要找的最小值其实就是右半部分的开头，故更新区间为 $[l,mid]$ 。</li><li>否则：说明mid落在了旋转数组的左半部分，那么右半部分的起点则在 $[mid+1, r]$</li></ul><p>总之，要找满足 $nums[mid] &lt;= nums[nums.length-1]$ 的最小值</p><p>第二阶段</p><p>找到最小值后，假如最小值的下标是 min ，数组便可以分为有序的两半 $[l, min-1]$  和 $[min, r]$  此时判断 $target&lt;=nums[nums.length-1]$  。</p><p>若成立，可以再右半部分中找target，因为target如果在右半部分的话，一定大于 $nums[nums.length-1]$， 那么久应该去左半边 $[l, min-1]$ 中找 target</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">search</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(nums==<span class="keyword">null</span> || nums.length==<span class="number">0</span>) <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        <span class="comment">//1,找出数组中的最小值,即左右两边的分界点，便可以将数组分为有序的左右两边</span></span><br><span class="line">        <span class="comment">//2,判断target &lt;= nums[nums.length - 1]是否成立</span></span><br><span class="line">        <span class="comment">//  成立：target在旋转后的右半边</span></span><br><span class="line">        <span class="comment">//  不成立：target在旋转数组的左半边</span></span><br><span class="line">        <span class="keyword">int</span> l =<span class="number">0</span> ;</span><br><span class="line">        <span class="keyword">int</span> r = nums.length-<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(r&gt;<span class="number">0</span> &amp;&amp; nums[r]==nums[<span class="number">0</span>])&#123;</span><br><span class="line">            r--;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(l&lt;r)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = l+r &gt;&gt;<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &lt;= nums[nums.length-<span class="number">1</span>])&#123;</span><br><span class="line">                r=mid;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                l=mid+<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//上面while结束后，l = r，都指向旋转数组中的最小值</span></span><br><span class="line">        <span class="keyword">if</span>(target&lt;=nums[nums.length-<span class="number">1</span>])&#123;</span><br><span class="line">            <span class="comment">// target在右边， l本身就是指向右边起点的，不用更新，更新r为右边终点。</span></span><br><span class="line">            r = nums.length-<span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">//target在左半边</span></span><br><span class="line">            l = <span class="number">0</span>;<span class="comment">//左半边的起点</span></span><br><span class="line">            r--;<span class="comment">//让r指向最小值的前一个位置，即左半边的终点</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//定好了区间[l,r]后，可以在里面找target了</span></span><br><span class="line">        <span class="comment">//使用二分模板即可，找满足nums[mid] &gt;= target的最小值</span></span><br><span class="line">        <span class="keyword">while</span>(l&lt;r)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = l+r&gt;&gt;<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &gt;= target)&#123;</span><br><span class="line">                r=mid;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                l=mid+<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 判断最终找到的num[l]是否等于target</span></span><br><span class="line">        <span class="keyword">if</span>(nums[l] == target) <span class="keyword">return</span> l;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;旋转数组的最小数字-amp-搜索旋转排序数组&quot;&gt;&lt;a href=&quot;#旋转数组的最小数字-amp-搜索旋转排序数组&quot; class=&quot;headerlink&quot; title=&quot;旋转数组的最小数字&amp;amp;搜索旋转排序数组&quot;&gt;&lt;/a&gt;旋转数组的最小数字&amp;amp;搜索旋转排序</summary>
      
    
    
    
    
    <category term="刷题" scheme="http://example.com/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>GRAPH-BERT: Only Attention is Needed for Learning Graph Representations</title>
    <link href="http://example.com/2021/06/04/GRAPH-BERT-Only-Attention-is-Needed-for-Learning-Graph-Representations/"/>
    <id>http://example.com/2021/06/04/GRAPH-BERT-Only-Attention-is-Needed-for-Learning-Graph-Representations/</id>
    <published>2021-06-04T03:24:00.000Z</published>
    <updated>2021-06-28T08:29:41.892Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GRAPH-BERT-Only-Attention-is-Needed-for-Learning-Graph-Representations"><a href="#GRAPH-BERT-Only-Attention-is-Needed-for-Learning-Graph-Representations" class="headerlink" title="GRAPH-BERT: Only Attention is Needed for Learning Graph Representations"></a>GRAPH-BERT: Only Attention is Needed for Learning Graph Representations</h1><p>当前GNNs的主要方法是过度依赖图中的连接关系，这样造成了三大问题。</p><ol><li>模型假死 (<em>suspended animation problem</em>) : 随着神经网络层数的不断加深，模型对于输入的数据开始不进行反应。这个问题的原因论文没写，个人理解是由于层之间的非线性使得数据分布变换导致梯度消失。</li><li>过平滑 (<em>over-smoothing problem</em>) : 由于GNN大多依靠聚合操作 (mean,max,sum) 的信息更新方式，这样随着层的不断堆叠，每个节点都会大量收到其他信息节点的影响，从而使得每个节点的embedding预测趋同。</li><li>难以并行计算：由于内存的限制，尤其是在大型图里面，图中的关联关系难以并行计算。</li></ol><p>根据以上问题作者提出了一种新的图神经网络，即Graph-Based BERT，它完全基于注意力机制，不需要任何图卷积或聚集操作。</p><p>在模型输入部分，不会把一整个大图输入给模型，而是先采样得到大图的一些无边子图，只是抽取子节点，而不考虑这些节点之间的边关系。这样就解决了GNN不能并行的问题。</p><p>传统GNN由于图的结构多样性，不能进行跨任务的预训练工作，但Graph-Bert不考虑边之间的联系，因此并不受限于图结构，可以很好地进行预训练和迁移学习。</p><script type="math/tex; mode=display">e_j^{(r)} = \left[ sin(\frac {WL(v_j)}{10000^{\frac{2l}{d_h}}}) , cos(\frac {WL(v_j)}{10000^{\frac{2l+1}{d_h}}}) \right]_{l=0}^{[\frac {d_h}{2}]}</script><script type="math/tex; mode=display">\begin{equation}\begin{split} H^{(l)} &=  \mathrm{G\text{-}Transformer}(H^{(l-1)})\\ &=\mathrm{softmax}(\frac{QK^T}{\sqrt d_h})V + \mathrm{G\text{-}Res}(H^{(l-1)},X_i)+ \mathrm{features...}    \end{split}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}\begin{split}  H^{(l)} &=  \mathrm{G\text{-}Transformer}(H^{(l-1)})\\ &=\mathrm{softmax}(\frac{QK^T}{\sqrt d_h})V + \mathrm{features...}    \end{split}\end{equation}</script><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="符号定义"><a href="#符号定义" class="headerlink" title="符号定义"></a>符号定义</h3><p><img src="https://i.loli.net/2021/06/04/lRKbH2Aa71m3peP.png" alt=""></p><h3 id="无边子图采样"><a href="#无边子图采样" class="headerlink" title="无边子图采样"></a>无边子图采样</h3><h3 id="输入节点向量Embedding"><a href="#输入节点向量Embedding" class="headerlink" title="输入节点向量Embedding"></a>输入节点向量Embedding</h3><h4 id="原始特征embedding"><a href="#原始特征embedding" class="headerlink" title="原始特征embedding"></a>原始特征embedding</h4><h4 id="Weisfeiler-Lehman-绝对角色-Embedding"><a href="#Weisfeiler-Lehman-绝对角色-Embedding" class="headerlink" title="Weisfeiler-Lehman 绝对角色 Embedding"></a>Weisfeiler-Lehman 绝对角色 Embedding</h4><h4 id="基于亲密度的相对位置Embedding"><a href="#基于亲密度的相对位置Embedding" class="headerlink" title="基于亲密度的相对位置Embedding"></a>基于亲密度的相对位置Embedding</h4><h4 id="基于相对距离的Hop-Embedding"><a href="#基于相对距离的Hop-Embedding" class="headerlink" title="基于相对距离的Hop Embedding"></a>基于相对距离的Hop Embedding</h4><h3 id="Graph-Transformer-Encoder"><a href="#Graph-Transformer-Encoder" class="headerlink" title="Graph Transformer Encoder"></a>Graph Transformer Encoder</h3><h3 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h3><h4 id="节点原始属性重构"><a href="#节点原始属性重构" class="headerlink" title="节点原始属性重构"></a>节点原始属性重构</h4><h4 id="图结构恢复"><a href="#图结构恢复" class="headerlink" title="图结构恢复"></a>图结构恢复</h4>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;GRAPH-BERT-Only-Attention-is-Needed-for-Learning-Graph-Representations&quot;&gt;&lt;a href=&quot;#GRAPH-BERT-Only-Attention-is-Needed-for-Learning-G</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>Attention</title>
    <link href="http://example.com/2021/05/31/Attention/"/>
    <id>http://example.com/2021/05/31/Attention/</id>
    <published>2021-05-31T12:02:02.000Z</published>
    <updated>2021-05-31T14:43:04.087Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从RNN到LSTM到Transformer再到GNN"><a href="#从RNN到LSTM到Transformer再到GNN" class="headerlink" title="从RNN到LSTM到Transformer再到GNN"></a>从RNN到LSTM到Transformer再到GNN</h1><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><h3 id="单头注意力"><a href="#单头注意力" class="headerlink" title="单头注意力"></a>单头注意力</h3><p><img src="https://i.loli.net/2021/05/31/PkepyKTAUWOI3wB.png" alt=""></p><p>将句子 $S$ 中第 $i$ 个词的隐藏特征 $h$ 从 $l$ 层更新到 $l+1$ 层 :</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  h^{l+1}_i &= Attention(Q^lh_i^l, K^lh_j^l, V^lh_j^l)\\ i.e., \ h_i^{l+1} &= \sum_{j\in S} softmax_j(Q^lh^l_i,K^lh_j^l)    \end{split}\end{equation}</script><p>$Q^l, K^l , V^l$ 是可学习的线性权重（分别表示注意力计算中的Query，Key，Value）。句子中的每个单词并行执行注意力机制，从而可以一次性获得他们已更新的特征——这是Transformer相对RNNs的另一个加分点，它使得模型能够逐字更新特征。</p><p><img src="https://i.loli.net/2021/05/31/vx6helFfAStIKj4.png" alt=""></p><h3 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h3><p>事实证明，要让这种点积注意力机制起作用是很难的——如果随机初始化处理得不好会使得整个学习过程失去稳定性。我们可以通过并行执行多个注意力“头”并将结果连接起来（现在每个注意力头都有单独的可学习权重）来克服这个问题：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  h^{l+1}_i &= Concat(head_1,...,head_K) O^l\\ head_k &= Attention(Q^{k,l} h_i^l, \ K^{k,l} h_j^l, \ V^{k,l}h_j^l)    \end{split}\end{equation}</script><p>其中， $Q^{k,l} ,K^{k,l}, V^{k,l}$ 是第 $k$ 个注意力投的可学习权重，而 $O^l$ 是一个向下的投影，可以用以匹配跨层的 $h_i^{l+1}$ 和 $h^l_i$ 的尺寸。此外多头注意力形成多个子空间，可以让模型去关注不同方面的信息。</p><h2 id="Transformer架构"><a href="#Transformer架构" class="headerlink" title="Transformer架构"></a>Transformer架构</h2><p><img src="https://i.loli.net/2021/05/31/F8obteYNsUVGpPZ.png" alt=""></p><p>下面是上文的多头Attention结构，但为什么Transformer的结构为什么是这样的？</p><p>注意力机制之后的词的特征可能在不同尺度或重要性上:</p><ol><li><p>这可能是由于某些词在将其他词的特征累加时具有非常集中或非常分散的注意力权重 $w_{ij}$</p></li><li><p>在单个特征/向量输入级别，跨多个注意力头 (每个可能会以不同的比例输出值) 进行级联可以导致最终向量 $h_i^{l+1}$ 的输入具有一个大范围的值。遵循传统的机器学习思路，在上述流程中增加一个归一化层似乎是合理的选择。</p></li></ol><p>对于上面的两个问题，Transformer用LayerNorm客服了问题2，LayerNorm在特征层级上进行归一化并学习一种仿射变换。  <a href="https://zhuanlan.zhihu.com/p/113233908">batchNormalization与layerNormalization的区别</a></p><p>对于问题1，通过求特征维度的平方根，来做缩放点积。</p><p>在LayerNorm之后，是FF-MLP</p><p>是一个控制尺度问题的技巧，具有特殊结构的考虑位置的双层MLP，在多头注意力之后，他们通过一个可学习的权重将 $h_i^{l+1}$  投影到一个更高的维度，在该维度中， $h_i^{l+1}$ 经过ReLU 非线性变换，然后投影回其原始维度，然后再进行另一个归一化操作。</p><script type="math/tex; mode=display">h^{l+1}_i = LN (MLP(LN(h_l^{l+1})))</script><p>不确定超参数化前馈子层背后的确切理由是什么，似乎也没有人对此提出疑问！我认为LayerNorm和缩放的点积不能完全解决突出的问题，因此大型MLP是一种可以相互独立地重新缩放特征向量的手段。</p><p>Transformer架构也非常适合非常深的网络，使NLP界能够在模型参数和扩展数据这两方面进行延伸。每个多头注意力子层和前馈子层的输入和输出之间的残差连接是堆叠Transformer层的关键（但为了清楚起见，在上图中省略了）。</p><h2 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h2><p>图神经网络（GNNs）或图卷积网络（GCNs）在图数据中建立节点和边的表示。它们是通过邻域聚合（或消息传递）来实现的，在邻域聚合中，每个节点从其邻域收集特征，以更新其周围的局部图结构表示。通过堆叠多个GNN层使得该模型可以将每个节点的特征传播到整个图中，从其邻居传播到邻居的邻居，依此类推。</p><p><img src="https://i.loli.net/2021/05/31/VgT8NGlDXIxLB3H.png" alt=""></p><p>以这个表情符号社交网络为例：由GNN产生的节点特征可用于预测性任务，例如识别最有影响力的成员或提出潜在的联系。</p><p>在他们最基本的形式中，GNNs通过以下方法来更新节点 $i$ 在 $l$ 层的隐藏层特征 $h$ 。</p><p>也就是先将节点自身特征 $h_i^{l}$ 和每个邻居节点 $j \ \ (j\in N(i))$  特征 $h_j^{l}$ 的聚合相加，然后在整体做一个非线性变换， 如下:</p><script type="math/tex; mode=display">h_i^{l+1} = \sigma (U^{l} h_i^l + \sum_{j\in N(j)}(V^l h_j^l))</script><p>其中， $U^l, V^l$ 是GNN 层的可学习权重矩阵。</p><p>邻居节点 $j\in N(i)$ 上的求和可以被其他输入大小不变的聚合函数代替，例如简单的 均值/最大值函数或其他更强大的函数（如通过注意机制的加权和）。</p><p>如果是GAT的话其实就变成了Transformer了</p><p><img src="https://i.loli.net/2021/05/31/YRcsK5h38HoS4On.png" alt=""></p><p>如果我们要执行多个并行的邻域聚合头，并且用注意力机制（即加权和）替换领域 上的求和 ，我们将获得图注意力网络（GAT）。加上归一化和前馈MLP，瞧，我们就有了Graph Transformer！</p><h3 id="在NLP中，句子就是由词全连接而成的图"><a href="#在NLP中，句子就是由词全连接而成的图" class="headerlink" title="在NLP中，句子就是由词全连接而成的图"></a>在NLP中，句子就是由词全连接而成的图</h3><p>为了使连接更加清晰，可以将一个句子看作一个完全连接的图，其中每个单词都连接到其他每个单词。现在，我们可以使用GNN来为图（句子）中的每个节点（单词）构建特征，然后我们可以使用它来执行NLP任务。</p><p><img src="https://i.loli.net/2021/05/31/eV8aQgnZ64rsNyv.png" alt=""></p><p>广义上来讲，这就是Transformers正在做的事情：Transformers是以多头注意力作为邻聚合函数的GNNs。标准GNNs从其局部邻域节点 $j\in N(i)$ 聚合特征，而NLP的Transfors 将整个句子视为局部邻域，在每个层聚合来自每个单词 $j\in S$的特征。 而NLP的Transformers将整个句子视为局部邻域，在每个层聚合来自每个单词 $j\in S$ 的特征。</p><p>重要的是，各种特定于问题的技巧（如位置编码、因果/掩码聚合、学习率表和大量的预训练）对于Transformers的成功至关重要，但在GNN界中却很少出现。同时，从GNN的角度看Transformers可以启发我们摆脱模型结构中的许多花哨的玩意。</p><h3 id="全连接图是NLP的最佳输入格式吗？"><a href="#全连接图是NLP的最佳输入格式吗？" class="headerlink" title="全连接图是NLP的最佳输入格式吗？"></a>全连接图是NLP的最佳输入格式吗？</h3><p>在统计NLP和ML之前，Noam Chomsky等语言学家致力于发展语言结构的最新理论，如语法树/图。Tree LSTMs已经尝试过这一点，但是也许Transformers/GNNs是可以让语言理论和统计NLP的领域结合得更加紧密的更好的架构？</p><h3 id="如何学习到长期依赖？"><a href="#如何学习到长期依赖？" class="headerlink" title="如何学习到长期依赖？"></a><strong>如何学习到长期依赖？</strong></h3><p>完全连通图使得学习词与词之间非常长期的依赖关系变得非常困难，这是完全连通图的另一个问题。这仅仅是因为图中的边数与节点数成二次方关系，即在n个单词的句子中，Transformer/GNN 将在 $n^2$ 上对单词进行计算，如果n很大，那将会是一个非常棘手的问题。</p><p>NLP界对长序列和依赖性问题的看法很有意思，例如，使用注意力机制在输入大小方面稀疏或自适应，在每一层中添加递归或压缩，以及使用对局部性敏感的哈希法进行有效的注意，这些都是 优化Transformers 有希望的想法。</p><p>有趣的是，还可以看到一些GNN界的想法被混入其中，例如使用句子图稀疏化的二进制分区似乎是另一种令人兴奋的方法。</p><p><img src="https://i.loli.net/2021/05/31/k1XmCxvfYMwuNIt.png" alt=""></p><h3 id="Transformers在学习神经网络的句法吗？"><a href="#Transformers在学习神经网络的句法吗？" class="headerlink" title="Transformers在学习神经网络的句法吗？"></a>Transformers在学习神经网络的句法吗？</h3><p>NLP界有几篇关于Transformers可能学到什么的有趣论文。其基本前提是，对句子中的所有词对使用注意力机制（目的是确定哪些词对最有趣），可以让Transformers学习特定任务句法之类的东西。</p><p>多头注意力中的不同头也可能“关注”不同的句法属性。</p><p>从图的角度来看，通过在完全图上使用GNN，我们能否从GNN在每一层执行邻域聚合的方法中恢复最重要的边线及其可能带来的影响？我还不太相信这种观点。</p><h3 id="为什么要用多头注意力？为什么要用注意力机制？"><a href="#为什么要用多头注意力？为什么要用注意力机制？" class="headerlink" title="为什么要用多头注意力？为什么要用注意力机制？"></a><strong>为什么要用多头注意力？为什么要用注意力机制？</strong></h3><p>我更赞同多头机制的优化观点——拥有多个注意力可以改进学习，克服不好的随机初始化。例如，这些论文表明，Transformers头可以在训练后“修剪”或“删除”，并且不会产生重大的性能影响。</p><p>多头邻聚合机制在GNNs中也被证明是有效的，例如在GAT使用相同的多头注意力，MoNet使用多个高斯核来聚合特征。虽然多头技巧是为了稳定注意力机制而发明的，但它能否成为提炼出额外模型性能的标准？</p><p>相反，具有简单聚合函数（如sum或max）的GNNs不需要多个聚合头来维持稳定的训练。如果我们不需要计算句子中每个词对之间的成对兼容性，对Transformers来说不是很好吗？</p><p>Transformers能从抛弃注意力中获益吗？Yann Dauphin和合作者最近的工作提出了另一种ConvNet架构。Transformers也可能最终会做一些类似于ConvNets的事情。</p><p><img src="https://i.loli.net/2021/05/31/6nlhQ2GLpBSeudq.png" alt=""></p><h3 id="为什么Transformers这么难训练？"><a href="#为什么Transformers这么难训练？" class="headerlink" title="为什么Transformers这么难训练？"></a><strong>为什么Transformers这么难训练？</strong></h3><p>阅读新的Transformer论文让我觉得，在确定最佳学习率表、预热策略和衰减设置时，训练这些模型需要一些类似于黑魔法的东西。这可能仅仅是因为模型太大，而且所研究的NLP任务非常具有挑战性。</p><p>但是最近的结果表明，这也可能是由于结构中归一化和残差连接的特定组合导致的。</p><p>在这一点上我很在意，但是也让我感到怀疑：我们真的需要代价昂贵的成对的多头注意力结构，超参数化的MLP子层以及复杂的学习计划吗？</p><p>我们真的需要具有大量碳足迹的（译者注：有人提出现在训练一个模型相当于5辆汽车一天的排碳量）大规模模型吗？</p><p>具有良好归纳偏差的架构难道不容易训练吗？</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zhuanlan.zhihu.com/p/110805093">为什么说Transformer就是图神经网络？</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;从RNN到LSTM到Transformer再到GNN&quot;&gt;&lt;a href=&quot;#从RNN到LSTM到Transformer再到GNN&quot; class=&quot;headerlink&quot; title=&quot;从RNN到LSTM到Transformer再到GNN&quot;&gt;&lt;/a&gt;从RNN到LST</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Do Multi-Hop Question Answering Systems Know How to Answer the Single-Hop Sub-Questions?</title>
    <link href="http://example.com/2021/05/31/Do-Multi-Hop-Question-Answering-Systems-Know-How-to-Answer-the-Single-Hop-Sub-Questions/"/>
    <id>http://example.com/2021/05/31/Do-Multi-Hop-Question-Answering-Systems-Know-How-to-Answer-the-Single-Hop-Sub-Questions/</id>
    <published>2021-05-31T01:24:46.000Z</published>
    <updated>2021-05-31T05:15:59.425Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Do-Multi-Hop-Question-Answering-Systems-Know-How-to-Answer-the-Single-Hop-Sub-Questions"><a href="#Do-Multi-Hop-Question-Answering-Systems-Know-How-to-Answer-the-Single-Hop-Sub-Questions" class="headerlink" title="Do Multi-Hop Question Answering Systems Know How to Answer the Single-Hop Sub-Questions?"></a>Do Multi-Hop Question Answering Systems Know How to Answer the Single-Hop Sub-Questions?</h1><p>这是一篇比较有意思的工作，但是出发点是多跳阅读理解的本质问题。</p><p>多跳QA需要一个模型来检索和整合来自多个段落的信息来回答问题，作者认为现有的评估标准，EM和F1并不能证明在多大程度上学会了多跳推理能力。</p><p>所以作者根据多跳QA中的桥接实体生成了一千个相关的子问题，来测试模型的能力，并期望这样能说明一些问题。</p><h2 id="做法"><a href="#做法" class="headerlink" title="做法"></a>做法</h2><p>当设计一个多跳问题时，我们要求模型去检索一系列句子作为证据，然后对他们进行推理来回答问题。作者设计了一个HotpotQA干扰项集，的子问题集，期望模型如果具有了多跳的推理能力，多跳的问题可以回答的话，那么单跳问题也可以回答。但是这个单跳问题不是凭空出现的和原问题不相关的问题。如下图所示：</p><p><img src="https://i.loli.net/2021/05/31/YEQ2HtyLSrqU1FD.png" alt=""></p><p>这是一个典型的桥接问题，问题是：罗斯为阿诺德·施瓦辛格饰演的前纽约警探主演的一部电影做宣传是在哪一年？</p><p>想要回答问题，我们就必须先知道施瓦辛格在哪个电影里扮演了纽约警探，也就是必须找到桥梁实体 Gold Para2 中的电影《End of Days》才能回答。</p><p>那么子问题的建立就很自然的可以分为，</p><p>1、施瓦辛格正在哪个电影里扮演了纽约警探？</p><p>2、罗斯在那一年为电影《End of Days》做了宣传？</p><p>第一个问题的答案正好是桥梁实体，第二个问题的答案是最终答案。</p><p>作者认为只有模型能够完整的回答这些问题，说明模型就具备了多跳推理能力。</p><p>生成方法是半自动的：</p><ul><li>首先，我们通过预测断点将每个源问题分解成若干子串</li><li>其次，进行post processed，生成两个子问题。使用一些启发式方法从段落中提取子问题的答案。</li><li>最后，将生成的候选评价实例发送给人工验证。</li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者认为有些预测答案，虽然部分匹配EM=0但是语义上是正确的，也应该被算作预测正确。如下这种：</p><p><img src="https://i.loli.net/2021/05/31/8xBnLoPbdfKmDiI.png" alt=""></p><p>新的评估标准：给定黄金答案文本跨度 $a_g$ 和预测答案文本 $a_p$，如果满足以下两个要求之一，则它们部分匹配：</p><script type="math/tex; mode=display">f1 > 0.8\\f1>0.6 \land \{(a_g\ contains\ a_{p} ) \lor (a_p \ contains \ a_g) \}</script><p>$f1$ 值大于0.8，直接认为符合要求， 或者，$f1$ 大于0.6 ，且标准答案文本跨度包含了预测文本的答案或预测答案包含了标准答案。</p><p>Baseline 选用开源的CogQA、DFGN、DecompRC</p><p><img src="https://i.loli.net/2021/05/31/FI9gEo7fLJ3UjBO.png" alt=""></p><p>实验结果发现CogQA稍微好一些</p><p><img src="https://i.loli.net/2021/05/31/jq1aPVeZrJYBEQu.png" alt=""></p><p>模型有很高的概率没有答对其中一个问题。</p><p>作者将这些示例称为模型故障案例：模型故障案例在所有正确回答的多跳问题中所占的百分比被定义为模型故障率。</p><p>CogQA PM下的故障率: $(6.1+16.5+3.4)/(40.9+6.1+16.5+3.4) \times100\% = 38.86\%$ </p><p><img src="https://i.loli.net/2021/05/31/TfhW2lJBSLtbRP3.png" alt=""></p><p>所评估的所有三个模型都有很高的模型失败率，这表明这些模型学会了回答复杂的问题，而没有探索推理过程的多个步骤。当使用EM和PM分数进行评估时，也会出现同样的现象。</p><blockquote><p>After analyzing the model failure cases, we ob- serve a common phenomenon that there is a high similarity between the words in the second sub- question and the words near the answer in the con- text. The model has learned to answer multi-hop question by local pattern matching, instead of going through the multiple reasoning steps. For the ex- ample presented in Figure 1, the model may locate the answer “<em>1999</em>” for the multi-hop question by matching the surrounding words “ <em>Guns N Roses</em>” in the second sub-question. Despite answering the multi-hop question correctly, the model fails to identify the answer of the first sub-question which it is expected to retrieve as a multi-hop QA system.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Do-Multi-Hop-Question-Answering-Systems-Know-How-to-Answer-the-Single-Hop-Sub-Questions&quot;&gt;&lt;a href=&quot;#Do-Multi-Hop-Question-Answering-S</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch RNN之pack_padded_sequence()和pad_packed_sequence()</title>
    <link href="http://example.com/2021/05/29/Pytorch-RNN%E4%B9%8Bpack-padded-sequence-%E5%92%8Cpad-packed-sequence/"/>
    <id>http://example.com/2021/05/29/Pytorch-RNN%E4%B9%8Bpack-padded-sequence-%E5%92%8Cpad-packed-sequence/</id>
    <published>2021-05-29T02:20:02.000Z</published>
    <updated>2021-05-29T02:44:31.370Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pytorch-RNN之pack-padded-sequence-和pad-packed-sequence"><a href="#Pytorch-RNN之pack-padded-sequence-和pad-packed-sequence" class="headerlink" title="Pytorch RNN之pack_padded_sequence()和pad_packed_sequence()"></a>Pytorch RNN之pack_padded_sequence()和pad_packed_sequence()</h1><h2 id="为什么有pad和pack操作？"><a href="#为什么有pad和pack操作？" class="headerlink" title="为什么有pad和pack操作？"></a>为什么有pad和pack操作？</h2><p>先看一个例子，这个batch中有5个sample</p><p><img src="https://i.loli.net/2021/05/29/AZFV4WzLUKsgpfa.png" alt=""></p><p>如果不用pack和pad操作会有一个问题，什么问题呢？</p><p>比如上图，句子“Yes”只有一个单词，但是padding了多余的pad符号，这样会导致LSTM对它的表示通过了非常多无用的字符，这样得到的句子表示就会有误差，更直观的如下图：</p><p><img src="https://i.loli.net/2021/05/29/hT5ab7rQgunimtB.png" alt=""></p><p>那么我们正确的做法应该是怎么样呢？</p><p>在上面这个例子，我们想要得到的表示仅仅是LSTM过完单词”Yes”之后的表示，而不是通过了多个无用的“Pad”得到的表示：如下图：</p><p><img src="https://i.loli.net/2021/05/29/9efQa4sVrN8ulFd.png" alt=""></p><h2 id="torch-nn-utils-rnn-pack-padded-sequence"><a href="#torch-nn-utils-rnn-pack-padded-sequence" class="headerlink" title="torch.nn.utils.rnn.pack_padded_sequence()"></a>torch.nn.utils.rnn.pack_padded_sequence()</h2><p>这里的<code>pack</code>，理解成压紧比较好。 将一个 填充过的变长序列 压紧。（填充时候，会有冗余，所以压紧一下）</p><p>其中pack的过程为：（注意pack的形式，不是按行压，而是按列压）</p><p><img src="https://i.loli.net/2021/05/29/KTaQHmkbOIC3h8x.png" alt=""></p><p>​                                      （下面方框内为<code>PackedSequence</code>对象，由data和batch_sizes组成）</p><p>pack之后，原来填充的 PAD（一般初始化为0）占位符被删掉了。</p><p>输入的形状可以是(T×B×<em> )。<code>T</code>是最长序列长度，<code>B</code>是<code>batch size</code>，`</em><code>代表任意维度(可以是0)。如果</code>batch_first=True<code>的话，那么相应的</code>input size<code>就是</code>(B×T×*)`。</p><p><code>Variable</code>中保存的序列，应该按序列长度的长短排序，长的在前，短的在后。即<code>input[:,0]</code>代表的是最长的序列，<code>input[:, B-1]</code>保存的是最短的序列。</p><blockquote><p><code>NOTE：</code> 只要是维度大于等于2的<code>input</code>都可以作为这个函数的参数。你可以用它来打包<code>labels</code>，然后用<code>RNN</code>的输出和打包后的<code>labels</code>来计算<code>loss</code>。通过<code>PackedSequence</code>对象的<code>.data</code>属性可以获取 <code>Variable</code>。</p></blockquote><p>参数说明:</p><ul><li>input (Variable) – 变长序列 被填充后的 batch</li><li>lengths (list[int]) – <code>Variable</code> 中 每个序列的长度。</li><li>batch_first (bool, optional) – 如果是<code>True</code>，input的形状应该是<code>B*T*size</code>。</li></ul><p>返回值:</p><p>一个<code>PackedSequence</code> 对象。</p><h2 id="torch-nn-utils-rnn-pad-packed-sequence"><a href="#torch-nn-utils-rnn-pad-packed-sequence" class="headerlink" title="torch.nn.utils.rnn.pad_packed_sequence()"></a>torch.nn.utils.rnn.pad_packed_sequence()</h2><p>填充<code>packed_sequence</code>。</p><p>上面提到的函数的功能是将一个填充后的变长序列压紧。 这个操作和pack_padded_sequence()是相反的。把压紧的序列再填充回来。填充时会初始化为0。</p><p>返回的Varaible的值的<code>size</code>是 <code>T×B×*</code>, <code>T</code> 是最长序列的长度，<code>B</code> 是 batch_size,如果 <code>batch_first=True</code>,那么返回值是<code>B×T×*</code>。</p><p>Batch中的元素将会以它们长度的逆序排列。</p><p>参数说明:</p><ul><li>sequence (PackedSequence) – 将要被填充的 batch</li><li>batch_first (bool, optional) – 如果为True，返回的数据的格式为 <code>B×T×*</code>。</li></ul><p>返回值: 一个tuple，包含被填充后的序列，和batch中序列的长度列表</p><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p><img src="https://i.loli.net/2021/05/29/ok9UMKLO5RYAlWP.png" alt=""></p><p><img src="https://i.loli.net/2021/05/29/rkuqZlphAoPFyi7.png" alt=""></p><p>此时PackedSequence对象输入RNN后，输出RNN的还是PackedSequence对象 </p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/lindaxin/p/8052043.html">https://www.cnblogs.com/lindaxin/p/8052043.html</a></p><p><a href="https://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence">https://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence</a></p><p><a href="https://zhuanlan.zhihu.com/p/34418001?edition=yidianzixun&amp;utm_source=yidianzixun&amp;yidian_docid=0IVwLf60">https://zhuanlan.zhihu.com/p/34418001?edition=yidianzixun&amp;utm_source=yidianzixun&amp;yidian_docid=0IVwLf60</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Pytorch-RNN之pack-padded-sequence-和pad-packed-sequence&quot;&gt;&lt;a href=&quot;#Pytorch-RNN之pack-padded-sequence-和pad-packed-sequence&quot; class=&quot;heade</summary>
      
    
    
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Multi-hop Attention Graph Neural Networks</title>
    <link href="http://example.com/2021/05/27/Multi-hop-Attention-Graph-Neural-Networks/"/>
    <id>http://example.com/2021/05/27/Multi-hop-Attention-Graph-Neural-Networks/</id>
    <published>2021-05-27T02:27:12.000Z</published>
    <updated>2021-06-01T02:45:03.452Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Multi-hop-Attention-Graph-Neural-Networks"><a href="#Multi-hop-Attention-Graph-Neural-Networks" class="headerlink" title="Multi-hop Attention Graph Neural Networks"></a>Multi-hop Attention Graph Neural Networks</h1><p>GAT中的attention运算只能关注节点相连节点表达，这种机制不考虑不直接相连但又有很重要信息的节点表达。</p><p>所以提出了多跳注意力图神经网络(MAGNA)，这是一种将多跳上下文信息融入到注意力计算的每一层的方法。</p><p>其将注意力分数分散到整个网络，相当于增加了每一层的GNN的“感受野”。</p><p><img src="https://i.loli.net/2021/05/27/xCrtn9LhXwiZ7sK.png" alt=""></p><p>如左图，考虑A和D节点，普通的attention层只计算直接相连节点的注意力分数，如 $ \alpha<em>{A,D} $   , 但如果C的信息很重要，   $ \alpha</em>{C,D}=0 $  关注度却为0。并且，单个GAT层中A和D节点之间的运算只依赖于自己的表达，而不依赖于它们的图邻域上下文。其相当于每一层只关注了一阶邻居范围的感受野，虽然堆叠多层GNN可以扩大这个范围，但GNN层数一多就会有过平滑的问题。</p><p>再看右图，MAGNA 层的改进方法是</p><ul><li>通过扩散多跳注意力捕捉  $ \alpha<em>{D,C}’ $  表达为  $ \alpha</em>{D,C}’ = f([\alpha<em>{B,C},\alpha</em>{D,B}]) $</li><li>基于图邻接矩阵的权值，通过分散注意力来考虑节点之间的所有路径，从而增强图结构学习。MAGNA利用D的节点特征进行A和B之间的注意力计算，这意味着MAGNA中的两跳注意力是基于上下文的。</li></ul><p>总之，GAT中的一跳注意机制限制了探索更广泛的图形结构与注意权重之间关系的能力。</p><p>本文提出了多跳注意图神经网络(MAGNA)，这是一种针对图结构数据的有效的多跳自注意机制。Magna使用了一种新颖的图形注意力扩散层(图1)，其中我们首先计算边上的注意力权重(用实心箭头表示)，然后使用边上的注意力权重通过注意力扩散过程计算断开的节点对之间的自我注意力权重(虚线箭头)。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="参数定义"><a href="#参数定义" class="headerlink" title="参数定义"></a>参数定义</h3><p>图 $G =(V,E)$ , $E\in V\times V$ ,  V 节点集有 $N_n$ 个 ，E 边集有 $N_e$ 个</p><p>节点 v 到其类型的映射为 $ \phi: V \rightarrow \Tau $ , 边e 到其关系类型的映射 $\psi : E \rightarrow R$</p><p>节点的embedding : $X \in \mathbb{R}^{N_n\times d_n}$ , 边的embedding: $R\in \mathbb{R}^{N_r\times d_r}$</p><p>其中 $N_n = |V|, N_r=|R|$ , $d_n,d_r$ 是节点和边类型的embedding维度。</p><p>Embedding 的每行 $x_i = X[i:]$ 表示节点 $v_i (1\le i\le N_n)$ 的embedding ， $r_j=R[j:] ,  r_j(1\le j\le N_r)$  </p><p>首先看一下MAGNA 模块的整体结构</p><p><img src="https://i.loli.net/2021/05/27/sd32RDYOLyvK64l.png" alt=""></p><p>有点像Transformer block，现在GNN的包装越来越往Transformer based模型上靠了。</p><p>他传入节点和关系embedding，会首先经历一个对于每个节点的多头注意力层（这里和GAT一样），然后是注意力扩散、 Layer Norm、前向传播层和两个残差链接。</p><h3 id="Multi-hop-Attention-Diffusion"><a href="#Multi-hop-Attention-Diffusion" class="headerlink" title="Multi-hop Attention Diffusion"></a>Multi-hop Attention Diffusion</h3><p>Attention diffusion是每一层中用于计算MAGNA‘s的attention分数。首先第一阶段，计算每一条边上的attention分数。第二阶段，用扩散注意力计算多条邻居的注意力。</p><h4 id="Edge-Attention-Computation"><a href="#Edge-Attention-Computation" class="headerlink" title="Edge Attention Computation."></a>Edge Attention Computation.</h4><p>在每一层 $l$ 处，为每个三元组 $(v_i，r_k，v_j)$ 计算矢量消息。 为了计算在 $l+1$ 层的表示，将关联的三元组的所有消息聚合成一条消息，然后使用该消息更新 $v_j^{l+1}$。</p><p>在第一阶段， 一个边 $(v_i,r_k,v_j)$ 的注意力分数是由以下计算而来：</p><script type="math/tex; mode=display">s_{i,k,j}^{(l)} = LeakyRelu(v_a^{(l)} tanh(W_h^{(l)} h_i^{(l)} || W_t^{(l)}h_j^{(l)} || W_r^{(l)}r_k))</script><p>$ W_h^{(l)} , W_t^{(l)}\in \mathbb{R}^{d^{(l)}\times d^{(l)}} , W_r^{(l)}\in \mathbb{R}^{d^{(l)}\times d_r} , v_a^{(l)}\in \mathbb{R }^{1\times 3d^{(l)}}$   可共享的可训练参数。</p><p>$h_i^{(l)}\in \mathbb{R}^{d^{(l)}}$ 是第 $l$ 层第 $i$ 个节点的embedding。 $h_i^{(0)} = x_i$</p><p>$r_k (1\le k \le N_r)$ 是可训练的第 $k$ 个关系类型的embedding</p><p>将上式应用到graph中的每一条边后，得到 attention score matrix $ S^{(l)}$:</p><script type="math/tex; mode=display">S^{(l)}_{i,j} =\begin{cases}s_{i,j,k}^{(l)}, &\ (v_i,r_k,v_j)\ appears\ in\ G\\\infty, &otherwise\end{cases}</script><p>随后，我们通过对得分矩阵 $S^{(l)}$ 执行逐行Softmax来获得注意力矩阵 $A^{(l)}_{i,j} = Softamax(S^{(l)})$</p><p>$A^{(l)}_{i,j}$ 就定义为在第 $l$ 层中当 从节点 $j$ 和 节点 $i$ 聚合消息时的关注值。</p><p>这里其实和GAT差不多 只是多了不同种边和节点。 </p><h4 id="Attention-Diffusion-for-Multi-hop-Neighbors"><a href="#Attention-Diffusion-for-Multi-hop-Neighbors" class="headerlink" title="Attention Diffusion for Multi-hop Neighbors"></a>Attention Diffusion for Multi-hop Neighbors</h4><p>通过以下注意力扩散过程，在网络中将计算不直接连接的节点之间的注意力。</p><p>该过程基于1-hop 注意力矩阵A的幂 为：</p><script type="math/tex; mode=display">A = \sum^{\infty}_{i=0}\theta_iA^i \ \ \ where \sum_{i=0}^{\infty}\theta_i = 1 \ and \ \theta_i \gt 0</script><p>其中 $\theta<em>i$ 是 attention decay factor 并且 $\theta_i \gt \theta</em>{i+1}$ ，</p><p>注意矩阵的幂 $A^i$ 给出了从节点 $h$ 到节点 $t$ 的长度为 $i$ 的关系路径的数量，从而增加了注意的感受野。</p><p>重要的是，该机制允许两个节点之间的注意力不仅取决于它们之前的层表示，而且还考虑到节点之间的路径，从而有效地在不直接连接的节点之间创建 attention shotcuts</p><p>在实现过程中，作者使用几何分布 (geometric distribution)    $θ_i=α(1−α)^i$，其中 $α∈(0，1]$  。</p><p>该选择基于the inductive bias ，即较远的节点在消息聚合中应该被较小的权重，并且具有到目标节点的不同关系路径长度的节点以独立的方式被顺序加权。</p><p>此外，请注意，如果定义$θ<em>0=α∈(0，1] ，A_0=I$ ，则上面的公式。利用关注矩阵A和移动概率 $α$ ，给出了图上的<a href="https://blog.csdn.net/likeyou1314918273/article/details/106895794/">Personal Page Rank</a>。因此，扩散注意力权重 $A</em>{i,j}$ 可以看作是节点 $j$ 对节点 $i$ 的影响。 </p><p>同时对与目标节点关系路径长度不同的节点权重应该相互独立。因此，本文定义了基于特征聚合的graph attention diffusion：</p><script type="math/tex; mode=display">AttDiff(G,H^{(l)}, \Theta) = A H^{(l)}</script><p> 其中 $\Theta$ 为注意力参数集合。 </p><h4 id="Approximate-Computation-for-Attention-Diffusio"><a href="#Approximate-Computation-for-Attention-Diffusio" class="headerlink" title="Approximate Computation for Attention Diffusio"></a>Approximate Computation for Attention Diffusio</h4><p>对于大图，公式（3）的计算开销巨大，而DAGCN需要通过 $AH^l$进行信息聚合，本文通过定义一个数列 $Z^K$, 当 $K \rightarrow \infty$时，该数列能收敛到$AH^l$的值：</p><script type="math/tex; mode=display">Z^0 = H^L, Z^{k+1} = (1-\alpha)AZ^{(k)} + \alpha Z^0 \\lim_{K\rightarrow \infty} Z^{K} = AH^{l}</script><p>证明请参考原文。上述的近似使得attention的复杂度保持在$O(|E|)$。很多真实世界网络具有小世界（small-world ）特征，在这种情况下，较小的K值就足够。对于具有较大直径的图，选择较大的K和较小 $\alpha$ 。</p><h3 id="Multi-hop-Attention-based-GNN-Architecture"><a href="#Multi-hop-Attention-based-GNN-Architecture" class="headerlink" title="Multi-hop Attention based GNN Architecture"></a>Multi-hop Attention based GNN Architecture</h3><p>图2提供了可多次堆叠的MAGNA block 的架构概览。</p><h4 id="Multi-head-Graph-Attention-Diffusion-Layer"><a href="#Multi-head-Graph-Attention-Diffusion-Layer" class="headerlink" title="Multi-head Graph Attention Diffusion Layer"></a>Multi-head Graph Attention Diffusion Layer</h4><p>在不同的视角联合关注来自不同表示子空间的信息。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \hat H^{(l)} &= MultiHead(G, \hat H^{(l)}) =(||_{i=1}^M head_i) W_o \\head_i &=  AttDiff(G, \hat H^{(l)}, \Theta_i) \\\hat H^{(l)} &= LayerNorm(H^{(l)})    \end{split}\end{equation}</script><p>方程中以递归的方式计算注意力扩散。增加了层归一化，有助于稳定递归计算过程。</p><h4 id="Deep-Aggregation"><a href="#Deep-Aggregation" class="headerlink" title="Deep Aggregation"></a>Deep Aggregation</h4><p>此外，还包含一个完全连接的前馈子层，它由两层前馈网络组成。我们还在两个子层中添加了层标准化和残差连接，从而为每个block提供了更具表现力的聚合步骤</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \hat H^{(l+1)} &= \hat H^{(l)} + H^{(l)} \\ H^{(l+1)} &= W_2^{(l)} ReLU(W_1^{(l)} LayerNorm(\hat H^{(l+1)})) + \hat H^{(l+1)}    \end{split}\end{equation}</script><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://i.loli.net/2021/05/28/rVp3kq5IsGweYgR.png" alt=""></p><p><img src="https://i.loli.net/2021/05/28/ZzVJehsHGlPoFiY.png" alt=""></p><h2 id="Reviewer"><a href="#Reviewer" class="headerlink" title="Reviewer"></a>Reviewer</h2><blockquote><p>The central question of the reviewers’ discussion was whether the contribution of this paper was significant enough or too incremental. The discussion emphasized relevant literature which already considers multi-hop attention (e.g. <a href="https://openreview.net/forum?id=rkKvBAiiz">https://openreview.net/forum?id=rkKvBAiiz</a> [Cucurull et al.], <a href="https://ieeexplore.ieee.org/document/8683050">https://ieeexplore.ieee.org/document/8683050</a> [Feng et al.], <a href="https://arxiv.org/abs/2001.07620">https://arxiv.org/abs/2001.07620</a> [Isufi et al.]), and which should have served as baseline. In particular, the experiment suggested by R3 was in line with some of these previous works, which consider “a multi-hop adjacency matrix “ as a way to increase the GAT’s receptive field. This was as opposed to preserving the 1-hop adjacency matrix used in the original GAT and stacking multiple layers to enlarge the receptive field, which as noted by the authors, may result in over-smoothed node features. The reviewers acknowledged that there is indeed as slight difference between the formulation proposed in the paper and the one in e.g. [Cucurull et al.]. The difference consists in calculating attention and then computing the powers with a decay factor vs. increasing the receptive field first by using powers of the adjacency matrix and then computing attention. Still, the multi-hop GAT baseline of [Cucurull et al.] could be extended to use a multi-hop adjacency matrix computed with the diffusion process from [Klicpera 2019], as suggested by R3. In light of these works and the above-mentioned missing baselines, the reviewers agreed that the contribution may be viewed as rather incremental (combining multi-hop graph attention with graph diffusion). The discussion also highlighted the potential of the presented spectral analysis, which could be strengthened by developing new insights in order to become a stronger contribution (see R2’s suggestions).</p><p>Proposed methodology being more powerful than GAT is arguable:<br>When the attention scores for indirectly connected neighbors are still computed based on the immediate neighbors’ attention scores, it is not convincing enough to be argued as more powerful than GAT, which learns attention scores over contextualized immediate neighbors.Also, the approximate realization of the model described in Eqn: 5 follows a message-passing style to propagate attention scores. Suppose it is to be argued that standard message-passing-based diffusion is not powerful enough to get a good immediate neighbor representation that encodes neighbors’ information from far away. In that case, it is not immediately clear how a similar diffusion, when used for propagating attention scores from immediate neighbors to neighbors multiple hops away, will be more powerful. </p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Multi-hop-Attention-Graph-Neural-Networks&quot;&gt;&lt;a href=&quot;#Multi-hop-Attention-Graph-Neural-Networks&quot; class=&quot;headerlink&quot; title=&quot;Multi-hop </summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
    <link href="http://example.com/2021/05/24/GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training/"/>
    <id>http://example.com/2021/05/24/GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training/</id>
    <published>2021-05-24T12:17:55.000Z</published>
    <updated>2021-05-25T07:10:36.604Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training"><a href="#GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training" class="headerlink" title="GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training"></a>GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</h1><p>一个自监督的利用对比学习来学习GNN内在可迁移的先验知识的预训练框架，目的是想得到一个可迁移性强，迁移领域广的，通用的表达。为了捕捉跨多个网络的通用网络拓扑特性，预训练任务为，跨网络子图实例判别。所以预训练的重点在结构相似性层面上，并且不带节点属性。</p><h2 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h2><p>基本思想是对输入图中的实例进行采样，将每个实例视为自己的一个不同的类，并学习对这些实例进行编码和区分。</p><p>具体地说，作者认为GCC需要回答三个问题，才能学习到可转移性好的结构模式：</p><ul><li>实例是什么？</li><li>分辨规则是什么？</li><li>如何对实例进行编码？</li></ul><p>对于者三个问题作者展开研究。</p><h2 id="预训练任务——子图实例判别"><a href="#预训练任务——子图实例判别" class="headerlink" title="预训练任务——子图实例判别"></a>预训练任务——子图实例判别</h2><p>任务的目标是根据顶点的局部结构来区分它们。</p><p><img src="https://i.loli.net/2021/05/24/vH3s8Rm45liJPxh.png" alt=""></p><p>对于每个顶点，从它的多跳ego网络中抽取子图作为实例。</p><p>GCC的目的是区分从某个顶点采样的子图和从其他顶点采样的子图。并且不假设顶点和子图来自同一个图，所以图编码器被迫捕获不同输入图的通用模式。</p><h3 id="定义子图实例"><a href="#定义子图实例" class="headerlink" title="定义子图实例"></a>定义子图实例</h3><p>对比学习框架的成功很大程度上取决于数据实例的定义。CV和NLP任务可以直接将实例定义为图像或句子。</p><p> 但是，这些想法不能直接扩展到图形数据，因为图形中的实例没有明确定义。</p><p>因为节点是无属性的节点，所以要表示一个节点，就要采用以他为中心的局部结构。</p><p>具体地说，对于某个顶点v，定义一个实例为它的r-ego网络：</p><p>对于一个 r-ego 网络 $G = (V,E)$ ，$V$ 是节点集并且 $E \subseteq V\times V$</p><p>对于其中心点v， 他的 r-neighbors 定义为 $S_v={u:d(u,v)\le r}$ , 其中 $d(u,v)$ 为邻居u节点到v节点的最短路径。</p><p>顶点v的r-ego图，记为 $G_v$，是由 $S_v$ 引出的子图。 下图就是一个2-ege图，右边是预训练过程。</p><p><img src="https://i.loli.net/2021/05/25/Epkna1vfKVIU4hG.png" alt=""></p><h3 id="定义实例相似性判别准则"><a href="#定义实例相似性判别准则" class="headerlink" title="定义实例相似性判别准则"></a>定义实例相似性判别准则</h3><p>在cv中，同一图像的两个随机数据增加(例如，随机裁剪、随机调整大小、随机颜色抖动、随机翻转等)被视为相似的实例对。</p><p>在GCC中将同一r-ego网络的两个随机数据扩充看作一个相似实例对，并将数据扩充定义为图采样。</p><p>GCC的图采样遵循三个步骤</p><ul><li>重新启动的随机行走(RWR) ：从ego图的节点v出发，随机采样子图结构，并以一定概率返回到v节点。得到的采样子图可以被认为是一种数据扩增，像cv那样。</li><li>子图归纳：导出子图随机游走抽样(ISRW)。</li><li>匿名化：匿名化被采样的子图 $\hat G_v$ ,并重新排序。</li></ul><h3 id="定义图编码器"><a href="#定义图编码器" class="headerlink" title="定义图编码器"></a>定义图编码器</h3><p>给定两个采样子图 $x^q$ 和 $x^k$，GCC分别通过两个图神经网络编码器 $f_q$ 和 $f_k$ 对其进行编码。从技术上讲，任何图神经网络都可以作为这里的编码器，而GCC模型对不同的选择并不敏感。因为不考虑节点属性，而大多数GNN模型需要把节点特征/属性作为输入。为了弥补这一差距，作者建议利用每个采样子图的图结构来初始化顶点特征。</p><p>目标Loss采用对比学习经典的InfoNCE:</p><script type="math/tex; mode=display">L = -log \frac{exp(q^Tk_+/\tau)}{\sum_{i=0}^K exp(q^Tk_i/\tau)}</script><p>其中 $q = f_q(x^q) , k=f_k(x^k)$ ，$q$ 是query 对应的目录为 $K+1$ 个编码的keys: ${k_0,…,k_K}$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training&quot;&gt;&lt;a href=&quot;#GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-</summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
</feed>
