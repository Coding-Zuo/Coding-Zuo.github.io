<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Coding-Zuo</title>
  
  <subtitle>Coding And Studying</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-05-01T07:08:15.275Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Coding-Zuo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>海华阅读理解比赛复盘</title>
    <link href="http://example.com/2021/05/01/%E6%B5%B7%E5%8D%8E%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%AF%94%E8%B5%9B%E5%A4%8D%E7%9B%98/"/>
    <id>http://example.com/2021/05/01/%E6%B5%B7%E5%8D%8E%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%AF%94%E8%B5%9B%E5%A4%8D%E7%9B%98/</id>
    <published>2021-05-01T02:29:30.000Z</published>
    <updated>2021-05-01T07:08:15.275Z</updated>
    
    <content type="html"><![CDATA[<h1 id="海华阅读理解比赛复盘"><a href="#海华阅读理解比赛复盘" class="headerlink" title="海华阅读理解比赛复盘"></a>海华阅读理解比赛复盘</h1><p>比赛详情、EMA、Baseline，本文主要记录提分点和模型改进的验证</p><p>参考上文 <a href="https://coding-zuo.github.io/2021/04/06/%E6%B5%B7%E5%8D%8E%E4%B8%AD%E6%96%87%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%AF%94%E8%B5%9B%E6%A2%B3%E7%90%86-%E5%A4%9A%E5%8D%A1%E5%B9%B6%E8%A1%8C-transformers/">海华中文阅读理解比赛梳理/多卡并行/transformers</a></p><p><a href="https://github.com/Coding-Zuo/MRC_multiChoice">github</a></p><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p>数据增强的办法很多参考 <a href="https://zhuanlan.zhihu.com/p/145521255">https://zhuanlan.zhihu.com/p/145521255</a></p><p>我只采用了句子乱序和数据回译，都是将增强数据和原始数据挨着放到数据集中，在训练的时候停用shuffle。(可能有其他方法：每条数据根据概率来选择性增强)，我这种可能会让数据集臃肿，质量下降。</p><h3 id="句子乱序"><a href="#句子乱序" class="headerlink" title="句子乱序"></a>句子乱序</h3><p>没有提分，也没有降很多。</p><p>原因参考：<a href="https://zhuanlan.zhihu.com/p/107594976">从知觉谈中文乱序不影响阅读的原因</a></p><p>代码：<a href="https://github.com/Coding-Zuo/MRC_multiChoice/blob/main/train/data_process.py">https://github.com/Coding-Zuo/MRC_multiChoice/blob/main/train/data_process.py</a> 中的data_enhancement_sentence_order</p><h3 id="数据回译"><a href="#数据回译" class="headerlink" title="数据回译"></a>数据回译</h3><p>和句子乱序一样和回译到的数据和原始数据挨着放到数据集，没有提分，可能是回译到的数据质量不好。</p><p>使用的是百度API，百度限制一个账户免费200万字符，如果超了就多注册几个账户薅羊毛。</p><p>代码：<a href="https://github.com/Coding-Zuo/MRC_multiChoice/blob/main/TranslateAPI.py">https://github.com/Coding-Zuo/MRC_multiChoice/blob/main/TranslateAPI.py</a></p><h3 id="在训练集上打伪标签"><a href="#在训练集上打伪标签" class="headerlink" title="在训练集上打伪标签"></a>在训练集上打伪标签</h3><p>由于时间问题，没有直接提交伪标签训练的结果，就直接模型融合。验证集有提高。</p><p>用训练好的模型去inference测试集，取了模型认为有百分之85概率认为是正确答案的数据打上伪标签，加入到训练集训练。</p><h2 id="优化训练"><a href="#优化训练" class="headerlink" title="优化训练"></a>优化训练</h2><h3 id="EMA"><a href="#EMA" class="headerlink" title="EMA"></a>EMA</h3><p>滑动平均exponential moving average</p><p>没有提分，反而效果变差。具体原因，还在探索，可能和优化方法有关？</p><p>我一直使用的都是adamw，<a href="https://www.cnblogs.com/tfknight/p/13425532.html">比较Adam 和Adamw</a> <a href="https://zhuanlan.zhihu.com/p/39543160">一文告诉你Adam、AdamW、Amsgrad区别和联系</a>，AdamW是在Adam+L2正则化的基础上进行改进的算法。</p><p>可以和sgd搭配看看效果。(这方面因为时间问题没有尝试充足)</p><p><a href="https://blog.csdn.net/weixin_43002433/article/details/113531466">PyTorch指数移动平均(EMA)手册</a></p><p>指数移动平均EMA是用于估计变量的局部均值的，它可以使变量的更新不只取决于当前时刻的数据。</p><p>而是加权平均了近期一段时间内的历史数据，是的变量的更新更平滑，不易受到某次异常值的影响。</p><h3 id="labelSmoothing"><a href="#labelSmoothing" class="headerlink" title="labelSmoothing"></a>labelSmoothing</h3><p>精度提升不明显，但是缓解了验证集的loss上升。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelSmoothingCrossEntropy</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, eps=<span class="number">0.1</span>, reduction=<span class="string">&#x27;mean&#x27;</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LabelSmoothingCrossEntropy, self).__init__()</span><br><span class="line">        self.eps = eps</span><br><span class="line">        self.reduction = reduction</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, output, target</span>):</span></span><br><span class="line">        c = output.size()[-<span class="number">1</span>]</span><br><span class="line">        log_preds = F.log_softmax(output, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> self.reduction == <span class="string">&#x27;sum&#x27;</span>:</span><br><span class="line">            loss = -log_preds.<span class="built_in">sum</span>()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss = -log_preds.<span class="built_in">sum</span>(dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> self.reduction == <span class="string">&#x27;mean&#x27;</span>:</span><br><span class="line">                loss = loss.mean()</span><br><span class="line">        <span class="keyword">return</span> loss * self.eps / c + (<span class="number">1</span> - self.eps) * F.nll_loss(log_preds, target, reduction=self.reduction)</span><br></pre></td></tr></table></figure><h3 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h3><p>提升两个点以上</p><p>可参考我的 <a href="https://coding-zuo.github.io/adversary/index.html">ppt</a> 和以前文章</p><p>主要使用了fgm和pgd两个，都有提升的效果</p><p>但有时候pgd并没有提升，可能是在有些参数和加了伪标签的数据情况下，学习困难？</p><h3 id="早停"><a href="#早停" class="headerlink" title="早停"></a>早停</h3><p>bert的早停不太好控制，有时候一两个epoch之后还会更新，可能跟参数有关。</p><h2 id="模型改进"><a href="#模型改进" class="headerlink" title="模型改进"></a>模型改进</h2><h3 id="尝试用LongFormer"><a href="#尝试用LongFormer" class="headerlink" title="尝试用LongFormer"></a>尝试用LongFormer</h3><p>因为文本比较长，但因为没有时间测试而没有跑，不过已经基本调通，日后跑一跑。</p><h3 id="复现DUMA"><a href="#复现DUMA" class="headerlink" title="复现DUMA"></a>复现DUMA</h3><p>用co-attention 来分别处理 bert输出的文章编码和问题答案对编码，分别送到co-attention中。</p><p>我的方法是分别为文章和问题答案设置一个maxlen， 多的截掉，因为我机器只能最大总长度跑到400，而数据文章又比较长，可能这也会导致学习瓶颈的出现。</p><p>我的另一个实现想法但是没有时间做的是，把文章和问题答案拼在一起用sep分割送入bert，输出时只要找到sep的timesteps进行分割，对于得到的两个不等长的向量，在经过对其。送入co-attention。</p><p>训练刚开始有一个比较好的提分劲头，但随着深入训练后期效果乏力。可能是因为参数没有调好？DUMA那篇论文没有复现细节。</p><h3 id="尝试其他比赛前排模型"><a href="#尝试其他比赛前排模型" class="headerlink" title="尝试其他比赛前排模型"></a>尝试其他比赛前排模型</h3><p><img src="https://i.loli.net/2021/05/01/f1QIsuWtSVXCcBx.png" alt=""></p><p>移植后问题：训练集准确率很低，具体问题还需探究。</p><h3 id="尝试在bert后加self-attention层"><a href="#尝试在bert后加self-attention层" class="headerlink" title="尝试在bert后加self-attention层"></a>尝试在bert后加self-attention层</h3><p>用pool_output,投入自注意力，没有明显提升</p><p>在bert后加多层线性也没有明显提升。不过可以尝试加highway network。</p><h2 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h2><p>组合不同参数和结构的打包模型，用argmax的方法融合了九个，达到最好的51.7分，晋级分数最终为52分，遗憾落榜。</p><p>还尝试用实现vote投票来融合，并没有最终提交。</p><p>以后将会尝试实现bert的stacking融合。</p><h2 id="遇到的难题"><a href="#遇到的难题" class="headerlink" title="遇到的难题"></a>遇到的难题</h2><ol><li><p>bert换成roberta后始终不收敛，因为没有经验，学习率试过1e-5, 1e-6, 2e-5,和不同batch32、64、128进行组合都不收敛(浪费了很多时间)。最终发现学习率在1e-5,2e-5 ,batch 在8或16才会收敛。</p><p>并参照roberta论文附录中的参数，收敛了，但是效果没有达到预期，不过听说好多人也是用了roberta。</p></li></ol><p><img src="https://i.loli.net/2021/05/01/7vZQHiFus6DqJI2.png" alt=""></p><ol><li>调参没经验，浪费了很多时间。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>用了将近一个月的时间来做这个比赛，对模型训练体系、模型理解、微调下游任务、多卡并行、对抗训练。还有好多理论需要通过实践来加深理解。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;海华阅读理解比赛复盘&quot;&gt;&lt;a href=&quot;#海华阅读理解比赛复盘&quot; class=&quot;headerlink&quot; title=&quot;海华阅读理解比赛复盘&quot;&gt;&lt;/a&gt;海华阅读理解比赛复盘&lt;/h1&gt;&lt;p&gt;比赛详情、EMA、Baseline，本文主要记录提分点和模型改进的验证&lt;/p</summary>
      
    
    
    
    
    <category term="DataGame" scheme="http://example.com/tags/DataGame/"/>
    
  </entry>
  
  <entry>
    <title>阅读理解文献梳理</title>
    <link href="http://example.com/2021/04/29/%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%96%87%E7%8C%AE%E6%A2%B3%E7%90%86/"/>
    <id>http://example.com/2021/04/29/%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%96%87%E7%8C%AE%E6%A2%B3%E7%90%86/</id>
    <published>2021-04-29T08:49:50.000Z</published>
    <updated>2021-04-30T11:21:05.005Z</updated>
    
    <content type="html"><![CDATA[<h1 id="阅读理解文献梳理"><a href="#阅读理解文献梳理" class="headerlink" title="阅读理解文献梳理"></a>阅读理解文献梳理</h1><h2 id="多跳QA"><a href="#多跳QA" class="headerlink" title="多跳QA"></a>多跳QA</h2><h3 id="模型在任务中学习的多跳推理行为。"><a href="#模型在任务中学习的多跳推理行为。" class="headerlink" title="模型在任务中学习的多跳推理行为。"></a>模型在任务中学习的多跳推理行为。</h3><p>QFE (Nishida et al., 2019) regards evidence extraction as a query-focused summarization task, and reformulates the query in each hop.    将证据提取作为以查询为中心的摘要任务，并在每一跳中重构查询。—— HGN</p><p>Kosuke Nishida, Kyosuke Nishida, Masaaki Nagata, Atsushi Otsuka, Itsumi Saito, Hisako Asano, and Junji Tomita. 2019. Answering while summarizing: Multi-task learning for multi-hop qa with evidence extraction. In <em>ACL</em>.</p><hr><p> DecompRC (Min et al., 2019b) decomposes a compositional question into simpler sub-questions and leverages single-hop MRC mod- els to answer the sub-questions.  将作文问题分解为更简单的子问题，并利用单跳MRC模型答复子问题—— HGN</p><p>Sewon Min, Victor Zhong, Luke Zettlemoyer, and Han- naneh Hajishirzi. 2019b. Multi-hop reading compre- hension through question decomposition and rescor- ing. In <em>ACL</em>.</p><hr><p>A neural modular network is also proposed in Jiang and Bansal (2019b), where neural modules are dynamically assembled for more interpretable multi-hop rea- soning.一种神经模块网络，其中神经模块被动态地组装起来，以便更好地解释多跳推理。—— HGN</p><p>Yichen Jiang and Mohit Bansal. 2019b. Self- assembling modular networks for interpretable multi-hop reasoning. In <em>EMNLP</em>.</p><hr><p>其他</p><p>Jifan Chen and Greg Durrett. 2019. Understanding dataset design choices for multi-hop reasoning. In <em>NAACL</em>.—— HGN</p><p>Sewon Min, Eric Wallace, Sameer Singh, Matt Gard- ner, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019a. Compositional questions do not necessitate multi-hop reasoning. In <em>ACL</em>.—— HGN</p><p>Yichen Jiang and Mohit Bansal. 2019a. Avoiding rea- soning shortcuts: Adversarial evaluation, training, and model development for multi-hop qa. In <em>ACL</em>.—— HGN</p><hr><h3 id="与GNN相关的"><a href="#与GNN相关的" class="headerlink" title="与GNN相关的"></a>与GNN相关的</h3><p>Coref-GRN (Dhingra et al., 2018) construct an entity graph based on co-reference reso- lution or sliding windows.基于共引用解决方案或滑动窗口构建实体图。—— HGN</p><p>Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. 2018. Neural models for reasoning over multiple mentions using coreference. In <em>NAACL</em>.</p><hr><p>Entity-GCN (De Cao et al., 2019) considers three different types of edges that connect different entities in the entity graph.考虑连接实体图中不同实体的三种不同类型的边。—— HGN</p><p>Nicola De Cao, Wilker Aziz, and Ivan Titov. 2019. Question answering by reasoning across documents with graph convolutional networks. In <em>NAACL</em>.</p><hr><p><strong>(已读)</strong>HDE-Graph (Tu et al., 2019) enriches information in the entity graph by adding document nodes and creating interactions among documents, entities and answer candidates.通过添加文档节点并在文档、实体和候选答案之间创建交互，丰富了实体图中的信息。——HGN</p><hr><p><strong>(已读)</strong>Cognitive Graph QA employs an MRC model to predict answer spans and possible next-hop spans, and then organizes them into a cognitive graph.使用MRC模型预测答案跨度和可能的下一跳跨度，然后将它们组织到认知图中。——HGN</p><hr><p>DFGN (Xiao et al., 2019) constructs a dynamic entity graph, where in each reasoning step irrelevant en- tities are softly masked out and a fusion module is designed to improve the interaction between the entity graph and documents.构建了一个动态实体图，在每个推理步骤中，不相关的实体被软屏蔽，并设计了一个融合模块来改善实体图与文档之间的交互性。——HGN</p><p>Yunxuan Xiao, Yanru Qu, Lin Qiu, Hao Zhou, Lei Li, Weinan Zhang, and Yong Yu. 2019. Dynamically fused graph network for multi-hop reasoning. In <em>ACL</em>.</p><hr><p><strong>(已读)</strong>SAE (Tu et al., 2020) defines three types of edge in the sentence graph based on the named entities and noun phrases appearing in the question and sentences 根据问题和句子中出现的命名实体和名词短语，定义句子图中的三种边——HGN</p><hr><p>C2F Reader (Shao et al., 2020) uses graph attention or self-attention on entity graph, and argues that this graph may not be necessary for multi-hop reasoning. 在实体图上使用图注意或自我注意，并认为该图对于多跳推理可能不是必需的。——HGN</p><p>Nan Shao, Yiming Cui, Ting Liu, Wang, and Guop- ing Hu Hu. 2020. Is graph structure necessary for multi-hop reasoningt. <em>arXiv preprint arXiv:2004.03096</em>.</p><hr><p>Asai et al. (2020) proposes a new graph-based recurrent method to find evidence documents as reasoning paths, which is more focused on information retrieval.提出了一种新的基于图的递归方法来寻找证据文档作为推理路径，更侧重于信息检索。——HGN</p><p>Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In <em>ICLR</em>.</p><hr><p><strong>(已读)</strong>HGN 2020 提出的模型构建了一个层次图，有效地探索了不同粒度之间的关系，并使用不同的节点来执行不同的任务。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;阅读理解文献梳理&quot;&gt;&lt;a href=&quot;#阅读理解文献梳理&quot; class=&quot;headerlink&quot; title=&quot;阅读理解文献梳理&quot;&gt;&lt;/a&gt;阅读理解文献梳理&lt;/h1&gt;&lt;h2 id=&quot;多跳QA&quot;&gt;&lt;a href=&quot;#多跳QA&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>Hierarchical Graph Network for Multi-hop Question Answering</title>
    <link href="http://example.com/2021/04/29/Hierarchical-Graph-Network-for-Multi-hop-Question-Answering/"/>
    <id>http://example.com/2021/04/29/Hierarchical-Graph-Network-for-Multi-hop-Question-Answering/</id>
    <published>2021-04-29T05:19:33.000Z</published>
    <updated>2021-05-01T07:13:30.697Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hierarchical-Graph-Network-for-Multi-hop-Question-Answering"><a href="#Hierarchical-Graph-Network-for-Multi-hop-Question-Answering" class="headerlink" title="Hierarchical Graph Network for Multi-hop Question Answering"></a>Hierarchical Graph Network for Multi-hop Question Answering</h1><p><a href="https://arxiv.org/pdf/1911.03631.pdf">https://arxiv.org/pdf/1911.03631.pdf</a></p><p>这篇文章也是HotpotQA数据集上的，在干扰项排行榜和全维基排行榜都曾是前列。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Hierarchical-Graph-Network-for-Multi-hop-Question-Answering&quot;&gt;&lt;a href=&quot;#Hierarchical-Graph-Network-for-Multi-hop-Question-Answering&quot; </summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>二分模板</title>
    <link href="http://example.com/2021/04/29/%E4%BA%8C%E5%88%86%E6%A8%A1%E6%9D%BF/"/>
    <id>http://example.com/2021/04/29/%E4%BA%8C%E5%88%86%E6%A8%A1%E6%9D%BF/</id>
    <published>2021-04-29T01:23:15.000Z</published>
    <updated>2021-04-29T03:17:35.583Z</updated>
    
    <content type="html"><![CDATA[<h1 id="二分模板"><a href="#二分模板" class="headerlink" title="二分模板"></a>二分模板</h1><p><a href="https://www.acwing.com/problem/content/791/">https://www.acwing.com/problem/content/791/</a></p><p>二分的本质不是单调性, 单调性的题目一定可以二分, 可以二分的题目不一定有单调性</p><p>二分的本质是边界<br>二分法用于查找, 每次都选择答案所在的区间再次进行查找, 当区间长度为 1时, 就是答案</p><p><img src="https://i.loli.net/2021/04/29/Hy4vGqOtus8lQXp.png" alt=""></p><ol><li>根据 check(mid)来判断 r和 l的取值范围</li><li>根据取值范围选择 mid是否有 + 1操作<ul><li>mid归于左边, r = mid, mid选择 不 +1</li><li>mid归于右边, l = mid, mid选择 +1</li></ul></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> 模板<span class="title">_</span>二分 </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        BufferedReader input = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(System.in));</span><br><span class="line">        <span class="keyword">int</span>[] line1 = Arrays.asList(input.readLine().split(<span class="string">&quot; &quot;</span>)).stream().mapToInt(Integer::parseInt).toArray();</span><br><span class="line">        <span class="keyword">int</span> n = line1[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> q = line1[<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span>[] line2 = Arrays.asList(input.readLine().split(<span class="string">&quot; &quot;</span>)).stream().mapToInt(Integer::parseInt).toArray();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (q-- != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">int</span> target = Integer.parseInt(input.readLine());</span><br><span class="line">            <span class="comment">// 查找左边界 用第一个模板</span></span><br><span class="line">            <span class="keyword">int</span> index_l = bsearch_1(line2, <span class="number">0</span>, n - <span class="number">1</span>, target);</span><br><span class="line">            <span class="keyword">if</span> (line2[index_l] != target) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;-1 -1&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.print(index_l + <span class="string">&quot; &quot;</span>);</span><br><span class="line">                <span class="comment">// 查找右边界 用第二个模板</span></span><br><span class="line">                <span class="keyword">int</span> index_r = bsearch_2(line2, <span class="number">0</span>, n - <span class="number">1</span>, target);</span><br><span class="line">                System.out.print(index_r + <span class="string">&quot;\n&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">bsearch_1</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> l, <span class="keyword">int</span> r, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (l &lt; r) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = l + r &gt;&gt; <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span> (arr[mid] &gt;= target) &#123;</span><br><span class="line">                r = mid;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                l = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> l;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">bsearch_2</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> l, <span class="keyword">int</span> r, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (l &lt; r) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = l + r + <span class="number">1</span> &gt;&gt; <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span> (arr[mid] &lt;= target) &#123;</span><br><span class="line">                l = mid;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                r = mid - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> l;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;二分模板&quot;&gt;&lt;a href=&quot;#二分模板&quot; class=&quot;headerlink&quot; title=&quot;二分模板&quot;&gt;&lt;/a&gt;二分模板&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.acwing.com/problem/content/791/&quot;&gt;https://</summary>
      
    
    
    
    
    <category term="刷题" scheme="http://example.com/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>归并排序模板</title>
    <link href="http://example.com/2021/04/28/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%E6%A8%A1%E6%9D%BF/"/>
    <id>http://example.com/2021/04/28/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%E6%A8%A1%E6%9D%BF/</id>
    <published>2021-04-28T01:16:02.000Z</published>
    <updated>2021-04-28T02:06:37.453Z</updated>
    
    <content type="html"><![CDATA[<h1 id="归并排序模板"><a href="#归并排序模板" class="headerlink" title="归并排序模板"></a>归并排序模板</h1><p>分治思想</p><p><img src="https://i.loli.net/2021/04/28/8g1qixHscOBTv6Q.png" alt=""></p><ol><li><p>确定分界点：$mid = (l+r)/2$</p></li><li><p>先递归分成左右两边</p></li><li><p>将两个有序数组合并成一个有序序列——归并</p><p>使用两个指针：这个过程时间复杂度为$O(n)$</p></li></ol><p><img src="https://i.loli.net/2021/04/28/d23pUiKgLOswDZm.png" alt=""></p><p>整体时间复杂度$O(nlogn)$</p><p>因为分层用了$logn$次</p><p><img src="https://i.loli.net/2021/04/28/WeSTDRHymJbg5KN.png" alt=""></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> 模板<span class="title">_</span>归并排序 </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        BufferedReader input = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(System.in));</span><br><span class="line">        <span class="keyword">int</span> n = Integer.parseInt(input.readLine());</span><br><span class="line">        <span class="keyword">int</span>[] q = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">        String[] linelist = input.readLine().split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; linelist.length; i++) &#123;</span><br><span class="line">            q[i] = Integer.parseInt(linelist[i]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        merge_sort(q, <span class="number">0</span>, q.length - <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; q.length; i++) &#123;</span><br><span class="line">            System.out.print(q[i]);</span><br><span class="line">            System.out.print(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">merge_sort</span><span class="params">(<span class="keyword">int</span>[] q, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (l &gt;= r) <span class="keyword">return</span>;</span><br><span class="line">        <span class="comment">// 确定分界点</span></span><br><span class="line">        <span class="keyword">int</span> mid = l + ((r - l) &gt;&gt; <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 递归</span></span><br><span class="line">        merge_sort(q, l, mid);</span><br><span class="line">        merge_sort(q, mid + <span class="number">1</span>, r);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span>[] tmp = <span class="keyword">new</span> <span class="keyword">int</span>[r - l + <span class="number">1</span>]; <span class="comment">// 辅助数组</span></span><br><span class="line">        <span class="comment">// 归并</span></span><br><span class="line">        <span class="keyword">int</span> k = <span class="number">0</span>; <span class="comment">// 表示tmp中有多少个数</span></span><br><span class="line">        <span class="comment">// 两个指针</span></span><br><span class="line">        <span class="keyword">int</span> i = l, j = mid + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (i &lt;= mid &amp;&amp; j &lt;= r) &#123;</span><br><span class="line">            <span class="keyword">if</span> (q[i] &lt;= q[j]) &#123;</span><br><span class="line">                tmp[k++] = q[i++];</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                tmp[k++] = q[j++];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 剩余</span></span><br><span class="line">        <span class="keyword">while</span> (i &lt;= mid) tmp[k++] = q[i++];</span><br><span class="line">        <span class="keyword">while</span> (j &lt;= r) tmp[k++] = q[j++];</span><br><span class="line">        <span class="comment">// 放回</span></span><br><span class="line">        <span class="keyword">for</span> (i = l, j = <span class="number">0</span>; i &lt;= r; i++, j++) q[i] = tmp[j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;归并排序模板&quot;&gt;&lt;a href=&quot;#归并排序模板&quot; class=&quot;headerlink&quot; title=&quot;归并排序模板&quot;&gt;&lt;/a&gt;归并排序模板&lt;/h1&gt;&lt;p&gt;分治思想&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/04/28/8g</summary>
      
    
    
    
    
    <category term="刷题" scheme="http://example.com/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Ubantu18.04安装NVIDIA驱动+cuda10.1+cuDNN+Tensorflow2.1.0</title>
    <link href="http://example.com/2021/04/26/Ubantu18-04%E5%AE%89%E8%A3%85NVIDIA%E9%A9%B1%E5%8A%A8-cuda10-1-cuDNN-Tensorflow2-1-0/"/>
    <id>http://example.com/2021/04/26/Ubantu18-04%E5%AE%89%E8%A3%85NVIDIA%E9%A9%B1%E5%8A%A8-cuda10-1-cuDNN-Tensorflow2-1-0/</id>
    <published>2021-04-26T02:18:40.000Z</published>
    <updated>2021-04-26T02:31:39.722Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Ubantu18-04安装NVIDIA驱动-cuda10-1-cuDNN-Tensorflow2-1-0"><a href="#Ubantu18-04安装NVIDIA驱动-cuda10-1-cuDNN-Tensorflow2-1-0" class="headerlink" title="Ubantu18-04安装NVIDIA驱动-cuda10-1-cuDNN-Tensorflow2-1-0"></a>Ubantu18-04安装NVIDIA驱动-cuda10-1-cuDNN-Tensorflow2-1-0</h1><p>注意：TensorFlow2.1 要求 你的GPU算力要达到3.5，检查自己GPU算力</p><h2 id="安装和卸载NVIDIA驱动"><a href="#安装和卸载NVIDIA驱动" class="headerlink" title="安装和卸载NVIDIA驱动"></a>安装和卸载NVIDIA驱动</h2><p>首先要确保驱动已经卸载干净</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get purge nvidia*</span><br><span class="line">sudo apt-get autoremove</span><br></pre></td></tr></table></figure><p>检查自己GPU版本，之后到官网去下载，这种办法安装比较稳妥，其他网络安装办法有时候出错不知道咋回事。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lshw -numeric -C display</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/04/26/tDVckAHU8B7dnla.png" alt=""></p><p>下载驱动网址：<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn">https://www.nvidia.cn/Download/index.aspx?lang=cn</a></p><p><img src="https://i.loli.net/2021/04/26/ruzlX3qQ6Ndat9I.png" alt=""></p><p>禁用Nouveau</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Nouveau驱动禁用方法：</span><br><span class="line"></span><br><span class="line">sudo gedit &#x2F;etc&#x2F;modprobe.d&#x2F;blacklist.conf</span><br><span class="line">或者</span><br><span class="line">sudo vim &#x2F;etc&#x2F;modprobe.d&#x2F;blacklist.conf</span><br><span class="line"> </span><br><span class="line">在最后两行添加：</span><br><span class="line"> </span><br><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset&#x3D;0     &#x2F;&#x2F; 禁用nouveau第三方驱动，之后也不需要改回来</span><br><span class="line"> </span><br><span class="line">执行</span><br><span class="line"> </span><br><span class="line">sudo update -initramfs -u   &#x2F;&#x2F; 更新内核</span><br></pre></td></tr></table></figure><p>关闭lightdm</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo service lightdm stop</span><br><span class="line"></span><br><span class="line">　sudo init 3 # 遇见X Server报错执行 </span><br><span class="line"></span><br><span class="line"> rm -rf &#x2F;tmp&#x2F;.X*</span><br><span class="line"></span><br><span class="line"> .&#x2F;NVIDIA-Linux-x86_64-418.165.02.run #开始安装驱动 遇见continue就continue 遇见ok就ok</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/04/26/Wf7Imlx8PyKqtUG.png" alt=""></p><h2 id="安装cuda10-1"><a href="#安装cuda10-1" class="headerlink" title="安装cuda10.1"></a>安装cuda10.1</h2><p><a href="https://tensorflow.google.cn/install/source#linux">https://tensorflow.google.cn/install/source#linux</a></p><p>在这个网站上对好版本，版本不对可不行，全是坑 </p><p><a href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a> 选择版本</p><p>然后在这里下载cuda 我用的是deb的办法也是本地下载后安装的。<strong>（我这个网络可能是不行，总是apt-get update 总是报错 所以这个方法没成功用runfile成功了。。。）参考一下吧</strong> </p><p><img src="https://i.loli.net/2021/04/26/wVXLYjvD5zHTNRu.png" alt=""></p><p>安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb</span><br><span class="line">sudo apt-key add &#x2F;var&#x2F;cuda-repo-&lt;version&gt;&#x2F;7fa2af80.pub</span><br><span class="line">sudo dpkg -i cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install cuda</span><br></pre></td></tr></table></figure><p>添加环境变量：</p><p>打开 .bashrc</p><p> sudo vim ~/.bashrc</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda </span><br><span class="line">export PATH&#x3D;$PATH:$CUDA_HOME&#x2F;bin </span><br><span class="line">export LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure><p>source ~/.bashrc</p><p>nvcc -V</p><h2 id="runfile安装cuda"><a href="#runfile安装cuda" class="headerlink" title="runfile安装cuda"></a>runfile安装cuda</h2><p>下载runfile</p><p><img src="https://i.loli.net/2021/04/26/7G8c26kofdjJQBh.png" alt=""></p><p><img src="https://i.loli.net/2021/04/26/LI4shCiMqcKaNQB.png" alt=""></p><p>一定要取消掉driver 此处！！！，因为已经装了驱动了</p><p><img src="https://i.loli.net/2021/04/26/5CmNy6BrOIiDlkp.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure><p>我们在文件最后一行添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;NsightCompute-2019.1$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">$ export LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;lib64\</span><br><span class="line">                         $&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/04/26/VDZRTx76oi8MeuK.png" alt=""></p><h2 id="安装TensorFlow2-1-0-gpu"><a href="#安装TensorFlow2-1-0-gpu" class="headerlink" title="安装TensorFlow2.1.0_gpu"></a>安装TensorFlow2.1.0_gpu</h2><p>这上面虽然没写2.1.0_gpu 可是还得得装gpu版</p><p><img src="https://i.loli.net/2021/04/26/LNxBI3jmDrcGUVT.png" alt=""></p><p>完成后 </p><p>conda install cudatoolkit=10.1</p><p><img src="https://img2020.cnblogs.com/blog/1225390/202010/1225390-20201031135739329-731523260.png" alt=""></p><h2 id="安装cuDNN"><a href="#安装cuDNN" class="headerlink" title="安装cuDNN"></a>安装cuDNN</h2><p><a href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a></p><p>去下载对应版本，但是要登录一下</p><p>解压后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo cp cuda&#x2F;include&#x2F;cudnn.h &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;include</span><br><span class="line">sudo cp cuda&#x2F;lib64&#x2F;libcudnn* &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64</span><br><span class="line">sudo chmod a+r &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;include&#x2F;cudnn.h </span><br><span class="line">sudo chmod a+r &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64&#x2F;libcudnn*</span><br></pre></td></tr></table></figure><p>以配置cuDNN环境。</p><p>通过</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;include&#x2F;cudnn.h | grep CUDNN_MAJOR -A 2</span><br></pre></td></tr></table></figure><p>查看cuDNN版本</p><p>over</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Ubantu18-04安装NVIDIA驱动-cuda10-1-cuDNN-Tensorflow2-1-0&quot;&gt;&lt;a href=&quot;#Ubantu18-04安装NVIDIA驱动-cuda10-1-cuDNN-Tensorflow2-1-0&quot; class=&quot;headerl</summary>
      
    
    
    
    
    <category term="GPU" scheme="http://example.com/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>快速排序模板</title>
    <link href="http://example.com/2021/04/26/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E6%A8%A1%E6%9D%BF/"/>
    <id>http://example.com/2021/04/26/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E6%A8%A1%E6%9D%BF/</id>
    <published>2021-04-26T01:56:51.000Z</published>
    <updated>2021-04-26T03:27:56.883Z</updated>
    
    <content type="html"><![CDATA[<h1 id="快速排序模板"><a href="#快速排序模板" class="headerlink" title="快速排序模板"></a>快速排序模板</h1><p><img src="https://i.loli.net/2021/04/26/voAgqKV12i9Rxuz.png" alt=""></p><ol><li>先确定分界点：$q[l] 、 q[(l+r)/2]、 q[r]$ 或随机</li><li>调整区间：小于等于x的在左半边，大于等于x的在右半边 (如何去调整)</li><li>递归处理左右两段</li></ol><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>暴力：</p><ul><li>声明两个数组 a[] 、b[]</li><li>将$q[l~r]$ 遍历 </li><li>如果 $q[i] \le x$ 放到a[]中   </li><li>如果 $q[i] \ge x$ 放到b[]中   </li><li>再将a、b数组放回q中</li></ul><p>优美：</p><p>用两个指针，swap</p><p><a href="https://blog.csdn.net/qq_42369555/article/details/82745923">关于JAVA中IO流类：BufferredReader的简单用法</a></p><p>bufferreader要比scanner快</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> code;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.util.Collections;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> 快排模板 </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        BufferedReader input = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(System.in));</span><br><span class="line">        <span class="keyword">int</span> n = Integer.parseInt(input.readLine());</span><br><span class="line">        <span class="keyword">int</span>[] q = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">        String[] linelist = input.readLine().split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; linelist.length; i++) &#123;</span><br><span class="line">            q[i] = Integer.parseInt(linelist[i]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        quick_sort(q, <span class="number">0</span>, q.length - <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; q.length; i++) &#123;</span><br><span class="line">            System.out.print(q[i]);</span><br><span class="line">            System.out.print(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">quick_sort</span><span class="params">(<span class="keyword">int</span>[] q, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (l &gt;= r) <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">int</span> x = q[l];</span><br><span class="line">        <span class="keyword">int</span> i = l - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> j = r + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (i &lt; j) &#123;</span><br><span class="line">            <span class="keyword">do</span> i++; <span class="keyword">while</span> (q[i] &lt; x);</span><br><span class="line">            <span class="keyword">do</span> j--; <span class="keyword">while</span> (q[j] &gt; x);</span><br><span class="line">            <span class="keyword">if</span> (i &lt; j) &#123;</span><br><span class="line">                <span class="keyword">int</span> t = q[i];</span><br><span class="line">                q[i] = q[j];</span><br><span class="line">                q[j] = t;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        quick_sort(q, l, j);</span><br><span class="line">        quick_sort(q, j + <span class="number">1</span>, r);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;快速排序模板&quot;&gt;&lt;a href=&quot;#快速排序模板&quot; class=&quot;headerlink&quot; title=&quot;快速排序模板&quot;&gt;&lt;/a&gt;快速排序模板&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/04/26/voAgqKV12i9Rxu</summary>
      
    
    
    
    
    <category term="刷题" scheme="http://example.com/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>DP分析——石子合并</title>
    <link href="http://example.com/2021/04/24/DP%E5%88%86%E6%9E%90%E2%80%94%E2%80%94%E7%9F%B3%E5%AD%90%E5%90%88%E5%B9%B6/"/>
    <id>http://example.com/2021/04/24/DP%E5%88%86%E6%9E%90%E2%80%94%E2%80%94%E7%9F%B3%E5%AD%90%E5%90%88%E5%B9%B6/</id>
    <published>2021-04-24T02:19:10.000Z</published>
    <updated>2021-04-24T04:39:01.339Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DP分析——石子合并"><a href="#DP分析——石子合并" class="headerlink" title="DP分析——石子合并"></a>DP分析——石子合并</h1><p>设有 NN 堆石子排成一排，其编号为 1，2，3，…，N。</p><p>每堆石子有一定的质量，可以用一个整数来描述，现在要将这 N 堆石子合并成为一堆。</p><p>每次只能合并相邻的两堆，合并的代价为这两堆石子的质量之和，合并后与这两堆石子相邻的石子将和新堆相邻，合并时由于选择的顺序不同，合并的总代价也不相同。</p><p>例如有 4 堆石子分别为 <code>1 3 5 2</code>， 我们可以先合并 1、2堆，代价为 4，得到 <code>4 5 2</code>， 又合并 1，2 堆，代价为 9，得到 <code>9 2</code> ，再合并得到 11，总代价为 4+9+11=244+9+11=24；</p><p>如果第二步是先合并 2，3 堆，则代价为 7，得到 <code>4 7</code>，最后一次合并代价为 11，总代价为 4+7+11=22。</p><p>问题是：找出一种合理的方法，使总的代价最小，输出最小代价。</p><h4 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h4><p>第一行一个数 N 表示石子的堆数 N。</p><p>第二行 N 个数，表示每堆石子的质量(均不超过 1000)。</p><h4 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h4><p>输出一个整数，表示最小代价。</p><h4 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h4><p>1≤N≤300     1≤N≤300</p><h4 id="输入样例："><a href="#输入样例：" class="headerlink" title="输入样例："></a>输入样例：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4</span><br><span class="line">1 3 5 2</span><br></pre></td></tr></table></figure><h4 id="输出样例："><a href="#输出样例：" class="headerlink" title="输出样例："></a>输出样例：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">22</span><br></pre></td></tr></table></figure><h2 id="解"><a href="#解" class="headerlink" title="解"></a>解</h2><p><img src="https://i.loli.net/2021/04/24/CqE9QcaxYBzZKRw.png" alt=""></p><p><img src="https://i.loli.net/2021/04/24/gPlOsK5oXFcWutE.png" alt=""></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DP_</span>石子合并 </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="keyword">int</span> N = scanner.nextInt();</span><br><span class="line">        <span class="keyword">int</span>[] s = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>];  <span class="comment">//前缀和</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++) &#123;</span><br><span class="line">            s[i] = scanner.nextInt();</span><br><span class="line">            s[i] += s[i - <span class="number">1</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>][N + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> len = <span class="number">2</span>; len &lt;= N; len++) &#123;<span class="comment">//先枚举区间长度</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i + len - <span class="number">1</span> &lt;= N; i++) &#123;<span class="comment">//再枚举区间左端点</span></span><br><span class="line">                <span class="keyword">int</span> j = i + len - <span class="number">1</span>; <span class="comment">//右端点</span></span><br><span class="line">                dp[i][j] = <span class="number">100000000</span>;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> k = i; k &lt; j; k++) &#123;</span><br><span class="line">                    dp[i][j] = Math.min(dp[i][j], dp[i][k] + dp[k + <span class="number">1</span>][j] + s[j] - s[i - <span class="number">1</span>]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(dp[<span class="number">1</span>][N]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>$O(n^3)$ </p><hr><h1 id="最长公共子序列"><a href="#最长公共子序列" class="headerlink" title="最长公共子序列"></a>最长公共子序列</h1><p>给定两个长度分别为 N 和 M 的字符串 A 和 B，求既是 A 的子序列又是 B 的子序列的字符串长度最长是多少。</p><h4 id="输入格式-1"><a href="#输入格式-1" class="headerlink" title="输入格式"></a>输入格式</h4><p>第一行包含两个整数 N 和 M。</p><p>第二行包含一个长度为 N 的字符串，表示字符串 A。</p><p>第三行包含一个长度为 M 的字符串，表示字符串 B。</p><p>字符串均由小写字母构成。</p><h4 id="输出格式-1"><a href="#输出格式-1" class="headerlink" title="输出格式"></a>输出格式</h4><p>输出一个整数，表示最大长度。</p><h4 id="数据范围-1"><a href="#数据范围-1" class="headerlink" title="数据范围"></a>数据范围</h4><p>1≤N,M≤1000       1≤N,M≤1000</p><h4 id="输入样例：-1"><a href="#输入样例：-1" class="headerlink" title="输入样例："></a>输入样例：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">4 5</span><br><span class="line">acbd</span><br><span class="line">abedc</span><br></pre></td></tr></table></figure><h4 id="输出样例：-1"><a href="#输出样例：-1" class="headerlink" title="输出样例："></a>输出样例：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3</span><br></pre></td></tr></table></figure><h2 id="解-1"><a href="#解-1" class="headerlink" title="解"></a>解</h2><p>最坏情况下 aaaa,aaaaa，A中所有都是由 $2^n$ 个不同子序列。</p><p><img src="https://i.loli.net/2021/04/24/oxH4yliYPD23LeQ.png" alt=""></p><p><img src="https://i.loli.net/2021/04/24/dVY9UmoHTticMPC.png" alt=""></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="keyword">int</span> N = scanner.nextInt();</span><br><span class="line">        <span class="keyword">int</span> M = scanner.nextInt();</span><br><span class="line">        String strA = <span class="string">&quot; &quot;</span> + scanner.next();</span><br><span class="line">        String strB = <span class="string">&quot; &quot;</span> + scanner.next();</span><br><span class="line"><span class="comment">//        char[] A = strA.toCharArray();</span></span><br><span class="line"><span class="comment">//        char[] B = strB.toCharArray();</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>][M + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= M; j++) &#123;</span><br><span class="line">                dp[i][j] = Math.max(dp[i - <span class="number">1</span>][j], dp[i][j - <span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">if</span> (strA.charAt(i) == strB.charAt(j)) &#123;</span><br><span class="line">                    dp[i][j] = Math.max(dp[i][j], dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(dp[N][M]);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;DP分析——石子合并&quot;&gt;&lt;a href=&quot;#DP分析——石子合并&quot; class=&quot;headerlink&quot; title=&quot;DP分析——石子合并&quot;&gt;&lt;/a&gt;DP分析——石子合并&lt;/h1&gt;&lt;p&gt;设有 NN 堆石子排成一排，其编号为 1，2，3，…，N。&lt;/p&gt;
&lt;p&gt;每</summary>
      
    
    
    
    
    <category term="刷题" scheme="http://example.com/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Select, Answer and Explain-Interpretable Multi-hop Reading Comprehension over Multiple Documents</title>
    <link href="http://example.com/2021/04/22/Select-Answer-and-Explain-Interpretable-Multi-hop-Reading-Comprehension-over-Multiple-Documents/"/>
    <id>http://example.com/2021/04/22/Select-Answer-and-Explain-Interpretable-Multi-hop-Reading-Comprehension-over-Multiple-Documents/</id>
    <published>2021-04-21T16:47:09.000Z</published>
    <updated>2021-04-22T05:12:11.538Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Select-Answer-and-Explain-Interpretable-Multi-hop-Reading-Comprehension-over-Multiple-Documents"><a href="#Select-Answer-and-Explain-Interpretable-Multi-hop-Reading-Comprehension-over-Multiple-Documents" class="headerlink" title="Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents"></a>Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>选择、回答和解释(SAE)系统解决多文档RC问题。</p><p>首先主要创新，用文档分类器过滤掉与答案无关的文档，从而减少分散注意力的信息量。</p><p>然后将所选择的答案相关文档输入到模型以联合预测答案和支持句子。</p><p>该模型在答案预测的表征层和支持句子预测的句子层都设置了多任务学习目标，</p><p>并在这两个任务之间进行了基于注意力的交互，对模型进行了优化。</p><p>关键词：过滤无关文档、多任务学习、混合注意力交互</p><p><img src="https://i.loli.net/2021/04/22/yArGlY6zuT7eMsW.png" alt=""></p><h2 id="在HotpotQA中什么是gold-doc"><a href="#在HotpotQA中什么是gold-doc" class="headerlink" title="在HotpotQA中什么是gold doc"></a>在HotpotQA中什么是gold doc</h2><p>HotpotQA通过为答案提供支持句来鼓励可解释的QA模型，这些支持句通常来自多个文档，如果文档包含答案或包含对答案的支持句，则称为“黄金文档”。</p><p>应答文本，它可以是一段文本或“是/否”。</p><p>作者从答案和支持句标签导出GOLD文档标签。我们使用 $D_i$ 表示文档 i：如果Di是黄金文档，则标记为1，否则标记为0。还将答案类型标记为以下注释之一：“Span”、“Yes”和“No”。</p><h2 id="选择gold-doc-过滤文档"><a href="#选择gold-doc-过滤文档" class="headerlink" title="选择gold doc(过滤文档)"></a>选择gold doc(过滤文档)</h2><p>答案预测和支持句预测的上游任务。将分类排名最靠前的文档作为预测的黄金文档 gold doc。</p><p><img src="https://i.loli.net/2021/04/22/EapiyjuwNbPt49l.png" alt=""></p><p>做文档过滤最直接做法就是采用bert的CLS摘要向量，做交叉熵分类</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  L = -\sum_{i=0}^n t_ilogP(D_i) + (1-t_i) log(1-P(D_i))    \end{split}\end{equation}</script><p>$t_i$ 是 $D_i$ 的标签，n是文档数，$P(D_i)$ 是文档i在标签 $t_i$ 中的概率。</p><p>这种简单的方法的缺点：单独处理每个文档，而不考虑下游多跳推理任务所必需的文档间交互和关系。</p><p>为解决此问题，作者提出了一个新的模型，如图上图CLS后，加了一层多头注意力层。</p><p>意在：增加对从不同文档生成的“CLS”标记的关注的动机是鼓励文档间的交互。文档间交互对于文档间的多跳推理至关重要。</p><p>优化：采用了新的成对学习排序损失。还将问题从分类问题描述为两两学习排序问题，</p><p>通过将文档与所有其他文档进行比较，该模型能够更好地将一小部分黄金文档与睡觉分散注意力的文档区分开来。</p><p>给每个文档一共分数 $S(.)$</p><p>如果 $D_i$ 是gold doc $S(D_i) = 1 $, 否则 $S(D_i) = 0$</p><p>然后，标记每对输入文档：给定一对输入文档 $(D_i，D_j)$，标签 $l$设置为：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  l_{i,j} = \begin{cases} 1, &if\ S(D_i) \gt S(D_j) \\ 0 , &if\  S(D_i) \le S(D_j)\end{cases}    \end{split}\end{equation}</script><p>还认为包含答案范围的文档对于下游任务更重要。因此，如果$D_i$是包含答案跨度的黄金文献，$S(D_i)=2$。</p><p>再将MHSA输出传递给双线性层来输出每对文档的概率，双线性层基于二元交叉熵进行训练，如下所示：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  L = - \sum_{i=0,j=0}^n \sum_{j\neq i} l_{i,j} logP(D_i,D_j) + (1-l_{i,j}) log(1-P(D_i,D_j))    \end{split}\end{equation}</script><p>相关性定义为$ R_i=􏰅\sum_j^n(P(D_i，D_j)&gt;0.5)$。将来自该相关性排序的前k个排序的文档作为的过滤文档。</p><h2 id="答案和解释"><a href="#答案和解释" class="headerlink" title="答案和解释"></a>答案和解释</h2><p>模型采用多任务学习的方式进行训练，以联合预测答案和对黄金文档的支持意义。</p><p>基于GNN构建多跳推理图，将上下文语句嵌入作为节点，而不是像以往的工作那样以实体作为节点，直接输出带有答案预测的支持语句。</p><p>为什么不用NER因为作者认为：</p><p>目前GNN在QA任务中的应用通常需要实体作为图形节点，并且通过在具有上下文信息的节点上进行消息传递来实现推理。这仅在预定义的一组目标实体可用时才有可能。否则，需要使用命名实体识别(NER)工具来提取实体，这可能会导致图推理中的冗余实体和噪声实体。如果答案不是命名实体，则需要进一步处理以定位最终答案。</p><p> token-level and sentence-level 多任务学习</p><p>基于一种新的混合注意池机制</p><p>将GNN中使用的上下文语句嵌入归结到令牌表示上。注意力权重是根据令牌表示上的答案广度日志和自我注意输出来计算的。这种基于注意力的交互能够利用“回答”和“解释”任务之间的互补信息。</p><h3 id="答案预测"><a href="#答案预测" class="headerlink" title="答案预测"></a>答案预测</h3><p>针对bert输出的每一个$H_i$ 用两层MLP做答案起始位置预测 $L$ 为长度</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \hat Y &= f_{span} (H^i) \in R^{L\times2}\\ L ^{span} &= \frac{1}{2}(CE(\hat Y[:,0], y^{start}) + CE(\hat Y[:,1], y^{end}))    \end{split}\end{equation}</script><p>其中$\hat Y$的第一行是起始位置的逻辑，第二行是结束位置的逻辑。$y^{star}t$和 $y^{end}$ 是范围 [0，L-1] 中的开始位置和结束位置的标签。CE表示交叉熵损失函数。</p><h3 id="支持句预测"><a href="#支持句预测" class="headerlink" title="支持句预测"></a>支持句预测</h3><p>预测输入上下文中的句子是否为答案预测的支持证据。为了实现句子级预测，我们首先获得$H_i$中每个句子的序列表示。$H_i$ 是bert的token输出。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  S^j - H[j^s:j^e,:] \in R^{L^j\times d}    \end{split}\end{equation}</script><p>$S^j$是表示语句 j 内的标记嵌入的矩阵( 这里省略了样本索引i)。 $j^s$ 和 $j^e$ 定义了开始和结束位置，$L_j$ 是语句$j$ 的长度。</p><h3 id="多任务"><a href="#多任务" class="headerlink" title="多任务"></a>多任务</h3><p>答案预测任务和支持句预测任务可以相辅相成。</p><p>据观察，答案预测任务总是可以帮助支持句子预测任务，因为有答案的句子总是一条证据；</p><p>但反过来情况不是一样的，因为可能有多个支持句子，概率最高的句子可能不包含答案</p><p>所以答案预测任务总 可以帮助支持句子预测任务，因为有答案的句子总是一个证据；</p><p>反之亦然，因为可能有多个支持句子，概率最高的句子可能不包含答案。</p><p>因此，为了揭示这两个互补任务之间的相互作用，提出了一种基于注意力的总结句子表示法，以引入来自回答预测的互补信息。</p><p>注意力权重的计算方法如下：在Sj上用自我注意计算一部分注意力，另一部分来自答案预测任务的起始位置日志和结束位置日志的总和。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \alpha^j &= \sigma(f_{att}(S^j) + \hat Y[j^s:j^e,0] + \hat Y[j^s:j^e,1])\\ s^j &= \sum^{L^j}_{k=0} \alpha^j_k S^j[k,:] \in R^{1\times d}    \end{split}\end{equation}</script><p>Sj是表示语句j 的标记嵌入的矩阵</p><p>$f_{att}$ 是一个两层MLP输出size为1，$\sigma$是softmax</p><p>$α_j ∈ R^{L^j×1}$表示句子j的每个token上的关注度权重。</p><h3 id="构建GNN"><a href="#构建GNN" class="headerlink" title="构建GNN"></a>构建GNN</h3><p>接下来，在语句嵌入Sj上建立GNN模型，以显式地促进对预测gold doc中所有语句的多跳推理，从而更好地利用复杂的关系信息。使用语句嵌入Sj来初始化图的节点特征。采用基于多关系图卷积网络(GCN)的消息传递策略来更新图的节点特征，并将最终的节点特征输入到MLP中，得到每个句子的分类。</p><p><img src="https://i.loli.net/2021/04/22/kfrJdNaDZBqyAhV.png" alt=""></p><p>根据问题和句子中出现的命名实体和名词短语设计了三种类型的边：</p><ul><li>如果两个节点最初来自同一文档，则在这两个节点之间添加一条边(上图中的实节点)</li><li>如果表示两个节点的句子在问题中都具有命名实体或名词短语(可以是不同的)，则在来自不同文档的两个节点之间添加边。(图中的虚线)</li><li>如果表示两个节点的句子具有相同的命名实体或名词短语，则在来自不同文档的两个节点之间添加一条边。(图中的虚线)</li></ul><p>第一种类型的边的动机是希望GNN掌握每个文档中呈现的全局信息。</p><p>第二类和第三类边，为了以更好地捕捉这种跨文档推理路径。跨文档推理是通过从问题中的实体跳到未知的桥梁实体或比较问题中两个实体的属性来实现的 。</p><p>对于消息传递，使用具有门控机制的多关系GCN。</p><p>假设 $h^0_j$ 表示从语句嵌入 $S_j$的初始节点嵌入，则一跳(或一层)之后的节点嵌入计算可表示为:</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  h_j^{k+1} &= act(u_j^k) \odot g^k_j + h^k_j \odot (1-g^k_j) \\ u^k_j &= f_s(h^k_j) + \sum_{r\in R} \frac{1}{|N_j^r|} \sum_{n\in N^r_j} f_r(h_n^k)\\ g_j^k &= sigmoid (f_g([u_j^k; h^k_j]))     \end{split}\end{equation}</script><p>其中R 是一些列边类型， $N^r_j$ 是边类型为r的 j 节点的邻居。</p><p>$h^k_n$ 是节点n的第k层节点表示。</p><p>$f_r、f_s、f_g$中的每一个都定义了输入节点表示上的变换，并且可以使用MLP来实现。</p><p>门控$g_j^k$ 是由0和1之间的值组成的向量，用于控制来自计算的更新$u^k_j$ 或来自原始节点表示的信息。</p><p>函数$act$表示非线性激活函数。</p><p>最后得到每个节点的最终表示 $h_j$ 后用两层MLP 最终预测 。</p><p>$\hat y^{sp}<em>j = sigmoid(f</em>{sp}(h_j))$ </p><p>除了支持句子预测任务之外，还在GNN输出之上添加了另一个任务，以说明“Yes/No”类型的问题。</p><p>预测任务描述为3类(“Yes”、“No”和“span”)分类</p><p>引入：</p><p>$h = \sum_j a_jh_j$</p><p>$a = \sigma(\hat y^{sp})$</p><p>$\hat y^{ans} = f_{ans}(h)$</p><p>最终loss表达为：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  L = \gamma L^{span} + BCE(\hat y^{sp}, y^{sp}) + CE(\hat y^{ans}, y^{ans})    \end{split}\end{equation}</script><p>$BCE()$ 二元交叉熵函数</p><p>为了考虑不同损失的尺度差异，在跨度损失中加入了一个权重γ。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Select-Answer-and-Explain-Interpretable-Multi-hop-Reading-Comprehension-over-Multiple-Documents&quot;&gt;&lt;a href=&quot;#Select-Answer-and-Explain</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>DP分析法--01背包问题</title>
    <link href="http://example.com/2021/04/20/DP%E5%88%86%E6%9E%90%E6%B3%95%E2%80%94%E2%80%9401%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"/>
    <id>http://example.com/2021/04/20/DP%E5%88%86%E6%9E%90%E6%B3%95%E2%80%94%E2%80%9401%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/</id>
    <published>2021-04-20T01:14:15.000Z</published>
    <updated>2021-04-24T02:15:41.794Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DP分析法—01背包问题"><a href="#DP分析法—01背包问题" class="headerlink" title="DP分析法—01背包问题"></a>DP分析法—01背包问题</h1><p>从集合角度来分析DP问题，DP问题的题目一般都是从有限集中求得最值的问题。</p><p><img src="https://i.loli.net/2021/04/20/SlgJ96RzdyGp5fw.png" alt=""></p><p><a href="https://www.acwing.com/problem/content/2/">01背包问题</a></p><p>有 N 件物品和一个容量是 V 的背包。每件物品只能使用一次。</p><p>第 i 件物品的体积是 $v_i$，价值是 $w_i$。</p><p>求解将哪些物品装入背包，可使这些物品的总体积不超过背包容量，且总价值最大。<br>输出最大价值。</p><h4 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h4><p>第一行两个整数，N，V，用空格隔开，分别表示物品数量和背包容积。</p><p>接下来有 N 行，每行两个整数 $v_i,w_i$，用空格隔开，分别表示第 i 件物品的体积和价值。</p><h4 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h4><p>输出一个整数，表示最大价值。</p><h4 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h4><p>0&lt;N,V≤10000&lt;N,V≤1000<br>0&lt;vi,wi≤10000&lt;vi,wi≤1000</p><h4 id="输入样例"><a href="#输入样例" class="headerlink" title="输入样例"></a>输入样例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">4 5</span><br><span class="line">1 2</span><br><span class="line">2 4</span><br><span class="line">3 4</span><br><span class="line">4 5</span><br></pre></td></tr></table></figure><h4 id="输出样例："><a href="#输出样例：" class="headerlink" title="输出样例："></a>输出样例：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">8</span><br></pre></td></tr></table></figure><h2 id="解"><a href="#解" class="headerlink" title="解"></a>解</h2><p>最多$2^N$, 从$2^N$ 个方案里找总价值最大的方案。——有限集合的最值问题</p><p>状态表示：</p><ul><li><p>选择问题一般$f(i,j)$ 第一维表示前i个物品,第二维是限制 (经验)</p></li><li><p>集合：所有只考虑前i个物品，且总体积不超过j的选法的集合。</p></li><li>属性：集合中每一个方案的最大价值Max</li></ul><p>状态计算：</p><ul><li>所有不选第i个物品的方案 $f(i-1,j)$</li><li>所有选择第i个物品的方案 $f(i-1,j-v_i) + w_i$</li><li>$Max(f(i-1,j), f(i-1,j-v_i)+w_i)$</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 读入数据的代码</span></span><br><span class="line">        Scanner reader = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="comment">// 物品的数量为N</span></span><br><span class="line">        <span class="keyword">int</span> N = reader.nextInt();</span><br><span class="line">        <span class="comment">// 背包的容量为V</span></span><br><span class="line">        <span class="keyword">int</span> V = reader.nextInt();</span><br><span class="line">        <span class="comment">// 一个长度为N的数组，第i个元素表示第i个物品的体积；</span></span><br><span class="line">        <span class="keyword">int</span>[] v = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>] ;</span><br><span class="line">        <span class="comment">// 一个长度为N的数组，第i个元素表示第i个物品的价值；</span></span><br><span class="line">        <span class="keyword">int</span>[] w = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>] ;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span> ; i &lt;= N ; i++)&#123;</span><br><span class="line">            <span class="comment">// 接下来有 N 行，每行有两个整数:v[i],w[i]，用空格隔开，分别表示第i件物品的体积和价值</span></span><br><span class="line">            v[i] = reader.nextInt();</span><br><span class="line">            w[i] = reader.nextInt();</span><br><span class="line">        &#125;</span><br><span class="line">        reader.close() ;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 正式工作的代码</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        定义一个二阶矩阵dp[N+1][V+1],</span></span><br><span class="line"><span class="comment">        这里之所以要N+1和V+1，是因为第0行表示只能选择第0个物品的时候，即没有物品的时候</span></span><br><span class="line"><span class="comment">        第0列表示背包的体积为0的时候，即不能装任何东西的时候</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">        dp[i][j]表示在 只能选择前i个物品，背包容量为j的情况下，背包中物品的最大价值</span></span><br><span class="line"><span class="comment">        对于dp[i][j]有两种情况：</span></span><br><span class="line"><span class="comment">        1. 不选择当前的第i件物品/第i件物品比背包容量要大，则dp[i][j] = dp[i-1][j]</span></span><br><span class="line"><span class="comment">        2. 选择当前的第i件物品（潜在要求第i件物品体积小于等于背包总容量），则能装入的物品最大价值为：</span></span><br><span class="line"><span class="comment">            当前物品的价值 加上 背包剩余容量在只能选前i-1件物品的情况下的最大价值</span></span><br><span class="line"><span class="comment">            dp[i][j] = dp[i-1][j-v[i]] + w[i]</span></span><br><span class="line"><span class="comment">        dp[i][j]在两种情况中选择比较大的情况作为当前的最优解；</span></span><br><span class="line"><span class="comment">        即：</span></span><br><span class="line"><span class="comment">        if(j &gt;= v[i]):</span></span><br><span class="line"><span class="comment">            dp[i][j] = max(dp[i-1][j], dp[i-1][j-v[i]] + w[i])</span></span><br><span class="line"><span class="comment">        else:</span></span><br><span class="line"><span class="comment">            dp[i][j] = dp[i-1][j]</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[N+<span class="number">1</span>][V+<span class="number">1</span>];</span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= V; j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(j &gt;= v[i])&#123;</span><br><span class="line">                    dp[i][j] = Math.max(dp[i-<span class="number">1</span>][j], dp[i-<span class="number">1</span>][j-v[i]] + w[i]);</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(dp[N][V]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>优化后</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 读入数据的代码</span></span><br><span class="line">        Scanner reader = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="comment">// 物品的数量为N</span></span><br><span class="line">        <span class="keyword">int</span> N = reader.nextInt();</span><br><span class="line">        <span class="comment">// 背包的容量为V</span></span><br><span class="line">        <span class="keyword">int</span> V = reader.nextInt();</span><br><span class="line">        <span class="comment">// 一个长度为N的数组，第i个元素表示第i个物品的体积；</span></span><br><span class="line">        <span class="keyword">int</span>[] v = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>] ;</span><br><span class="line">        <span class="comment">// 一个长度为N的数组，第i个元素表示第i个物品的价值；</span></span><br><span class="line">        <span class="keyword">int</span>[] w = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>] ;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span> ; i &lt;= N ; i++)&#123;</span><br><span class="line">            <span class="comment">// 接下来有 N 行，每行有两个整数:v[i],w[i]，用空格隔开，分别表示第i件物品的体积和价值</span></span><br><span class="line">            v[i] = reader.nextInt();</span><br><span class="line">            w[i] = reader.nextInt();</span><br><span class="line">        &#125;</span><br><span class="line">        reader.close() ;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 正式算法的代码</span></span><br><span class="line">        <span class="comment">// 将dp优化为一维数组</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        注意，这里第二层循环的时候，还是小到大循环的话，那么</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">        dp[i][j] = Math.max(dp[i-1][j], dp[i-1][j-v[i]] + w[i])</span></span><br><span class="line"><span class="comment">        实际上变成了</span></span><br><span class="line"><span class="comment">        dp[i][j] = Math.max(dp[i][j], dp[i][j-v[i]] + w[i]);</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">        因为i-1的值已经在前面被更新过了，覆盖了</span></span><br><span class="line"><span class="comment">        为了避免这个问题，所以要逆序更新，即先更新第i个，然后更新第i-1个，从而保证第i-1个不被覆盖</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">        如果不逆序的话，输出结果为10，dp数组实际为：</span></span><br><span class="line"><span class="comment">        0 0 0 0 0 0 </span></span><br><span class="line"><span class="comment">        0 2 4 6 8 10</span></span><br><span class="line"><span class="comment">        0 2 4 6 8 10</span></span><br><span class="line"><span class="comment">        0 2 4 6 8 10</span></span><br><span class="line"><span class="comment">        0 2 4 6 8 10</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[V+<span class="number">1</span>];</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = V; j &gt;= v[i]; j--)&#123;</span><br><span class="line">                dp[j] = Math.max(dp[j], dp[j-v[i]] + w[i]);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// for(int j = 0; j &lt;= V; j++)&#123;</span></span><br><span class="line">            <span class="comment">//     System.out.print(dp[j]);</span></span><br><span class="line">            <span class="comment">//     System.out.print(&quot; &quot;);</span></span><br><span class="line">            <span class="comment">// &#125;</span></span><br><span class="line">            <span class="comment">// System.out.print(&quot;\n&quot;);</span></span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(dp[V]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">o1bagSolutionOptimization</span><span class="params">(<span class="keyword">int</span>[] weight, <span class="keyword">int</span>[] value, <span class="keyword">int</span> bagWeight)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> num = weight.length;</span><br><span class="line">    <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[bagWeight + <span class="number">1</span>];</span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= num; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = bagWeight; j &gt;= <span class="number">1</span>; j--) &#123;</span><br><span class="line">            <span class="keyword">if</span> (j &gt;= weight[i - <span class="number">1</span>]) &#123;</span><br><span class="line">                dp[j] = Math.max(dp[j], dp[j - weight[i - <span class="number">1</span>]] + value[i - <span class="number">1</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dp[bagWeight];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Scanner sc = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">    <span class="keyword">int</span> itemsNumber = sc.nextInt();</span><br><span class="line">    <span class="keyword">int</span> bagWeight = sc.nextInt();</span><br><span class="line">    <span class="keyword">int</span>[][] arr = <span class="keyword">new</span> <span class="keyword">int</span>[itemsNumber][<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">int</span>[] weight = <span class="keyword">new</span> <span class="keyword">int</span>[itemsNumber];</span><br><span class="line">    <span class="keyword">int</span>[] value = <span class="keyword">new</span> <span class="keyword">int</span>[itemsNumber];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; itemsNumber; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; j++) &#123;</span><br><span class="line">            arr[i][j] = sc.nextInt();</span><br><span class="line">        &#125;</span><br><span class="line">        weight[i] = arr[i][<span class="number">0</span>];</span><br><span class="line">        value[i]=   arr[i][<span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(o1bagSolutionOptimization(weight, value, bagWeight));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="完全背包问题"><a href="#完全背包问题" class="headerlink" title="完全背包问题"></a>完全背包问题</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> 完全背包问题 </span>&#123;</span><br><span class="line">    <span class="comment">// 完全背包和01背包的区别是完全背包中每个物品可以用无限次</span></span><br><span class="line"><span class="comment">// 01背包：f[i][j] = max(f[i-1][j], f[i-1][j-v]+w)</span></span><br><span class="line"><span class="comment">// 完全背包：f[i][j] = max(f[i-1][j], f[i][j-v]+w)</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Scanner reader = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="keyword">int</span> N = reader.nextInt();</span><br><span class="line">        <span class="keyword">int</span> V = reader.nextInt();</span><br><span class="line">        <span class="keyword">int</span>[] v = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span>[] w = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++) &#123;</span><br><span class="line">            v[i] = reader.nextInt();</span><br><span class="line">            w[i] = reader.nextInt();</span><br><span class="line">        &#125;</span><br><span class="line">        reader.close();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[V + <span class="number">1</span>];</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= V; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (j &gt;= v[i]) &#123;</span><br><span class="line">                    dp[j] = Math.max(dp[j], dp[j - v[i]] + w[i]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(dp[V]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// int nativeDp(int n,int m)&#123;</span></span><br><span class="line">    <span class="comment">//     int[] f = new int[maxN];</span></span><br><span class="line">    <span class="comment">//     for(int i=1;i&lt;=n;i++)&#123;</span></span><br><span class="line">    <span class="comment">//         for(int j=m;j&gt;=v[i];j--)&#123;</span></span><br><span class="line">    <span class="comment">//             for(int k=0;k*v[i]&lt;=j;k++)&#123;</span></span><br><span class="line">    <span class="comment">//                 f[j] = Math.max(f[j], f[j-k*v[i]]+k*w[i]);</span></span><br><span class="line">    <span class="comment">//             &#125;</span></span><br><span class="line">    <span class="comment">//         &#125;</span></span><br><span class="line">    <span class="comment">//     &#125;</span></span><br><span class="line">    <span class="comment">// &#125;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;DP分析法—01背包问题&quot;&gt;&lt;a href=&quot;#DP分析法—01背包问题&quot; class=&quot;headerlink&quot; title=&quot;DP分析法—01背包问题&quot;&gt;&lt;/a&gt;DP分析法—01背包问题&lt;/h1&gt;&lt;p&gt;从集合角度来分析DP问题，DP问题的题目一般都是从有限集中求</summary>
      
    
    
    
    
    <category term="刷题" scheme="http://example.com/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>LongFormer:The Long-Document Transformer</title>
    <link href="http://example.com/2021/04/18/LongFormer-The-Long-Document-Transformer/"/>
    <id>http://example.com/2021/04/18/LongFormer-The-Long-Document-Transformer/</id>
    <published>2021-04-18T06:40:10.000Z</published>
    <updated>2021-04-18T16:00:17.349Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LongFormer-The-Long-Document-Transformer"><a href="#LongFormer-The-Long-Document-Transformer" class="headerlink" title="LongFormer:The Long-Document Transformer"></a>LongFormer:The Long-Document Transformer</h1><p>主要记录一些Longfromer的原理和使用时的细节。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>针对的问题：</p><ul><li>基于Transformer的模型，由于self-attention的操作，导致不能处理很长的序列。</li><li>self-attention的处理规模和序列长度是成二次关系的。</li></ul><p><img src="https://i.loli.net/2021/04/18/3YtkrO18p2dAvmV.png" alt=""></p><p>因为self-attention对于每个token都要计算打分，也就是缩放点积中的$QK^T$ 矩阵运算。</p><p>这相当于对每个token之间都照顾到了注意信息。</p><p>每个token代表一个小格，自注意力机制的QK都是自己，所以是个正方形。</p><p>为解决这个问题，作者引入了三种具有随序列长度线性缩放的注意机制，将规模缩减成线性。</p><p>分别是局部窗口注意和任务激活的全局注意力。</p><p>并且还提供了LongFormer的预训练模型。</p><p>定义了生成结构为Long-Forward-Encoding-Decoder(LED) </p><h2 id="引入-amp-相关工作"><a href="#引入-amp-相关工作" class="headerlink" title="引入&amp;相关工作"></a>引入&amp;相关工作</h2><p>熟知的Bert等预训练模型，最大长度为512，多的就要截断，这样可能会潜在地导致重要的跨分区信息丢失问题。</p><p>然而当时已有的针对解决长文本的方法，都是基于自回归语言模型的。</p><p>而LongFormer是可以应用于迁移学习环境中的文档级NLP任务的。</p><p><img src="https://i.loli.net/2021/04/18/KNJa3dZx6fCG8eH.png" alt=""></p><p>之后可能会读几篇。ltr从左到右的模型，其受益于双向语境(自回归或从左到右的语言建模被粗略地定义为在给定输入序列中的先前符号/字符的情况下估计现有符号/字符的概率分布)。</p><p>spare代表模型通过稀疏性来进行优化。</p><p>Generating long se-quences with sparse transformers.其使用由BlockSparse提供的大小为8x8的块的扩展滑动窗口的形式，但没有探索预训练设置。等等</p><h2 id="LongFormer"><a href="#LongFormer" class="headerlink" title="LongFormer"></a>LongFormer</h2><p>原始Transformer的自注意力机制有$O(n^2)$ 的时间和空间内存复杂度。</p><p>为了解决这个问题，作者根据指定相互关注的输入位置对的“注意模式”来稀疏完整的自我注意矩阵</p><p>与full self-attention不同的是，提出的注意力模式与输入序列成线性关系，这使得它对较长的序列是有效的。</p><h3 id="注意力模式"><a href="#注意力模式" class="headerlink" title="注意力模式"></a>注意力模式</h3><h4 id="滑动窗口-Sliding-Window"><a href="#滑动窗口-Sliding-Window" class="headerlink" title="滑动窗口 (Sliding Window)"></a>滑动窗口 (Sliding Window)</h4><p>设固定窗口大小为 w，transformer层数为$l$, token的每边 $\frac{1}{2}w$  计算复杂度为$O(n\times w)$</p><p><img src="https://i.loli.net/2021/04/18/XaDokntURBWdNSe.png" alt=""></p><p>作者认为：根据应用程序的不同，为每个图层使用不同的w值可能有助于在效率和模型表达能力之间取得平衡。</p><h4 id="空洞滑窗-Dilated-Sliding-Window"><a href="#空洞滑窗-Dilated-Sliding-Window" class="headerlink" title="空洞滑窗(Dilated Sliding Window)"></a>空洞滑窗(Dilated Sliding Window)</h4><p>类似于CNN的空洞卷积</p><p>空洞尺寸 $d$ 感受野是 $l\times d\times w$</p><p><img src="https://i.loli.net/2021/04/18/SxDhujGwCVIvt2g.png" alt=""></p><p>在多头注意力中，每个注意力头部计算不同的注意力分数。</p><p>作者发现，每个头具有不同扩张配置设置的话效果会好：</p><p>允许一些没有空洞的头部专注于局部语境，而另一些带空洞的则专注于更长的语境，从而提高了性能。</p><h4 id="全局注意力-Global-Attention"><a href="#全局注意力-Global-Attention" class="headerlink" title="全局注意力(Global Attention)"></a>全局注意力(Global Attention)</h4><p><img src="https://i.loli.net/2021/04/18/tVGNpUa3o9gIluf.png" alt=""></p><p>例如对于QA，问题和文档连接在一起，允许模型通过自我关注将问题与文档进行比较。</p><p>有时需要使用特殊的全局CLS作为整体的表达，所以就需要再这某些个关键点地方计算全局注意力，关注每一个token。其他的还是滑窗的形式。</p><p>我们在几个预先选择的输入位置添加了“全局关注”。</p><p>由于这样的记号token的数量相对于n很小，并且与n无关，因此组合的局部和全局注意的复杂度仍然是O(N)。</p><p>这时，计算打分函数就可以分为两组QKV，分别是全局的$Q_g,K_g,V_g$ 和 滑窗局部的 $Q_s,K_s,V_s$</p><p>昂贵的运算是矩阵乘法 $QK^T$，因为Q和K都具有n(序列长度)投影。对于LongFormer，空洞滑动窗口注意只计算固定数量$QK^T$的对角线。</p><p>在实现的时候主要用到了带状乘法。还定制了特别的CUDA内核。。</p><h3 id="对于自回归的语言模型"><a href="#对于自回归的语言模型" class="headerlink" title="对于自回归的语言模型"></a>对于自回归的语言模型</h3><p>可以使用空洞滑动窗口注意力，并且可以跨层使用不同尺寸的窗口，效果可能更佳。</p><p>对较低层使用较小的窗口大小，并在移动到较高层时增加窗口大小</p><p>这允许顶层了解整个序列的较高级别表示，同时使较低层捕获本地信息。此外，它还在效率和性能之间取得平衡。</p><p>(窗口大小越小，非零值越少，计算开销越小)</p><p>(窗口大小越大，表示能力更丰富，通常会带来性能提升)</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>和训练长文本的模型进行对比 ，BPC值越小越好</p><p><img src="https://i.loli.net/2021/04/18/Oxo2A1SCsaIeDL8.png" alt=""></p><h2 id="在QA上的Finetuning"><a href="#在QA上的Finetuning" class="headerlink" title="在QA上的Finetuning"></a>在QA上的Finetuning</h2><p>分别采用了我比较关注的多文档数据集 WikiHop/HotpotQA(干扰榜)/TriviaQA</p><p>将问题和文档连接成一个长序列放入Longformer，最后加一个预测层。</p><p><img src="https://i.loli.net/2021/04/18/lqTB5cSPrD8Z3w7.png" alt=""></p><h3 id="WikiHop"><a href="#WikiHop" class="headerlink" title="WikiHop"></a>WikiHop</h3><p>数据特点：</p><ul><li><p>候选答案个数由2个到79个不等。</p></li><li><p>文章段落数量由3段到63段不等</p></li></ul><p>数据集不为多跳推理链提供任何中间注释，需要模型代之以从间接答案监督中推断它们。</p><p>数据预处理：</p><p>将问题和答案与特殊令牌连接在一起</p><p>$ [q] question [/q] [ent] candidate1 [/ent] … [ent] candidateN [/ent] $</p><p>上下文也是使用文档分隔符进行间隔</p><p>$&lt;/s&gt; context1 &lt;/s&gt; … &lt;/s&gt; contextM &lt;/s&gt;$</p><p>在准备好输入数据后，从每个模型的顶层开始计算活动。获取问题和答案候选并将它们连接到尽可能多的上下文直到模型序列长度(Roberta为512，LongFormer为4,096)，在模型中运行序列，收集输出激活，并重复，直到用尽所有上下文(除了LongFormor-Large之外的所有模型，由于存储器要求，我们只包括第一个4,096长度的序列)。然后，将所有块的所有激活连接成一个长序列。在Longformer的下，使用全局注意力来关注整个问答候选序列。</p><p>最终预测，对每个[ent] 附加一个线性层，输出一个logit，最后平均所有候选答案的logits。 用softmax和交叉熵得出最终答案。</p><p>优化策略：</p><p>Adam、Linear warmup超过200梯度更新对于最大LR，然后linear decay剩余训练。</p><p>使用梯度累积最终batch达到32</p><p>其他超参Dropout weight decay 都和Roberta相同。</p><p>对LR[2e-5，3e-5，5e-5]和epoch[5，10，15]进行网格搜索。</p><p>LR=3e-5，15个epoch是最好的Longform-Base配置。</p><h3 id="TriviaQA"><a href="#TriviaQA" class="headerlink" title="TriviaQA"></a>TriviaQA</h3><p>TriviaQA有超过10万个问题、答案、文档。</p><p>文档是维基百科文章，答案是文章中提到的命名实体。</p><p>回答问题的跨度没有注释，但可以使用简单的文本匹配找到它。</p><p>数据预处理：</p><p>$[s] question [/s]document [/s]$</p><p>在所有问题符号上都使用全局注意力。</p><h2 id="HotpotQA"><a href="#HotpotQA" class="headerlink" title="HotpotQA"></a>HotpotQA</h2><p>使用两阶段首先确定相关段落，然后确定最终答案范围和证据。</p><p>这主要是因为首先删除分散注意力的段落，可以降低最终认识和范围检测的噪声，这一点也被发现非常重要此数据集中最新的最新方法。</p><p>数据预处理：</p><p>$[CLS] [q] question [/q] ⟨t⟩ title1 ⟨/t⟩ sent1,1 [s] sent1,2 [s] …⟨t⟩ title2 ⟨/t⟩ sent2,1 [s] sent2,2 [s] …$</p><p>使用全局注意力来问句标记、段落计时开始标记以及句子标记。</p><p>在段落标题顶部增加了前馈层，用于预测相关段落的开始标记，以及用于预测证据句子的句子标记。</p><p>在对第一阶段模型进行训练后，预测了训练集和开发集的相关段落得分。然后，保留最多5个原始得分高于预先指定的阈值(-3.0)的段落，并从上下文中删除其他段落。然后，根据得到的缩短上下文训练第二阶段模型。</p><p>将跨度、问题分类、句子和段落损失结合起来，使用线性损失组合对模型进行多任务训练。</p><p>使用ADAM优化器对模型进行了训练，并进行了线性warmup(1000步)和线性衰减。我们使用最小超参数调整，使用3E-5和5E-5的LR和3到7的epoch，发现LR为3E-5和5个历元的模型效果最好。</p><p><img src="https://i.loli.net/2021/04/19/LuOCHUx1eMPwDW9.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;LongFormer-The-Long-Document-Transformer&quot;&gt;&lt;a href=&quot;#LongFormer-The-Long-Document-Transformer&quot; class=&quot;headerlink&quot; title=&quot;LongFormer:T</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle上传dataset的方法</title>
    <link href="http://example.com/2021/04/15/Kaggle%E4%B8%8A%E4%BC%A0dataset%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <id>http://example.com/2021/04/15/Kaggle%E4%B8%8A%E4%BC%A0dataset%E7%9A%84%E6%96%B9%E6%B3%95/</id>
    <published>2021-04-15T04:34:33.000Z</published>
    <updated>2021-04-15T04:50:41.636Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kaggle快速上传dataset的方法"><a href="#Kaggle快速上传dataset的方法" class="headerlink" title="Kaggle快速上传dataset的方法"></a>Kaggle快速上传dataset的方法</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>从国内上传到有cdn的地方(如GitHub), 再在kaggle的kernel上下载下来，直接上传dataset。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>首先需要掌握kaggle-api的使用，kaggle-api是kaggle官方提供的命令行工具，可以从命理完成比赛数据的下载、dataset下载上传，获取榜单等操作。</p><p><a href="https://github.com/Kaggle/kaggle-api">https://github.com/Kaggle/kaggle-api</a></p><p>本地安装：pip install kaggle</p><p>Kaggle已经安装好了，不用再安装</p><p>步骤1：下载账户API json</p><p><a href="https://www.kaggle.com/me/account">https://www.kaggle.com/me/account</a></p><p>步骤2：在页面创建一个dataset</p><p><a href="https://www.kaggle.com/datasets">https://www.kaggle.com/datasets</a></p><p>步骤3：下载dataset的metadata</p><p>运行：kaggle datasets metadata shopee-models</p><p>步骤4：下载数据集并上传到dataset</p><p>完整代码：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将API json文件写到这里</span></span><br><span class="line">!mkdir /root/.kaggle</span><br><span class="line">lines = <span class="string">&#x27;&#x27;</span><span class="string">&#x27;&#123;&quot;username&quot;:&quot;写你的用户名&quot;,&quot;key&quot;:&quot;写你的key&quot;&#125;&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">with open(<span class="string">&#x27;/root/.kaggle/kaggle.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) as up:    </span><br><span class="line">up.write(lines)</span><br><span class="line"><span class="comment"># 创建文件夹，写入dataset的metadata</span></span><br><span class="line">!mkdir hubmapkidneysegmentation</span><br><span class="line">lines = <span class="string">&#x27;&#x27;</span><span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">&quot;id&quot;: &quot;finlay/shopee-models&quot;,</span></span><br><span class="line"><span class="string">&quot;id_no&quot;: 122348,</span></span><br><span class="line"><span class="string">&quot;title&quot;: &quot;shopee_models&quot;,</span></span><br><span class="line"><span class="string">&quot;subtitle&quot;: &quot;&quot;,</span></span><br><span class="line"><span class="string">&quot;description&quot;: &quot;&quot;,</span></span><br><span class="line"><span class="string">&quot;keywords&quot;: [],</span></span><br><span class="line"><span class="string">&quot;resources&quot;: []</span></span><br><span class="line"><span class="string">&#125;&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">with open(<span class="string">&#x27;hubmapkidneysegmentation/dataset-metadata.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) as up:</span><br><span class="line">up.write(lines)</span><br><span class="line"><span class="comment"># 下载文件，这里用axel多线程下载，直接用wget也可以的。</span></span><br><span class="line">!apt-get install axel</span><br><span class="line">!axel -n 12 https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth -o hubmapkidneysegmentation/baseline_fold0_densenet_224_epoch50.pth</span><br><span class="line"><span class="comment"># 上传文件，这里会覆盖上传</span></span><br><span class="line">!kaggle datasets version -p ./hubmapkidneysegmentation -m <span class="string">&quot;Updated data fcn&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Kaggle快速上传dataset的方法&quot;&gt;&lt;a href=&quot;#Kaggle快速上传dataset的方法&quot; class=&quot;headerlink&quot; title=&quot;Kaggle快速上传dataset的方法&quot;&gt;&lt;/a&gt;Kaggle快速上传dataset的方法&lt;/h1&gt;&lt;</summary>
      
    
    
    
    
    <category term="DataGame" scheme="http://example.com/tags/DataGame/"/>
    
  </entry>
  
  <entry>
    <title>DUMA: Reading Comprehension with Transposition Thinking</title>
    <link href="http://example.com/2021/04/14/DUMA-Reading-Comprehension-with-Transposition-Thinking/"/>
    <id>http://example.com/2021/04/14/DUMA-Reading-Comprehension-with-Transposition-Thinking/</id>
    <published>2021-04-14T04:21:00.000Z</published>
    <updated>2021-04-18T16:04:16.645Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DUMA-Reading-Comprehension-with-Transposition-Thinking"><a href="#DUMA-Reading-Comprehension-with-Transposition-Thinking" class="headerlink" title="DUMA: Reading Comprehension with Transposition Thinking"></a>DUMA: Reading Comprehension with Transposition Thinking</h1><p>DUMA：DUal Multi-head Co-Attention model</p><p>这是一篇针对解决多项选择任务的MRC网络结构。题目中的Transposition Think，被作者赋义为分别从文章和问题的角度来考虑对方的关注点。</p><p>主要特点：</p><ul><li>基于预训练语言模型(得到表示编码，替代复杂的匹配网络)</li><li>衔接多层co-attention(从三元组中捕捉关系)</li></ul><p>多项选择任务可以抽象为(文章P，问题q，选项a) 三元组。</p><p>针对多项选择的特点多项选择MRC尤其依赖于匹配网络的设计，它被认为是有效地捕捉文章、问题和答案三元组之间的关系。(不能只考虑推理如何做的更好，还要考虑答案出现的关键位置也就是匹配网络的作用)</p><p>文中总结的人在做阅读理解题时的特点：</p><ul><li>快速通读文章的整体内容，问题和回答选项，以建立全局印象，然后进行换角度思考过程。</li><li>根据问答选项的特有信息，重新考虑文章的细节，收集问答选项的支持证据。</li><li>根据文章中的特有信息，重新考虑问题和答案选项，以确定正确的选项，排除错误的选项。</li></ul><p>当人们重读文章时，他们倾向于根据对问答选项的印象提取关键信息，重读问答选项时也是如此</p><hr><h2 id="DUMA"><a href="#DUMA" class="headerlink" title="DUMA"></a>DUMA</h2><p>多项选择问题可以定义模型需要学习一个概率分布$F(A_1,A_2,…,A_t|P,Q)$</p><p><img src="https://i.loli.net/2021/04/14/IE9asGiRTlLJNV2.png" alt=""></p><p>Encoder 接受文本输入生成一个全局序列表达，这个过程类似人类第一次阅读整个内容以获得总体印象。</p><p>Decoder则收集所有信息的答案预测以选择正确答案选项。</p><p>DUMA层位于encoder和decoder之间，意在模仿人类转换思考角度的过程，从问题文章和关键词中捕捉关系信息。</p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>作者用的是PrLMs，其将文章、问题和所有不同的候选答案拼接作为输入。</p><p>$P=[p_1,p_2,..,p_m]$    ， $Q=[q_1,q_2,…,q_n]$ ,   $A=[a_1,a_2,…,a_k]$</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  E = Enc(P \oplus Q \oplus A ) \end{split}\end{equation}</script><p>这个输入到预训练的方式可能会遇到点问题，一般预训练语言模型比如bert都会限制一个输入的大小，如果文章过长的话，模型看不到问题和选项可能会导致训练效果不佳。可以改为 Q、A、P的形式，因为一般Q和A都比较短。</p><p>$E = [e<em>1,e_2,…,e</em>{m+n+k}]$  </p><p>$e<em>i$ 为固定维度$d</em>{model}$ 的向量，是各自的token。</p><h3 id="Dual-Multi-head-Co-Attention"><a href="#Dual-Multi-head-Co-Attention" class="headerlink" title="Dual Multi-head Co-Attention"></a>Dual Multi-head Co-Attention</h3><p>使用双多头共同注意模型来计算文章和问答的attention表征。(可堆叠k层)</p><p>其实就是一个多头co-attention，定义一个Q、K、V (Q不是上面的问题Q)</p><p>先从E中分离出$E^P = [e^P<em>1,e^P_2,…,E^P</em>{t<em>p}]$、$E^{QA} = [e^{qA},e^{qA},…,E^{qA}</em>{t_{q_a}}]$</p><p>使用两种计算attention的方法：</p><ul><li><p>$E^P$ 做Query ，$E^{QA}$ 做 Key和Value</p></li><li><p>$E^{QA}$ 做Query ，$E^{P}$ 做 Key和Value</p></li></ul><script type="math/tex; mode=display">\begin{equation}\begin{split}  Attention(E^P,E^{QA},E^{QA}) &= softmax(\frac{E^P(E^{QA})^T}{\sqrt{d_k}})E^{QA}\\ head_i &= Attention(E^PW^Q_i,E^{QA}W^K_i)\\ MIIA(E^P, E^{QA}, E^{QA}) &= Concat(head_1,head_2,...,head_h) W^O\\ MHA_1 &= MHA(E^P, E^{QA}, E^{QA}) \\ MHA_2 &= MHA(E^{QA}, E^{P}, E^P) \\ DUMA (E^P, E^{QA}) &= Fuse(MHA_1,MHA_2)\\    \end{split}\end{equation}</script><p>其中$W<em>i^Q \in R^{d</em>{model} \times d<em>q}$ 、 $W_i^K \in R^{d</em>{model} \times d<em>k}$、  $W_i^V \in R^{d</em>{model} \times d<em>q}$ 、$W_i^O \in R^{hd_v \times d</em>{model}}$  : h 头数</p><p>$MHA$: 多头注意力</p><p>$Fuse$ 函数先使用均值池化来汇集$MHA(·)$的序列输出，然后再聚合两个池化的输出。</p><p>后文实验了三种聚合方法 元素乘法  元素相加  concat</p><p>表示在决定哪个是最佳答案选项之前，对所有关键信息进行混合。</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><script type="math/tex; mode=display">\begin{equation}\begin{split}  O_i &= DUMA(E^P, E^{QA_i}) \\ L(A_r|P,Q) &= -log\frac{exp(W^TO_r)}{\sum_{i=1}^s exp(W^TO_i)} \end{split}\end{equation}</script><p>s 是选项数量</p><h2 id="Multi-choice-MRC数据集"><a href="#Multi-choice-MRC数据集" class="headerlink" title="Multi-choice MRC数据集"></a>Multi-choice MRC数据集</h2><p>DREAM and RACE</p><p><img src="https://i.loli.net/2021/04/14/63cBOaFhfGIgd58.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://i.loli.net/2021/04/14/3tQ1BCvzHSo9bTU.png" alt=""></p><p><img src="https://i.loli.net/2021/04/14/NchfAZRWxCIuQeS.png" alt=""></p><p><img src="https://i.loli.net/2021/04/14/d6JDXcVaTEmZPLF.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;DUMA-Reading-Comprehension-with-Transposition-Thinking&quot;&gt;&lt;a href=&quot;#DUMA-Reading-Comprehension-with-Transposition-Thinking&quot; class=&quot;hea</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>FREELB: ENHANCED ADVERSARIAL TRAINING FOR NATURAL LANGUAGE UNDERSTANDING</title>
    <link href="http://example.com/2021/04/09/FREELB-ENHANCED-ADVERSARIAL-TRAINING-FOR-NATURAL-LANGUAGE-UNDERSTANDING/"/>
    <id>http://example.com/2021/04/09/FREELB-ENHANCED-ADVERSARIAL-TRAINING-FOR-NATURAL-LANGUAGE-UNDERSTANDING/</id>
    <published>2021-04-09T11:56:02.000Z</published>
    <updated>2021-04-18T16:01:34.644Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FreeLB-Enhanced-Adversarial-Training-For-Natural-Language-Understanding"><a href="#FreeLB-Enhanced-Adversarial-Training-For-Natural-Language-Understanding" class="headerlink" title="FreeLB: Enhanced Adversarial Training For Natural Language Understanding"></a>FreeLB: Enhanced Adversarial Training For Natural Language Understanding</h1><p><a href="https://coding-zuo.github.io/2021/04/07/nlp%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83-%E4%B8%8Ebert%E7%BB%93%E5%90%88/">nlp中的对抗训练</a></p><p>承接上文，上文主要讲对抗训练的原理与物理意义与发展，对抗性训练是创建健壮神经网络的一种方法。在对抗性训练期间，小批次的训练样本受到对抗性扰动的污染，然后用于更新网络参数，直到得到的模型学会抵抗此类攻击，并且对模型起到了正则化的效果，提高模型泛化能力并且防止过拟合。</p><p>这篇论文结合现在流行的预训练模型或transformer模型只能结合到下游任务的embedding中。</p><p>提出FreeLB算法在GLUE上结合Roberta达到了当时的SOTA，是基于Transformer的自然语言理解和常识推理任务模型来做对抗。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>对抗性训练可以最小化标签保留输入扰动的最大风险，已被证明对提高语言模型的泛化能力是有效的。</p><p>FreeLB (Free Large-Batch)，通过在单词嵌入中添加对抗性扰动，并最小化输入样本周围不同区域内的对抗性风险，从而提高了嵌入空间的不变性。</p><p>在GLUE基准上的实验表明，当仅应用于精调阶段时，它能够将BERT-BASE模型的整体测试分数从78.3提高到79.4，将Roberta-Large模型的测试分数从88.5提高到88.8。</p><h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><p>针对PGD算法的问题：当K较小时，基于PGD的对抗性训练仍然会导致高度卷积和非线性的损失面，在更强的对手下很容易被打破，当K大时计算开销又很大。</p><p>利用最近提出的“Free”训练策略在不同范数约束下用多样化的对抗样本来丰富训练数据，</p><p>“Free”的对抗性训练算法在一次反向传递中同时更新模型参数和对抗性扰动。</p><p>还使用将大部分对抗性更新限制在第一层，有效减少的对抗过程正反向传播总量。</p><p>比PGD计算成本小，能在大规模的预训练模型上进行对抗训练。</p><h2 id="文本的对抗样本-对手"><a href="#文本的对抗样本-对手" class="headerlink" title="文本的对抗样本(对手)"></a>文本的对抗样本(对手)</h2><ol><li>黑盒环境下对embedding进行扰动(对手不是从样本进行攻击)</li><li>在输入中添加分散注意力的句子(人工)</li><li>用GANs将输入投影到潜在空间，并搜索接近原始的文本对手</li></ol><p><img src="https://i.loli.net/2021/04/09/daKoDQkv35hwmEP.png" alt=""></p><p>第二三算是一种辅助模型，数据增强的一种形式。</p><p>如何在没有人工评估的情况下通过单词/字符替换来构建保留标签的对抗性示例仍然不清楚，因为每个单词/字符的含义取决于上下文。</p><p>所以主要还是采用第一种进行对抗训练。</p><p>因为词的输入表达有很多种，像词embedding、句子embedding和位置embedding。作者和其他对抗训练一样只干扰词embedding和拼接词的embedding。</p><p>注意，基于Embedding的对手严格来说比更传统的基于文本的对手更强大，因为对手可以在单词嵌入上进行在文本域中不可能进行的操作。因为CV都是从样本层面进行扰动，这个扰动从embedding上扰动，相当于在更高级的层面，所以更强大。</p><h2 id="FreeLB"><a href="#FreeLB" class="headerlink" title="FreeLB"></a>FreeLB</h2><p>此前预训练语言模型对于下游任务已被证实很有效。</p><p>作者的目标是通过在下游语言理解任务的精调过程中增强它们在嵌入空间中的鲁棒性，进一步提高这些预先训练的语言模型在下游语言理解任务上的泛化能力。</p><p>由于这篇论文只对对抗性训练的效果感兴趣，而不是产生实际的对抗性示例，因此使用基于梯度的方法在输入句子的嵌入中添加范数有界的对抗性扰动。</p><p>定义模型的输入One-hot向量为 $ Z=[z_1,z_2,…,z_n]$</p><p>嵌入矩阵为V</p><p>语言模型看成是一个 $y=f_{\theta}(X), X=VZ$ , y是模型输出 $\theta$是可学习参数。</p><p>定义对抗扰动为 $\delta$ </p><p>新的预测输出变为 $y’=f_{\theta}(X+\delta)$</p><p>为了保持语义，我们将δ的范数限制为较小，并假设模型的预测在扰动后不会改变。</p><p>上面的定义和其他人的研究基本都是相同的，FreeLB区别在于不要求X归一化。</p><p>FreeLB吸取了FreeAT和YOPO加速方法, 几乎不需要任何开销就可以获得参数的梯度。实现了与标准PGD训练模型相当的健壮性和泛化能力，只使用与自然训练相同或略多的正反向传播。</p><h3 id="FreeAT-Free-Adversarial-Training-NIPS2019"><a href="#FreeAT-Free-Adversarial-Training-NIPS2019" class="headerlink" title="FreeAT (Free Adversarial Training): NIPS2019"></a>FreeAT (Free Adversarial Training): NIPS2019</h3><p>从FGSM到PGD，主要是优化对抗扰动的计算，虽然取得了更好的效果，但计算量也一步步增加。对于每个样本，FGSM和FGM都只用计算两次，一次是计算x的前后向，一次是计算x+r的前后向。而PGD则计算了K+1次，消耗了更多的计算资源。因此FreeAT被提了出来，在PGD的基础上进行训练速度的优化。</p><p>FreeAT的思想是在对每个样本x连续重复m次训练，计算r时复用上一步的梯度，为了保证速度，整体epoch会除以m。r的更新公式为：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  r_{t+1} = r_t + \epsilon \cdot sign(g)    \end{split}\end{equation}</script><p>伪代码：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">初始化r=0</span><br><span class="line">对于epoch=1...N/m:</span><br><span class="line">  对于每个x:</span><br><span class="line">    对于每步m:</span><br><span class="line">      1.利用上一步的r，计算x+r的前后向，得到梯度</span><br><span class="line">      2.根据梯度更新参数</span><br><span class="line">      3.根据梯度更新r</span><br></pre></td></tr></table></figure><p>缺点：FreeLB指出，FreeAT的问题在于每次的r对于当前的参数都是次优的（无法最大化loss），因为当前r是由r(t-1)和theta(t-1)计算出来的，是对于theta(t-1)的最优。</p><p>代码：<a href="https://link.zhihu.com/?target=https%3A//github.com/mahyarnajibi/FreeAdversarialTraining/blob/d70774030871fa3207e09ce8528c1b84cd690603/main_free.py%23L160">https://github.com/mahyarnajibi…</a></p><h3 id="YOPO-You-Only-Propagate-Once-NIPS2019"><a href="#YOPO-You-Only-Propagate-Once-NIPS2019" class="headerlink" title="YOPO (You Only Propagate Once): NIPS2019"></a>YOPO (You Only Propagate Once): NIPS2019</h3><p>代码：<a href="https://link.zhihu.com/?target=https%3A//github.com/a1600012888/YOPO-You-Only-Propagate-Once">https://github.com/a1600012888/YOPO-You-Only-Propagate-Once</a></p><p>可以参考<a href="https://zhuanlan.zhihu.com/p/95904001">加速对抗训练——YOPO算法浅析</a></p><p>极大值原理PMP(Pontryagin’s maximum principle)是optimizer的一种，它将神经网络看作动力学系统。这个方法的优点是在优化网络参数时，层之间是解藕的。通过这个思想，我们可以想到，既然扰动是加在embedding层的，为什么每次还要计算完整的前后向传播呢？</p><p>基于这个想法，作者想复用后几层的梯度，假设p为定值：</p><p><img src="https://www.zhihu.com/equation?tex=p+%3D+%5Cnabla_%7Bg_%7B%5Ctilde%5Ctheta%7D%7D%28l%28g_%7B%5Ctilde%5Ctheta%7D%28f_0%28x_i%2Br_i%5E%7Bj%2C0%7D%2C+%5Ctheta_0%29%29%2Cy_i%29%29%5Ccdot%5Cnabla_%7Bf_0%7D%28g_%7B%5Ctilde%5Ctheta%7D%28f_0%28x_i%2Br_i%5E%7Bj%2C0%7D%2C+%5Ctheta_0%29%29%29+%5C%5C" alt="[公式]"></p><p>则对r的更新就可以变为</p><p><img src="https://www.zhihu.com/equation?tex=r_i%5E%7Bj%2Cs%2B1%7D+%3D+r_i%5E%7Bj%2Cs%7D%2B%5Calpha_1p%5Ccdot%5Cnabla_%7Br_i%7Df_0%28x_i%2Br_i%5E%7Bj%2Cs%7D%2C%5Ctheta_0%29+%5C%5C" alt="[公式]"></p><p>我们可以先写出YOPO的梯度下降版本：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">对于每个样本x</span><br><span class="line">初始化r(1,0)</span><br><span class="line">对于j=1,2,...,m:</span><br><span class="line">  1.根据r(j,0),计算p</span><br><span class="line">  对于s=0,1,...,n-1:</span><br><span class="line">    2.计算r(j,s+1)</span><br><span class="line">  3.另r(j+1,0)=r(j,n)</span><br></pre></td></tr></table></figure><p>作者又提出了PMP版本的YOPO，并证明SGD的YOPO是PMP版的一种特殊形式。这样每次迭代r就只用到embedding的梯度就可以了。</p><p>YOPO还主张在每次反向传播后，应将第一隐层的梯度作为常数，利用该常数与网络第一层的雅可比的乘积对对手进行多次额外更新，以获得强对手。</p><h3 id="回到FreeLB"><a href="#回到FreeLB" class="headerlink" title="回到FreeLB"></a>回到FreeLB</h3><p>与FreeAT不同的是，YOPO从每个上升步长开始累加参数的梯度，并且只在K个内上升步长之后更新一次参数。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}   min_{\theta}\mathbb{E}_{(Z,y)∼ D ,{m∼M }} [\frac {1}{K}\sum_{t=0}^{K-1} max_{\delta_t\in \Omega_t    }L(f_{\theta}(x+\delta_t),y)]  \end{split}\end{equation}</script><p>对比 PGD:</p><script type="math/tex; mode=display">\begin{equation}\begin{split}   min_{\theta}\mathbb{E}_{(x,y)∼ D} [max_{\Delta x\in \Omega    }L(x+\Delta x,y;\Theta)]     \end{split}\end{equation}</script><p>FreeLB和PGD主要有两点区别：</p><ol><li>PGD是迭代K次r后取最后一次扰动的梯度更新参数，FreeLB是取K次迭代中的平均梯度</li><li>PGD的扰动范围都在epsilon内，因为伪代码第3步将梯度归0了，每次投影都会回到以第1步x为圆心，半径是epsilon的圆内，而FreeLB每次的x都会迭代，所以r的范围更加灵活，更可能接近局部最优：</li></ol><p><img src="https://i.loli.net/2021/04/10/ZMxvfdq4FXRn69S.jpg" alt=""></p><p>它执行多次PGD迭代来构造对抗性实例，并在每次迭代中同时累积“free”参数梯度∇θL。</p><p>伪代码：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">对于每个x:</span><br><span class="line">  1.通过均匀分布初始化r，梯度g为0</span><br><span class="line">  对于每步t=1...K:</span><br><span class="line">    2.根据x+r计算前后向，累计梯度g</span><br><span class="line">    3.更新r</span><br><span class="line">  4.根据g/K更新梯度</span><br></pre></td></tr></table></figure><p>论文中还指出了很重要的一点，就是<strong>对抗训练和dropout不能同时使用</strong>，加上dropout相当于改变了网络结构，会影响r的计算。如果要用的话需要在<strong>K步中都使用同一个mask</strong>。</p><p><img src="https://i.loli.net/2021/04/10/WTh7O4Ui5YenzNM.png" alt=""></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zhuanlan.zhihu.com/p/103593948">一文搞懂NLP中的对抗训练FGSM/FGM/PGD/FreeAT/YOPO/FreeLB/SMART</a></p><p><a href="https://blog.csdn.net/weixin_41712499/article/details/110878322">对抗训练的理解，以及FGM、PGD和FreeLB的详细介绍</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;FreeLB-Enhanced-Adversarial-Training-For-Natural-Language-Understanding&quot;&gt;&lt;a href=&quot;#FreeLB-Enhanced-Adversarial-Training-For-Natural-</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>图神经网络的对抗攻击</title>
    <link href="http://example.com/2021/04/08/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/"/>
    <id>http://example.com/2021/04/08/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/</id>
    <published>2021-04-08T14:01:43.000Z</published>
    <updated>2021-04-10T06:08:47.884Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图神经网络的对抗攻击"><a href="#图神经网络的对抗攻击" class="headerlink" title="图神经网络的对抗攻击"></a>图神经网络的对抗攻击</h1><p>最近要汇报一个关于安全方面的研究。本来打算讲一些和安全擦边的关于nlp对抗训练提升模型鲁棒性的内容，正好和最近学习的阅读理解比赛相关，可以作为一个提分trick。</p><p>但老师强调要和安全相关少讲过程。而nlp中的对抗样本不可以加在原始样本中，只能在embedding中加入扰动，这样就没法攻击，多数用来提升模型鲁棒性。所以就拍马研究了一下图网络的对抗攻击。</p><p>刚开始了解，希望可以从中找出可以和我研究方向结合的地方。</p><p>如有不对的地方还希望联系我指点一下。</p><p><a href="https://coding-zuo.github.io/2021/04/07/nlp%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83-%E4%B8%8Ebert%E7%BB%93%E5%90%88/">nlp中的对抗训练&amp;与bert结合</a></p><p>在上一篇文章中主要介绍的是对抗训练，其实是一种防御的策略，对提高模型而言FGM相当于加了一个正则项。 </p><h2 id="图网络攻击难点"><a href="#图网络攻击难点" class="headerlink" title="图网络攻击难点"></a>图网络攻击难点</h2><ul><li><p>离散的结构/特征，难以直接利用现有的基于梯度的方法。</p></li><li><p>对于“无法感知”的扰动如何定义。</p></li><li><p>节点分类往往属于直推式学习，训练数据和测试数据联合使用以学习模型，这就使得攻击方法注定是与poisoning/causative attack相关，而非仅是evasion attack。</p></li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zhuanlan.zhihu.com/p/88934914">图对抗攻击 Graph Adversarial Attack</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;图神经网络的对抗攻击&quot;&gt;&lt;a href=&quot;#图神经网络的对抗攻击&quot; class=&quot;headerlink&quot; title=&quot;图神经网络的对抗攻击&quot;&gt;&lt;/a&gt;图神经网络的对抗攻击&lt;/h1&gt;&lt;p&gt;最近要汇报一个关于安全方面的研究。本来打算讲一些和安全擦边的关于nlp对抗训</summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>nlp中的对抗训练&amp;与bert结合</title>
    <link href="http://example.com/2021/04/07/nlp%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83-%E4%B8%8Ebert%E7%BB%93%E5%90%88/"/>
    <id>http://example.com/2021/04/07/nlp%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83-%E4%B8%8Ebert%E7%BB%93%E5%90%88/</id>
    <published>2021-04-07T09:45:46.000Z</published>
    <updated>2021-04-10T08:13:12.154Z</updated>
    
    <content type="html"><![CDATA[<h1 id="nlp中的对抗训练学习"><a href="#nlp中的对抗训练学习" class="headerlink" title="nlp中的对抗训练学习"></a>nlp中的对抗训练学习</h1><p>PPT : <a href="https://coding-zuo.github.io/adversary/index.html">https://coding-zuo.github.io/adversary/index.html</a></p><p>由于深度神经网络强大的表示学习能力，在许多领域都取得了很大的成功，包括计算机视觉、自然语言处理、语音识别等。然而，在其卓越性能的背后，深度神经网络作为一个黑箱，缺乏可解释性与鲁棒性，使得它易受到对抗攻击而对抗性攻击的存在可能是深度学习模型的一个固有弱点。</p><p>深度学习的对抗一般有两种含义：</p><ul><li>一是生成对抗网络(Generative Adversarial Network,GAN) 代表一大类先进的生成模型。(这方面我不是很了解)</li><li>另一个则是跟对抗攻击、对抗样本相关的领域。(主要关心模型在小扰动下的稳健性)</li></ul><h2 id="方法介绍"><a href="#方法介绍" class="headerlink" title="方法介绍"></a>方法介绍</h2><p>在CV领域，我们需要通过对模型的对抗攻击和防御来增强模型的稳健型，比如在自动驾驶系统中，要防止模型因为一些随机噪声就将红灯识别为绿灯。</p><p>在NLP领域，类似的对抗训练也是存在的，不过NLP中的对抗训练更多是作为一种正则化手段来提高模型的泛化能力！</p><p>这使得对抗训练成为了NLP刷榜的“神器”之一，前有微软通过RoBERTa+对抗训练在<a href="https://gluebenchmark.com/leaderboard">GLUE</a>上超过了原生RoBERTa。</p><h2 id="对抗样本"><a href="#对抗样本" class="headerlink" title="对抗样本"></a>对抗样本</h2><p>要认识对抗训练，首先要了解“对抗样本”，它首先出现在论文<a href="http://https//arxiv.org/abs/1312.6199">《Intriguing properties of neural networks》</a>之中。简单来说，它是指对于人类来说“看起来”几乎一样、但对于模型来说预测结果却完全不一样的样本，比如下面的经典例子：</p><p><img src="https://i.loli.net/2021/04/07/EizJ85fCHyX2drj.png" alt=""></p><p>“对抗攻击”，其实就是想办法造出更多的对抗样本。</p><p>“对抗防御”，就是想办法让模型能正确识别更多的对抗样本。</p><p>所谓对抗训练，则是属于对抗防御的一种，它构造了一些对抗样本加入到原数据集中，希望增强模型对对抗样本的鲁棒性；同时，如本文开篇所提到的，在NLP中它通常还能提高模型的表现。</p><p>用对抗训练的思路来提升NLP模型，有两个实现角度：</p><ol><li><p>因为nlp的输入通常是one-hot向量，两个one-hot向量其欧式距离恒为$\sqrt 2$ ，理论上不存在微小的扰动，不想cv图像那样可以对连续实数向量来做。比如，$\Delta x$ 是实数向量，$x+\Delta x$还是一个有意义的图。所以很多研究都是在embedding层上做扰动的，因为embedding层是我们自己训练的，所以不太可能出现认为的恶意对抗攻击。</p><p><img src="https://i.loli.net/2021/04/09/daTOFDIU3EtfyGs.png" alt=""></p></li><li><p>这种角度不知道还算不算对抗，但可以说是一种数据增强手段。如上图中下面的问题，经过缩写，添加标点，或者同义词近义词替换等等。通过辅助模型提升鲁棒性。</p></li></ol><h2 id="Min-Max"><a href="#Min-Max" class="headerlink" title="Min-Max"></a>Min-Max</h2><p>对抗训练可以统一写成如下格式：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  min_{\theta}\mathbb{E}_{(x,y)∼ D} [max_{\Delta x\in \Omega    }L(x+\Delta x,y;\Theta)]     \end{split}\end{equation}</script><p>其中$D$ 代表训练集，x代表输入，y代表标签，θ是可学习模型参数，L(x,y;θ)是单个样本的loss，Δx是对抗扰动，Ω是扰动空间。</p><p>理解为：</p><ol><li><p>$max_{\Delta x\in \Omega}L(x+\Delta x,y;\theta)$ ，往输入x中注入扰动$\Delta x$， 目的是希望 $ L(x+\Delta x,y;\theta)$ 损失越大越好，也就是让现有模型的预测出错;</p></li><li><p>当然$\Delta x$ 不能太大、无约束，否则达不到“看起来几乎一样”的效果，所以$Δx$要满足一定的约束，常规的约束是$‖Δx‖≤ϵ$，其中$ϵ$是一个常数；</p></li><li><p>构造好对抗样本后，用$x+\Delta x,y$作为数据去最小化loss，来更新参数$\theta$ (梯度下降)</p></li><li>重复执行1.2.3步。</li></ol><p>整个对抗训练优化过程是一个max和min交替执行的过程：通过注入max损失，在梯度下降让损失变min。</p><h2 id="如何计算-Delta-x-——快速梯度FGM"><a href="#如何计算-Delta-x-——快速梯度FGM" class="headerlink" title="如何计算$\Delta x$——快速梯度FGM"></a>如何计算$\Delta x$——快速梯度FGM</h2><p>$\Delta x$的目的是增大Loss，而我们知道让loss减少的方法是梯度下降，那反过来，让loss增大的方法自然就是梯度上升，因此可以简单地取</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \Delta x &= ϵ∇_xL(x,y;θ)\\ ∇_xL(x,y;θ) &= (\frac {\partial L }{\partial x})    \end{split}\end{equation}</script><p>求loss对x的梯度，然后根据梯度给Δx赋值，来实现对输入的干扰，完成干扰之后再执行常规的梯度下降。</p><p>为了防止$\Delta x$过大，通常要对 $∇xL(x,y;θ)$ 标准化，常见方式为：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}      \Delta x = ϵ \frac {∇_xL(x,y;θ)}{||∇_xL(x,y;θ)||} \text{或} \Delta x=  ϵsign(∇_xL(x,y;θ))    \end{split}\end{equation}</script><p>采用右边的取扰动值的算法叫FGSM(ICLR2015)，理解为扰动是沿着梯度方向向损失值的极大值走。</p><p>采用左边取扰动值的算法叫FGM(ICLR2017)，理解为在每个方向上都走相同的一步找到更好的对抗样本。</p><p>有了$\Delta x$，得到：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  min_{\theta}\mathbb{E}_{(x,y)∼ D} [L(x+\Delta x,y;\Theta)]     \end{split}\end{equation}</script><p>这就构成了一种对抗训练方法，被称为<strong>Fast Gradient Method（FGM）</strong>，它由GAN之父Goodfellow在论文<a href="https://arxiv.org/abs/1412.6572">《Explaining and Harnessing Adversarial Examples》</a>首先提出。</p><p>此外，对抗训练还有一种方法，叫做<strong>Projected Gradient Descent（PGD）</strong>，其实就是通过多迭代几步来达到让$L(x+Δx,y;θ)$更大的$Δx$（如果迭代过程中模长超过了$ϵ$，<a href="https://arxiv.org/abs/1706.06083">《Towards Deep Learning Models Resistant to Adversarial Attacks》</a>。在后文….</p><h3 id="梯度惩罚"><a href="#梯度惩罚" class="headerlink" title="梯度惩罚"></a>梯度惩罚</h3><p>假设已经得到对抗扰动 $\Delta x$ ,更新 $\theta$ 时，对 L 进行泰勒展开：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  min_{\theta} \mathbb{E}_{(x,y)∼ D} [L(x+\Delta x,y;\theta)] &\approx min_{\theta} \mathbb{E}_{(x,y)∼ D}[L(x,y;\theta) + <∇_xL(x,y;θ), \Delta x>] \\ &= min_{\theta} \mathbb{E}_{(x,y)∼ D}[L(x,y;\theta) + ∇_xL(x,y;θ) \cdot \Delta  x ] \\ &= min_{\theta} \mathbb{E}_{(x,y)∼ D}[L(x,y;\theta) + ∇_xL(x,y;θ)^T  \Delta  x ]     \end{split}\end{equation}</script><p>对应的 $\theta$ 的梯度为:</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  ∇_{\theta} L(x,y;\theta) + ∇_{\theta} ∇_xL(x,y;θ)^T  \Delta  x     \end{split}\end{equation}</script><p>代入 $ \Delta x = ϵ∇_xL(x,y;θ)$:</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  &∇_{\theta} L(x,y;\theta) + ϵ ∇_{\theta} ∇_xL(x,y;θ)^T  ∇_xL(x,y;θ)\\ &= ∇_{\theta}(L(x,y;θ) + \frac{1}{2} ϵ ||∇_xL(x,y;θ)||^2)    \end{split}\end{equation}</script><p>这个结果表示，对输入样本施加 $ϵ∇xL(x,y;θ)$ 的对抗扰动，一定程度上等价于往loss里边加入“梯度惩罚”</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \frac{1}{2} ϵ ||∇_xL(x,y;θ)||^2    \end{split}\end{equation}</script><p>如果对抗扰动是$ϵ‖∇xL(x,y;θ)‖$ ，那么对应的梯度惩罚项则是$ϵ‖∇xL(x,y;θ)‖$（少了个1/2，也少了个2次方）。</p><h3 id="几何图像"><a href="#几何图像" class="headerlink" title="几何图像"></a>几何图像</h3><p>事实上，关于梯度惩罚，我们有一个非常直观的几何图像。以常规的分类问题为例，假设有n个类别，那么模型相当于挖了n个坑，然后让同类的样本放到同一个坑里边去：</p><p><img src="https://i.loli.net/2021/04/09/xeOucXmabjkrS4A.png" alt=""></p><p>梯度惩罚则说“同类样本不仅要放在同一个坑内，还要放在坑底”，这就要求每个坑的内部要长这样：</p><p><img src="https://i.loli.net/2021/04/09/tHylhowkCpvP2IM.png" alt=""></p><p>为什么要在坑底呢？因为物理学告诉我们，坑底最稳定呀，所以就越不容易受干扰呀，这不就是对抗训练的目的么？</p><p>那坑底意味着什么呢？极小值点呀，导数（梯度）为零呀，所以不就是希望‖∇xL(x,y;θ)‖‖∇xL(x,y;θ)‖越小越好么？这便是梯度惩罚的几何意义了。</p><p><img src="https://kexue.fm/usr/uploads/2020/03/3963498733.gif" alt=""></p><p>苏神代码基于keras的：</p><blockquote><p><a href="https://github.com/bojone/keras_adversarial_training">https://github.com/bojone/keras_adversarial_training</a></p></blockquote><h2 id="Projected-Gradient-Descent-PGD"><a href="#Projected-Gradient-Descent-PGD" class="headerlink" title="Projected Gradient Descent (PGD)"></a>Projected Gradient Descent (PGD)</h2><p>内部max的过程，本质上是一个非凹的约束优化问题，FGM解决的思路其实就是梯度上升，<strong>那么FGM简单粗暴的“一步到位”，是不是有可能并不能走到约束内的最优点呢？</strong>当然是有可能的。于是，一个很intuitive的改进诞生了：Madry在18年的ICLR中，提出了用Projected Gradient Descent（PGD）的方法，简单的说，就是<strong>“小步走，多走几步”</strong>，如果走出了扰动半径为$\epsilon$的空间，就映射回“球面”上，以保证扰动不要过大：</p><p>其中$\mathcal{S}={r\in\mathbb{R}^d:||r||_2 \leq \epsilon}$ 为扰动的约束空间，$\alpha$为小步的步长。</p><p>作者将这一类通过一阶梯度得到的对抗样本称之为“一阶对抗”，在实验中，作者发现，经过PGD训练过的模型，对于所有的一阶对抗都能得到一个低且集中的损失值，如下图所示：</p><p><img src="https://i.loli.net/2021/04/09/SosrVAW9UGYNm6T.png" alt=""></p><p>我们可以看到，面对约束空间 $\mathcal{S}$ 内随机采样的十万个扰动，PGD模型能够得到一个<strong>非常低且集中的loss分布</strong>，因此，在论文中，作者称PGD为<strong>“一阶最强对抗”</strong>。也就是说，只要能搞定PGD对抗，别的一阶对抗就不在话下了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对于每个x:</span><br><span class="line">  1.计算x的前向loss、反向传播得到梯度并备份</span><br><span class="line">  对于每步t:</span><br><span class="line">      2.根据embedding矩阵的梯度计算出r，并加到当前embedding上，相当于x+r(超出范围则投影回epsilon内)</span><br><span class="line">      3.t不是最后一步: 将梯度归0，根据1的x+r计算前后向并得到梯度</span><br><span class="line">      4.t是最后一步: 恢复(1)的梯度，计算最后的x+r并将梯度累加到(1)上</span><br><span class="line">  5.将embedding恢复为(1)时的值</span><br><span class="line">  6.根据(4)的梯度对参数进行更新</span><br></pre></td></tr></table></figure><p>基于PGD的对抗性训练被广泛认为是最有效的，因为它在很大程度上避免了模糊的梯度问题。它将一类对抗性训练算法转化为求解交叉熵损失的极大极小问题，该问题可以通过多次投影梯度上升步骤和随后的SGD步骤可靠地实现。</p><h2 id="Virtual-Adversarial-Training"><a href="#Virtual-Adversarial-Training" class="headerlink" title="Virtual Adversarial Training"></a>Virtual Adversarial Training</h2><p>除了监督训练，对抗训练还可以用在半监督任务中，尤其对于NLP任务来说，很多时候输入的无监督文本多的很，但是很难大规模地进行标注，那么就可以参考[13]中提到的Virtual Adversarial Training进行半监督训练。</p><p>首先，我们抽取一个随机标准正态扰动（$d\sim \mathcal{N}(0, I)\in \mathbb{R}^d$），加到embedding上，并用KL散度计算梯度：</p><p>然后，用得到的梯度，计算对抗扰动，并进行对抗训练：</p><p><img src="https://i.loli.net/2021/04/09/KYX3zILWf4A1Htg.png" alt=""></p><p>实现方法跟FGM差不多</p><h2 id="FreeAT-amp-YOPO-amp-FreeLB"><a href="#FreeAT-amp-YOPO-amp-FreeLB" class="headerlink" title="FreeAT &amp; YOPO &amp; FreeLB"></a>FreeAT &amp; YOPO &amp; FreeLB</h2><p><strong>优化的主要方向有两点：得到更优的扰动 &amp; 提升训练速度</strong></p><p>其实PGD效果不错但是它迭代多步计算开销很大，所以出现了这些针对效率上的优化，并且结合预训练语言模型。</p><p>具体的就搜这些论文来看吧。</p><p>FGSM: Explaining and Harnessing Adversarial Examples</p><p>FGM: Adversarial Training Methods for Semi-Supervised Text Classification</p><p>FreeAT: Adversarial Training for Free!</p><p>YOPO: You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle</p><p>FreeLB: Enhanced Adversarial Training for Language Understanding</p><p>SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[<a href="https://kexue.fm/archives/7234">对抗训练浅谈：意义、方法和思考（附Keras实现）</a>]</p><p><a href="https://fyubang.com/2019/10/15/adversarial-train/">【炼丹技巧】功守道：NLP中的对抗训练 + PyTorch实现</a></p><p><a href="https://blog.csdn.net/chencas/article/details/103551852">NLP —- &gt;对抗学习：从FGM, PGD到FreeLB</a></p><p><a href="https://arxiv.org/pdf/2004.14543v3.pdf">TAVAT: Token-Aware Virtual Adversarial Training for Language Understanding</a></p><p><a href="https://arxiv.org/abs/1605.07725">Adversarial Training Methods for Semi-Supervised Text Classification</a><br><a href="https://github.com/tensorflow/models/blob/e97e22dfcde0805379ffa25526a53835f887a860/research/adversarial_text/adversarial_losses.py">Adversarial Text Classification原作实现</a></p><p><a href="https://blog.csdn.net/ganxiwu9686/article/details/105931668">NLP(文本)中的对抗训练</a></p><p><a href="https://blog.csdn.net/weixin_41712499/article/details/110878322">对抗训练的理解，以及FGM、PGD和FreeLB的详细介绍</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;nlp中的对抗训练学习&quot;&gt;&lt;a href=&quot;#nlp中的对抗训练学习&quot; class=&quot;headerlink&quot; title=&quot;nlp中的对抗训练学习&quot;&gt;&lt;/a&gt;nlp中的对抗训练学习&lt;/h1&gt;&lt;p&gt;PPT : &lt;a href=&quot;https://coding-zuo.</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>数组范围内计数</title>
    <link href="http://example.com/2021/04/07/%E6%95%B0%E7%BB%84%E8%8C%83%E5%9B%B4%E5%86%85%E8%AE%A1%E6%95%B0/"/>
    <id>http://example.com/2021/04/07/%E6%95%B0%E7%BB%84%E8%8C%83%E5%9B%B4%E5%86%85%E8%AE%A1%E6%95%B0/</id>
    <published>2021-04-07T02:07:32.000Z</published>
    <updated>2021-04-08T14:13:18.358Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数组范围内计数"><a href="#数组范围内计数" class="headerlink" title="数组范围内计数"></a>数组范围内计数</h1><p>数组为 3,2,2,3,1, 查询为（0,3,2)。</p><p>意思是在数组里下标 0-3 这个范围上，有几个 2? </p><p>假设给一个数组 arr，对这个数组的查询非常频繁请返回所有查询的结果。</p><p>给出：arr[    ]</p><p>要查多个范围：</p><p>[[0,3,2],</p><p>[1,4,0]]</p><p>结果返回：[第一个数组结果，第二个数组结果]</p><p>暴力解法直接遍历肯定不可以。</p><h2 id="解法"><a href="#解法" class="headerlink" title="解法"></a>解法</h2><p>一、做一个map映射，遍历一遍数组，将每个值和每个值出现的下标位置数组做成key-value</p><p>在根据查询的V，在所在值的数组内做二分查找。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">QueryBox2</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> HashMap&lt;Integer, ArrayList&lt;Integer&gt;&gt; map;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">QueryBox2</span><span class="params">(<span class="keyword">int</span>[] arr)</span> </span>&#123;</span><br><span class="line">            map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; arr.length; i++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (!map.containsKey(arr[i])) &#123;</span><br><span class="line">                    map.put(arr[i], <span class="keyword">new</span> ArrayList&lt;&gt;());</span><br><span class="line">                &#125;</span><br><span class="line">                map.get(arr[i]).add(i);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">query</span><span class="params">(<span class="keyword">int</span> L, <span class="keyword">int</span> R, <span class="keyword">int</span> value)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (!map.containsKey(value)) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            ArrayList&lt;Integer&gt; indexArr = map.get(value);</span><br><span class="line">            <span class="comment">// 查询&lt;L的下标有几个</span></span><br><span class="line">            <span class="keyword">int</span> a = countLess(indexArr, L);</span><br><span class="line">            <span class="comment">// 查询&lt;R+1的下标有几个</span></span><br><span class="line">            <span class="keyword">int</span> b = countLess(indexArr, R + <span class="number">1</span>);</span><br><span class="line">            <span class="keyword">return</span> b - a;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 在有序数组中，用二分法数出&lt;limit 的数有几个</span></span><br><span class="line">        <span class="comment">// 也就是用二分法，找到&lt;limit的数中最右的位置</span></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">countLess</span><span class="params">(ArrayList&lt;Integer&gt; arr, <span class="keyword">int</span> limit)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> L = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> R = arr.size() - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">int</span> mostRight = -<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">while</span> (L &lt;= R) &#123;</span><br><span class="line">                <span class="keyword">int</span> mid = L + ((R - L) &gt;&gt; <span class="number">1</span>);</span><br><span class="line">                <span class="keyword">if</span> (arr.get(mid) &lt; limit) &#123;</span><br><span class="line">                    mostRight = mid;</span><br><span class="line">                    L = mid + <span class="number">1</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    R = mid - <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> mostRight + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>$O(mlogn)$</p><p>扩展：</p><p>如果是求范围内累加和</p><p>可以生成一个前缀和数组类似本题</p><h2 id="腾讯原题"><a href="#腾讯原题" class="headerlink" title="腾讯原题"></a>腾讯原题</h2><p>给定整数 power。给定一个数组 arr。给定一个数组 reverse。含义如下</p><p>arr 的长度一定是 2 的 power 次方，reverse1 每个值一定都在 0 ~ power范围。</p><p>例如 power=2, ar={3,1,4,2}, reverse={0,1,0,2}</p><p>针对reverse的数组中每一个值 </p><p>如第一个值为0，就是对$2^0=1$ 每一个arr数组以1个数为单位逆序。</p><p>如第二个值为1，就是对$2^1=2$ 对arr数组每两个数逆序</p><p>任何一个在前的数字可以和任何一个在后的数组，构成一对数。可能是升序关系、相等关系或者降序关系。</p><p>最后求调整完的arr数组有多少个降序对。</p><p>比如 arr 开始时有如下的降序对：(3,1)、（3.2)、（4.2），一共 3 个。</p><p>以2个数调整后arr由{3,1,4,2} 变成{1,3,2,4} 降序对有{3,2} ，共1个</p><p>经典做法每次都reverse</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// originArr长度一定是2的power次方</span></span><br><span class="line">    <span class="comment">// reverseArr中每一个值，都是0-power范围上的数</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] reversePair1(<span class="keyword">int</span>[] originArr, <span class="keyword">int</span>[] reverseArr, <span class="keyword">int</span> power) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] ans = <span class="keyword">new</span> <span class="keyword">int</span>[reverseArr.length];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; reverseArr.length; i++) &#123;</span><br><span class="line">            <span class="comment">// 1 &lt;&lt; (reverseArr[i]) == r[i]的2次方</span></span><br><span class="line">            reverseArray(originArr, <span class="number">1</span> &lt;&lt; (reverseArr[i]));</span><br><span class="line">            ans[i] = countReversePair(originArr);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">reverseArray</span><span class="params">(<span class="keyword">int</span>[] originArr, <span class="keyword">int</span> teamSize)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (teamSize &lt; <span class="number">2</span>) <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; originArr.length; i += teamSize) &#123;</span><br><span class="line">            reversePart(originArr, i, i + teamSize - <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">reversePart</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> L, <span class="keyword">int</span> R)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (L &lt; R) &#123;</span><br><span class="line">            <span class="keyword">int</span> temp = arr[L];</span><br><span class="line">            arr[L++] = arr[R];</span><br><span class="line">            arr[R--] = temp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">countReversePair</span><span class="params">(<span class="keyword">int</span>[] originArr)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; originArr.length; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; originArr.length; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (originArr[i] &gt; originArr[j]) &#123;</span><br><span class="line">                    ans++;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>优化方案</p><p>[3,2 4,5, 0,1 3,5]</p><p>两个数一组：  1个逆序对，3个升序对</p><p>四个数一组： 0个逆序对，8个升序对</p><p>八个数一组：10个逆序对，4个升序对</p><p>每个数组的逆序对数是2、4、9个一组的加和</p><p>当数组进行几个数一组翻转时</p><p>翻转后的逆序对和升序对数量，和翻转前的升序对和逆序对相等，数量调换了。</p><p>小的数调整后数量不影响大数量调整后的数量。</p><p>所以直接查2、4、8、16个 数量再交换相加。</p><p>如何高效生成预处理记录</p><p>输入数据状况</p><p>power范围[0,20]</p><p>arr长度范围[1,10e7]</p><p>reverse长度范围[1,10e6]</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] reversePair2(<span class="keyword">int</span>[] originArr, <span class="keyword">int</span>[] reverseArr, <span class="keyword">int</span> power) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] originReverse = Arrays.copyOf(originArr, originArr.length);</span><br><span class="line">        reversePart(originReverse, <span class="number">0</span>, originReverse.length - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">int</span>[] recordDown = <span class="keyword">new</span> <span class="keyword">int</span>[power + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span>[] recordUp = <span class="keyword">new</span> <span class="keyword">int</span>[power + <span class="number">1</span>];</span><br><span class="line">        process(originArr, <span class="number">0</span>, originArr.length - <span class="number">1</span>, power, recordDown);</span><br><span class="line">        process(originReverse, <span class="number">0</span>, originReverse.length - <span class="number">1</span>, power, recordUp);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// recordDown[i] 2的i次方个数一组的划分中，降序的数量</span></span><br><span class="line">        <span class="comment">// recordUp[i] 2的i次方个数一组的划分中，升序的数量</span></span><br><span class="line">        <span class="keyword">int</span>[] ans = <span class="keyword">new</span> <span class="keyword">int</span>[reverseArr.length];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; reverseArr.length; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> curPower = reverseArr[i]; <span class="comment">// =3  2的1次方、2次方、3次方 要调整</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> p = <span class="number">1</span>; p &lt;= curPower; p++) &#123;</span><br><span class="line">                <span class="keyword">int</span> tmp = recordDown[p];</span><br><span class="line">                recordDown[p] = recordUp[p];</span><br><span class="line">                recordUp[p] = tmp;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> p = <span class="number">1</span>; p &lt;= power; p++) &#123;</span><br><span class="line">                ans[i] += recordDown[p];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(<span class="keyword">int</span>[] originArr, <span class="keyword">int</span> L, <span class="keyword">int</span> R, <span class="keyword">int</span> power, <span class="keyword">int</span>[] record)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (L == R) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> mid = L + ((R - L) &gt;&gt; <span class="number">1</span>);</span><br><span class="line">        process(originArr, L, mid, power - <span class="number">1</span>, record);</span><br><span class="line">        process(originArr, mid + <span class="number">1</span>, R, power - <span class="number">1</span>, record);</span><br><span class="line">        record[power] += merge(originArr, L, mid, R);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">merge</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> L, <span class="keyword">int</span> m, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] help = <span class="keyword">new</span> <span class="keyword">int</span>[r - L + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> p1 = L;</span><br><span class="line">        <span class="keyword">int</span> p2 = m + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (p1 &lt;= m &amp;&amp; p2 &lt;= r) &#123;</span><br><span class="line">            ans += arr[p1] &lt;= arr[p2] ? arr[p1++] : arr[p2++];</span><br><span class="line">            help[i++] = arr[p1] &lt;= arr[p2] ? arr[p1++] : arr[p2++];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (p1 &lt;= m) &#123;</span><br><span class="line">            help[i++] = arr[p1++];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (p2 &lt;= r) &#123;</span><br><span class="line">            help[i++] = arr[p2++];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; help.length; i++) &#123;</span><br><span class="line">            arr[L + i] = help[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;数组范围内计数&quot;&gt;&lt;a href=&quot;#数组范围内计数&quot; class=&quot;headerlink&quot; title=&quot;数组范围内计数&quot;&gt;&lt;/a&gt;数组范围内计数&lt;/h1&gt;&lt;p&gt;数组为 3,2,2,3,1, 查询为（0,3,2)。&lt;/p&gt;
&lt;p&gt;意思是在数组里下标 0-3 这个</summary>
      
    
    
    
    
    <category term="刷题" scheme="http://example.com/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>海华中文阅读理解比赛梳理/多卡并行/transformers</title>
    <link href="http://example.com/2021/04/06/%E6%B5%B7%E5%8D%8E%E4%B8%AD%E6%96%87%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%AF%94%E8%B5%9B%E6%A2%B3%E7%90%86-%E5%A4%9A%E5%8D%A1%E5%B9%B6%E8%A1%8C-transformers/"/>
    <id>http://example.com/2021/04/06/%E6%B5%B7%E5%8D%8E%E4%B8%AD%E6%96%87%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%AF%94%E8%B5%9B%E6%A2%B3%E7%90%86-%E5%A4%9A%E5%8D%A1%E5%B9%B6%E8%A1%8C-transformers/</id>
    <published>2021-04-06T12:28:26.000Z</published>
    <updated>2021-05-01T03:47:02.764Z</updated>
    
    <content type="html"><![CDATA[<h1 id="海华中文阅读理解比赛梳理"><a href="#海华中文阅读理解比赛梳理" class="headerlink" title="海华中文阅读理解比赛梳理"></a>海华中文阅读理解比赛梳理</h1><p>文文言文古诗词现代诗词</p><p>1 字词解释 2 标点符号作用 3 句子解释 4 填空 5 选择正读音 6 推理总结 7 态度情感 8 外部知识</p><p>不需要先验知识的问题</p><p>如一个问题能够在文档中进行匹配，回答起来就几乎不需要先验知识需要先验知识的问題</p><p>1、关于语言的知识：需要词汇/语法知识,例如:习语、谚语、否定、反义词、同义词语法转换</p><p>2、特定领域的知识：需要但不限于些事实上的知识，这些事实与特定领域的概念概念定义和属性，概念之间的关系</p><p>3、一般世界的知识：需要有关世界如何运作的一般知识，或者被称为常识。比如百科全书中的知识</p><p>这个赛题的难点是有些预训练语言模型没有学到的先验知识怎么学</p><h2 id="赛题概述"><a href="#赛题概述" class="headerlink" title="赛题概述"></a>赛题概述</h2><ul><li>train 训练集提供了6313条数据数据格式是和中小学生做的阅读题一样，一篇文章有两到三个问题每个问题有两到四个答案选项。</li><li>validation 验证集提供了1000条数据。</li></ul><p>原始单条数据格式如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;ID&quot;</span>: <span class="string">&quot;0001&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;Content&quot;</span>: <span class="string">&quot;春之怀古张晓风春天必然曾经是这样的：从绿意内敛的山头，一把雪再也撑不住了，噗嗤的一声，将冷面笑成花面，一首澌澌然的歌便从云端唱到山麓，从山麓唱到低低的荒村。。。。。很多省略。&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;Questions&quot;</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;Q_id&quot;</span>: <span class="string">&quot;000101&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;Question&quot;</span>: <span class="string">&quot;鸟又可以开始丈量天空了。”这句话的意思是   （   ）&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;Choices&quot;</span>: [</span><br><span class="line">          <span class="string">&quot;A．鸟又可以飞了。&quot;</span>,</span><br><span class="line">          <span class="string">&quot;B． 鸟又要远飞了。&quot;</span>,</span><br><span class="line">          <span class="string">&quot;C．鸟又可以筑巢了。&quot;</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">&quot;Answer&quot;</span>: <span class="string">&quot;A&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;Q_id&quot;</span>: <span class="string">&quot;000102&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;Question&quot;</span>: <span class="string">&quot;本文写景非常含蓄，请读一读找一找哪些不在作者的笔下有所描述&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;Choices&quot;</span>: [</span><br><span class="line">          <span class="string">&quot;A．冰雪融化&quot;</span>,</span><br><span class="line">          <span class="string">&quot;B． 蝴蝶在花间飞舞&quot;</span>,</span><br><span class="line">          <span class="string">&quot;C．白云在空中飘&quot;</span>,</span><br><span class="line">          <span class="string">&quot;D．小鸟在空中自由地飞&quot;</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">&quot;Answer&quot;</span>: <span class="string">&quot;C&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="EDA-与预处理"><a href="#EDA-与预处理" class="headerlink" title="EDA 与预处理"></a>EDA 与预处理</h2><p>将原始数据每个问题抽出来以 [文章- 问题 -答案] 作为一条数据。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;Question&quot;</span>: <span class="string">&quot;下列对这首诗的理解和赏析，不正确的一项是&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;Choices&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;A．作者写作此诗之时，皮日休正患病居家，闭门谢客，与外界不通音讯。&quot;</span>,</span><br><span class="line">        <span class="string">&quot;B．由于友人患病，原有的约会被暂时搁置，作者游春的诗篇也未能写出。&quot;</span>,</span><br><span class="line">        <span class="string">&quot;C．作者虽然身在书斋从事教学，但心中盼望能走进自然，领略美好春光。&quot;</span>,</span><br><span class="line">        <span class="string">&quot;D．尾联使用了关于沈约的典故，可以由此推测皮日休所患的疾病是目疾。&quot;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;Answer&quot;</span>: <span class="string">&quot;A&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;Q_id&quot;</span>: <span class="string">&quot;000101&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;Content&quot;</span>: <span class="string">&quot;奉和袭美抱疾杜门见寄次韵  陆龟蒙虽失春城醉上期，下帷裁遍未裁诗。因吟郢岸百亩蕙，欲采商崖三秀芝。栖野鹤笼宽使织，施山僧饭别教炊。但医沈约重瞳健，不怕江花不满枝。&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>训练集从6313变为15421条数据，相当于有15421个问题</p><p>验证集从1000变为2444条数据，相当于有2444个问题</p><p>接下来看看文章的长度如何？</p><p><img src="https://i.loli.net/2021/04/06/UXPvh1c6CIRY54J.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">count    15421.000000</span><br><span class="line">mean      1039.781272</span><br><span class="line">std        435.583878</span><br><span class="line">min         38.000000</span><br><span class="line">25%        744.000000</span><br><span class="line">50%       1067.000000</span><br><span class="line">75%       1251.000000</span><br><span class="line">max       3047.000000</span><br><span class="line">Name: content_len, dtype: float64</span><br><span class="line">count    2444.000000</span><br><span class="line">mean      927.508592</span><br><span class="line">std       481.552693</span><br><span class="line">min        40.000000</span><br><span class="line">25%       596.000000</span><br><span class="line">50%       938.000000</span><br><span class="line">75%      1179.500000</span><br><span class="line">max      3047.000000</span><br><span class="line">Name: content_len, dtype: float64</span><br></pre></td></tr></table></figure><p>发现content文章都非常长，绝大多数都超过了512。</p><p>使用预训练模型bert的话，如何训练很长的文章是是个提高的点。</p><p>我的想法是bert模型一个这个提高的点、看看最近比较火的Longformer怎么做，再用几个和长度无关的模型像lstm等最后做集成。</p><p><img src="https://i.loli.net/2021/04/06/gBUFiwHtaK195rq.png" alt=""></p><p>答案中选C的居多，点歌都选C。。。</p><p>在提供的测试集中有一个特别的地方，赛方给出了文章的类型。</p><p>00 现代文 11文言文 22 古诗词 33现代诗词</p><p><img src="https://i.loli.net/2021/04/06/ZoVXJGdhrf7wga5.png" alt=""></p><p>测试集还给了难度，使用想法：</p><p>可以训练一个模型预测文本的难度和类型，标注训练集，可能会有提升。</p><p>接下来将标签从ABCD转成0123</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_df[<span class="string">&#x27;label&#x27;</span>] = train_df[<span class="string">&#x27;Answer&#x27;</span>].apply(<span class="keyword">lambda</span> x:[<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;C&#x27;</span>,<span class="string">&#x27;D&#x27;</span>].index(x)) </span><br><span class="line"></span><br><span class="line">test_df[<span class="string">&#x27;label&#x27;</span>] = <span class="number">0</span></span><br></pre></td></tr></table></figure><h2 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h2><h3 id="分词器"><a href="#分词器" class="headerlink" title="分词器"></a>分词器</h3><p>采用transformers提供的bert分词器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;model&#x27;</span>) <span class="comment">#加载bert的分词器</span></span><br></pre></td></tr></table></figure><p>这里我试过如果要将bert改成roberta，分词器还是要采用BertTokenizer，如果用RobertaTokenizer会报错。</p><p>参考<a href="https://zhuanlan.zhihu.com/p/121787628">关于transformers库中不同模型的Tokenizer</a></p><p><strong>由于中文的特殊性不太适合采用byte级别的编码，所以大部分开源的中文Roberta预训练模型仍然采用的是单字词表，所以直接使用<code>BertTokenizer</code>读取即可，</strong> 不需要使用<code>RobertaTokenizer</code>。</p><h3 id="模型部分"><a href="#模型部分" class="headerlink" title="模型部分"></a>模型部分</h3><p>BertForMultipleChoice <a href="https://huggingface.co/transformers/model_doc/bert.html#bertformultiplechoice">https://huggingface.co/transformers/model_doc/bert.html#bertformultiplechoice</a></p><p>把每个问题和文章的不同选项拆开拼成一个输入。如下图第一行</p><p><img src="https://i.loli.net/2021/04/08/sgbWnyIqdT9hcrE.png" alt=""></p><p>baseline采用transformers提供的调包，封装好的BertForMultipleChoice (多项选择任务)，它的源码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertForMultipleChoice</span>(<span class="params">BertPreTrainedModel</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(config)</span><br><span class="line"></span><br><span class="line">        self.bert = BertModel(config)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line">        self.classifier = nn.Linear(config.hidden_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        input_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        token_type_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        position_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        inputs_embeds=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        labels=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">       </span><br><span class="line">        num_choices = input_ids.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        input_ids = input_ids.view(-<span class="number">1</span>, input_ids.size(-<span class="number">1</span>))</span><br><span class="line">        attention_mask = attention_mask.view(-<span class="number">1</span>, attention_mask.size(-<span class="number">1</span>)) <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        token_type_ids = token_type_ids.view(-<span class="number">1</span>, token_type_ids.size(-<span class="number">1</span>)) <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        position_ids = position_ids.view(-<span class="number">1</span>, position_ids.size(-<span class="number">1</span>)) <span class="keyword">if</span> position_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"><span class="comment"># 将bert三个输入展平 输入到bertmodel</span></span><br><span class="line">        outputs = self.bert(</span><br><span class="line">            input_ids,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">            position_ids=position_ids,</span><br><span class="line">            head_mask=head_mask,</span><br><span class="line">            inputs_embeds=inputs_embeds,</span><br><span class="line">        )</span><br><span class="line"><span class="comment"># 隐层输出</span></span><br><span class="line">        <span class="comment"># last_hidden_state: [32=4*batch, seq_len,768]</span></span><br><span class="line">        <span class="comment"># pooler_ouput: [32=4*batch,768]</span></span><br><span class="line">        pooled_output = outputs[<span class="number">1</span>] <span class="comment"># CLS https://www.cnblogs.com/webbery/p/12167552.html</span></span><br><span class="line">        <span class="comment"># bert_output = outputs[0] # last_hidden</span></span><br><span class="line"></span><br><span class="line">        pooled_output = self.dropout(pooled_output)</span><br><span class="line">        logits = self.classifier(pooled_output)</span><br><span class="line">        reshaped_logits = logits.view(-<span class="number">1</span>, num_choices)</span><br><span class="line"></span><br><span class="line">        outputs = (reshaped_logits,) + outputs[<span class="number">2</span>:]  <span class="comment"># add hidden states and attention if they are here</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss_fct = CrossEntropyLoss()</span><br><span class="line">            loss = loss_fct(reshaped_logits, labels)</span><br><span class="line">            outputs = (loss,) + outputs</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs  <span class="comment"># (loss), reshaped_logits, (hidden_states), (attentions)</span></span><br></pre></td></tr></table></figure><p>做bert方面的模型扩展可以参考上面，其实就是BertModel加上了线性层。</p><h3 id="制造模型输入数据"><a href="#制造模型输入数据" class="headerlink" title="制造模型输入数据"></a>制造模型输入数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dataframe</span>):</span></span><br><span class="line">        self.df = dataframe</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.df)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span> </span><br><span class="line">      <span class="comment">#将一条数据从(文章,问题,4个选项)转成(文章,问题,选项1)、(文章,问题,选项2)...</span></span><br><span class="line">        label = self.df.label.values[idx]</span><br><span class="line">        question = self.df.Question.values[idx]</span><br><span class="line">        content = self.df.Content.values[idx]</span><br><span class="line">        choice = self.df.Choices.values[idx][<span class="number">2</span>:-<span class="number">2</span>].split(<span class="string">&#x27;\&#x27;, \&#x27;&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(choice) &lt; <span class="number">4</span>: <span class="comment">#如果选项不满四个，就补“不知道”</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>-<span class="built_in">len</span>(choice)):</span><br><span class="line">                choice.append(<span class="string">&#x27;D．不知道&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        content = [content <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(choice))]</span><br><span class="line">        pair = [question + <span class="string">&#x27; &#x27;</span> + i[<span class="number">2</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> choice]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> content, pair, label</span><br></pre></td></tr></table></figure><p>$61536 = 15325\times 4 + 71 \times3+ 25\times2 $</p><p>数据将变成61536条</p><p>如果用五折交叉验证： 训练集 49228 验证集12307</p><p>如果Using 8 dataloader workers every process</p><p>每个batch 8条数据的话  约等于每个epoch 训练集运行772次，验证集193次</p><p>(这个地方不知道算的对不对)</p><p>将数据做成bert需要的三种编码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span>(<span class="params">data</span>):</span> </span><br><span class="line">  <span class="comment"># 将文章问题选项拼在一起后，得到分词后的数字id，输出的size是(batch, n_choices, max_len)</span></span><br><span class="line">    input_ids, attention_mask, token_type_ids = [], [], []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> data:</span><br><span class="line">        text = tokenizer(x[<span class="number">1</span>],</span><br><span class="line">                         text_pair=x[<span class="number">0</span>],</span><br><span class="line">                         padding=<span class="string">&#x27;max_length&#x27;</span>,  <span class="comment"># 填充到使用参数max_length指定的最大长度，或者填充到模型的最大可接受输入长度(如果未提供该参数)。</span></span><br><span class="line">                         truncation=<span class="literal">True</span>,</span><br><span class="line">                         <span class="comment"># TRUE或‘LIMEST_FIRST’：截断到使用参数max_length指定的最大长度，或者截断到模型的最大可接受输入长度(如果没有提供该参数)。这将逐个令牌截断令牌，如果提供了一对序列(或一批对)，则从该对中最长的序列中删除一个令牌。</span></span><br><span class="line">                         max_length=Param[<span class="string">&#x27;max_len&#x27;</span>],</span><br><span class="line">                         return_tensors=<span class="string">&#x27;pt&#x27;</span>)  <span class="comment"># 返回pytorch tensor格式</span></span><br><span class="line">        input_ids.append(text[<span class="string">&#x27;input_ids&#x27;</span>].tolist())</span><br><span class="line">        attention_mask.append(text[<span class="string">&#x27;attention_mask&#x27;</span>].tolist())</span><br><span class="line">        token_type_ids.append(text[<span class="string">&#x27;token_type_ids&#x27;</span>].tolist())</span><br><span class="line">    input_ids = torch.tensor(input_ids)</span><br><span class="line">    attention_mask = torch.tensor(attention_mask)</span><br><span class="line">    token_type_ids = torch.tensor(token_type_ids)</span><br><span class="line">    label = torch.tensor([x[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data])</span><br><span class="line">    <span class="keyword">return</span> input_ids, attention_mask, token_type_ids, label</span><br></pre></td></tr></table></figure><p>DataLoader</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">train_set = utils.MyDataset(train)</span><br><span class="line">val_set = utils.MyDataset(val)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;单卡直接写&quot;&quot;&quot;</span></span><br><span class="line">train_loader = DataLoader(train_set, batch_size=CFG[<span class="string">&#x27;train_bs&#x27;</span>], collate_fn=collate_fn, shuffle=<span class="literal">True</span>, num_workers=CFG[<span class="string">&#x27;num_workers&#x27;</span>])</span><br><span class="line">val_loader = DataLoader(val_set, batch_size=CFG[<span class="string">&#x27;valid_bs&#x27;</span>], collate_fn=collate_fn, shuffle=<span class="literal">False</span>, num_workers=CFG[<span class="string">&#x27;num_workers&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;多卡写法&quot;&quot;&quot;</span></span><br><span class="line"> <span class="comment"># 给每个rank对应的进程分配训练的样本索引</span></span><br><span class="line">train_sampler = DistributedSampler(train_set)</span><br><span class="line">val_sampler = DistributedSampler(val_set)</span><br><span class="line"> <span class="comment"># 将样本索引每batch_size个元素组成一个list 验证集不用</span></span><br><span class="line">train_batch_sampler = torch.utils.data.BatchSampler(train_sampler, batch_size=args.batch_size, drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_set, batch_sampler=train_batch_sampler, pin_memory=<span class="literal">False</span>,</span><br><span class="line">                                  collate_fn=collate_fn, num_workers=<span class="number">2</span>)</span><br><span class="line">val_loader = DataLoader(val_set, batch_size=args.batch_size, sampler=val_sampler, pin_memory=<span class="literal">False</span>, collate_fn=collate_fn, num_workers=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>DistributedSampler/BatchSampler:</p><p><a href="https://blog.csdn.net/m0_37400316/article/details/107210970">四Sampler源码</a></p><p><a href="https://www.freesion.com/article/6505681767/">（TORCH.NN.PARALLEL.DISTRIBUTEDDATAPARALLEL）时，DISTRIBUTEDSAMPLER(DATASET)用法解释</a></p><p>Dataloader 中的 num_workers:</p><p>加快训练进程<br>为了加快训练过程，使用DataLoader类的num workers可选属性。<br>num workers属性告诉数据加载器实例要使用多少子进程来加载数据。默认情况下，num  workers值设置为0，值为0告诉加载程序在主进程内加载数据。<br>这意味着训练将在主进程中按顺序工作。在训练过程中使用了一个batch，并且需要另一个batch之后，从磁盘读取批数据。现在，如果我们有一个worker进程，我们可以利用机器多个核的。这意味着在主进程准备好进行另一批处理时，下一批处理已经可以加载并准备就绪。这就是加速的来源。批处理使用其他工作进程加载，并在内存中排队。</p><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><h4 id="优化配置"><a href="#优化配置" class="headerlink" title="优化配置"></a>优化配置</h4><p>多层不同学习率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fc_para = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">id</span>, model.module.classifier.parameters()))</span><br><span class="line">lstm_para = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">id</span>, model.module.lstm.parameters()))</span><br><span class="line">gru_para = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">id</span>, model.module.gru.parameters()))</span><br><span class="line">base_para = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: <span class="built_in">id</span>(p) <span class="keyword">not</span> <span class="keyword">in</span> fc_para, model.module.parameters())</span><br><span class="line">params = [&#123;<span class="string">&#x27;params&#x27;</span>: base_para&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;params&#x27;</span>: model.module.lstm.parameters(), <span class="string">&#x27;lr&#x27;</span>: args.other_lr&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;params&#x27;</span>: model.module.gru.parameters(), <span class="string">&#x27;lr&#x27;</span>: args.other_lr&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;params&#x27;</span>: model.module.classifier.parameters(), <span class="string">&#x27;lr&#x27;</span>: args.fc_lr&#125;]</span><br><span class="line">scaler = GradScaler() <span class="comment"># 有v100的话还可以开半精度</span></span><br><span class="line">optimizer = AdamW(model.module.parameters(), lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line"><span class="comment"># criterion = nn.CrossEntropyLoss().cuda(local_rank)</span></span><br><span class="line">criterion = utils.LabelSmoothingCrossEntropy().cuda(local_rank) <span class="comment"># 标签平滑</span></span><br></pre></td></tr></table></figure><h4 id="梯度累积"><a href="#梯度累积" class="headerlink" title="梯度累积"></a>梯度累积</h4><p>由于机器显存限制，不得不用梯度累积来达到目的batch数。</p><p>用多次小的 mini-batch 来模拟一个较大的 mini-batch，即：global_batch_size = batch_size*iter_size</p><p>batch size 和 learning rate 要等比例放大。但需要注意：特别大的 batch size 还需要再加上其他 trick 如 warmup 才能保证训练顺利（因为太大的初始 lr 很容易 train 出 nan）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">loss = criterion(output, y) / args.accum_iter</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ((step + <span class="number">1</span>) % args.accum_iter == <span class="number">0</span>) <span class="keyword">or</span> ((step + <span class="number">1</span>) == <span class="built_in">len</span>(train_loader)):</span><br><span class="line">    scaler.step(optimizer)</span><br><span class="line">    scaler.update()</span><br><span class="line">    scheduler.step()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure><p>苏神:<a href="https://kexue.fm/archives/6794"> 用时间换取效果：Keras梯度累积优化器</a></p><h4 id="loss计算与warmup"><a href="#loss计算与warmup" class="headerlink" title="loss计算与warmup"></a>loss计算与warmup</h4><p>warmup顾名思义就是热身，在刚刚开始训练时以很小的学习率进行训练，使得网络熟悉数据，随着训练的进行学习率慢慢变大，到了一定程度，以设置的初始学习率进行训练，接着过了一些inter后，学习率再慢慢变小；学习率变化：上升——平稳——下降；</p><p>warm up setp（一般等于epoch*inter_per_epoch），当step小于warm up setp时，学习率等于基础学习率×(当前step/warmup_step)，由于后者是一个小于1的数值，因此在整个warm up的过程中，学习率是一个递增的过程！当warm up结束后，学习率以基础学习率进行训练，再学习率开始递减</p><p>1、当网络非常容易nan时候，采用warm up进行训练，可使得网络正常训练；</p><p>2、如果训练集损失很低，准确率高，但测试集损失大，准确率低，可用warm up；具体可看：<a href="https://blog.csdn.net/u011995719/article/details/77884728">https://blog.csdn.net/u011995719/article/details/77884728</a></p><p><a href="https://blog.zhujian.life/posts/f311f0.html">[LR Scheduler]warmup</a></p><h4 id="模型保存与加载"><a href="#模型保存与加载" class="headerlink" title="模型保存与加载"></a>模型保存与加载</h4><p>这里有个小地方要注意，因为多卡并行时model用DistributedDataParallel包装了，所以在save时不时直接的model.state_dict()，而是model.module.state_dict()。 这个问题当时困扰了我好久，模型保存完的都是没经过学习的参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> val_acc &gt; best_acc:</span><br><span class="line">    best_acc = val_acc</span><br><span class="line">    print(<span class="string">&quot;best:&quot;</span>, best_acc)</span><br><span class="line">    <span class="keyword">if</span> distribute_utils.is_main_process():</span><br><span class="line">        torch.save(model.module.state_dict(),</span><br><span class="line">                   <span class="string">&#x27;spawn_adv_pgd_&#123;&#125;_fold_&#123;&#125;.pt&#x27;</span>.<span class="built_in">format</span>(args.model.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>], fold))</span><br></pre></td></tr></table></figure><h2 id="提升点"><a href="#提升点" class="headerlink" title="提升点"></a>提升点</h2><p>更长的文本（512、sliding window、xinet、longformer） </p><ul><li>滑动窗口把文章截成很多段然后取平均softmax</li><li>xlnet 不限制长度，时间长</li><li>longformer 4096 transformers有提供</li></ul><p>更好的模型（roberta、large、DUMA）</p><ul><li>DUMA bert上再加attention</li></ul><p>更多的数据（爬虫、C3）</p><ul><li>先训练C3中文的有提升 但新改了规则说不让用外部数据了。</li></ul><p>比赛复盘<a href="">海华阅读理解比赛复盘</a></p><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><ol><li>五折交叉验证有的轮次收敛有的轮次不收敛</li></ol><p>数据shuffle过，要加warmup用cosine lr，学习率往小调从2e-5调到1e-5</p><p>还有一种情况是因为label不均衡造成的，每折数据不一样</p><ol><li>验证集loss和acc都上涨</li></ol><p>现象很常见，原因是过拟合或者训练验证数据分布不一致造成。就是在训练后期，预测的结果趋向于极端，使少数预测错的样本主导了loss，但同时少数样本不影响整体的验证acc情况。</p><h2 id="可能用到的外部数据"><a href="#可能用到的外部数据" class="headerlink" title="可能用到的外部数据"></a>可能用到的外部数据</h2><p>1、RACE dataset<br>2、SQuAD2.0 and CoQA dataset<br>3、ARC dataset<br>4、DREAM dataset<br>5、ChineseSquad，<a href="https://github.com/zengjunjun/ChineseSquad">https://github.com/zengjunjun/ChineseSquad</a><br>6、cmrc2018，<a href="https://github.com/ymcui/cmrc2018">https://github.com/ymcui/cmrc2018</a><br>7、c3 dataset<br>8、dureader dataset</p><p>1、 爬取中学语文阅读理解试题（全部选项、无标注） <a href="https://github.com/sz128/ext_data_for_haihua_ai_mrc">https://github.com/sz128/ext_data_for_haihua_ai_mrc</a> （内含网盘下载链接）<br>2、C3数据：<a href="https://github.com/nlpdata/c3">https://github.com/nlpdata/c3</a><br>3、 开源的中文预训练语言模型：</p><p>MacBERT (<a href="https://github.com/ymcui/MacBERT">https://github.com/ymcui/MacBERT</a>)<br>Chinese-BERT-wwm（<a href="https://github.com/ymcui/Chinese-BERT-wwm）">https://github.com/ymcui/Chinese-BERT-wwm）</a><br>Chinese-ELECTRA（<a href="https://github.com/ymcui/Chinese-ELECTRA">https://github.com/ymcui/Chinese-ELECTRA</a>)<br>ALBERT-zh (<a href="https://github.com/brightmart/albert_zh">https://github.com/brightmart/albert_zh</a>)<br>guwenBERT (<a href="https://github.com/Ethan-yt/guwenbert">https://github.com/Ethan-yt/guwenbert</a>);</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;海华中文阅读理解比赛梳理&quot;&gt;&lt;a href=&quot;#海华中文阅读理解比赛梳理&quot; class=&quot;headerlink&quot; title=&quot;海华中文阅读理解比赛梳理&quot;&gt;&lt;/a&gt;海华中文阅读理解比赛梳理&lt;/h1&gt;&lt;p&gt;文文言文古诗词现代诗词&lt;/p&gt;
&lt;p&gt;1 字词解释 2 标点</summary>
      
    
    
    
    
    <category term="DataGame" scheme="http://example.com/tags/DataGame/"/>
    
  </entry>
  
  <entry>
    <title>bfprt算法</title>
    <link href="http://example.com/2021/04/06/bfprt%E7%AE%97%E6%B3%95/"/>
    <id>http://example.com/2021/04/06/bfprt%E7%AE%97%E6%B3%95/</id>
    <published>2021-04-06T01:39:50.000Z</published>
    <updated>2021-04-06T03:55:24.559Z</updated>
    
    <content type="html"><![CDATA[<h1 id="bfprt算法-求TopK"><a href="#bfprt算法-求TopK" class="headerlink" title="bfprt算法 求TopK"></a>bfprt算法 求TopK</h1><p><strong>中位数的中位数算法</strong> 最坏时间复杂度 $O(n)$</p><p>在做topk问题时，最容易想到的就是先对所有数据进行一次排序，然后取其前k个。但问题有二：</p><ul><li>快排时间复杂度 $O(nlogn)$ ，但最坏时间复杂度 $O(n^2)$</li><li>我们只要前k大的，而对其余的数也进行了排序，浪费了大量排序时间。</li></ul><p>除了这种方法堆排序也是一个比较好的选择，可以维护一个大小为k的堆，时间复杂度为 $O(nlogk)$</p><h2 id="堆排序topk"><a href="#堆排序topk" class="headerlink" title="堆排序topk"></a>堆排序topk</h2><ul><li>Heap是一种数据结构具有以下的特点：<br>1）<strong>完全二叉树</strong>；<br>2）heap中存储的值是<strong>偏序</strong>；</li><li><strong>Min-heap</strong>: 父节点的值小于或等于子节点的值；<br><strong>Max-heap</strong>: 父节点的值大于或等于子节点的值；</li><li>一般都用数组来表示堆，i结点的父结点下标就为(i–1)/2。它的左右子结点下标分别为2 <em> i + 1和2 </em> i + 2</li><li>堆中每次都删除第0个数据。为了便于重建堆，实际的操作是将最后一个数据的值赋给根结点，然后再从根结点开始进行一次从上向下的调整。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># 大根堆比较器 top小</span><br><span class="line"><span class="keyword">public</span>  <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MaxheapComparator</span> <span class="keyword">implements</span> <span class="title">Comparator</span>&lt;<span class="title">Integer</span>&gt;</span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Integer o1, Integer o2)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> o2-o1;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> PriorityQueue <span class="title">getMinKNumsByHeap</span><span class="params">(<span class="keyword">int</span>[] arr,<span class="keyword">int</span> k)</span></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(k&lt;<span class="number">1</span> || k&gt;arr.length)&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  PriorityQueue&lt;Integer&gt; kHeap = <span class="keyword">new</span> PriorityQueue&lt;Integer&gt;(k, <span class="keyword">new</span> MaxheapComparator());</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i!=k;i++)&#123;</span><br><span class="line">    kHeap.add(arr[i]);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=k;i!=arr.length;i++)&#123;</span><br><span class="line">    <span class="keyword">if</span>(arr[i]&lt;kHeap.peek())&#123; <span class="comment">// 返回第一个元素，而不从此PriorityQueue中删除一个元素。</span></span><br><span class="line">      kHeap.poll(); <span class="comment">// 返回第一个元素，并从此PriorityQueue中删除一个元素。</span></span><br><span class="line">      kHeap.add(arr[i]);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> kHeap;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">int</span>[] arr = &#123; <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">9</span> &#125;;</span><br><span class="line">         <span class="comment">// 测试普通方法</span></span><br><span class="line">         System.out.println(getMinKNumsByHeap(arr, <span class="number">1</span>).peek());</span><br><span class="line">         System.out.println(getMinKNumsByHeap(arr, <span class="number">2</span>).peek());</span><br><span class="line">         System.out.println(getMinKNumsByHeap(arr, <span class="number">3</span>).peek());</span><br><span class="line">         System.out.println(getMinKNumsByHeap(arr, <span class="number">4</span>).peek());</span><br><span class="line">         System.out.println(getMinKNumsByHeap(arr, <span class="number">5</span>).peek());</span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure><h2 id="原理过程"><a href="#原理过程" class="headerlink" title="原理过程"></a>原理过程</h2><p>在快排基础上，先通过判断主元位置与k的大小使递归的规模变小。</p><p>再通过修改快速排序中主元的选取方法来降低快速排序在最坏情况下的时间复杂度。</p><ul><li><p>选取主元</p></li><li><p>以选取的主元为分界点，把小于主元的放到左边，大于主元的放到右边</p></li><li><p>分别对左边和右边进行递归，重复上述过程</p></li></ul><ol><li><p>数组被划分为了 N/5 个小部分，每个部分的5个数排序需要 O(1) ，所有部分排完需要 O(N/5)=O(N)</p></li><li><p>取出每个小部分的中位数，一共有 N/5 个，递归调用BFPRT算法得到这些数中第 (N/5)/2 小的数（即这些数 的中位数），记为 pivot</p></li><li><p>以 pivot 作为比较，将整个数组划分为 <pivot , =pivot , >pivot 三个区域</p></li><li><p>判断第K小的数在哪个区域，如果在 = 区域则直接返回 pivot ，如果在 &lt; 或 &gt; 区域，则将这个区域的数递 归调用BFPRT算法</p></li><li><p>base case ：在某次递归调用BFPRT算法时发现这个区域只有一个数，那么这个数就是我们要找的数</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getMinKthNum</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> k)</span></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(arr==<span class="keyword">null</span> || k&gt;arr.length)&#123;</span><br><span class="line">    <span class="keyword">return</span> Integer.MIN_VALUE;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">int</span>[] copyArr = Arrays.copyOf(arr, arr.length);</span><br><span class="line">  <span class="keyword">return</span> BFPRT(copyArr, <span class="number">0</span>, arr.length-<span class="number">1</span>, k-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 取出每个小部分的中位数，一共有 N/5 个，递归调用BFPRT算法得到这些数中第 (N/5)/2 小的数（即这些数 的中位数），记为 pivot. 以 pivot 作为比较，将整个数组划分为 &lt;pivot , =pivot , &gt;pivot 三个区域</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">BFPRT</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> begin, <span class="keyword">int</span> end, <span class="keyword">int</span> i)</span></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(begin==end) <span class="keyword">return</span> arr[begin];</span><br><span class="line">  <span class="keyword">int</span> pivot = medianOfMedians(arr, begin, end);</span><br><span class="line">  <span class="keyword">int</span>[] pivotRange = partition(arr, begin, end, pivot);</span><br><span class="line">  <span class="keyword">if</span>(i&gt;= pivotRange[<span class="number">0</span>] &amp;&amp; i&lt;=pivotRange[<span class="number">1</span>])&#123;</span><br><span class="line">    <span class="keyword">return</span> arr[i];</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span>(i&lt;pivotRange[<span class="number">0</span>])&#123;</span><br><span class="line">    <span class="keyword">return</span> BFPRT(arr, begin, pivotRange[<span class="number">0</span>]-<span class="number">1</span>, i);</span><br><span class="line">  &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> BFPRT(arr, pivotRange[<span class="number">1</span>]+<span class="number">1</span>, end, i);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span>[] partition(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> begin, <span class="keyword">int</span> end, <span class="keyword">int</span> pivot)&#123;</span><br><span class="line">  <span class="keyword">int</span> L= begin-<span class="number">1</span>;</span><br><span class="line">  <span class="keyword">int</span> R= end + <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">int</span> cur = begin;</span><br><span class="line">  <span class="keyword">while</span>(cur!=R)&#123;</span><br><span class="line">    <span class="keyword">if</span>(arr[cur]&gt;pivot)&#123;</span><br><span class="line">      swap(arr, cur, --R);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span>(arr[cur]&lt;pivot)&#123;</span><br><span class="line">      swap(arr, cur++, ++L)</span><br><span class="line">    &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">      cur++;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;L+<span class="number">1</span>, R-<span class="number">1</span>&#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">medianOfMedians</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> begin, <span class="keyword">int</span> end)</span></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> num = end - begin +<span class="number">1</span>;</span><br><span class="line">  <span class="keyword">int</span> offset = num % <span class="number">5</span> ==<span class="number">0</span> ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">int</span>[] medians = <span class="keyword">new</span> <span class="keyword">int</span>[num/<span class="number">5</span> + offset];</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;medians.length; i++)&#123;</span><br><span class="line">    <span class="keyword">int</span> beginI = begin+i*<span class="number">5</span>;</span><br><span class="line">    <span class="keyword">int</span> endI = beginI + <span class="number">4</span>;</span><br><span class="line">    medians[i] = getMedian(arr, beginI, Math.min(endI, end));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> BFPRT(medians, <span class="number">0</span>, medians.length-<span class="number">1</span>, medians.length/<span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> get <span class="title">Median</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> begin, <span class="keyword">int</span> end)</span></span>&#123;</span><br><span class="line">  insertionSort(arr, begin, end);</span><br><span class="line">  <span class="keyword">int</span> sum = end+begin;</span><br><span class="line">  <span class="keyword">int</span> mid = (sum/<span class="number">2</span>) + (sum%<span class="number">2</span>);</span><br><span class="line">  <span class="keyword">return</span> arr[mid];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">insertionSort</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> begin, <span class="keyword">int</span> end)</span></span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (begin&gt;=end) <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=begin+<span class="number">1</span>; i&lt;=end; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=i; j&gt;begin; j--)&#123;</span><br><span class="line">      <span class="keyword">if</span>(arr[j] &lt; arr[j-<span class="number">1</span>])&#123;</span><br><span class="line">        swap(arr, j, j-<span class="number">1</span>);</span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h2><p>最坏情况下是 $O(n)$</p><p>令 $T(n)$ 为所求的时间复杂度，则：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  T(n) \le T(\frac {n}{5}) + T(\frac{7n}{10}) + c\cdot n    \end{split}\end{equation}</script><ul><li>$T(\frac n 5)$ 来自 GetPivotIndex()，n 个元素，5 个一组，共有 $⌊\frac n5⌋$ 个中位数；</li><li>$T(\frac {7n}{10})$ 来自 BFPRT()，在 $⌊\frac n5⌋$ 个中位数中，主元 x 大于其中 $\frac 12⋅\frac n5=\frac n{10}$ 的中位数，而每个中位数在其本来的 5 个数的小组中又大于或等于其中的 3 个数，所以主元 x 至少大于所有数中的 $\frac n{10}⋅3=\frac {3n}{10}$ 个。即划分之后，任意一边的长度至少为 $\frac 3{10}$，在最坏情况下，每次选择都选到了 $\frac 7{10}$ 的那一部分。</li><li>$c⋅n$ 来自其它操作，比如 InsertSort()，以及 GetPivotIndex() 和 Partition() 里所需的一些额外操作。</li></ul><p>设 $T(n)=t⋅n$，其中 t 为未知，它可以是一个正常数，也可以是一个关于 n 的函数，代入上式：</p><script type="math/tex; mode=display">\begin{align} t⋅n&≤\frac {t⋅n}5+\frac{7t⋅n}{10}+c⋅n \tag{两边消去 n}\\ t&≤\frac t 5+\frac {7t}{10}+c \tag{再化简}\\ t&≤10c \tag{c 为一个正常数} \end{align}</script><p>其中 c 为一个正常数，故t也是一个正常数，即 $T(n)≤10c⋅n$，因此 $T(n)=O(n)$，至此证明结束。</p><p>接下来我们再来探讨下 BFPRT 算法为何选 5 作为分组主元，而不是 2, 3, 7, 9 呢？</p><p>首先排除偶数，对于偶数我们很难取舍其中位数，而奇数很容易。再者对于 3 而言，会有 $T(n)≤T(\frac n 3)+T(\frac {2n}3)+c⋅n$，它本身还是操作了 n 个元素，与以 5 为主元的 $\frac {9n}{10}$ 相比，其复杂度并没有减少。对于 7，9，… 而言，上式中的 10c，其整体都会增加，所以与 5 相比，5 更适合。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;bfprt算法-求TopK&quot;&gt;&lt;a href=&quot;#bfprt算法-求TopK&quot; class=&quot;headerlink&quot; title=&quot;bfprt算法 求TopK&quot;&gt;&lt;/a&gt;bfprt算法 求TopK&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;中位数的中位数算法&lt;/strong&gt;</summary>
      
    
    
    
    
    <category term="刷题" scheme="http://example.com/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>RoBERTa &amp; Albert</title>
    <link href="http://example.com/2021/04/05/RoBERTa-Albert/"/>
    <id>http://example.com/2021/04/05/RoBERTa-Albert/</id>
    <published>2021-04-05T07:31:04.000Z</published>
    <updated>2021-04-05T11:50:32.791Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RoBERTa-amp-Albert"><a href="#RoBERTa-amp-Albert" class="headerlink" title="RoBERTa &amp; Albert"></a>RoBERTa &amp; Albert</h1><p>2021年了，bert的改进体也越来越多，Roberta和Albert是比较出名的两个改进体。</p><p>Roberta主要针对bert的预训练任务如NSP，mask进行改进。并且扩大了batchsize和使用更长的序列训练，这两点可能在长文本竞赛上有作用。</p><p>Albert主要针对bert参数量太大，训练慢来进行改进。引入了跨层参数共享，embedding解绑分解，取消dropout和添加SOP预训练任务。</p><h2 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h2><ol><li>使用更大的batch在更大的数据集上对Bert进行深度训练</li><li>不再使用NSP(Next Sentence Prediction)任务</li><li>使用更长的序列进行训练</li><li>动态改变训练数据的MASK模式</li></ol><h3 id="静态Masking-vs-动态Masking"><a href="#静态Masking-vs-动态Masking" class="headerlink" title="静态Masking vs 动态Masking"></a>静态Masking vs 动态Masking</h3><ul><li>静态Masking:在数据预处理期间Mask矩阵就已经生成好了，每个样本只会进行一次随机Mask，每个epoch都是相同的。</li><li>修改版静态Masking: 在预处理时将数据拷贝10份，每一份拷贝都采用不同的Mask，也就是说，同样的一句话有十种不同的mask 方式，然后每份数据都训练N/10个epoch</li><li>动态Masking:每次向模型输入一个序列时，都会生成一种新的Mask方式，即不在预处理的时候进行mask，而是在向模型提供输入时动态生成Mask。</li></ul><p><img src="https://i.loli.net/2021/04/05/zFrQXsCIgc98pu3.png" alt=""></p><h3 id="取消NSP任务"><a href="#取消NSP任务" class="headerlink" title="取消NSP任务"></a>取消NSP任务</h3><p>RoBERTa 实验了 4 种方法：</p><ul><li>SEGMENT-PAIR + NSP：输入包含两部分，每个部分是来自同一文档或者不同文档的 segment （segment 是连续的多个句子），这两个 segment 的 token 总数少于 512 。预训练包含 MLM 任务和 NSP 任务。这是原始 BERT 的做法</li><li>SENTENCE-PAIR + NSP：输入也是包含两部分，每个部分是来自同一个文档或者不同文档的单个句子，这两个句子的 token 总数少于 512 。由于这些输入明显少于 512 个 tokens，因此增加 batch size 的大小，以使 tokens 总数保持与 SEGMENT-PAIR + NSP 相似。预训练包含 MLM 任务和 NSP 任务</li><li>FULL-SENTENCES：输入只有一部分（而不是两部分），来自同一个文档或者不同文档的连续多个句子，token 总数不超过 512 。输入可能跨越文档边界，如果跨文档，则在上一个文档末尾添加标志文档边界的 token 。预训练不包含 NSP 任务</li><li>DOC-SENTENCES：输入只有一部分（而不是两部分），输入的构造类似于 FULL-SENTENCES，只是不需要跨越文档边界，其输入来自同一个文档的连续句子，token 总数不超过 512 。在文档末尾附近采样的输入可以短于 512 个 tokens， 因此在这些情况下动态增加 batch size 大小以达到与 FULL-SENTENCES 相同的 tokens 总数。预训练不包含 NSP 任务</li></ul><p><img src="https://i.loli.net/2021/04/05/TqvNDS2WIXHBnAl.png" alt=""></p><h3 id="扩大Batch-Size"><a href="#扩大Batch-Size" class="headerlink" title="扩大Batch Size"></a>扩大Batch Size</h3><p>公认的因素：降低batch size会显著降低实验效果，具体可参考BERT，XLNet目录的相关Issue。</p><p>Roberta 作者也证实了这一点。</p><p><img src="https://i.loli.net/2021/04/05/jRdWv3gEVMlLk14.png" alt=""></p><p>其中，bsz 是 Batch Size；steps 是训练步数（为了保证 bsz*steps 近似相同，所以大 bsz 必定对应小 steps）；lr 是学习率；ppl 是困惑度，越小越好；最后两项是不同任务的准确率。</p><h3 id="文本编码"><a href="#文本编码" class="headerlink" title="文本编码"></a>文本编码</h3><ul><li>基于 char-level ：原始 BERT 的方式，它通过对输入文本进行启发式的词干化之后处理得到。</li><li>基于 bytes-level：与 char-level 的区别在于bytes-level 使用 bytes 而不是 unicode 字符作为 sub-word 的基本单位，因此可以编码任何输入文本而不会引入 UNKOWN 标记。</li></ul><h2 id="Albert"><a href="#Albert" class="headerlink" title="Albert"></a>Albert</h2><p>最近在 NLP 领域的研究趋势是使用越来越大的模型，以获得更好的性能。ALBERT 的研究表明，无脑堆叠模型参数可能导致效果降低</p><p>在论文中，作者做了一个有趣的实验</p><blockquote><p>如果更大的模型可以带来更好的性能，为什么不将最大的 BERT 模型 (BERT-large) 的隐含层单元增加一倍，从 1024 个单元增加到 2048 个单元呢？</p></blockquote><p>他们称之为 “BERT-xlarge”。令人惊讶的是，无论是在语言建模任务还是阅读理解测试（RACE）中，这个更大的模型的表现都不如 BERT-large</p><p><img src="https://i.loli.net/2021/04/05/5kJfDmlt8xiUFCv.png" alt=""></p><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>ALBERT 利用了参数共享、矩阵分解等技术大大减少了模型参数，用 SOP（Sentence Order Prediction） Loss 取代 NSP（Next Sentence Prediction） Loss 提升了下游任务的表现。但是 ALBERT 的层数并未减少，因此<strong>推理时间（Inference Time）还是没有得到改进</strong>。不过参数减少的确使得训练变快，同时 ALBERT 可以扩展到比 BERT 更大的模型（ALBERT-xxlarge），因此能得到更好的表现</p><p>具体的创新部分有三个：</p><ol><li>embedding 层参数因式分解</li><li>跨层参数共享</li><li>将 NSP 任务改为 SOP 任务</li></ol><h4 id="Factorized-Embedding-Parameterization"><a href="#Factorized-Embedding-Parameterization" class="headerlink" title="Factorized Embedding Parameterization"></a>Factorized Embedding Parameterization</h4><p>原始的 BERT 模型以及各种依据 Transformer 的预训连语言模型都有一个共同特点，即 E=H，其中 E 指的是 Embedding Dimension，H 指的是 Hidden Dimension。这就会导致一个问题，当提升 Hidden Dimension 时，Embedding Dimension 也需要提升，最终会导致参数量呈平方级的增加。</p><p>所以 ALBERT 的作者将 <strong>E 和 H 进行解绑</strong>，具体的操作就是<strong>在 Embedding 后面加入一个矩阵进行维度变换</strong>。E 的维度是不变的，如果 H 增大了，我们只需要在 E 后面进行一个升维操作即可</p><p><img src="https://i.loli.net/2021/04/05/8FjUN5XrKWAqvsP.png" alt=""></p><p>所以，ALBERT 不直接将原本的 one-hot 向量映射到 hidden space size of H，而是分解成两个矩阵，原本参数数量为 V∗H，V 表示的是 Vocab Size。分解成两步则减少为 V∗E+E∗H，当 H 的值很大时，这样的做法能够大幅降低参数数量</p><blockquote><p>V∗H=30000∗768=23,040,000</p><p>V∗E+E∗H=30000∗256+256∗768=7,876,608</p><p>举个例子，当 V 为 30000，H 为 768，E 为 256 时，参数量从 2300 万降低到 780 万</p></blockquote><p>通过因式分解 Embedding 的实验可以看出，对于参数不共享的版本，随着 E 的增大，效果是不断提升的。但是参数共享的版本似乎不是这样，E 最大并不是效果最好。同时也能发现参数共享对于效果可能带来 1-2 个点的下降</p><p><img src="https://i.loli.net/2021/04/05/5W9ytZiukCfdLmQ.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">  self.emb = nn.Embedding(vocab_size, <span class="number">128</span>)</span><br><span class="line">  self.fc = nn.Linear(<span class="number">128</span>, <span class="number">1024</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">  x = self.emb(x)</span><br><span class="line">  x = self.fc(x) <span class="comment"># [batch_size, seq_len, 1024]</span></span><br></pre></td></tr></table></figure><h4 id="Cross-Layer-Parameter-Sharing"><a href="#Cross-Layer-Parameter-Sharing" class="headerlink" title="Cross-Layer Parameter Sharing"></a>Cross-Layer Parameter Sharing</h4><p>传统 Transformer 的每一层参数都是独立的，包括各层的 self-attention、全连接。这样就导致层数增加时，参数量也会明显上升。之前有工作试过单独将 self-attention 或者全连接层进行共享，都取得了一些效果。ALBERT 作者尝试将所有层的参数进行共享，相当于只学习第一层的参数，并在剩下的所有层中重用该层的参数，而不是每个层都学习不同的参数</p><p><img src="https://i.loli.net/2021/04/05/zOjWTLiGyaXMvnq.png" alt=""></p><p>使用参数共享提升了模型 的稳定性，曲线更平滑了。</p><p>BERT-base 和 ALBERT 使用相同的层数以及 768 个隐藏单元，结果 BERT-base 共有 1.1 亿个参数，而 ALBERT 只有 3100 万个参数。通过实验发现，feed-forward 层的参数共享会对精度产生比较大的影响；共享注意力参数的影响是最小的</p><p><img src="https://i.loli.net/2021/04/05/V3Tf6EhcAiuXder.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数共享例子</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    self.enc_layer = TransformerEncoder()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">12</span>):</span><br><span class="line">        x = self.enc_layer(x)</span><br><span class="line"><span class="comment"># 参数不共享例子        </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    self.enc_layer1 = TransformerEncoder()</span><br><span class="line">    self.enc_layer2 = TransformerEncoder()</span><br><span class="line">    ....</span><br><span class="line">    self.enc_layer12 = TransformerEncoder()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    x = self.enc_layer1(x)</span><br><span class="line">    x = self.enc_layer2(x)</span><br><span class="line">    ....</span><br><span class="line">    x = self.enc_layer12(x)</span><br><span class="line"><span class="comment"># 分组参数共享</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    self.enc_layer1 = TransformerEncoder()</span><br><span class="line">    self.enc_layer2 = TransformerEncoder()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">      enc_inputs = self.encoder_layer(enc_inputs)</span><br><span class="line">    x = self.enc_layer2(x)</span><br></pre></td></tr></table></figure><h4 id="Sentence-Order-Prediciton-SOP"><a href="#Sentence-Order-Prediciton-SOP" class="headerlink" title="Sentence-Order Prediciton (SOP)"></a>Sentence-Order Prediciton (SOP)</h4><p><strong>BERT</strong> 引入了一个叫做<strong>下一个句子预测</strong>的二分类问题。这是专门为提高使用句子对，如 “自然语言推理” 的下游任务的性能而创建的。但是像 RoBERTa 和 XLNet 这样的论文已经阐明了 NSP 的无效性，并且发现它对下游任务的影响是不可靠的</p><p>因此，ALBERT 提出了另一个任务 —— <strong>句子顺序预测</strong>。关键思想是：</p><ul><li>从同一个文档中取两个连续的句子作为一个正样本</li><li>交换这两个句子的顺序，并使用它作为一个负样本</li></ul><p><img src="https://i.loli.net/2021/04/05/XHDghABQW6Y2Fdf.png" alt=""></p><p><img src="https://i.loli.net/2021/04/05/K7vIQg5C2GthUda.png" alt=""></p><h4 id="Adding-Data-amp-Remove-Dropout"><a href="#Adding-Data-amp-Remove-Dropout" class="headerlink" title="Adding Data &amp; Remove Dropout"></a>Adding Data &amp; Remove Dropout</h4><p>以上 ALBERT 都是使用跟 BERT 相同的训练数据。但是增加训练数据或许可以提升模型的表现，于是 ALBERT 加上 STORIES Dataset 后总共训练了 157G 的数据。另外，训练到 1M 步的时候，模型还没有对训练集 Overfit，所以作者直接把 Dropout 移除，最终在 MLM 验证集上的效果得到了大幅提升</p><p><img src="https://i.loli.net/2021/04/05/8u3sZJXQ4EFcn2C.png" alt=""></p><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>刚开始看这篇文章是很惊喜的，因为它直接把同等量级的 BERT 缩小了 10 + 倍，让普通用户有了运行可能。但是仔细看了实验后才发现参数量的减小是需要付出代价的</p><p><img src="https://i.loli.net/2021/04/05/U37dpafWzxT4lqD.png" alt=""></p><p>需要注意的是，Speedup 是训练时间而不是 Inference 时间。Inference 时间并未得到改善，因为即使是使用了共享参数机制，还是得跑完 12 层 Encoder，故 Inference 时间跟 BERT 是差不多的</p><p>实验用的参数如下</p><p><img src="https://i.loli.net/2021/04/05/svcV1HRtMd7jxX8.png" alt=""></p><p>可以得出的结论是：</p><ol><li>在相同的训练时间下，ALBERT 得到的效果确实比 BERT 好</li><li>在相同的 Inference 时间下，ALBERT base 和 large 的效果都没有 BERT 好，而且差了 2-3 个点，作者在最后也提到了会继续寻找提高速度的方法（Sparse attention 和 Block attention）</li></ol><p>另外，结合 <strong>Universal Transformer</strong> 可以想到的是，在训练和 Inference 阶段可以动态地调整 Transformer 层数（告别 12、24、48 的配置）。同时可以想办法去避免纯参数共享带来的效果下降，毕竟 Transformer 中越深层学到的任务相关信息越多，可以改进 Transformer 模块，加入记忆单元、每层个性化的 Embedding</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;RoBERTa-amp-Albert&quot;&gt;&lt;a href=&quot;#RoBERTa-amp-Albert&quot; class=&quot;headerlink&quot; title=&quot;RoBERTa &amp;amp; Albert&quot;&gt;&lt;/a&gt;RoBERTa &amp;amp; Albert&lt;/h1&gt;&lt;p&gt;20</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
</feed>
