<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Coding-Zuo</title>
  
  <subtitle>Coding And Studying</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-05-12T15:56:43.849Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Coding-Zuo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Strategies for Pre-training Graph Neural Networks</title>
    <link href="http://example.com/2021/05/12/Strategies-for-Pre-training-Graph-Neural-Networks/"/>
    <id>http://example.com/2021/05/12/Strategies-for-Pre-training-Graph-Neural-Networks/</id>
    <published>2021-05-12T09:10:47.000Z</published>
    <updated>2021-05-12T15:56:43.849Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Strategies-for-Pre-training-Graph-Neural-Networks"><a href="#Strategies-for-Pre-training-Graph-Neural-Networks" class="headerlink" title="Strategies for Pre-training Graph Neural Networks"></a>Strategies for Pre-training Graph Neural Networks</h1><p>目前深度学习各个领域的预训练都搞的热火朝天，GNN也是肯定要搞的。那么预训练之后下一个热潮会是什么呢？</p><p>ICLR2020 首次系统的探索了大规模GNN预训练</p><p>提出了一种结合节点级和图级表示的预训练方法来训练模型。</p><p>在节点级，使用了两种自监督方法，即上下文预测和属性预测。</p><p>在图形级，使用有监督的图级属性预测和结构相似性预测</p><p>同时作者建立了两个新的预训练数据集，2M graph的化学数据集和一个有395K graph的生物数据集。</p><p>接下来介绍作者这么做的理由</p><h2 id="发现"><a href="#发现" class="headerlink" title="发现"></a>发现</h2><p>因为对于特定任务的有标签数据是很稀少的，但无标签数据却有很多，所以为了充分利用无标签数据，各种自监督方法开始兴起。</p><p>所以作者分别在图级和节点级层面上提出了两大类预测方法</p><ul><li>属性预测：属性mask(节点)、有监督的属性预测(图级)</li><li>结构预测：上下文预测(节点)、结构相似性预测(图级)</li></ul><p>以往的一些研究表明(Xu et al., 2017; Ching et al., 2018; Wang et al., 2019),一个成功的迁移学习不仅仅是增加与下游任务来自同一领域的标注好的预训练数据集的数量。相反，它需要大量的领域专业知识来仔细选择与感兴趣的下游任务相关的示例和目标标签。否则，知识从相关的预训练任务转移到新的下游任务可能会损害泛化，这被称为负迁移(Rosenstein等人，2005年)，并极大地限制了预训练模型的适用性和可靠性。</p><p>作者研究发现朴素的策略要么在整个图的层面上预先训练GNN，要么在单个节点层面上预先训练GNN，所给出的改进有限，甚至可能导致许多下游任务的负迁移。在只有图级的预训练下大约有1/4的任务出现了负迁移。</p><p><img src="https://i.loli.net/2021/05/12/z5CEtxbX9Tj1WwN.png" alt=""></p><p>图(a.i)当仅使用节点级预训练时，可以很好地分离不同形状的节点(语义上不同的节点)，但汇集节点级嵌入创建的结果，图嵌入是不可分离的(图嵌入由+和−表示)</p><p>图(a.ii)仅在图级预训练的情况下，图嵌入可以很好地分离，但是单个节点的嵌入并不一定捕获它们特定于领域的语义。</p><p>图(a.iii) 高质量的节点嵌入使得不同类型的节点能够很好地分开，同时嵌入空间也是可组合的。这允许对整个图形进行准确和健壮的表示，并允许将预先训练的模型健壮地传输到各种下游任务。</p><h2 id="预训练策略"><a href="#预训练策略" class="headerlink" title="预训练策略"></a>预训练策略</h2><p>在预训练策略的技术核心是在单个节点以及整个图的级别预先训练。这一概念鼓励GNN在两个级别捕获特定域的语义。</p><h3 id="节点级预训练"><a href="#节点级预训练" class="headerlink" title="节点级预训练"></a>节点级预训练</h3><p>两种自监督方法，上下文预测和属性mask。</p><p><img src="https://i.loli.net/2021/05/12/RFS46a2tozyNGkp.png" alt=""></p><p>图(a)在上下文预测中，子图是所选中心节点周围的K跳邻域，其中K是GNN层的数量，上图中设置为K=2。环境定义为中心节点r1-和r2-Hop之间的周围图结构，上图中使用r1=1和r2=4。</p><p>图(b) 在属性mask中，输入节点/边属性(例如，分子图中的原子类型)被随机mask，并且要求GNN预测它们。</p><h4 id="上下文预测：利用图结构的分布性"><a href="#上下文预测：利用图结构的分布性" class="headerlink" title="上下文预测：利用图结构的分布性"></a>上下文预测：利用图结构的分布性</h4><p>使用子图来预测其周围的图结构。目标是预先训练GNN，以便它将出现在类似结构上下文中的节点映射到附近的嵌入。</p><p>通过三个步骤：</p><ul><li><p>邻居节点和上下文图</p><p>对于每个节点v，定义v的邻居和上下文图。因为GNN信息聚合的是K层邻居，所以节点v的嵌入$h_v$ 依赖于距离v至多k跳节点。上下文图由两个超参数r1和r2来描述，并且它表示远离v的r1跳和r2跳之间的子图(即它是宽度为r2−r1的环)。并且r1&lt;K，以便在邻域和上下文图之间共享一些节点，我们将这些节点称为上下文锚节点。这些锚节点提供关于邻居图和上下文图如何彼此连接的信息。</p></li><li><p>使用一个辅助GNN把上下文编码成固定向量</p><p>由于图的组合性，直接预测上下文图是很困难的。这与自然语言处理不同，在自然语言处理中，单词来自固定和有限的词汇表。为了实现上下文预测，将上下文图编码为固定长度的向量。为此，引入一个上下文GNN作为辅助编码，就是图中的GNN‘。首先用其获得上下文图中的节点嵌入，然后对上下文锚点的嵌入进行平均，得到固定长度的上下文嵌入。对于图G中的节点v，将其对应的上下文嵌入表示为$c^G_v$</p></li><li><p>负采样</p><p>主要的GNN编码邻居节点获取节点的embedding—— $h_v^{(K)}$ ，上下文GNN编码上下文图获取上下文embedding——$c^G_v$。学习目标是一个二分类：是否特定邻域和特定上下文图是否属于同一节点。</p><script type="math/tex; mode=display">\sigma(h^{(k)T}_v c_{v'}^{G'}) \approx 1 \{\text{v and v' are the same nodes}\}</script></li></ul><p>  让v‘=v并且G’=G(即正例)，或者我们从随机选择的图G‘中随机抽样v’(即负例)。</p><h4 id="属性mask-利用图属性的分布性"><a href="#属性mask-利用图属性的分布性" class="headerlink" title="属性mask:利用图属性的分布性"></a>属性mask:利用图属性的分布性</h4><p>目标是通过学习图结构上节点/边属性的分布规律来获取领域知识。</p><p>属性mask有节点mask和属性mask两类</p><p>工作原理：掩蔽节点/边缘属性，然后让GNN基于相邻结构预测这些属性，这参考了bert的mask。</p><p>具体地说，通过用特殊的屏蔽指示符替换输入节点/边属性(例如分子图中的原子类型)来随机屏蔽它们。然后应用GNNs来获得相应的节点/边嵌入(边嵌入:为边的端点的节点嵌入之和来获得)。</p><p>最后，在嵌入的基础上应用线性模型来预测被mask的节点/边属性。有趣的是bert的mask其实相当于在全连通的token图上应用了消息传递。</p><p>在图结构数据中是对非全连通图进行操作，目的是捕捉节点/边属性在不同图结构上的分布规律。</p><h3 id="图级别预训练"><a href="#图级别预训练" class="headerlink" title="图级别预训练"></a>图级别预训练</h3><p>我们的目标是确保节点和图嵌入都是高质量的，以便图嵌入是健壮的，并且可以跨下游任务传输。</p><p>有两个用于图级预训练的选项：预测整个图的特定于域的属性(监督标签)，或者预测图结构。</p><h4 id="有监督的图级属性预测"><a href="#有监督的图级属性预测" class="headerlink" title="有监督的图级属性预测"></a>有监督的图级属性预测</h4><p>由于图形级表示 $h_G$ 直接用于对下游预测任务进行微调，希望将特定于域的信息直接编码成 $h_G$。</p><p>考虑了一种对图表示进行预训练的实用方法：图级多任务监督预训练，用于联合预测单个图的不同监督标签集。例如，在分子性质预测中，我们可以预先训练GNN来预测到目前为止实验测量的分子的所有性质。在蛋白质功能预测中，目标是预测给定的蛋白质是否具有给定的功能，我们可以预先训练GNN来预测到目前为止已经验证的各种蛋白质功能的存在。</p><p>重要的是，单独进行大量的多任务图级预训练可能无法给出可转移的图级表示。(问题来了)</p><p>这是因为一些有监督的预训练任务可能与下游感兴趣的任务无关，甚至会损害下游的绩效（负迁移）。一种解决办法是选择“真正相关的”有监督的训练前任务，只对这些任务进行训练前GNN训练。然而，这样的解决方案成本极高，因为选择相关任务需要大量的领域专业知识，并且需要针对不同的下游任务分别进行预训练。</p><p>为了缓解这个问题，作者的见解是，多任务监督的预训练只提供图形级的监督；因此，创建图形级嵌入的本地节点嵌入可能没有意义。这种无用的节点嵌入可能会加剧负迁移问题，因为许多不同的预训练任务在节点嵌入空间中更容易相互干扰。受此启发，在执行图级预训练之前，先通过上文描述的节点级预训练方法在单个节点级别对GNN进行正则化。正如作者所料，组合策略产生了更多可转移的图形表示。并且在没有专家选择监督的预训练任务的情况下稳健地改善了下游性能。</p><h4 id="结构相似性预测"><a href="#结构相似性预测" class="headerlink" title="结构相似性预测"></a>结构相似性预测</h4><p>目标是对两个图的结构相似性进行建模</p><p>此类任务的示例包括对图形编辑距离进行建模(Bai等人，2019年)或预测图形结构相似性(Navarin等人，2018年)。</p><p>这里好像作者感觉比较难没有全部实现，留到了以后的工作中</p><h3 id="总体预训练策略"><a href="#总体预训练策略" class="headerlink" title="总体预训练策略"></a>总体预训练策略</h3><p>预训练策略是首先进行节点级的自监督预训练，然后进行图级多任务监督的预训练。当GNN预训练完成后，我们对下游任务的预训练GNN模型进行微调。具体地说，我们在图级表示的基础上添加线性分类器来预测下游的图标签。随后以端到端的方式微调整个模型，即预先训练的GNN和下游线性分类器。</p><h2 id="进一步相关工作"><a href="#进一步相关工作" class="headerlink" title="进一步相关工作"></a>进一步相关工作</h2><p>关于图中单个节点的无监督表示学习的文献非常丰富，大致分为两类。</p><p>第一类是使用基于局部随机行走的目标的方法(Grover&amp;Leskovec，2016；Perozzi等人，2014；Don等人，2015)以及例如通过预测边的存在来重建图的邻接矩阵的方法。</p><p>在第二类中是诸如Deep Graph Infomax的方法，其训练最大化局部节点表示和聚集的全局图表示之间的互信息的节点编码器。(基于对比学习互信息的最近也要研究研究)</p><p>这两种方法都鼓励附近的节点具有相似的嵌入表示，最初是针对节点分类和链路预测提出和评估的。然而，这对于图级预测任务来说可能是次优的，在图级预测任务中，捕捉局部邻域的结构相似性通常比捕捉图中节点的位置信息更重要</p><p>所以该预训练策略既考虑了节点级的预训练任务，也考虑了图级的预训练任务，并且正如在实验中所显示的，为了使预训练模型获得良好的性能，必须同时使用这两种类型的任务。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://i.loli.net/2021/05/12/cFBosvWCfURYdhx.png" alt=""></p><p>阴影单元格表示负迁移，即预训练模型的ROC-AUC比未预训练模型的ROC-AUC差。借此说明两个级别共用的重要性。</p><p><img src="https://i.loli.net/2021/05/12/HvFtBiY5RadqGMw.png" alt=""></p><p>在有无预培训的情况下测试不同GNN架构的ROC-AUC(%)性能。</p><p>这里表达能力越强的结构预训练效果越好，表达能力较弱的GNN收益较小，甚至有时未负。这一发现证实了先前的观察结果(例如，Erhan等人)。(2010))，使用富有表现力的模型对于充分利用预培训至关重要，当用于表达能力有限的模型(如GCN、GraphSAGE和GAT)时，预培训甚至会影响性能。</p><p>并且GAT的表现反而下降了不少。作者认为GAT属于表达能力有限的模型，还有人认为GAT attention的参数比较多，模型结构比较复杂导致。</p><p><img src="https://i.loli.net/2021/05/12/kFYCKXIvcU2iLeZ.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Strategies-for-Pre-training-Graph-Neural-Networks&quot;&gt;&lt;a href=&quot;#Strategies-for-Pre-training-Graph-Neural-Networks&quot; class=&quot;headerlink&quot; t</summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Meta Learning(李宏毅)</title>
    <link href="http://example.com/2021/05/09/Meta-Learning-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    <id>http://example.com/2021/05/09/Meta-Learning-%E6%9D%8E%E5%AE%8F%E6%AF%85/</id>
    <published>2021-05-09T15:15:57.000Z</published>
    <updated>2021-05-10T07:26:22.165Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Meta-Learning"><a href="#Meta-Learning" class="headerlink" title="Meta Learning"></a>Meta Learning</h1><p>李宏毅：<a href="https://www.bilibili.com/video/BV15b411g7Wd?p=57&amp;spm_id_from=pageDriver">https://www.bilibili.com/video/BV15b411g7Wd?p=57&amp;spm_id_from=pageDriver</a></p><p>一个不错的科普：<a href="https://www.bilibili.com/video/BV1KB4y1c7gg?from=search&amp;seid=2922012165894972973">https://www.bilibili.com/video/BV1KB4y1c7gg?from=search&amp;seid=2922012165894972973</a></p><h2 id="什么是元学习"><a href="#什么是元学习" class="headerlink" title="什么是元学习"></a>什么是元学习</h2><p>Meta Learning = Learn to Learn (学习如何去做学习这件事)</p><p>机器在学习了很多task后，在获得过去的任务下所汲取的经验后，学习到了更多的学习技巧，成为了一个更厉害的学习者。</p><p>从而有一个新任务，他可以学的更快更好。</p><p>比如：task1你教机器去学语音识别，task2你教他去做图片识别，那么task3你让他去学习文字识别，那么他可能学的会更好。</p><p>元学习的输入是训练数据，输出的是可以用于下一个任务的function，function也就是万能函数模拟器神经网络的模型参数</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  f^* = F(D_{train})    \end{split}\end{equation}</script><p>其中F 代表元学习算法，D是数据，f就是function。理解下图：</p><p><img src="https://i.loli.net/2021/05/09/rMLgmoSywHJcq5u.png" alt=""></p><h3 id="和机器学习的区别"><a href="#和机器学习的区别" class="headerlink" title="和机器学习的区别"></a>和机器学习的区别</h3><p>机器学习：定义一系列function—-&gt;定一个function好坏的指标——-&gt; 用gradient decent找到一个最好的function</p><p>元学习(也是找一个function)：定义一系列大Function——-&gt;定一个评价大Function好坏的指标——-&gt;找到一个最好的大Function</p><h3 id="和终身学习-Life-long-learning-有些像？"><a href="#和终身学习-Life-long-learning-有些像？" class="headerlink" title="和终身学习(Life-long learning)有些像？"></a>和终身学习(Life-long learning)有些像？</h3><p><a href="https://blog.csdn.net/zyy617532750/article/details/104217399">持续/终身学习</a>：是让同一个模型可以同时学会很多任务技能</p><p>而元学习是不同的任务仍然有不同的模型，我们期待的是模型通过以前的学习经历可以让他在未来别的任务上学的好。</p><h2 id="元学习过程"><a href="#元学习过程" class="headerlink" title="元学习过程"></a>元学习过程</h2><h3 id="定义一系列学习算法"><a href="#定义一系列学习算法" class="headerlink" title="定义一系列学习算法"></a>定义一系列学习算法</h3><p>为什么是一系列学习算法，其实不同的模型参数、不同的结构、不同的学习参数的组合都是不同的学习算法。</p><p><img src="https://i.loli.net/2021/05/09/P6iANVszETHWB37.png" alt=""></p><p>以梯度下降法为例，首先定义一个网络结构，初始化一个参数，通过训练数据计算一个梯度g，再通过学习率更新参数。</p><p>迭代多次最后得到最终参数$\hat \theta$</p><p>但上图中红色框框内的都是人为定义的。元学习就是想让这红框内的东西，不让人来设计，让机器根据先验知识来自己学习设计。</p><h3 id="评估function参数好坏"><a href="#评估function参数好坏" class="headerlink" title="评估function参数好坏"></a>评估function参数好坏</h3><p>让模型先学一些任务，去解一些问题看看。</p><p>比如Task1：用一些$D<em>{train}$ 数据去训练模型得到$f_1$ ,再用Task1的$D</em>{test}$ 去衡量 $f_1$ 得到一个loss $l_1$</p><p>一个任务不够，再多找些任务来</p><p>Task2：用一些$D<em>{train}$ 数据去训练模型得到$f_2$ ,再用Task2的$D</em>{test}$ 去衡量 $f_2$得到一个loss $l_2$</p><p>最后得到评价F好坏的Loss：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  L(F) &= \sum_{n=1}^Nl_n\\ F^* &= argmin_FL(F)    \end{split}\end{equation}</script><p>N 为任务数</p><p>meta learning 通常会把task的Train叫做Suppot set，Test叫做Query set</p><h2 id="MAML-Model-Agnostic-Meta-Learning"><a href="#MAML-Model-Agnostic-Meta-Learning" class="headerlink" title="MAML(Model Agnostic Meta-Learning)"></a>MAML(Model Agnostic Meta-Learning)</h2><p>学一个初始化的参数</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  L(\phi) = \sum_{n=1}^N l^n(\hat \theta^n)    \end{split}\end{equation}</script><p>$\phi$ 输入的初始化参数，$\hat \theta^n$ 在第n个task上学出来的model，$\hat \theta^n$ 取决于$\phi$ </p><p>$l^n(\hat \theta^n)$: 把$\hat \theta^n$这组参数拿到第n个task的测试集中去看看效果怎么样</p><p>怎么确定初始化的参数好不好，就用初始化参数到不同task上去做训练</p><p>最小化$L(\phi)$ : $\phi \gets \phi-\alpha ▽_{\phi}L(\phi)$</p><h3 id="和迁移学习-Transfer-learning-预训练有些像？"><a href="#和迁移学习-Transfer-learning-预训练有些像？" class="headerlink" title="和迁移学习(Transfer learning) 预训练有些像？"></a>和迁移学习(Transfer learning) 预训练有些像？</h3><p>迁移学习：某一个任务的数据很少，但另外一个任务的数据多。就把model预训练在多的数据上，再fine-tuning在少的数据上。</p><p>他的loss function：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}   L(\phi) = \sum_{n=1}^N l^n(\phi)    \end{split}\end{equation}</script><p>在MAML里面loss是用$\phi$ 训练完后的model计算出来的，是训练过后的model</p><p>在pretrain里是用现在这个model直接去下游任务中衡量表现怎么样。</p><p>有的文章把预训练改成MAML的形式，以缓解预训练任务和下游任务直接目标不同产生的gap。</p><p>在MAML中，我们不在意$\phi$ 在training task上的表现，在意的是用$\phi$ 训练出来的$\hat \theta^n$的表现如何</p><p>（面向的是<strong>学习的过程</strong>，并不是<strong>学习的结果</strong>）</p><p><img src="https://i.loli.net/2021/05/10/7V2Uua4g8e9R1tk.png" alt=""></p><p><img src="https://i.loli.net/2021/05/10/epRfZzxlFTgSjVI.png" alt=""></p><p>如上图虽然$\phi$ 本身表现不够好，但$\phi$经过训练以后可以变得很强 (潜力如何)</p><p>而pretrain在意的是现在这个$\phi$表现的怎么样，是在找寻在所有task都最好的$\phi$, 并不保证训练以后会得到好的 $ \hat \theta^n$ （现在表现如何）</p><p>并且MAML只训练很少的步数，因为</p><ul><li>为了快速</li><li>希望在训练一步就得到很好的结果</li><li>在使用算法模型时可以多update</li><li>为了适应Few-shot learning </li></ul><h3 id="Toy-Example"><a href="#Toy-Example" class="headerlink" title="Toy Example"></a>Toy Example</h3><p>每一个任务：</p><ul><li>给一个目标sin函数 $y = a sin(x+b)$ 其中 a、b 都是随机数，每一组 a、b 对应一条正弦曲线</li><li>从目标函数中采样k个点</li><li>使用采样点去估计目标函数</li></ul><p>希望拟合的y越好越好。随机采样不同的a和b就可以得到不同的任务。</p><p><img src="https://i.loli.net/2021/05/10/9YnTfrxqoBDgVCU.png" alt=""></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zhuanlan.zhihu.com/p/367684934">元学习-总结</a></p><p><a href="https://zhuanlan.zhihu.com/p/108503451">元学习（Meta-learning）——李宏毅老师教学视频笔记</a></p><p><a href="https://zhuanlan.zhihu.com/p/181709693">[meta-learning] 对MAML的深度解析</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Meta-Learning&quot;&gt;&lt;a href=&quot;#Meta-Learning&quot; class=&quot;headerlink&quot; title=&quot;Meta Learning&quot;&gt;&lt;/a&gt;Meta Learning&lt;/h1&gt;&lt;p&gt;李宏毅：&lt;a href=&quot;https://www.b</summary>
      
    
    
    
    
    <category term="Meta Learning" scheme="http://example.com/tags/Meta-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Learning to Pre-train Graph Neural Networks</title>
    <link href="http://example.com/2021/05/08/Learning-to-Pre-train-Graph-Neural-Networks/"/>
    <id>http://example.com/2021/05/08/Learning-to-Pre-train-Graph-Neural-Networks/</id>
    <published>2021-05-08T14:19:46.000Z</published>
    <updated>2021-05-10T12:18:03.591Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Learning-to-Pre-train-Graph-Neural-Networks"><a href="#Learning-to-Pre-train-Graph-Neural-Networks" class="headerlink" title="Learning to Pre-train Graph Neural Networks"></a>Learning to Pre-train Graph Neural Networks</h1><h2 id="动机与挑战"><a href="#动机与挑战" class="headerlink" title="动机与挑战"></a>动机与挑战</h2><p>图神经网络也是有预训练模型的，预训练之所以可以提升，可以解释为获取了有用的先验知识，并迁移到任务中。</p><p>常规的GNN预训练步骤和其他网络一样分为两个步骤：</p><ul><li>在大量未标记的图数据上预先训练GNN模型，其导出编码固有图属性的通用可转移知识</li><li>在特定于任务的图形数据上对预先训练的GNN模型进行微调，以使通用知识适用于下游任务。</li></ul><p>但之前有人已经研究过直接进行fine-tuning效果不提反降，产生负迁移效果。应该是出自(Strategies for Pre-training Graph Neural Networks 如何解决的以后看了这篇论文再说) </p><p>而这篇文章的主要想解决的是由于预训练和fine-tuning优化目标的不同，两者之间存在明显差距，损害了模型的泛化效果。</p><p>引出了第一个挑战：如何缩小不同优化目标带来的差距？ —-&gt;&gt;元学习思想</p><p>那GNN的预训练模型的特点是不仅要考虑局部的节点级先验知识还要获取图级别的全局先验知识 (现有方法要么只考虑节点级的预训练，或者仍然需要用有监督的图级预训练)</p><p>引出了第二个挑战：如何利用完全未标记的图数据同时保留节点级和图形级信息？</p><p>提出了L2P-GNN，计了节点级和图级的双重自适应机制，并且是完全自监督的方式。</p><h2 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h2><h3 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h3><p>首先定义一个图 $G = (V,E,X,Z)$ , 其中 $V$ 是节点、$E$ 是边、$X \in R ^{|V|\times d_v}$ 是节点特征、 $Z \in R^{|E|\times d_e}$ 是边的特征。</p><p>GNN 一般包含两个关键的计算，一个是聚合信息的操作AGGREGATE，另一个是更新操作UPDATE</p><p>节点表示：节点v的l层表示由下式给出：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  h_v^l &= \Psi (\psi, A, X,Z)^l\\ &= \text{UPDATAE}(h_v^{l-1}, AGGREGATE(\{(h_v^{l-1}, h_u^{l-1}, z_{uv}): u\in N_v\}))\end{split}\end{equation}</script><p>其中 $z_{uv}$ 是u到v的边特征向量，A是邻接矩阵 ，$N_v$ 是v的邻居节点。$\Psi$ 是聚合和更新操作的定义，$\psi$ 是可学习参数。</p><p>图级的表示：通常用READOUT </p><script type="math/tex; mode=display">\begin{equation}\begin{split}  h_G = \Omega(w ; H^l) = \text{READOUT} (\{h_v^l| v\in V\})    \end{split}\end{equation}</script><p>其中$H^l = [h_v^l]$ 是节点级表达矩阵。READOUT的典型实现有sum、max、mean池化，或者用其他复杂一点的方法。</p><h3 id="常规GNN的预训练"><a href="#常规GNN的预训练" class="headerlink" title="常规GNN的预训练"></a>常规GNN的预训练</h3><ol><li>预训练：定义 $D^{pre}$ 为预训练图数据，$L^{pre}$ 预训练的loss ，优化目标为：</li></ol><script type="math/tex; mode=display">\theta_0 = argmin_{\theta}  L^{pre} (f_\theta; D^{pre})</script><ol><li>fine-tuning：目标是，在对下游任务的训练集图数据$D^{tr}$进行微调之后，最大化下游测试集图数据$D^{te}$上的表现 </li></ol><p>所谓的微调根据预先训练的参数$\theta<em>0$来初始化模型，并且用在(通常是批处理的)$D</em>{tr}$上的多步梯度下降来更新GNN模型 $f_{\theta}$。</p><script type="math/tex; mode=display">\theta_1 = \theta_0 - \eta ▽_{\theta_0} L^{fine}(f_{\theta_0};D^{tr})</script><p>其中 $ \eta$ 学习率</p><p>可见常规的预训练和finetuing是解耦的，参数$\theta_0$ 和下游没有适应性的联系形式。</p><p>为此，作者提出通过构建预训练阶段来模拟下游任务的微调过程，从而直接优化预训练模型对下游任务的快速适应性。</p><h3 id="新的预训练方法"><a href="#新的预训练方法" class="headerlink" title="新的预训练方法"></a>新的预训练方法</h3><p>其实就是元学习的思想 参考上文 <a href="https://coding-zuo.github.io/2021/05/09/Meta-Learning-%E6%9D%8E%E5%AE%8F%E6%AF%85/">Meta Learning(李宏毅)</a></p><p>现有$G\in D^{pre}$ 从中采样一些子图 定义为$D^{tr}<em>{T_G}$ 作为模拟下游任务$T_G$的训练数据——元学习中的support sets，再采样一些子图作为$D^{te}</em>{T_G}$ 作为模拟的验证集——元学习中的query sets。</p><script type="math/tex; mode=display">\theta_0 = argmin_{\theta} \sum_{G\in D^{pre}} L^{pre}(f_{\theta - \alpha ▽_{\theta}L^{pre}(f_{\theta}; D^{tr}_{T_G})}; D^{te}_{T_G})</script><p>$\theta - \alpha ▽<em>{\theta}L^{pre}(f</em>{\theta}; D^{tr}<em>{T_G})$ 相当于在$D^{tr}</em>{T_G}$ 预训练的测试集先进行了一次fine-tuning</p><p>作者认为：因此，预培训输出$θ_0$并不是为了直接优化任何特定任务的训练或测试数据。相反，θ0通常是最佳的，因为它允许快速适应新任务。</p><p>我认为：这类似元学习的思想，还可以从元知识的角度来描述。还有一个点，这个预训练数据集和下游任务相不相关呢？如果相关度不大会不会有用，如果相关会不会更好？</p><h2 id="L2P-GNN"><a href="#L2P-GNN" class="headerlink" title="L2P_GNN"></a>L2P_GNN</h2><p>两个特点：</p><ul><li>从局部和全局角度捕捉图形中的结构和属性</li><li>套用MAML获得元学习的先验知识可以适应新的任务或图表</li></ul><h3 id="任务实施"><a href="#任务实施" class="headerlink" title="任务实施"></a>任务实施</h3><p>定义每个任务的图数据随机采样得到 $T_G = (S_G,Q_G)$ , $S_G$ 为Support set ，$Q_G$ 为Query set</p><p><img src="/Users/zuoyuhui/Library/Application Support/typora-user-images/image-20210510173143614.png" alt=""></p><p>多个任务的支持集合查询集为: $S_G =(S_G^1,S_G^2,…,S_G^k) ,Q_G =(Q_G^1,Q_G^2,…,Q_G^k)$</p><p>在给定父任务和子任务的情况下，作者设计了一个节点级聚集和图级汇集的自监督基本GNN模型，分别学习节点和图的表示。其核心思想是利用无标签图数据的内在结构作为节点级和图级的自我监督。</p><p>节点级：自监督预测u和v节点有边链接的目标函数</p><script type="math/tex; mode=display">L^{node}(\psi;S_G^c) = \sum_{(u,v)\in S_G^e} -ln(\sigma(h_u^Th_v)) -ln(\sigma(-h_u^Th_{v'}))</script><p>其中 $v’$ 是负采样节点，是没有和u有边的节点。</p><p>图级：通过图池化获得图表达$h<em>G$，每个任务的支持集图表达为 $h</em>{S_G^c} = \Omega(w;{h_u|\forall u,\exists v:(u,v) \in S_G^c})$</p><script type="math/tex; mode=display">L^{graph} (w; S_G) = \sum_{c=1}^k -log(\sigma(h_{S_G^c}^T h_G)) -log(\sigma(-h^T_{S_G^c}h_{G'}))</script><p>两个级别的loss综合到一起：</p><script type="math/tex; mode=display">L_{T_G}(\theta;S_G) = L^{graph}(w;S_G) + \frac{1}{k} \sum_{c=1}^k L^{node}(\psi;S_G^c)</script><p>其中$\theta = {\psi,w}$ 是可学习参数，就是可迁移的先验知识</p><h3 id="双重适应-图级和节点级"><a href="#双重适应-图级和节点级" class="headerlink" title="双重适应(图级和节点级)"></a>双重适应(图级和节点级)</h3><p><img src="https://i.loli.net/2021/05/10/ahQdOwrHvFgKIxy.png" alt=""></p><p>节点级：支持loss采用一个或几个梯度下降步骤，以获得子任务的适应先验 $ψ$。例如，当使用一个具有节点级学习率$α$的梯度更新时:</p><script type="math/tex; mode=display">\psi' = \psi - \alpha \frac{\partial\sum_{c=1}^k L^{node}(\psi;S_G^c)}{\partial\psi}</script><p>图级：</p><script type="math/tex; mode=display">w' = \psi - \beta \frac{\partial  L^{graph}(w;S_G^c)}{\partial w}</script><p>所有任务的更新参数过程</p><script type="math/tex; mode=display">\theta \gets \theta - \gamma\frac{\partial\sum_{G\in D^{pre}}L_{T_G}(\theta';Q_G) }{\partial \theta}</script><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验的主要目的：要验证有没有缩小预训练和微调的gap图级和节点级预训练策略是否奏效</p><p>作者对预训练的GNN模型在下游任务微调前后（命名为model-P和model-F）进行了对比分析，并考虑了三个比较视角：model-P和model-F参数之间的中心核对齐相似性（CKA），训练损失（delta损失）和下游任务测试性能（delta RUC-AUC或Micro-F1）的变化。</p><p><img src="https://i.loli.net/2021/05/10/wiGa9eSAcgNy62u.png" alt=""></p><p>如图所示，观察到L2P-GNN参数在微调前后的CKA相似性通常比基线的CKA相似性小，这表明L2P-GNN经历了更大的变化，以便更好地适应下游任务。</p><p>CKA 是测量神经网络表示相似性的，可以对迁移学习任务进行评估，值越小越相似。</p><p>此外，L2P-GNN的训练损失变化较小，说明L2P-GNN通过快速适应可以很容易地达到新任务的最优点。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><h3 id="GNN预训练的论文"><a href="#GNN预训练的论文" class="headerlink" title="GNN预训练的论文"></a>GNN预训练的论文</h3><p>Hu, W.; Liu, B.; Gomes, J.; Zitnik, M.; Liang, P.; Pande, V. S.; and Leskovec, J. 2020. Strategies for Pre-training Graph Neural Networks. In <em>Proceedings of ICLR</em>.</p><p>Hu, Z.; Fan, C.; Chen, T.; Chang, K.; and Sun, Y. 2019. Pre-Training Graph Neural Networks for Generic Structural Feature Extraction. <em>CoRR</em> abs/1905.13728.</p><p>Navarin, N.; Tran, D. V.; and Sperduti, A. 2018. Pre-training Graph Neural Networks with Kernels. <em>CoRR</em> abs/1811.06930.</p><h3 id="元学习"><a href="#元学习" class="headerlink" title="元学习"></a>元学习</h3><p>Finn, C.; Abbeel, P.; and Levine, S. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. In <em>Pro- ceedings of ICML</em>, 1126–1135.</p><p>Lu, Y.; Fang, Y.; and Shi, C. 2020. Meta-learning on Hetero- geneous Information Networks for Cold-start Recommenda- tion. In <em>Proceedings of KDD</em>, 1563–1573.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Learning-to-Pre-train-Graph-Neural-Networks&quot;&gt;&lt;a href=&quot;#Learning-to-Pre-train-Graph-Neural-Networks&quot; class=&quot;headerlink&quot; title=&quot;Learni</summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>DP——最大子序和</title>
    <link href="http://example.com/2021/05/06/DP%E2%80%94%E2%80%94%E6%9C%80%E5%A4%A7%E5%AD%90%E5%BA%8F%E5%92%8C/"/>
    <id>http://example.com/2021/05/06/DP%E2%80%94%E2%80%94%E6%9C%80%E5%A4%A7%E5%AD%90%E5%BA%8F%E5%92%8C/</id>
    <published>2021-05-06T01:30:10.000Z</published>
    <updated>2021-05-06T03:21:31.900Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DP——最大子序和"><a href="#DP——最大子序和" class="headerlink" title="DP——最大子序和"></a>DP——最大子序和</h1><p><a href="https://www.acwing.com/problem/content/137/">https://www.acwing.com/problem/content/137/</a></p><p>输入一个长度为 n 的整数序列，从中找出一段长度不超过 m 的连续子序列，使得子序列中所有数的和最大。</p><p><strong>注意：</strong> 子序列的长度至少是 1。</p><h4 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h4><p>第一行输入两个整数 n,m。</p><p>第二行输入 n 个数，代表长度为 n 的整数序列。</p><p>同一行数之间用空格隔开。</p><h4 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h4><p>输出一个整数，代表该序列的最大子序和。</p><h4 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h4><p>1≤n,m≤300000</p><h4 id="输入样例："><a href="#输入样例：" class="headerlink" title="输入样例："></a>输入样例：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">6 4</span><br><span class="line">1 -3 5 1 -2 3</span><br></pre></td></tr></table></figure><h4 id="输出样例："><a href="#输出样例：" class="headerlink" title="输出样例："></a>输出样例：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">7</span><br></pre></td></tr></table></figure><h2 id="解"><a href="#解" class="headerlink" title="解"></a>解</h2><p><img src="https://i.loli.net/2021/05/06/QHsdPGY1q7FrtpB.png" alt=""></p><p>状态转移方程：集合代表的喊一声所有以i结尾的子段，如果i=3的话，那么集合可能是{1,num[i]}、{1,-3,num[i]}、{1,5,num[i]}、{num[i]} ，目标是求这些集合中的最大值，因为每个集合都有num[i]可先不考虑num[i]。</p><p>所以只要考虑f[i-1]+num[i] ,和只有num[i]的集合的最大值。</p><p>也就是考虑f[i-1]和0谁最大。</p><p>最终的答案是所有集合的值取最大</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>LeetCode53 不限制最大子序列长度</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="keyword">int</span>[] num)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> last = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> res = Integer.MIN_VALUE;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num.length; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> now = Math.max(last, <span class="number">0</span>) + num[i];</span><br><span class="line">            res = Math.max(res, now);</span><br><span class="line">            last = now;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>acwing 135 限制最大子序列长度</p><p>不同的是，对于每一个i，要求前面长度为m这个段内，求一个最小值</p><p><img src="https://i.loli.net/2021/05/06/KfOS1Mzt4GxnoUd.png" alt=""></p><script type="math/tex; mode=display">max\{Sum_i - Sum_j\} , i-m 到 i-1</script><p>可以用一个队列来维护m个数</p><p>每次i向后移动，就插入一个数同时队首出列</p><ul><li>用一个单调队列</li><li>把没用的数删去</li><li>变成单调递增的序列</li><li>用$0(1)$ 把 min或max找出</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = jin.nextInt();</span><br><span class="line">        <span class="keyword">int</span> m = jin.nextInt();</span><br><span class="line">        nums.add(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; n ; i++) nums.add(jin.nextInt());</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span> ; i &lt;= n ; i++) nums.set(i, nums.get(i)+nums.get(i-<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> res = Integer.MIN_VALUE;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n ; i++)&#123;</span><br><span class="line">            <span class="keyword">while</span>(!queue.isEmpty() &amp;&amp; i - queue.peekFirst() &gt; m) queue.removeFirst();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (!queue.isEmpty()) res = Math.max(res, nums.get(i) - nums.get(queue.peekFirst())); <span class="comment">// why not peekF -1 ?</span></span><br><span class="line">            <span class="keyword">else</span> res = Math.max(res, nums.get(i));                                              <span class="comment">// 差点漏掉了</span></span><br><span class="line">            <span class="keyword">while</span>(!queue.isEmpty() &amp;&amp; nums.get(i) &lt;= nums.get(queue.peekLast())) queue.removeLast();</span><br><span class="line">            queue.offerLast(i);</span><br><span class="line">        &#125;</span><br><span class="line">        res = Math.max(res, nums.get(n) - nums.get(queue.peekFirst()-<span class="number">1</span>));</span><br><span class="line">        System.out.println(res);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    List&lt;Integer&gt; nums = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    Deque&lt;Integer&gt; queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> Scanner jin = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;<span class="keyword">new</span> Main().run();&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;DP——最大子序和&quot;&gt;&lt;a href=&quot;#DP——最大子序和&quot; class=&quot;headerlink&quot; title=&quot;DP——最大子序和&quot;&gt;&lt;/a&gt;DP——最大子序和&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.acwing.com/problem/co</summary>
      
    
    
    
    
    <category term="刷题" scheme="http://example.com/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>海华阅读理解比赛复盘</title>
    <link href="http://example.com/2021/05/01/%E6%B5%B7%E5%8D%8E%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%AF%94%E8%B5%9B%E5%A4%8D%E7%9B%98/"/>
    <id>http://example.com/2021/05/01/%E6%B5%B7%E5%8D%8E%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%AF%94%E8%B5%9B%E5%A4%8D%E7%9B%98/</id>
    <published>2021-05-01T02:29:30.000Z</published>
    <updated>2021-05-01T07:08:15.275Z</updated>
    
    <content type="html"><![CDATA[<h1 id="海华阅读理解比赛复盘"><a href="#海华阅读理解比赛复盘" class="headerlink" title="海华阅读理解比赛复盘"></a>海华阅读理解比赛复盘</h1><p>比赛详情、EMA、Baseline，本文主要记录提分点和模型改进的验证</p><p>参考上文 <a href="https://coding-zuo.github.io/2021/04/06/%E6%B5%B7%E5%8D%8E%E4%B8%AD%E6%96%87%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%AF%94%E8%B5%9B%E6%A2%B3%E7%90%86-%E5%A4%9A%E5%8D%A1%E5%B9%B6%E8%A1%8C-transformers/">海华中文阅读理解比赛梳理/多卡并行/transformers</a></p><p><a href="https://github.com/Coding-Zuo/MRC_multiChoice">github</a></p><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p>数据增强的办法很多参考 <a href="https://zhuanlan.zhihu.com/p/145521255">https://zhuanlan.zhihu.com/p/145521255</a></p><p>我只采用了句子乱序和数据回译，都是将增强数据和原始数据挨着放到数据集中，在训练的时候停用shuffle。(可能有其他方法：每条数据根据概率来选择性增强)，我这种可能会让数据集臃肿，质量下降。</p><h3 id="句子乱序"><a href="#句子乱序" class="headerlink" title="句子乱序"></a>句子乱序</h3><p>没有提分，也没有降很多。</p><p>原因参考：<a href="https://zhuanlan.zhihu.com/p/107594976">从知觉谈中文乱序不影响阅读的原因</a></p><p>代码：<a href="https://github.com/Coding-Zuo/MRC_multiChoice/blob/main/train/data_process.py">https://github.com/Coding-Zuo/MRC_multiChoice/blob/main/train/data_process.py</a> 中的data_enhancement_sentence_order</p><h3 id="数据回译"><a href="#数据回译" class="headerlink" title="数据回译"></a>数据回译</h3><p>和句子乱序一样和回译到的数据和原始数据挨着放到数据集，没有提分，可能是回译到的数据质量不好。</p><p>使用的是百度API，百度限制一个账户免费200万字符，如果超了就多注册几个账户薅羊毛。</p><p>代码：<a href="https://github.com/Coding-Zuo/MRC_multiChoice/blob/main/TranslateAPI.py">https://github.com/Coding-Zuo/MRC_multiChoice/blob/main/TranslateAPI.py</a></p><h3 id="在训练集上打伪标签"><a href="#在训练集上打伪标签" class="headerlink" title="在训练集上打伪标签"></a>在训练集上打伪标签</h3><p>由于时间问题，没有直接提交伪标签训练的结果，就直接模型融合。验证集有提高。</p><p>用训练好的模型去inference测试集，取了模型认为有百分之85概率认为是正确答案的数据打上伪标签，加入到训练集训练。</p><h2 id="优化训练"><a href="#优化训练" class="headerlink" title="优化训练"></a>优化训练</h2><h3 id="EMA"><a href="#EMA" class="headerlink" title="EMA"></a>EMA</h3><p>滑动平均exponential moving average</p><p>没有提分，反而效果变差。具体原因，还在探索，可能和优化方法有关？</p><p>我一直使用的都是adamw，<a href="https://www.cnblogs.com/tfknight/p/13425532.html">比较Adam 和Adamw</a> <a href="https://zhuanlan.zhihu.com/p/39543160">一文告诉你Adam、AdamW、Amsgrad区别和联系</a>，AdamW是在Adam+L2正则化的基础上进行改进的算法。</p><p>可以和sgd搭配看看效果。(这方面因为时间问题没有尝试充足)</p><p><a href="https://blog.csdn.net/weixin_43002433/article/details/113531466">PyTorch指数移动平均(EMA)手册</a></p><p>指数移动平均EMA是用于估计变量的局部均值的，它可以使变量的更新不只取决于当前时刻的数据。</p><p>而是加权平均了近期一段时间内的历史数据，是的变量的更新更平滑，不易受到某次异常值的影响。</p><h3 id="labelSmoothing"><a href="#labelSmoothing" class="headerlink" title="labelSmoothing"></a>labelSmoothing</h3><p>精度提升不明显，但是缓解了验证集的loss上升。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelSmoothingCrossEntropy</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, eps=<span class="number">0.1</span>, reduction=<span class="string">&#x27;mean&#x27;</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LabelSmoothingCrossEntropy, self).__init__()</span><br><span class="line">        self.eps = eps</span><br><span class="line">        self.reduction = reduction</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, output, target</span>):</span></span><br><span class="line">        c = output.size()[-<span class="number">1</span>]</span><br><span class="line">        log_preds = F.log_softmax(output, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> self.reduction == <span class="string">&#x27;sum&#x27;</span>:</span><br><span class="line">            loss = -log_preds.<span class="built_in">sum</span>()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss = -log_preds.<span class="built_in">sum</span>(dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> self.reduction == <span class="string">&#x27;mean&#x27;</span>:</span><br><span class="line">                loss = loss.mean()</span><br><span class="line">        <span class="keyword">return</span> loss * self.eps / c + (<span class="number">1</span> - self.eps) * F.nll_loss(log_preds, target, reduction=self.reduction)</span><br></pre></td></tr></table></figure><h3 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h3><p>提升两个点以上</p><p>可参考我的 <a href="https://coding-zuo.github.io/adversary/index.html">ppt</a> 和以前文章</p><p>主要使用了fgm和pgd两个，都有提升的效果</p><p>但有时候pgd并没有提升，可能是在有些参数和加了伪标签的数据情况下，学习困难？</p><h3 id="早停"><a href="#早停" class="headerlink" title="早停"></a>早停</h3><p>bert的早停不太好控制，有时候一两个epoch之后还会更新，可能跟参数有关。</p><h2 id="模型改进"><a href="#模型改进" class="headerlink" title="模型改进"></a>模型改进</h2><h3 id="尝试用LongFormer"><a href="#尝试用LongFormer" class="headerlink" title="尝试用LongFormer"></a>尝试用LongFormer</h3><p>因为文本比较长，但因为没有时间测试而没有跑，不过已经基本调通，日后跑一跑。</p><h3 id="复现DUMA"><a href="#复现DUMA" class="headerlink" title="复现DUMA"></a>复现DUMA</h3><p>用co-attention 来分别处理 bert输出的文章编码和问题答案对编码，分别送到co-attention中。</p><p>我的方法是分别为文章和问题答案设置一个maxlen， 多的截掉，因为我机器只能最大总长度跑到400，而数据文章又比较长，可能这也会导致学习瓶颈的出现。</p><p>我的另一个实现想法但是没有时间做的是，把文章和问题答案拼在一起用sep分割送入bert，输出时只要找到sep的timesteps进行分割，对于得到的两个不等长的向量，在经过对其。送入co-attention。</p><p>训练刚开始有一个比较好的提分劲头，但随着深入训练后期效果乏力。可能是因为参数没有调好？DUMA那篇论文没有复现细节。</p><h3 id="尝试其他比赛前排模型"><a href="#尝试其他比赛前排模型" class="headerlink" title="尝试其他比赛前排模型"></a>尝试其他比赛前排模型</h3><p><img src="https://i.loli.net/2021/05/01/f1QIsuWtSVXCcBx.png" alt=""></p><p>移植后问题：训练集准确率很低，具体问题还需探究。</p><h3 id="尝试在bert后加self-attention层"><a href="#尝试在bert后加self-attention层" class="headerlink" title="尝试在bert后加self-attention层"></a>尝试在bert后加self-attention层</h3><p>用pool_output,投入自注意力，没有明显提升</p><p>在bert后加多层线性也没有明显提升。不过可以尝试加highway network。</p><h2 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h2><p>组合不同参数和结构的打包模型，用argmax的方法融合了九个，达到最好的51.7分，晋级分数最终为52分，遗憾落榜。</p><p>还尝试用实现vote投票来融合，并没有最终提交。</p><p>以后将会尝试实现bert的stacking融合。</p><h2 id="遇到的难题"><a href="#遇到的难题" class="headerlink" title="遇到的难题"></a>遇到的难题</h2><ol><li><p>bert换成roberta后始终不收敛，因为没有经验，学习率试过1e-5, 1e-6, 2e-5,和不同batch32、64、128进行组合都不收敛(浪费了很多时间)。最终发现学习率在1e-5,2e-5 ,batch 在8或16才会收敛。</p><p>并参照roberta论文附录中的参数，收敛了，但是效果没有达到预期，不过听说好多人也是用了roberta。</p></li></ol><p><img src="https://i.loli.net/2021/05/01/7vZQHiFus6DqJI2.png" alt=""></p><ol><li>调参没经验，浪费了很多时间。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>用了将近一个月的时间来做这个比赛，对模型训练体系、模型理解、微调下游任务、多卡并行、对抗训练。还有好多理论需要通过实践来加深理解。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;海华阅读理解比赛复盘&quot;&gt;&lt;a href=&quot;#海华阅读理解比赛复盘&quot; class=&quot;headerlink&quot; title=&quot;海华阅读理解比赛复盘&quot;&gt;&lt;/a&gt;海华阅读理解比赛复盘&lt;/h1&gt;&lt;p&gt;比赛详情、EMA、Baseline，本文主要记录提分点和模型改进的验证&lt;/p</summary>
      
    
    
    
    
    <category term="DataGame" scheme="http://example.com/tags/DataGame/"/>
    
  </entry>
  
  <entry>
    <title>阅读理解文献梳理</title>
    <link href="http://example.com/2021/04/29/%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%96%87%E7%8C%AE%E6%A2%B3%E7%90%86/"/>
    <id>http://example.com/2021/04/29/%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%96%87%E7%8C%AE%E6%A2%B3%E7%90%86/</id>
    <published>2021-04-29T08:49:50.000Z</published>
    <updated>2021-05-10T07:14:59.431Z</updated>
    
    <content type="html"><![CDATA[<h1 id="阅读理解文献梳理"><a href="#阅读理解文献梳理" class="headerlink" title="阅读理解文献梳理"></a>阅读理解文献梳理</h1><h2 id="多跳QA"><a href="#多跳QA" class="headerlink" title="多跳QA"></a>多跳QA</h2><h3 id="模型在任务中学习的多跳推理行为。"><a href="#模型在任务中学习的多跳推理行为。" class="headerlink" title="模型在任务中学习的多跳推理行为。"></a>模型在任务中学习的多跳推理行为。</h3><p>QFE (Nishida et al., 2019) regards evidence extraction as a query-focused summarization task, and reformulates the query in each hop.    将证据提取作为以查询为中心的摘要任务，并在每一跳中重构查询。—— HGN</p><p>Kosuke Nishida, Kyosuke Nishida, Masaaki Nagata, Atsushi Otsuka, Itsumi Saito, Hisako Asano, and Junji Tomita. 2019. Answering while summarizing: Multi-task learning for multi-hop qa with evidence extraction. In <em>ACL</em>.</p><hr><p> DecompRC (Min et al., 2019b) decomposes a compositional question into simpler sub-questions and leverages single-hop MRC mod- els to answer the sub-questions.  将作文问题分解为更简单的子问题，并利用单跳MRC模型答复子问题—— HGN</p><p>Sewon Min, Victor Zhong, Luke Zettlemoyer, and Han- naneh Hajishirzi. 2019b. Multi-hop reading compre- hension through question decomposition and rescor- ing. In <em>ACL</em>.</p><hr><p>A neural modular network is also proposed in Jiang and Bansal (2019b), where neural modules are dynamically assembled for more interpretable multi-hop rea- soning.一种神经模块网络，其中神经模块被动态地组装起来，以便更好地解释多跳推理。—— HGN</p><p>Yichen Jiang and Mohit Bansal. 2019b. Self- assembling modular networks for interpretable multi-hop reasoning. In <em>EMNLP</em>.</p><hr><p>其他</p><p>Jifan Chen and Greg Durrett. 2019. Understanding dataset design choices for multi-hop reasoning. In <em>NAACL</em>.—— HGN</p><p>Sewon Min, Eric Wallace, Sameer Singh, Matt Gard- ner, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019a. Compositional questions do not necessitate multi-hop reasoning. In <em>ACL</em>.—— HGN</p><p>Yichen Jiang and Mohit Bansal. 2019a. Avoiding rea- soning shortcuts: Adversarial evaluation, training, and model development for multi-hop qa. In <em>ACL</em>.—— HGN</p><hr><h3 id="与GNN相关的"><a href="#与GNN相关的" class="headerlink" title="与GNN相关的"></a>与GNN相关的</h3><p>Coref-GRN (Dhingra et al., 2018) construct an entity graph based on co-reference reso- lution or sliding windows.基于共引用解决方案或滑动窗口构建实体图。—— HGN</p><p>Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. 2018. Neural models for reasoning over multiple mentions using coreference. In <em>NAACL</em>.</p><hr><p>Entity-GCN (De Cao et al., 2019) considers three different types of edges that connect different entities in the entity graph.考虑连接实体图中不同实体的三种不同类型的边。—— HGN</p><p>Nicola De Cao, Wilker Aziz, and Ivan Titov. 2019. Question answering by reasoning across documents with graph convolutional networks. In <em>NAACL</em>.</p><hr><p><strong>(已读)</strong>HDE-Graph (Tu et al., 2019) enriches information in the entity graph by adding document nodes and creating interactions among documents, entities and answer candidates.通过添加文档节点并在文档、实体和候选答案之间创建交互，丰富了实体图中的信息。——HGN</p><hr><p><strong>(已读)</strong>Cognitive Graph QA employs an MRC model to predict answer spans and possible next-hop spans, and then organizes them into a cognitive graph.使用MRC模型预测答案跨度和可能的下一跳跨度，然后将它们组织到认知图中。——HGN</p><hr><p>DFGN (Xiao et al., 2019) constructs a dynamic entity graph, where in each reasoning step irrelevant en- tities are softly masked out and a fusion module is designed to improve the interaction between the entity graph and documents.构建了一个动态实体图，在每个推理步骤中，不相关的实体被软屏蔽，并设计了一个融合模块来改善实体图与文档之间的交互性。——HGN</p><p>Yunxuan Xiao, Yanru Qu, Lin Qiu, Hao Zhou, Lei Li, Weinan Zhang, and Yong Yu. 2019. Dynamically fused graph network for multi-hop reasoning. In <em>ACL</em>.</p><hr><p><strong>(已读)</strong>SAE (Tu et al., 2020) defines three types of edge in the sentence graph based on the named entities and noun phrases appearing in the question and sentences 根据问题和句子中出现的命名实体和名词短语，定义句子图中的三种边——HGN</p><hr><p>C2F Reader (Shao et al., 2020) uses graph attention or self-attention on entity graph, and argues that this graph may not be necessary for multi-hop reasoning. 在实体图上使用图注意或自我注意，并认为该图对于多跳推理可能不是必需的。——HGN</p><p>Nan Shao, Yiming Cui, Ting Liu, Wang, and Guop- ing Hu Hu. 2020. Is graph structure necessary for multi-hop reasoningt. <em>arXiv preprint arXiv:2004.03096</em>.</p><hr><p>Asai et al. (2020) proposes a new graph-based recurrent method to find evidence documents as reasoning paths, which is more focused on information retrieval.提出了一种新的基于图的递归方法来寻找证据文档作为推理路径，更侧重于信息检索。——HGN</p><p>Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In <em>ICLR</em>.</p><hr><p><strong>(已读)</strong>HGN 2020 提出的模型构建了一个层次图，有效地探索了不同粒度之间的关系，并使用不同的节点来执行不同的任务。</p><h2 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h2><h3 id="GNN结构机制"><a href="#GNN结构机制" class="headerlink" title="GNN结构机制"></a>GNN结构机制</h3><ul><li>GCN</li><li>SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS(ICLR 2017)</li><li>GAT</li><li>GraphSage</li><li>MPGNN</li><li>HGN</li><li>HAN</li></ul><h3 id="预训练GNN"><a href="#预训练GNN" class="headerlink" title="预训练GNN"></a>预训练GNN</h3><p>Hu, W.; Liu, B.; Gomes, J.; Zitnik, M.; Liang, P.; Pande, V. S.; and Leskovec, J. 2020. Strategies for Pre-training Graph Neural Networks. In <em>Proceedings of ICLR</em>.——L2P-GNN</p><p>提出了不同的策略来预训练图神经网络的节点和图级，虽然在图级需要标记的数据。</p><hr><p>Hu, Z.; Fan, C.; Chen, T.; Chang, K.; and Sun, Y. 2019. Pre-Training Graph Neural Networks for Generic Structural Feature Extraction. <em>CoRR</em> abs/1905.13728..——L2P-GNN</p><p>使用三个非监督任务预先培训图形编码器，以捕获图形的不同方面。</p><hr><p>Navarin, N.; Tran, D. V.; and Sperduti, A. 2018. Pre-training Graph Neural Networks with Kernels. <em>CoRR</em> abs/1811.06930..——L2P-GNN</p><p>利用图内核进行预培训</p><h2 id="元学习及应用"><a href="#元学习及应用" class="headerlink" title="元学习及应用"></a>元学习及应用</h2><p>Finn, C.; Abbeel, P.; and Levine, S. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. In <em>Pro- ceedings of ICML</em>, 1126–1135.</p><p>Lu, Y.; Fang, Y.; and Shi, C. 2020. Meta-learning on Hetero- geneous Information Networks for Cold-start Recommenda- tion. In <em>Proceedings of KDD</em>, 1563–1573.</p><h2 id="预训练语言模型"><a href="#预训练语言模型" class="headerlink" title="预训练语言模型"></a>预训练语言模型</h2><ul><li>ALBERT</li><li>Roberta</li><li>Bert</li><li>LongFormer</li></ul><h2 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h2><ul><li>FGM</li><li>PGD</li><li>FreeLB</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;阅读理解文献梳理&quot;&gt;&lt;a href=&quot;#阅读理解文献梳理&quot; class=&quot;headerlink&quot; title=&quot;阅读理解文献梳理&quot;&gt;&lt;/a&gt;阅读理解文献梳理&lt;/h1&gt;&lt;h2 id=&quot;多跳QA&quot;&gt;&lt;a href=&quot;#多跳QA&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>Hierarchical Graph Network for Multi-hop Question Answering</title>
    <link href="http://example.com/2021/04/29/Hierarchical-Graph-Network-for-Multi-hop-Question-Answering/"/>
    <id>http://example.com/2021/04/29/Hierarchical-Graph-Network-for-Multi-hop-Question-Answering/</id>
    <published>2021-04-29T05:19:33.000Z</published>
    <updated>2021-05-05T11:13:16.766Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hierarchical-Graph-Network-for-Multi-hop-Question-Answering"><a href="#Hierarchical-Graph-Network-for-Multi-hop-Question-Answering" class="headerlink" title="Hierarchical Graph Network for Multi-hop Question Answering"></a>Hierarchical Graph Network for Multi-hop Question Answering</h1><p><a href="https://arxiv.org/pdf/1911.03631.pdf">https://arxiv.org/pdf/1911.03631.pdf</a></p><p>这篇文章也是HotpotQA数据集上的关于解决多跳问答场景的，在干扰项排行榜和全维基排行榜都曾是前列。</p><p>多跳QA和HotpotQA数据集 : <a href="https://coding-zuo.github.io/2021/03/23/HotpotQA%E6%95%B0%E6%8D%AE%E9%9B%86/">HotpotQA数据集：A Dataset for Diverse, Explainable Multi-hop Question Answering</a></p><p>其特点是</p><ul><li>通过在问题、段落、句子、实体等不同粒度上构建层次图(HGN)</li><li>通过HGN这种节点粒度层次可以区分，进一步进行多任务推理：段落选择、支持句预测、实体抽取、最终答案预测</li></ul><h2 id="如何构图"><a href="#如何构图" class="headerlink" title="如何构图"></a>如何构图</h2><p>首先来介绍下作者是如何构图的。</p><p><img src="https://i.loli.net/2021/05/01/sux1NI4eMgf2bcz.png" alt=""></p><p>段落由句子组成，每个句子包含多个实体。这个图自然是以层次结构编码的，它也激发了作者构建层次图的动机。</p><p>四种节点类型：</p><ul><li>问题节点Q</li><li>实体节点E</li><li>段落节点P：对于每个段落节点，在段落中的所有句子之间添加边。</li><li>句子节点S：对于每个句子节点，提取句子中的所有实体，并在段落句子节点和这些实体节点之间添加边。</li></ul><p>七种边类型：</p><ul><li>问题节点和定位段落节点有边</li><li>问题节点和问题中的实体节点有边</li><li>段落节点和段落中的句子节点有边</li><li>句子节点与其链接的段落节点之间的边(超链接链接)</li><li>句子节点和句子中所提取的实体节点有边</li><li>段落和段落之间有边(论文是取和问题最相关的前两个段落)</li><li>存在同一个段落的句子节点</li></ul><h2 id="挑战与动机"><a href="#挑战与动机" class="headerlink" title="挑战与动机"></a>挑战与动机</h2><p>HotpotQA的方案一般是先用一个检索器去找到包含正确答案的段落。然后在用MRC模型去选择段落去预测答案。</p><p>目前的挑战：即使通过多个段落成功地确认了推理链，如何从分散的段落中收集不同粒度级别的证据共同回答并支持事实预测，仍然是一个关键的挑战。</p><p>作者认为多跳阅读推理直观的步骤：</p><ul><li>找到与问题相关的段落</li><li>在段落中选择强有力的证据</li><li>从获得的证据中查明正确答案</li></ul><p>作者也是这么实现的，并创新的采用了多个层级的粒度信息去构图推理。</p><h2 id="HGN"><a href="#HGN" class="headerlink" title="HGN"></a>HGN</h2><p><img src="https://i.loli.net/2021/05/05/6eL2w9WqRcPdIMj.png" alt=""></p><p>模型包含四个模块：图构造模块、上下文编码模块、图推理模块、多任务预测模块</p><h3 id="图构造模块"><a href="#图构造模块" class="headerlink" title="图构造模块"></a>图构造模块</h3><p>就是构造上文的七种边四种节点，形成层级图</p><p>一共要考虑两步：</p><ul><li><p>选择相关段落：</p><p>第一跳：用预训练模型加一个二分类判断段落中是否包含支撑事实，</p><p>如果返回多个段落则选择排名靠前的两个作为段落节点。</p><p>如果标题匹配没有结果，则进一步搜索段落中包含问题实体的。</p><p>如果还是搜索失败，将会从段落排序中选择排名最高的段落。</p><p>确定第一跳后：下一步就是段落中找到可以通向其他相关段落的事实和实体(不再依赖实体链接，这可能会很引入噪音，而是在第一跳段落中使用超链接来发现第二跳段落。)</p></li></ul><ul><li>添加表示所选段落内的句子/实体之间的连接的边。</li></ul><h3 id="上下文编码模块"><a href="#上下文编码模块" class="headerlink" title="上下文编码模块"></a>上下文编码模块</h3><p>给出构建的层次图，下一步是获得所有图节点的初始表示。首先将所有选定的段落合并到上下文C中，将其与问题Q连接起来，输入Roberta。</p><p>$C = {c<em>0,c_1,…,c</em>{n-1}} \in \text{R}^{n\times d } , Q ={q<em>0,q_1,…,q</em>{m-1}}\in \text{R}^{m\times d}$</p><p>问题Q随后是一个双向注意力层。(2017. Bidirectional attention flow for machine comprehension. <em>ICLR</em>.)</p><p>在上下文表示C之上用BiLSTM，并且从BILSTM的输出中提取不同节点的表示，表示为$M∈R^{n×2d}$。</p><p>在BiLSTM后通过预测开始和结束位置来得到句子和实体节点。</p><p>$p_i$ 第i段落节点、$s_i$ 第i句子节点、 $e_i$ 第i个实体节点、q 问题节点 $\in \text{R}^d$</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  p_i &= MLP_1 ([M[P^{(i)}_{start}][d:]; M[P^{(i)}_{end}][:d] ])\\  s_i &= MLP_2 ([M[S^{(i)}_{start}][d:]; M[S^{(i)}_{end}][:d] ])\\  e_i &= MLP_3 ([M[E^{(i)}_{start}][d:]; M[E^{(i)}_{end}][:d] ])\\  q &= \text{max-pooling}(Q)    \end{split}\end{equation}</script><h3 id="图推理模块"><a href="#图推理模块" class="headerlink" title="图推理模块"></a>图推理模块</h3><p>获得层次图所需要的节点：</p><ul><li>段落节点：$P = {p<em>i}^{n_p}</em>{i=1} , n_p=4$   </li><li>句子节点：$S = {s<em>i}^{n_s}</em>{i=1}, n_s=40$</li><li>实体节点：$E = {e<em>i}^{n_e}</em>{i=1}, n_e=60$</li></ul><p>定义图的临界矩阵为$H =  {q,P,S,E} \in \text{R}^{g\times d }  , g= n_p+n_s+n_e+1$</p><p> 经过GAT后，得到更新过后的每个节点表示$P’,S’,E’,q’$</p><p>为了让图信息进一步提取上下文答案跨度，这里还用更新后的节点表示H‘和之前的上下文表示M，通过一个门控注意力机制，用于答案跨度的预测。</p><p>具体表示为：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  C &= Relu(MW_m) \cdot Relu(H'W'_m)^T\\ \hat H &= Softmax(C)\cdot H'\\ G &= \sigma([M;\hat H]W_s) \cdot Tanh([M;\hat H]W_t)    \end{split}\end{equation}</script><p>其中：$W_m \in \text{R}^{2d\times 2d}$ ，$W’_m \in \text{R}^{2d\times 2d}$ ，$W_s \in \text{R}^{4d\times 4d}$ , $W_t\in \text{R}^{4d\times 4d}$</p><h3 id="多任务预测模块"><a href="#多任务预测模块" class="headerlink" title="多任务预测模块"></a>多任务预测模块</h3><ul><li>基于段落节点的段落选择</li><li>基于句子节点的支撑事实预测</li><li>基于实体节点和上下文G表示的答案预测</li></ul><p>最终目标函数</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \text{L}_{joint} = \text{L}_{start} +\text{L}_{end} + \lambda_1\text{L}_{para} + \lambda_2\text{L}_{sent} + \lambda_3\text{L}_{entity} + \lambda_4 \text{L}_{type}    \end{split}\end{equation}</script><p>其中$\lambda_{1,2,3,4}$ 超参数权重</p><p>$L_{type}$ 是预测答案类型的损失</p><h2 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h2><p><img src="https://i.loli.net/2021/05/05/ephOUw7zXAcK3bH.png" alt=""></p><p>作者在这分析了模型的弱点(为将来的工作提供了见解)，在dev集中随机抽样了100个答案f1为0的示例。</p><p>作者总结了六类错误</p><ul><li><p>Annotation批注：数据集中提供的批注不正确 </p><p>上图第一行：“Tonka”和“101只斑点狗”是在同一个十年上映的吗？数据集给的答案和实际情况不一样？这种应该是数据集错误吧， 这种错误占了9%。这种问题应该不是模型的弱点吧？</p></li><li><p>Multiple-Answers：问题可能有多个正确答案，但数据集中只提供一个答案</p><p>迈克尔·J·亨特取代了成为哪家机构管理员的律师？答案EPA是预测答案的缩写，这种问题也比较难解决，占了24%是比重最多的。</p></li><li><p>Discrete Reasoning:  这种类型的错误经常出现在“比较”题中，需要离散推理才能正确回答问题； 16%</p><p>在Mastodon和Hole这两个乐队中，哪个成员更多？ 可能是已知这两个乐队人数，要比较这两个数的大小</p></li><li><p>Commonsense &amp; External Knowledge： 要回答这类问题，需要常识或外部知识</p><p>迷你专辑Code#01的艺人第二次延长演出的名字是什么？</p></li><li><p>Multi-hop：模型不能进行多跳推理，从错误的段落中找到最终答案 16%</p><p>这部根据5：15出现的摇滚歌剧改编的电影是谁导演的？</p></li><li><p>MRC:  模型正确地找到了支持段落和句子，但预测了错误的答案跨度。 20%</p><p>艾达·洛夫莱斯，第一位计算机程序员，在“恰尔德·拜伦”中与拜伦勋爵有什么关系？答案是他的女儿，模型回答成紧张的关系，说明模型没有完全理解问题中的关系。</p></li></ul><p>可以看出HGN对于阅读理解的进行鲁棒性的回答还是有所不足，面对相同答案的多样性还有进一步的改进空间。</p><p>对于句子理解和推理定位还不够特别准确。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Hierarchical-Graph-Network-for-Multi-hop-Question-Answering&quot;&gt;&lt;a href=&quot;#Hierarchical-Graph-Network-for-Multi-hop-Question-Answering&quot; </summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>二分模板</title>
    <link href="http://example.com/2021/04/29/%E4%BA%8C%E5%88%86%E6%A8%A1%E6%9D%BF/"/>
    <id>http://example.com/2021/04/29/%E4%BA%8C%E5%88%86%E6%A8%A1%E6%9D%BF/</id>
    <published>2021-04-29T01:23:15.000Z</published>
    <updated>2021-04-29T03:17:35.583Z</updated>
    
    <content type="html"><![CDATA[<h1 id="二分模板"><a href="#二分模板" class="headerlink" title="二分模板"></a>二分模板</h1><p><a href="https://www.acwing.com/problem/content/791/">https://www.acwing.com/problem/content/791/</a></p><p>二分的本质不是单调性, 单调性的题目一定可以二分, 可以二分的题目不一定有单调性</p><p>二分的本质是边界<br>二分法用于查找, 每次都选择答案所在的区间再次进行查找, 当区间长度为 1时, 就是答案</p><p><img src="https://i.loli.net/2021/04/29/Hy4vGqOtus8lQXp.png" alt=""></p><ol><li>根据 check(mid)来判断 r和 l的取值范围</li><li>根据取值范围选择 mid是否有 + 1操作<ul><li>mid归于左边, r = mid, mid选择 不 +1</li><li>mid归于右边, l = mid, mid选择 +1</li></ul></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> 模板<span class="title">_</span>二分 </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        BufferedReader input = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(System.in));</span><br><span class="line">        <span class="keyword">int</span>[] line1 = Arrays.asList(input.readLine().split(<span class="string">&quot; &quot;</span>)).stream().mapToInt(Integer::parseInt).toArray();</span><br><span class="line">        <span class="keyword">int</span> n = line1[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> q = line1[<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span>[] line2 = Arrays.asList(input.readLine().split(<span class="string">&quot; &quot;</span>)).stream().mapToInt(Integer::parseInt).toArray();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (q-- != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">int</span> target = Integer.parseInt(input.readLine());</span><br><span class="line">            <span class="comment">// 查找左边界 用第一个模板</span></span><br><span class="line">            <span class="keyword">int</span> index_l = bsearch_1(line2, <span class="number">0</span>, n - <span class="number">1</span>, target);</span><br><span class="line">            <span class="keyword">if</span> (line2[index_l] != target) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;-1 -1&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.print(index_l + <span class="string">&quot; &quot;</span>);</span><br><span class="line">                <span class="comment">// 查找右边界 用第二个模板</span></span><br><span class="line">                <span class="keyword">int</span> index_r = bsearch_2(line2, <span class="number">0</span>, n - <span class="number">1</span>, target);</span><br><span class="line">                System.out.print(index_r + <span class="string">&quot;\n&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">bsearch_1</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> l, <span class="keyword">int</span> r, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (l &lt; r) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = l + r &gt;&gt; <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span> (arr[mid] &gt;= target) &#123;</span><br><span class="line">                r = mid;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                l = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> l;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">bsearch_2</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> l, <span class="keyword">int</span> r, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (l &lt; r) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = l + r + <span class="number">1</span> &gt;&gt; <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span> (arr[mid] &lt;= target) &#123;</span><br><span class="line">                l = mid;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                r = mid - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> l;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;二分模板&quot;&gt;&lt;a href=&quot;#二分模板&quot; class=&quot;headerlink&quot; title=&quot;二分模板&quot;&gt;&lt;/a&gt;二分模板&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.acwing.com/problem/content/791/&quot;&gt;https://</summary>
      
    
    
    
    
    <category term="刷题" scheme="http://example.com/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>归并排序模板</title>
    <link href="http://example.com/2021/04/28/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%E6%A8%A1%E6%9D%BF/"/>
    <id>http://example.com/2021/04/28/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%E6%A8%A1%E6%9D%BF/</id>
    <published>2021-04-28T01:16:02.000Z</published>
    <updated>2021-04-28T02:06:37.453Z</updated>
    
    <content type="html"><![CDATA[<h1 id="归并排序模板"><a href="#归并排序模板" class="headerlink" title="归并排序模板"></a>归并排序模板</h1><p>分治思想</p><p><img src="https://i.loli.net/2021/04/28/8g1qixHscOBTv6Q.png" alt=""></p><ol><li><p>确定分界点：$mid = (l+r)/2$</p></li><li><p>先递归分成左右两边</p></li><li><p>将两个有序数组合并成一个有序序列——归并</p><p>使用两个指针：这个过程时间复杂度为$O(n)$</p></li></ol><p><img src="https://i.loli.net/2021/04/28/d23pUiKgLOswDZm.png" alt=""></p><p>整体时间复杂度$O(nlogn)$</p><p>因为分层用了$logn$次</p><p><img src="https://i.loli.net/2021/04/28/WeSTDRHymJbg5KN.png" alt=""></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> 模板<span class="title">_</span>归并排序 </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        BufferedReader input = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(System.in));</span><br><span class="line">        <span class="keyword">int</span> n = Integer.parseInt(input.readLine());</span><br><span class="line">        <span class="keyword">int</span>[] q = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">        String[] linelist = input.readLine().split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; linelist.length; i++) &#123;</span><br><span class="line">            q[i] = Integer.parseInt(linelist[i]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        merge_sort(q, <span class="number">0</span>, q.length - <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; q.length; i++) &#123;</span><br><span class="line">            System.out.print(q[i]);</span><br><span class="line">            System.out.print(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">merge_sort</span><span class="params">(<span class="keyword">int</span>[] q, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (l &gt;= r) <span class="keyword">return</span>;</span><br><span class="line">        <span class="comment">// 确定分界点</span></span><br><span class="line">        <span class="keyword">int</span> mid = l + ((r - l) &gt;&gt; <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 递归</span></span><br><span class="line">        merge_sort(q, l, mid);</span><br><span class="line">        merge_sort(q, mid + <span class="number">1</span>, r);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span>[] tmp = <span class="keyword">new</span> <span class="keyword">int</span>[r - l + <span class="number">1</span>]; <span class="comment">// 辅助数组</span></span><br><span class="line">        <span class="comment">// 归并</span></span><br><span class="line">        <span class="keyword">int</span> k = <span class="number">0</span>; <span class="comment">// 表示tmp中有多少个数</span></span><br><span class="line">        <span class="comment">// 两个指针</span></span><br><span class="line">        <span class="keyword">int</span> i = l, j = mid + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (i &lt;= mid &amp;&amp; j &lt;= r) &#123;</span><br><span class="line">            <span class="keyword">if</span> (q[i] &lt;= q[j]) &#123;</span><br><span class="line">                tmp[k++] = q[i++];</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                tmp[k++] = q[j++];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 剩余</span></span><br><span class="line">        <span class="keyword">while</span> (i &lt;= mid) tmp[k++] = q[i++];</span><br><span class="line">        <span class="keyword">while</span> (j &lt;= r) tmp[k++] = q[j++];</span><br><span class="line">        <span class="comment">// 放回</span></span><br><span class="line">        <span class="keyword">for</span> (i = l, j = <span class="number">0</span>; i &lt;= r; i++, j++) q[i] = tmp[j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;归并排序模板&quot;&gt;&lt;a href=&quot;#归并排序模板&quot; class=&quot;headerlink&quot; title=&quot;归并排序模板&quot;&gt;&lt;/a&gt;归并排序模板&lt;/h1&gt;&lt;p&gt;分治思想&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/04/28/8g</summary>
      
    
    
    
    
    <category term="刷题" scheme="http://example.com/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Ubantu18.04安装NVIDIA驱动+cuda10.1+cuDNN+Tensorflow2.1.0</title>
    <link href="http://example.com/2021/04/26/Ubantu18-04%E5%AE%89%E8%A3%85NVIDIA%E9%A9%B1%E5%8A%A8-cuda10-1-cuDNN-Tensorflow2-1-0/"/>
    <id>http://example.com/2021/04/26/Ubantu18-04%E5%AE%89%E8%A3%85NVIDIA%E9%A9%B1%E5%8A%A8-cuda10-1-cuDNN-Tensorflow2-1-0/</id>
    <published>2021-04-26T02:18:40.000Z</published>
    <updated>2021-04-26T02:31:39.722Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Ubantu18-04安装NVIDIA驱动-cuda10-1-cuDNN-Tensorflow2-1-0"><a href="#Ubantu18-04安装NVIDIA驱动-cuda10-1-cuDNN-Tensorflow2-1-0" class="headerlink" title="Ubantu18-04安装NVIDIA驱动-cuda10-1-cuDNN-Tensorflow2-1-0"></a>Ubantu18-04安装NVIDIA驱动-cuda10-1-cuDNN-Tensorflow2-1-0</h1><p>注意：TensorFlow2.1 要求 你的GPU算力要达到3.5，检查自己GPU算力</p><h2 id="安装和卸载NVIDIA驱动"><a href="#安装和卸载NVIDIA驱动" class="headerlink" title="安装和卸载NVIDIA驱动"></a>安装和卸载NVIDIA驱动</h2><p>首先要确保驱动已经卸载干净</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get purge nvidia*</span><br><span class="line">sudo apt-get autoremove</span><br></pre></td></tr></table></figure><p>检查自己GPU版本，之后到官网去下载，这种办法安装比较稳妥，其他网络安装办法有时候出错不知道咋回事。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lshw -numeric -C display</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/04/26/tDVckAHU8B7dnla.png" alt=""></p><p>下载驱动网址：<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn">https://www.nvidia.cn/Download/index.aspx?lang=cn</a></p><p><img src="https://i.loli.net/2021/04/26/ruzlX3qQ6Ndat9I.png" alt=""></p><p>禁用Nouveau</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Nouveau驱动禁用方法：</span><br><span class="line"></span><br><span class="line">sudo gedit &#x2F;etc&#x2F;modprobe.d&#x2F;blacklist.conf</span><br><span class="line">或者</span><br><span class="line">sudo vim &#x2F;etc&#x2F;modprobe.d&#x2F;blacklist.conf</span><br><span class="line"> </span><br><span class="line">在最后两行添加：</span><br><span class="line"> </span><br><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset&#x3D;0     &#x2F;&#x2F; 禁用nouveau第三方驱动，之后也不需要改回来</span><br><span class="line"> </span><br><span class="line">执行</span><br><span class="line"> </span><br><span class="line">sudo update -initramfs -u   &#x2F;&#x2F; 更新内核</span><br></pre></td></tr></table></figure><p>关闭lightdm</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo service lightdm stop</span><br><span class="line"></span><br><span class="line">　sudo init 3 # 遇见X Server报错执行 </span><br><span class="line"></span><br><span class="line"> rm -rf &#x2F;tmp&#x2F;.X*</span><br><span class="line"></span><br><span class="line"> .&#x2F;NVIDIA-Linux-x86_64-418.165.02.run #开始安装驱动 遇见continue就continue 遇见ok就ok</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/04/26/Wf7Imlx8PyKqtUG.png" alt=""></p><h2 id="安装cuda10-1"><a href="#安装cuda10-1" class="headerlink" title="安装cuda10.1"></a>安装cuda10.1</h2><p><a href="https://tensorflow.google.cn/install/source#linux">https://tensorflow.google.cn/install/source#linux</a></p><p>在这个网站上对好版本，版本不对可不行，全是坑 </p><p><a href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a> 选择版本</p><p>然后在这里下载cuda 我用的是deb的办法也是本地下载后安装的。<strong>（我这个网络可能是不行，总是apt-get update 总是报错 所以这个方法没成功用runfile成功了。。。）参考一下吧</strong> </p><p><img src="https://i.loli.net/2021/04/26/wVXLYjvD5zHTNRu.png" alt=""></p><p>安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb</span><br><span class="line">sudo apt-key add &#x2F;var&#x2F;cuda-repo-&lt;version&gt;&#x2F;7fa2af80.pub</span><br><span class="line">sudo dpkg -i cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install cuda</span><br></pre></td></tr></table></figure><p>添加环境变量：</p><p>打开 .bashrc</p><p> sudo vim ~/.bashrc</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda </span><br><span class="line">export PATH&#x3D;$PATH:$CUDA_HOME&#x2F;bin </span><br><span class="line">export LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure><p>source ~/.bashrc</p><p>nvcc -V</p><h2 id="runfile安装cuda"><a href="#runfile安装cuda" class="headerlink" title="runfile安装cuda"></a>runfile安装cuda</h2><p>下载runfile</p><p><img src="https://i.loli.net/2021/04/26/7G8c26kofdjJQBh.png" alt=""></p><p><img src="https://i.loli.net/2021/04/26/LI4shCiMqcKaNQB.png" alt=""></p><p>一定要取消掉driver 此处！！！，因为已经装了驱动了</p><p><img src="https://i.loli.net/2021/04/26/5CmNy6BrOIiDlkp.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure><p>我们在文件最后一行添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;NsightCompute-2019.1$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">$ export LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.1&#x2F;lib64\</span><br><span class="line">                         $&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/04/26/VDZRTx76oi8MeuK.png" alt=""></p><h2 id="安装TensorFlow2-1-0-gpu"><a href="#安装TensorFlow2-1-0-gpu" class="headerlink" title="安装TensorFlow2.1.0_gpu"></a>安装TensorFlow2.1.0_gpu</h2><p>这上面虽然没写2.1.0_gpu 可是还得得装gpu版</p><p><img src="https://i.loli.net/2021/04/26/LNxBI3jmDrcGUVT.png" alt=""></p><p>完成后 </p><p>conda install cudatoolkit=10.1</p><p><img src="https://img2020.cnblogs.com/blog/1225390/202010/1225390-20201031135739329-731523260.png" alt=""></p><h2 id="安装cuDNN"><a href="#安装cuDNN" class="headerlink" title="安装cuDNN"></a>安装cuDNN</h2><p><a href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a></p><p>去下载对应版本，但是要登录一下</p><p>解压后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo cp cuda&#x2F;include&#x2F;cudnn.h &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;include</span><br><span class="line">sudo cp cuda&#x2F;lib64&#x2F;libcudnn* &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64</span><br><span class="line">sudo chmod a+r &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;include&#x2F;cudnn.h </span><br><span class="line">sudo chmod a+r &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64&#x2F;libcudnn*</span><br></pre></td></tr></table></figure><p>以配置cuDNN环境。</p><p>通过</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;include&#x2F;cudnn.h | grep CUDNN_MAJOR -A 2</span><br></pre></td></tr></table></figure><p>查看cuDNN版本</p><p>over</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Ubantu18-04安装NVIDIA驱动-cuda10-1-cuDNN-Tensorflow2-1-0&quot;&gt;&lt;a href=&quot;#Ubantu18-04安装NVIDIA驱动-cuda10-1-cuDNN-Tensorflow2-1-0&quot; class=&quot;headerl</summary>
      
    
    
    
    
    <category term="GPU" scheme="http://example.com/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>快速排序模板</title>
    <link href="http://example.com/2021/04/26/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E6%A8%A1%E6%9D%BF/"/>
    <id>http://example.com/2021/04/26/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E6%A8%A1%E6%9D%BF/</id>
    <published>2021-04-26T01:56:51.000Z</published>
    <updated>2021-05-14T03:12:20.106Z</updated>
    
    <content type="html"><![CDATA[<p>快速排序模板</p><p><img src="https://i.loli.net/2021/04/26/voAgqKV12i9Rxuz.png" alt=""></p><ol><li>先确定分界点：$q[l] 、 q[(l+r)/2]、 q[r]$ 或随机</li><li>调整区间：小于等于x的在左半边，大于等于x的在右半边 (如何去调整)</li><li>递归处理左右两段</li></ol><h2 id="由数据反推算法复杂度和算法内容"><a href="#由数据反推算法复杂度和算法内容" class="headerlink" title="由数据反推算法复杂度和算法内容"></a>由数据反推算法复杂度和算法内容</h2><p><img src="https://i.loli.net/2021/05/14/i9EQsUqAYdW1rcB.png" alt=""></p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>暴力：</p><ul><li>声明两个数组 a[] 、b[]</li><li>将$q[l~r]$ 遍历 </li><li>如果 $q[i] \le x$ 放到a[]中   </li><li>如果 $q[i] \ge x$ 放到b[]中   </li><li>再将a、b数组放回q中</li></ul><p>优美：</p><p>用两个指针，swap</p><p><a href="https://blog.csdn.net/qq_42369555/article/details/82745923">关于JAVA中IO流类：BufferredReader的简单用法</a></p><p>bufferreader要比scanner快</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> code;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.util.Collections;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> 快排模板 </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        BufferedReader input = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(System.in));</span><br><span class="line">        <span class="keyword">int</span> n = Integer.parseInt(input.readLine());</span><br><span class="line">        <span class="keyword">int</span>[] q = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">        String[] linelist = input.readLine().split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; linelist.length; i++) &#123;</span><br><span class="line">            q[i] = Integer.parseInt(linelist[i]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        quick_sort(q, <span class="number">0</span>, q.length - <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; q.length; i++) &#123;</span><br><span class="line">            System.out.print(q[i]);</span><br><span class="line">            System.out.print(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">quick_sort</span><span class="params">(<span class="keyword">int</span>[] q, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (l &gt;= r) <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">int</span> x = q[l];</span><br><span class="line">        <span class="keyword">int</span> i = l - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> j = r + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (i &lt; j) &#123;</span><br><span class="line">            <span class="keyword">do</span> i++; <span class="keyword">while</span> (q[i] &lt; x);</span><br><span class="line">            <span class="keyword">do</span> j--; <span class="keyword">while</span> (q[j] &gt; x);</span><br><span class="line">            <span class="keyword">if</span> (i &lt; j) &#123;</span><br><span class="line">                <span class="keyword">int</span> t = q[i];</span><br><span class="line">                q[i] = q[j];</span><br><span class="line">                q[j] = t;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        quick_sort(q, l, j);</span><br><span class="line">        quick_sort(q, j + <span class="number">1</span>, r);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;快速排序模板&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/04/26/voAgqKV12i9Rxuz.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;先确定分界点：$q[l] 、 q[(l+r)/2]、 q[r]$ 或随机&lt;/li&gt;</summary>
      
    
    
    
    
    <category term="刷题" scheme="http://example.com/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>DP分析——石子合并</title>
    <link href="http://example.com/2021/04/24/DP%E5%88%86%E6%9E%90%E2%80%94%E2%80%94%E7%9F%B3%E5%AD%90%E5%90%88%E5%B9%B6/"/>
    <id>http://example.com/2021/04/24/DP%E5%88%86%E6%9E%90%E2%80%94%E2%80%94%E7%9F%B3%E5%AD%90%E5%90%88%E5%B9%B6/</id>
    <published>2021-04-24T02:19:10.000Z</published>
    <updated>2021-04-24T04:39:01.339Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DP分析——石子合并"><a href="#DP分析——石子合并" class="headerlink" title="DP分析——石子合并"></a>DP分析——石子合并</h1><p>设有 NN 堆石子排成一排，其编号为 1，2，3，…，N。</p><p>每堆石子有一定的质量，可以用一个整数来描述，现在要将这 N 堆石子合并成为一堆。</p><p>每次只能合并相邻的两堆，合并的代价为这两堆石子的质量之和，合并后与这两堆石子相邻的石子将和新堆相邻，合并时由于选择的顺序不同，合并的总代价也不相同。</p><p>例如有 4 堆石子分别为 <code>1 3 5 2</code>， 我们可以先合并 1、2堆，代价为 4，得到 <code>4 5 2</code>， 又合并 1，2 堆，代价为 9，得到 <code>9 2</code> ，再合并得到 11，总代价为 4+9+11=244+9+11=24；</p><p>如果第二步是先合并 2，3 堆，则代价为 7，得到 <code>4 7</code>，最后一次合并代价为 11，总代价为 4+7+11=22。</p><p>问题是：找出一种合理的方法，使总的代价最小，输出最小代价。</p><h4 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h4><p>第一行一个数 N 表示石子的堆数 N。</p><p>第二行 N 个数，表示每堆石子的质量(均不超过 1000)。</p><h4 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h4><p>输出一个整数，表示最小代价。</p><h4 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h4><p>1≤N≤300     1≤N≤300</p><h4 id="输入样例："><a href="#输入样例：" class="headerlink" title="输入样例："></a>输入样例：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4</span><br><span class="line">1 3 5 2</span><br></pre></td></tr></table></figure><h4 id="输出样例："><a href="#输出样例：" class="headerlink" title="输出样例："></a>输出样例：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">22</span><br></pre></td></tr></table></figure><h2 id="解"><a href="#解" class="headerlink" title="解"></a>解</h2><p><img src="https://i.loli.net/2021/04/24/CqE9QcaxYBzZKRw.png" alt=""></p><p><img src="https://i.loli.net/2021/04/24/gPlOsK5oXFcWutE.png" alt=""></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DP_</span>石子合并 </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="keyword">int</span> N = scanner.nextInt();</span><br><span class="line">        <span class="keyword">int</span>[] s = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>];  <span class="comment">//前缀和</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++) &#123;</span><br><span class="line">            s[i] = scanner.nextInt();</span><br><span class="line">            s[i] += s[i - <span class="number">1</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>][N + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> len = <span class="number">2</span>; len &lt;= N; len++) &#123;<span class="comment">//先枚举区间长度</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i + len - <span class="number">1</span> &lt;= N; i++) &#123;<span class="comment">//再枚举区间左端点</span></span><br><span class="line">                <span class="keyword">int</span> j = i + len - <span class="number">1</span>; <span class="comment">//右端点</span></span><br><span class="line">                dp[i][j] = <span class="number">100000000</span>;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> k = i; k &lt; j; k++) &#123;</span><br><span class="line">                    dp[i][j] = Math.min(dp[i][j], dp[i][k] + dp[k + <span class="number">1</span>][j] + s[j] - s[i - <span class="number">1</span>]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(dp[<span class="number">1</span>][N]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>$O(n^3)$ </p><hr><h1 id="最长公共子序列"><a href="#最长公共子序列" class="headerlink" title="最长公共子序列"></a>最长公共子序列</h1><p>给定两个长度分别为 N 和 M 的字符串 A 和 B，求既是 A 的子序列又是 B 的子序列的字符串长度最长是多少。</p><h4 id="输入格式-1"><a href="#输入格式-1" class="headerlink" title="输入格式"></a>输入格式</h4><p>第一行包含两个整数 N 和 M。</p><p>第二行包含一个长度为 N 的字符串，表示字符串 A。</p><p>第三行包含一个长度为 M 的字符串，表示字符串 B。</p><p>字符串均由小写字母构成。</p><h4 id="输出格式-1"><a href="#输出格式-1" class="headerlink" title="输出格式"></a>输出格式</h4><p>输出一个整数，表示最大长度。</p><h4 id="数据范围-1"><a href="#数据范围-1" class="headerlink" title="数据范围"></a>数据范围</h4><p>1≤N,M≤1000       1≤N,M≤1000</p><h4 id="输入样例：-1"><a href="#输入样例：-1" class="headerlink" title="输入样例："></a>输入样例：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">4 5</span><br><span class="line">acbd</span><br><span class="line">abedc</span><br></pre></td></tr></table></figure><h4 id="输出样例：-1"><a href="#输出样例：-1" class="headerlink" title="输出样例："></a>输出样例：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3</span><br></pre></td></tr></table></figure><h2 id="解-1"><a href="#解-1" class="headerlink" title="解"></a>解</h2><p>最坏情况下 aaaa,aaaaa，A中所有都是由 $2^n$ 个不同子序列。</p><p><img src="https://i.loli.net/2021/04/24/oxH4yliYPD23LeQ.png" alt=""></p><p><img src="https://i.loli.net/2021/04/24/dVY9UmoHTticMPC.png" alt=""></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="keyword">int</span> N = scanner.nextInt();</span><br><span class="line">        <span class="keyword">int</span> M = scanner.nextInt();</span><br><span class="line">        String strA = <span class="string">&quot; &quot;</span> + scanner.next();</span><br><span class="line">        String strB = <span class="string">&quot; &quot;</span> + scanner.next();</span><br><span class="line"><span class="comment">//        char[] A = strA.toCharArray();</span></span><br><span class="line"><span class="comment">//        char[] B = strB.toCharArray();</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>][M + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= M; j++) &#123;</span><br><span class="line">                dp[i][j] = Math.max(dp[i - <span class="number">1</span>][j], dp[i][j - <span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">if</span> (strA.charAt(i) == strB.charAt(j)) &#123;</span><br><span class="line">                    dp[i][j] = Math.max(dp[i][j], dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(dp[N][M]);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;DP分析——石子合并&quot;&gt;&lt;a href=&quot;#DP分析——石子合并&quot; class=&quot;headerlink&quot; title=&quot;DP分析——石子合并&quot;&gt;&lt;/a&gt;DP分析——石子合并&lt;/h1&gt;&lt;p&gt;设有 NN 堆石子排成一排，其编号为 1，2，3，…，N。&lt;/p&gt;
&lt;p&gt;每</summary>
      
    
    
    
    
    <category term="刷题" scheme="http://example.com/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Select, Answer and Explain-Interpretable Multi-hop Reading Comprehension over Multiple Documents</title>
    <link href="http://example.com/2021/04/22/Select-Answer-and-Explain-Interpretable-Multi-hop-Reading-Comprehension-over-Multiple-Documents/"/>
    <id>http://example.com/2021/04/22/Select-Answer-and-Explain-Interpretable-Multi-hop-Reading-Comprehension-over-Multiple-Documents/</id>
    <published>2021-04-21T16:47:09.000Z</published>
    <updated>2021-04-22T05:12:11.538Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Select-Answer-and-Explain-Interpretable-Multi-hop-Reading-Comprehension-over-Multiple-Documents"><a href="#Select-Answer-and-Explain-Interpretable-Multi-hop-Reading-Comprehension-over-Multiple-Documents" class="headerlink" title="Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents"></a>Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>选择、回答和解释(SAE)系统解决多文档RC问题。</p><p>首先主要创新，用文档分类器过滤掉与答案无关的文档，从而减少分散注意力的信息量。</p><p>然后将所选择的答案相关文档输入到模型以联合预测答案和支持句子。</p><p>该模型在答案预测的表征层和支持句子预测的句子层都设置了多任务学习目标，</p><p>并在这两个任务之间进行了基于注意力的交互，对模型进行了优化。</p><p>关键词：过滤无关文档、多任务学习、混合注意力交互</p><p><img src="https://i.loli.net/2021/04/22/yArGlY6zuT7eMsW.png" alt=""></p><h2 id="在HotpotQA中什么是gold-doc"><a href="#在HotpotQA中什么是gold-doc" class="headerlink" title="在HotpotQA中什么是gold doc"></a>在HotpotQA中什么是gold doc</h2><p>HotpotQA通过为答案提供支持句来鼓励可解释的QA模型，这些支持句通常来自多个文档，如果文档包含答案或包含对答案的支持句，则称为“黄金文档”。</p><p>应答文本，它可以是一段文本或“是/否”。</p><p>作者从答案和支持句标签导出GOLD文档标签。我们使用 $D_i$ 表示文档 i：如果Di是黄金文档，则标记为1，否则标记为0。还将答案类型标记为以下注释之一：“Span”、“Yes”和“No”。</p><h2 id="选择gold-doc-过滤文档"><a href="#选择gold-doc-过滤文档" class="headerlink" title="选择gold doc(过滤文档)"></a>选择gold doc(过滤文档)</h2><p>答案预测和支持句预测的上游任务。将分类排名最靠前的文档作为预测的黄金文档 gold doc。</p><p><img src="https://i.loli.net/2021/04/22/EapiyjuwNbPt49l.png" alt=""></p><p>做文档过滤最直接做法就是采用bert的CLS摘要向量，做交叉熵分类</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  L = -\sum_{i=0}^n t_ilogP(D_i) + (1-t_i) log(1-P(D_i))    \end{split}\end{equation}</script><p>$t_i$ 是 $D_i$ 的标签，n是文档数，$P(D_i)$ 是文档i在标签 $t_i$ 中的概率。</p><p>这种简单的方法的缺点：单独处理每个文档，而不考虑下游多跳推理任务所必需的文档间交互和关系。</p><p>为解决此问题，作者提出了一个新的模型，如图上图CLS后，加了一层多头注意力层。</p><p>意在：增加对从不同文档生成的“CLS”标记的关注的动机是鼓励文档间的交互。文档间交互对于文档间的多跳推理至关重要。</p><p>优化：采用了新的成对学习排序损失。还将问题从分类问题描述为两两学习排序问题，</p><p>通过将文档与所有其他文档进行比较，该模型能够更好地将一小部分黄金文档与睡觉分散注意力的文档区分开来。</p><p>给每个文档一共分数 $S(.)$</p><p>如果 $D_i$ 是gold doc $S(D_i) = 1 $, 否则 $S(D_i) = 0$</p><p>然后，标记每对输入文档：给定一对输入文档 $(D_i，D_j)$，标签 $l$设置为：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  l_{i,j} = \begin{cases} 1, &if\ S(D_i) \gt S(D_j) \\ 0 , &if\  S(D_i) \le S(D_j)\end{cases}    \end{split}\end{equation}</script><p>还认为包含答案范围的文档对于下游任务更重要。因此，如果$D_i$是包含答案跨度的黄金文献，$S(D_i)=2$。</p><p>再将MHSA输出传递给双线性层来输出每对文档的概率，双线性层基于二元交叉熵进行训练，如下所示：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  L = - \sum_{i=0,j=0}^n \sum_{j\neq i} l_{i,j} logP(D_i,D_j) + (1-l_{i,j}) log(1-P(D_i,D_j))    \end{split}\end{equation}</script><p>相关性定义为$ R_i=􏰅\sum_j^n(P(D_i，D_j)&gt;0.5)$。将来自该相关性排序的前k个排序的文档作为的过滤文档。</p><h2 id="答案和解释"><a href="#答案和解释" class="headerlink" title="答案和解释"></a>答案和解释</h2><p>模型采用多任务学习的方式进行训练，以联合预测答案和对黄金文档的支持意义。</p><p>基于GNN构建多跳推理图，将上下文语句嵌入作为节点，而不是像以往的工作那样以实体作为节点，直接输出带有答案预测的支持语句。</p><p>为什么不用NER因为作者认为：</p><p>目前GNN在QA任务中的应用通常需要实体作为图形节点，并且通过在具有上下文信息的节点上进行消息传递来实现推理。这仅在预定义的一组目标实体可用时才有可能。否则，需要使用命名实体识别(NER)工具来提取实体，这可能会导致图推理中的冗余实体和噪声实体。如果答案不是命名实体，则需要进一步处理以定位最终答案。</p><p> token-level and sentence-level 多任务学习</p><p>基于一种新的混合注意池机制</p><p>将GNN中使用的上下文语句嵌入归结到令牌表示上。注意力权重是根据令牌表示上的答案广度日志和自我注意输出来计算的。这种基于注意力的交互能够利用“回答”和“解释”任务之间的互补信息。</p><h3 id="答案预测"><a href="#答案预测" class="headerlink" title="答案预测"></a>答案预测</h3><p>针对bert输出的每一个$H_i$ 用两层MLP做答案起始位置预测 $L$ 为长度</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \hat Y &= f_{span} (H^i) \in R^{L\times2}\\ L ^{span} &= \frac{1}{2}(CE(\hat Y[:,0], y^{start}) + CE(\hat Y[:,1], y^{end}))    \end{split}\end{equation}</script><p>其中$\hat Y$的第一行是起始位置的逻辑，第二行是结束位置的逻辑。$y^{star}t$和 $y^{end}$ 是范围 [0，L-1] 中的开始位置和结束位置的标签。CE表示交叉熵损失函数。</p><h3 id="支持句预测"><a href="#支持句预测" class="headerlink" title="支持句预测"></a>支持句预测</h3><p>预测输入上下文中的句子是否为答案预测的支持证据。为了实现句子级预测，我们首先获得$H_i$中每个句子的序列表示。$H_i$ 是bert的token输出。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  S^j - H[j^s:j^e,:] \in R^{L^j\times d}    \end{split}\end{equation}</script><p>$S^j$是表示语句 j 内的标记嵌入的矩阵( 这里省略了样本索引i)。 $j^s$ 和 $j^e$ 定义了开始和结束位置，$L_j$ 是语句$j$ 的长度。</p><h3 id="多任务"><a href="#多任务" class="headerlink" title="多任务"></a>多任务</h3><p>答案预测任务和支持句预测任务可以相辅相成。</p><p>据观察，答案预测任务总是可以帮助支持句子预测任务，因为有答案的句子总是一条证据；</p><p>但反过来情况不是一样的，因为可能有多个支持句子，概率最高的句子可能不包含答案</p><p>所以答案预测任务总 可以帮助支持句子预测任务，因为有答案的句子总是一个证据；</p><p>反之亦然，因为可能有多个支持句子，概率最高的句子可能不包含答案。</p><p>因此，为了揭示这两个互补任务之间的相互作用，提出了一种基于注意力的总结句子表示法，以引入来自回答预测的互补信息。</p><p>注意力权重的计算方法如下：在Sj上用自我注意计算一部分注意力，另一部分来自答案预测任务的起始位置日志和结束位置日志的总和。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \alpha^j &= \sigma(f_{att}(S^j) + \hat Y[j^s:j^e,0] + \hat Y[j^s:j^e,1])\\ s^j &= \sum^{L^j}_{k=0} \alpha^j_k S^j[k,:] \in R^{1\times d}    \end{split}\end{equation}</script><p>Sj是表示语句j 的标记嵌入的矩阵</p><p>$f_{att}$ 是一个两层MLP输出size为1，$\sigma$是softmax</p><p>$α_j ∈ R^{L^j×1}$表示句子j的每个token上的关注度权重。</p><h3 id="构建GNN"><a href="#构建GNN" class="headerlink" title="构建GNN"></a>构建GNN</h3><p>接下来，在语句嵌入Sj上建立GNN模型，以显式地促进对预测gold doc中所有语句的多跳推理，从而更好地利用复杂的关系信息。使用语句嵌入Sj来初始化图的节点特征。采用基于多关系图卷积网络(GCN)的消息传递策略来更新图的节点特征，并将最终的节点特征输入到MLP中，得到每个句子的分类。</p><p><img src="https://i.loli.net/2021/04/22/kfrJdNaDZBqyAhV.png" alt=""></p><p>根据问题和句子中出现的命名实体和名词短语设计了三种类型的边：</p><ul><li>如果两个节点最初来自同一文档，则在这两个节点之间添加一条边(上图中的实节点)</li><li>如果表示两个节点的句子在问题中都具有命名实体或名词短语(可以是不同的)，则在来自不同文档的两个节点之间添加边。(图中的虚线)</li><li>如果表示两个节点的句子具有相同的命名实体或名词短语，则在来自不同文档的两个节点之间添加一条边。(图中的虚线)</li></ul><p>第一种类型的边的动机是希望GNN掌握每个文档中呈现的全局信息。</p><p>第二类和第三类边，为了以更好地捕捉这种跨文档推理路径。跨文档推理是通过从问题中的实体跳到未知的桥梁实体或比较问题中两个实体的属性来实现的 。</p><p>对于消息传递，使用具有门控机制的多关系GCN。</p><p>假设 $h^0_j$ 表示从语句嵌入 $S_j$的初始节点嵌入，则一跳(或一层)之后的节点嵌入计算可表示为:</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  h_j^{k+1} &= act(u_j^k) \odot g^k_j + h^k_j \odot (1-g^k_j) \\ u^k_j &= f_s(h^k_j) + \sum_{r\in R} \frac{1}{|N_j^r|} \sum_{n\in N^r_j} f_r(h_n^k)\\ g_j^k &= sigmoid (f_g([u_j^k; h^k_j]))     \end{split}\end{equation}</script><p>其中R 是一些列边类型， $N^r_j$ 是边类型为r的 j 节点的邻居。</p><p>$h^k_n$ 是节点n的第k层节点表示。</p><p>$f_r、f_s、f_g$中的每一个都定义了输入节点表示上的变换，并且可以使用MLP来实现。</p><p>门控$g_j^k$ 是由0和1之间的值组成的向量，用于控制来自计算的更新$u^k_j$ 或来自原始节点表示的信息。</p><p>函数$act$表示非线性激活函数。</p><p>最后得到每个节点的最终表示 $h_j$ 后用两层MLP 最终预测 。</p><p>$\hat y^{sp}<em>j = sigmoid(f</em>{sp}(h_j))$ </p><p>除了支持句子预测任务之外，还在GNN输出之上添加了另一个任务，以说明“Yes/No”类型的问题。</p><p>预测任务描述为3类(“Yes”、“No”和“span”)分类</p><p>引入：</p><p>$h = \sum_j a_jh_j$</p><p>$a = \sigma(\hat y^{sp})$</p><p>$\hat y^{ans} = f_{ans}(h)$</p><p>最终loss表达为：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  L = \gamma L^{span} + BCE(\hat y^{sp}, y^{sp}) + CE(\hat y^{ans}, y^{ans})    \end{split}\end{equation}</script><p>$BCE()$ 二元交叉熵函数</p><p>为了考虑不同损失的尺度差异，在跨度损失中加入了一个权重γ。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Select-Answer-and-Explain-Interpretable-Multi-hop-Reading-Comprehension-over-Multiple-Documents&quot;&gt;&lt;a href=&quot;#Select-Answer-and-Explain</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>DP分析法--01背包问题</title>
    <link href="http://example.com/2021/04/20/DP%E5%88%86%E6%9E%90%E6%B3%95%E2%80%94%E2%80%9401%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"/>
    <id>http://example.com/2021/04/20/DP%E5%88%86%E6%9E%90%E6%B3%95%E2%80%94%E2%80%9401%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/</id>
    <published>2021-04-20T01:14:15.000Z</published>
    <updated>2021-04-24T02:15:41.794Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DP分析法—01背包问题"><a href="#DP分析法—01背包问题" class="headerlink" title="DP分析法—01背包问题"></a>DP分析法—01背包问题</h1><p>从集合角度来分析DP问题，DP问题的题目一般都是从有限集中求得最值的问题。</p><p><img src="https://i.loli.net/2021/04/20/SlgJ96RzdyGp5fw.png" alt=""></p><p><a href="https://www.acwing.com/problem/content/2/">01背包问题</a></p><p>有 N 件物品和一个容量是 V 的背包。每件物品只能使用一次。</p><p>第 i 件物品的体积是 $v_i$，价值是 $w_i$。</p><p>求解将哪些物品装入背包，可使这些物品的总体积不超过背包容量，且总价值最大。<br>输出最大价值。</p><h4 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h4><p>第一行两个整数，N，V，用空格隔开，分别表示物品数量和背包容积。</p><p>接下来有 N 行，每行两个整数 $v_i,w_i$，用空格隔开，分别表示第 i 件物品的体积和价值。</p><h4 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h4><p>输出一个整数，表示最大价值。</p><h4 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h4><p>0&lt;N,V≤10000&lt;N,V≤1000<br>0&lt;vi,wi≤10000&lt;vi,wi≤1000</p><h4 id="输入样例"><a href="#输入样例" class="headerlink" title="输入样例"></a>输入样例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">4 5</span><br><span class="line">1 2</span><br><span class="line">2 4</span><br><span class="line">3 4</span><br><span class="line">4 5</span><br></pre></td></tr></table></figure><h4 id="输出样例："><a href="#输出样例：" class="headerlink" title="输出样例："></a>输出样例：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">8</span><br></pre></td></tr></table></figure><h2 id="解"><a href="#解" class="headerlink" title="解"></a>解</h2><p>最多$2^N$, 从$2^N$ 个方案里找总价值最大的方案。——有限集合的最值问题</p><p>状态表示：</p><ul><li><p>选择问题一般$f(i,j)$ 第一维表示前i个物品,第二维是限制 (经验)</p></li><li><p>集合：所有只考虑前i个物品，且总体积不超过j的选法的集合。</p></li><li>属性：集合中每一个方案的最大价值Max</li></ul><p>状态计算：</p><ul><li>所有不选第i个物品的方案 $f(i-1,j)$</li><li>所有选择第i个物品的方案 $f(i-1,j-v_i) + w_i$</li><li>$Max(f(i-1,j), f(i-1,j-v_i)+w_i)$</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 读入数据的代码</span></span><br><span class="line">        Scanner reader = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="comment">// 物品的数量为N</span></span><br><span class="line">        <span class="keyword">int</span> N = reader.nextInt();</span><br><span class="line">        <span class="comment">// 背包的容量为V</span></span><br><span class="line">        <span class="keyword">int</span> V = reader.nextInt();</span><br><span class="line">        <span class="comment">// 一个长度为N的数组，第i个元素表示第i个物品的体积；</span></span><br><span class="line">        <span class="keyword">int</span>[] v = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>] ;</span><br><span class="line">        <span class="comment">// 一个长度为N的数组，第i个元素表示第i个物品的价值；</span></span><br><span class="line">        <span class="keyword">int</span>[] w = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>] ;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span> ; i &lt;= N ; i++)&#123;</span><br><span class="line">            <span class="comment">// 接下来有 N 行，每行有两个整数:v[i],w[i]，用空格隔开，分别表示第i件物品的体积和价值</span></span><br><span class="line">            v[i] = reader.nextInt();</span><br><span class="line">            w[i] = reader.nextInt();</span><br><span class="line">        &#125;</span><br><span class="line">        reader.close() ;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 正式工作的代码</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        定义一个二阶矩阵dp[N+1][V+1],</span></span><br><span class="line"><span class="comment">        这里之所以要N+1和V+1，是因为第0行表示只能选择第0个物品的时候，即没有物品的时候</span></span><br><span class="line"><span class="comment">        第0列表示背包的体积为0的时候，即不能装任何东西的时候</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">        dp[i][j]表示在 只能选择前i个物品，背包容量为j的情况下，背包中物品的最大价值</span></span><br><span class="line"><span class="comment">        对于dp[i][j]有两种情况：</span></span><br><span class="line"><span class="comment">        1. 不选择当前的第i件物品/第i件物品比背包容量要大，则dp[i][j] = dp[i-1][j]</span></span><br><span class="line"><span class="comment">        2. 选择当前的第i件物品（潜在要求第i件物品体积小于等于背包总容量），则能装入的物品最大价值为：</span></span><br><span class="line"><span class="comment">            当前物品的价值 加上 背包剩余容量在只能选前i-1件物品的情况下的最大价值</span></span><br><span class="line"><span class="comment">            dp[i][j] = dp[i-1][j-v[i]] + w[i]</span></span><br><span class="line"><span class="comment">        dp[i][j]在两种情况中选择比较大的情况作为当前的最优解；</span></span><br><span class="line"><span class="comment">        即：</span></span><br><span class="line"><span class="comment">        if(j &gt;= v[i]):</span></span><br><span class="line"><span class="comment">            dp[i][j] = max(dp[i-1][j], dp[i-1][j-v[i]] + w[i])</span></span><br><span class="line"><span class="comment">        else:</span></span><br><span class="line"><span class="comment">            dp[i][j] = dp[i-1][j]</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[N+<span class="number">1</span>][V+<span class="number">1</span>];</span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= V; j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(j &gt;= v[i])&#123;</span><br><span class="line">                    dp[i][j] = Math.max(dp[i-<span class="number">1</span>][j], dp[i-<span class="number">1</span>][j-v[i]] + w[i]);</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(dp[N][V]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>优化后</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 读入数据的代码</span></span><br><span class="line">        Scanner reader = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="comment">// 物品的数量为N</span></span><br><span class="line">        <span class="keyword">int</span> N = reader.nextInt();</span><br><span class="line">        <span class="comment">// 背包的容量为V</span></span><br><span class="line">        <span class="keyword">int</span> V = reader.nextInt();</span><br><span class="line">        <span class="comment">// 一个长度为N的数组，第i个元素表示第i个物品的体积；</span></span><br><span class="line">        <span class="keyword">int</span>[] v = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>] ;</span><br><span class="line">        <span class="comment">// 一个长度为N的数组，第i个元素表示第i个物品的价值；</span></span><br><span class="line">        <span class="keyword">int</span>[] w = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>] ;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span> ; i &lt;= N ; i++)&#123;</span><br><span class="line">            <span class="comment">// 接下来有 N 行，每行有两个整数:v[i],w[i]，用空格隔开，分别表示第i件物品的体积和价值</span></span><br><span class="line">            v[i] = reader.nextInt();</span><br><span class="line">            w[i] = reader.nextInt();</span><br><span class="line">        &#125;</span><br><span class="line">        reader.close() ;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 正式算法的代码</span></span><br><span class="line">        <span class="comment">// 将dp优化为一维数组</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        注意，这里第二层循环的时候，还是小到大循环的话，那么</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">        dp[i][j] = Math.max(dp[i-1][j], dp[i-1][j-v[i]] + w[i])</span></span><br><span class="line"><span class="comment">        实际上变成了</span></span><br><span class="line"><span class="comment">        dp[i][j] = Math.max(dp[i][j], dp[i][j-v[i]] + w[i]);</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">        因为i-1的值已经在前面被更新过了，覆盖了</span></span><br><span class="line"><span class="comment">        为了避免这个问题，所以要逆序更新，即先更新第i个，然后更新第i-1个，从而保证第i-1个不被覆盖</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">        如果不逆序的话，输出结果为10，dp数组实际为：</span></span><br><span class="line"><span class="comment">        0 0 0 0 0 0 </span></span><br><span class="line"><span class="comment">        0 2 4 6 8 10</span></span><br><span class="line"><span class="comment">        0 2 4 6 8 10</span></span><br><span class="line"><span class="comment">        0 2 4 6 8 10</span></span><br><span class="line"><span class="comment">        0 2 4 6 8 10</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[V+<span class="number">1</span>];</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = V; j &gt;= v[i]; j--)&#123;</span><br><span class="line">                dp[j] = Math.max(dp[j], dp[j-v[i]] + w[i]);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// for(int j = 0; j &lt;= V; j++)&#123;</span></span><br><span class="line">            <span class="comment">//     System.out.print(dp[j]);</span></span><br><span class="line">            <span class="comment">//     System.out.print(&quot; &quot;);</span></span><br><span class="line">            <span class="comment">// &#125;</span></span><br><span class="line">            <span class="comment">// System.out.print(&quot;\n&quot;);</span></span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(dp[V]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">o1bagSolutionOptimization</span><span class="params">(<span class="keyword">int</span>[] weight, <span class="keyword">int</span>[] value, <span class="keyword">int</span> bagWeight)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> num = weight.length;</span><br><span class="line">    <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[bagWeight + <span class="number">1</span>];</span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= num; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = bagWeight; j &gt;= <span class="number">1</span>; j--) &#123;</span><br><span class="line">            <span class="keyword">if</span> (j &gt;= weight[i - <span class="number">1</span>]) &#123;</span><br><span class="line">                dp[j] = Math.max(dp[j], dp[j - weight[i - <span class="number">1</span>]] + value[i - <span class="number">1</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dp[bagWeight];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Scanner sc = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">    <span class="keyword">int</span> itemsNumber = sc.nextInt();</span><br><span class="line">    <span class="keyword">int</span> bagWeight = sc.nextInt();</span><br><span class="line">    <span class="keyword">int</span>[][] arr = <span class="keyword">new</span> <span class="keyword">int</span>[itemsNumber][<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">int</span>[] weight = <span class="keyword">new</span> <span class="keyword">int</span>[itemsNumber];</span><br><span class="line">    <span class="keyword">int</span>[] value = <span class="keyword">new</span> <span class="keyword">int</span>[itemsNumber];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; itemsNumber; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; j++) &#123;</span><br><span class="line">            arr[i][j] = sc.nextInt();</span><br><span class="line">        &#125;</span><br><span class="line">        weight[i] = arr[i][<span class="number">0</span>];</span><br><span class="line">        value[i]=   arr[i][<span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(o1bagSolutionOptimization(weight, value, bagWeight));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="完全背包问题"><a href="#完全背包问题" class="headerlink" title="完全背包问题"></a>完全背包问题</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> 完全背包问题 </span>&#123;</span><br><span class="line">    <span class="comment">// 完全背包和01背包的区别是完全背包中每个物品可以用无限次</span></span><br><span class="line"><span class="comment">// 01背包：f[i][j] = max(f[i-1][j], f[i-1][j-v]+w)</span></span><br><span class="line"><span class="comment">// 完全背包：f[i][j] = max(f[i-1][j], f[i][j-v]+w)</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Scanner reader = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="keyword">int</span> N = reader.nextInt();</span><br><span class="line">        <span class="keyword">int</span> V = reader.nextInt();</span><br><span class="line">        <span class="keyword">int</span>[] v = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span>[] w = <span class="keyword">new</span> <span class="keyword">int</span>[N + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++) &#123;</span><br><span class="line">            v[i] = reader.nextInt();</span><br><span class="line">            w[i] = reader.nextInt();</span><br><span class="line">        &#125;</span><br><span class="line">        reader.close();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[V + <span class="number">1</span>];</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= V; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (j &gt;= v[i]) &#123;</span><br><span class="line">                    dp[j] = Math.max(dp[j], dp[j - v[i]] + w[i]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(dp[V]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// int nativeDp(int n,int m)&#123;</span></span><br><span class="line">    <span class="comment">//     int[] f = new int[maxN];</span></span><br><span class="line">    <span class="comment">//     for(int i=1;i&lt;=n;i++)&#123;</span></span><br><span class="line">    <span class="comment">//         for(int j=m;j&gt;=v[i];j--)&#123;</span></span><br><span class="line">    <span class="comment">//             for(int k=0;k*v[i]&lt;=j;k++)&#123;</span></span><br><span class="line">    <span class="comment">//                 f[j] = Math.max(f[j], f[j-k*v[i]]+k*w[i]);</span></span><br><span class="line">    <span class="comment">//             &#125;</span></span><br><span class="line">    <span class="comment">//         &#125;</span></span><br><span class="line">    <span class="comment">//     &#125;</span></span><br><span class="line">    <span class="comment">// &#125;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;DP分析法—01背包问题&quot;&gt;&lt;a href=&quot;#DP分析法—01背包问题&quot; class=&quot;headerlink&quot; title=&quot;DP分析法—01背包问题&quot;&gt;&lt;/a&gt;DP分析法—01背包问题&lt;/h1&gt;&lt;p&gt;从集合角度来分析DP问题，DP问题的题目一般都是从有限集中求</summary>
      
    
    
    
    
    <category term="刷题" scheme="http://example.com/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>LongFormer:The Long-Document Transformer</title>
    <link href="http://example.com/2021/04/18/LongFormer-The-Long-Document-Transformer/"/>
    <id>http://example.com/2021/04/18/LongFormer-The-Long-Document-Transformer/</id>
    <published>2021-04-18T06:40:10.000Z</published>
    <updated>2021-04-18T16:00:17.349Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LongFormer-The-Long-Document-Transformer"><a href="#LongFormer-The-Long-Document-Transformer" class="headerlink" title="LongFormer:The Long-Document Transformer"></a>LongFormer:The Long-Document Transformer</h1><p>主要记录一些Longfromer的原理和使用时的细节。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>针对的问题：</p><ul><li>基于Transformer的模型，由于self-attention的操作，导致不能处理很长的序列。</li><li>self-attention的处理规模和序列长度是成二次关系的。</li></ul><p><img src="https://i.loli.net/2021/04/18/3YtkrO18p2dAvmV.png" alt=""></p><p>因为self-attention对于每个token都要计算打分，也就是缩放点积中的$QK^T$ 矩阵运算。</p><p>这相当于对每个token之间都照顾到了注意信息。</p><p>每个token代表一个小格，自注意力机制的QK都是自己，所以是个正方形。</p><p>为解决这个问题，作者引入了三种具有随序列长度线性缩放的注意机制，将规模缩减成线性。</p><p>分别是局部窗口注意和任务激活的全局注意力。</p><p>并且还提供了LongFormer的预训练模型。</p><p>定义了生成结构为Long-Forward-Encoding-Decoder(LED) </p><h2 id="引入-amp-相关工作"><a href="#引入-amp-相关工作" class="headerlink" title="引入&amp;相关工作"></a>引入&amp;相关工作</h2><p>熟知的Bert等预训练模型，最大长度为512，多的就要截断，这样可能会潜在地导致重要的跨分区信息丢失问题。</p><p>然而当时已有的针对解决长文本的方法，都是基于自回归语言模型的。</p><p>而LongFormer是可以应用于迁移学习环境中的文档级NLP任务的。</p><p><img src="https://i.loli.net/2021/04/18/KNJa3dZx6fCG8eH.png" alt=""></p><p>之后可能会读几篇。ltr从左到右的模型，其受益于双向语境(自回归或从左到右的语言建模被粗略地定义为在给定输入序列中的先前符号/字符的情况下估计现有符号/字符的概率分布)。</p><p>spare代表模型通过稀疏性来进行优化。</p><p>Generating long se-quences with sparse transformers.其使用由BlockSparse提供的大小为8x8的块的扩展滑动窗口的形式，但没有探索预训练设置。等等</p><h2 id="LongFormer"><a href="#LongFormer" class="headerlink" title="LongFormer"></a>LongFormer</h2><p>原始Transformer的自注意力机制有$O(n^2)$ 的时间和空间内存复杂度。</p><p>为了解决这个问题，作者根据指定相互关注的输入位置对的“注意模式”来稀疏完整的自我注意矩阵</p><p>与full self-attention不同的是，提出的注意力模式与输入序列成线性关系，这使得它对较长的序列是有效的。</p><h3 id="注意力模式"><a href="#注意力模式" class="headerlink" title="注意力模式"></a>注意力模式</h3><h4 id="滑动窗口-Sliding-Window"><a href="#滑动窗口-Sliding-Window" class="headerlink" title="滑动窗口 (Sliding Window)"></a>滑动窗口 (Sliding Window)</h4><p>设固定窗口大小为 w，transformer层数为$l$, token的每边 $\frac{1}{2}w$  计算复杂度为$O(n\times w)$</p><p><img src="https://i.loli.net/2021/04/18/XaDokntURBWdNSe.png" alt=""></p><p>作者认为：根据应用程序的不同，为每个图层使用不同的w值可能有助于在效率和模型表达能力之间取得平衡。</p><h4 id="空洞滑窗-Dilated-Sliding-Window"><a href="#空洞滑窗-Dilated-Sliding-Window" class="headerlink" title="空洞滑窗(Dilated Sliding Window)"></a>空洞滑窗(Dilated Sliding Window)</h4><p>类似于CNN的空洞卷积</p><p>空洞尺寸 $d$ 感受野是 $l\times d\times w$</p><p><img src="https://i.loli.net/2021/04/18/SxDhujGwCVIvt2g.png" alt=""></p><p>在多头注意力中，每个注意力头部计算不同的注意力分数。</p><p>作者发现，每个头具有不同扩张配置设置的话效果会好：</p><p>允许一些没有空洞的头部专注于局部语境，而另一些带空洞的则专注于更长的语境，从而提高了性能。</p><h4 id="全局注意力-Global-Attention"><a href="#全局注意力-Global-Attention" class="headerlink" title="全局注意力(Global Attention)"></a>全局注意力(Global Attention)</h4><p><img src="https://i.loli.net/2021/04/18/tVGNpUa3o9gIluf.png" alt=""></p><p>例如对于QA，问题和文档连接在一起，允许模型通过自我关注将问题与文档进行比较。</p><p>有时需要使用特殊的全局CLS作为整体的表达，所以就需要再这某些个关键点地方计算全局注意力，关注每一个token。其他的还是滑窗的形式。</p><p>我们在几个预先选择的输入位置添加了“全局关注”。</p><p>由于这样的记号token的数量相对于n很小，并且与n无关，因此组合的局部和全局注意的复杂度仍然是O(N)。</p><p>这时，计算打分函数就可以分为两组QKV，分别是全局的$Q_g,K_g,V_g$ 和 滑窗局部的 $Q_s,K_s,V_s$</p><p>昂贵的运算是矩阵乘法 $QK^T$，因为Q和K都具有n(序列长度)投影。对于LongFormer，空洞滑动窗口注意只计算固定数量$QK^T$的对角线。</p><p>在实现的时候主要用到了带状乘法。还定制了特别的CUDA内核。。</p><h3 id="对于自回归的语言模型"><a href="#对于自回归的语言模型" class="headerlink" title="对于自回归的语言模型"></a>对于自回归的语言模型</h3><p>可以使用空洞滑动窗口注意力，并且可以跨层使用不同尺寸的窗口，效果可能更佳。</p><p>对较低层使用较小的窗口大小，并在移动到较高层时增加窗口大小</p><p>这允许顶层了解整个序列的较高级别表示，同时使较低层捕获本地信息。此外，它还在效率和性能之间取得平衡。</p><p>(窗口大小越小，非零值越少，计算开销越小)</p><p>(窗口大小越大，表示能力更丰富，通常会带来性能提升)</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>和训练长文本的模型进行对比 ，BPC值越小越好</p><p><img src="https://i.loli.net/2021/04/18/Oxo2A1SCsaIeDL8.png" alt=""></p><h2 id="在QA上的Finetuning"><a href="#在QA上的Finetuning" class="headerlink" title="在QA上的Finetuning"></a>在QA上的Finetuning</h2><p>分别采用了我比较关注的多文档数据集 WikiHop/HotpotQA(干扰榜)/TriviaQA</p><p>将问题和文档连接成一个长序列放入Longformer，最后加一个预测层。</p><p><img src="https://i.loli.net/2021/04/18/lqTB5cSPrD8Z3w7.png" alt=""></p><h3 id="WikiHop"><a href="#WikiHop" class="headerlink" title="WikiHop"></a>WikiHop</h3><p>数据特点：</p><ul><li><p>候选答案个数由2个到79个不等。</p></li><li><p>文章段落数量由3段到63段不等</p></li></ul><p>数据集不为多跳推理链提供任何中间注释，需要模型代之以从间接答案监督中推断它们。</p><p>数据预处理：</p><p>将问题和答案与特殊令牌连接在一起</p><p>$ [q] question [/q] [ent] candidate1 [/ent] … [ent] candidateN [/ent] $</p><p>上下文也是使用文档分隔符进行间隔</p><p>$&lt;/s&gt; context1 &lt;/s&gt; … &lt;/s&gt; contextM &lt;/s&gt;$</p><p>在准备好输入数据后，从每个模型的顶层开始计算活动。获取问题和答案候选并将它们连接到尽可能多的上下文直到模型序列长度(Roberta为512，LongFormer为4,096)，在模型中运行序列，收集输出激活，并重复，直到用尽所有上下文(除了LongFormor-Large之外的所有模型，由于存储器要求，我们只包括第一个4,096长度的序列)。然后，将所有块的所有激活连接成一个长序列。在Longformer的下，使用全局注意力来关注整个问答候选序列。</p><p>最终预测，对每个[ent] 附加一个线性层，输出一个logit，最后平均所有候选答案的logits。 用softmax和交叉熵得出最终答案。</p><p>优化策略：</p><p>Adam、Linear warmup超过200梯度更新对于最大LR，然后linear decay剩余训练。</p><p>使用梯度累积最终batch达到32</p><p>其他超参Dropout weight decay 都和Roberta相同。</p><p>对LR[2e-5，3e-5，5e-5]和epoch[5，10，15]进行网格搜索。</p><p>LR=3e-5，15个epoch是最好的Longform-Base配置。</p><h3 id="TriviaQA"><a href="#TriviaQA" class="headerlink" title="TriviaQA"></a>TriviaQA</h3><p>TriviaQA有超过10万个问题、答案、文档。</p><p>文档是维基百科文章，答案是文章中提到的命名实体。</p><p>回答问题的跨度没有注释，但可以使用简单的文本匹配找到它。</p><p>数据预处理：</p><p>$[s] question [/s]document [/s]$</p><p>在所有问题符号上都使用全局注意力。</p><h2 id="HotpotQA"><a href="#HotpotQA" class="headerlink" title="HotpotQA"></a>HotpotQA</h2><p>使用两阶段首先确定相关段落，然后确定最终答案范围和证据。</p><p>这主要是因为首先删除分散注意力的段落，可以降低最终认识和范围检测的噪声，这一点也被发现非常重要此数据集中最新的最新方法。</p><p>数据预处理：</p><p>$[CLS] [q] question [/q] ⟨t⟩ title1 ⟨/t⟩ sent1,1 [s] sent1,2 [s] …⟨t⟩ title2 ⟨/t⟩ sent2,1 [s] sent2,2 [s] …$</p><p>使用全局注意力来问句标记、段落计时开始标记以及句子标记。</p><p>在段落标题顶部增加了前馈层，用于预测相关段落的开始标记，以及用于预测证据句子的句子标记。</p><p>在对第一阶段模型进行训练后，预测了训练集和开发集的相关段落得分。然后，保留最多5个原始得分高于预先指定的阈值(-3.0)的段落，并从上下文中删除其他段落。然后，根据得到的缩短上下文训练第二阶段模型。</p><p>将跨度、问题分类、句子和段落损失结合起来，使用线性损失组合对模型进行多任务训练。</p><p>使用ADAM优化器对模型进行了训练，并进行了线性warmup(1000步)和线性衰减。我们使用最小超参数调整，使用3E-5和5E-5的LR和3到7的epoch，发现LR为3E-5和5个历元的模型效果最好。</p><p><img src="https://i.loli.net/2021/04/19/LuOCHUx1eMPwDW9.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;LongFormer-The-Long-Document-Transformer&quot;&gt;&lt;a href=&quot;#LongFormer-The-Long-Document-Transformer&quot; class=&quot;headerlink&quot; title=&quot;LongFormer:T</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle上传dataset的方法</title>
    <link href="http://example.com/2021/04/15/Kaggle%E4%B8%8A%E4%BC%A0dataset%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <id>http://example.com/2021/04/15/Kaggle%E4%B8%8A%E4%BC%A0dataset%E7%9A%84%E6%96%B9%E6%B3%95/</id>
    <published>2021-04-15T04:34:33.000Z</published>
    <updated>2021-04-15T04:50:41.636Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kaggle快速上传dataset的方法"><a href="#Kaggle快速上传dataset的方法" class="headerlink" title="Kaggle快速上传dataset的方法"></a>Kaggle快速上传dataset的方法</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>从国内上传到有cdn的地方(如GitHub), 再在kaggle的kernel上下载下来，直接上传dataset。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>首先需要掌握kaggle-api的使用，kaggle-api是kaggle官方提供的命令行工具，可以从命理完成比赛数据的下载、dataset下载上传，获取榜单等操作。</p><p><a href="https://github.com/Kaggle/kaggle-api">https://github.com/Kaggle/kaggle-api</a></p><p>本地安装：pip install kaggle</p><p>Kaggle已经安装好了，不用再安装</p><p>步骤1：下载账户API json</p><p><a href="https://www.kaggle.com/me/account">https://www.kaggle.com/me/account</a></p><p>步骤2：在页面创建一个dataset</p><p><a href="https://www.kaggle.com/datasets">https://www.kaggle.com/datasets</a></p><p>步骤3：下载dataset的metadata</p><p>运行：kaggle datasets metadata shopee-models</p><p>步骤4：下载数据集并上传到dataset</p><p>完整代码：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将API json文件写到这里</span></span><br><span class="line">!mkdir /root/.kaggle</span><br><span class="line">lines = <span class="string">&#x27;&#x27;</span><span class="string">&#x27;&#123;&quot;username&quot;:&quot;写你的用户名&quot;,&quot;key&quot;:&quot;写你的key&quot;&#125;&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">with open(<span class="string">&#x27;/root/.kaggle/kaggle.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) as up:    </span><br><span class="line">up.write(lines)</span><br><span class="line"><span class="comment"># 创建文件夹，写入dataset的metadata</span></span><br><span class="line">!mkdir hubmapkidneysegmentation</span><br><span class="line">lines = <span class="string">&#x27;&#x27;</span><span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">&quot;id&quot;: &quot;finlay/shopee-models&quot;,</span></span><br><span class="line"><span class="string">&quot;id_no&quot;: 122348,</span></span><br><span class="line"><span class="string">&quot;title&quot;: &quot;shopee_models&quot;,</span></span><br><span class="line"><span class="string">&quot;subtitle&quot;: &quot;&quot;,</span></span><br><span class="line"><span class="string">&quot;description&quot;: &quot;&quot;,</span></span><br><span class="line"><span class="string">&quot;keywords&quot;: [],</span></span><br><span class="line"><span class="string">&quot;resources&quot;: []</span></span><br><span class="line"><span class="string">&#125;&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">with open(<span class="string">&#x27;hubmapkidneysegmentation/dataset-metadata.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) as up:</span><br><span class="line">up.write(lines)</span><br><span class="line"><span class="comment"># 下载文件，这里用axel多线程下载，直接用wget也可以的。</span></span><br><span class="line">!apt-get install axel</span><br><span class="line">!axel -n 12 https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth -o hubmapkidneysegmentation/baseline_fold0_densenet_224_epoch50.pth</span><br><span class="line"><span class="comment"># 上传文件，这里会覆盖上传</span></span><br><span class="line">!kaggle datasets version -p ./hubmapkidneysegmentation -m <span class="string">&quot;Updated data fcn&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Kaggle快速上传dataset的方法&quot;&gt;&lt;a href=&quot;#Kaggle快速上传dataset的方法&quot; class=&quot;headerlink&quot; title=&quot;Kaggle快速上传dataset的方法&quot;&gt;&lt;/a&gt;Kaggle快速上传dataset的方法&lt;/h1&gt;&lt;</summary>
      
    
    
    
    
    <category term="DataGame" scheme="http://example.com/tags/DataGame/"/>
    
  </entry>
  
  <entry>
    <title>DUMA: Reading Comprehension with Transposition Thinking</title>
    <link href="http://example.com/2021/04/14/DUMA-Reading-Comprehension-with-Transposition-Thinking/"/>
    <id>http://example.com/2021/04/14/DUMA-Reading-Comprehension-with-Transposition-Thinking/</id>
    <published>2021-04-14T04:21:00.000Z</published>
    <updated>2021-04-18T16:04:16.645Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DUMA-Reading-Comprehension-with-Transposition-Thinking"><a href="#DUMA-Reading-Comprehension-with-Transposition-Thinking" class="headerlink" title="DUMA: Reading Comprehension with Transposition Thinking"></a>DUMA: Reading Comprehension with Transposition Thinking</h1><p>DUMA：DUal Multi-head Co-Attention model</p><p>这是一篇针对解决多项选择任务的MRC网络结构。题目中的Transposition Think，被作者赋义为分别从文章和问题的角度来考虑对方的关注点。</p><p>主要特点：</p><ul><li>基于预训练语言模型(得到表示编码，替代复杂的匹配网络)</li><li>衔接多层co-attention(从三元组中捕捉关系)</li></ul><p>多项选择任务可以抽象为(文章P，问题q，选项a) 三元组。</p><p>针对多项选择的特点多项选择MRC尤其依赖于匹配网络的设计，它被认为是有效地捕捉文章、问题和答案三元组之间的关系。(不能只考虑推理如何做的更好，还要考虑答案出现的关键位置也就是匹配网络的作用)</p><p>文中总结的人在做阅读理解题时的特点：</p><ul><li>快速通读文章的整体内容，问题和回答选项，以建立全局印象，然后进行换角度思考过程。</li><li>根据问答选项的特有信息，重新考虑文章的细节，收集问答选项的支持证据。</li><li>根据文章中的特有信息，重新考虑问题和答案选项，以确定正确的选项，排除错误的选项。</li></ul><p>当人们重读文章时，他们倾向于根据对问答选项的印象提取关键信息，重读问答选项时也是如此</p><hr><h2 id="DUMA"><a href="#DUMA" class="headerlink" title="DUMA"></a>DUMA</h2><p>多项选择问题可以定义模型需要学习一个概率分布$F(A_1,A_2,…,A_t|P,Q)$</p><p><img src="https://i.loli.net/2021/04/14/IE9asGiRTlLJNV2.png" alt=""></p><p>Encoder 接受文本输入生成一个全局序列表达，这个过程类似人类第一次阅读整个内容以获得总体印象。</p><p>Decoder则收集所有信息的答案预测以选择正确答案选项。</p><p>DUMA层位于encoder和decoder之间，意在模仿人类转换思考角度的过程，从问题文章和关键词中捕捉关系信息。</p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>作者用的是PrLMs，其将文章、问题和所有不同的候选答案拼接作为输入。</p><p>$P=[p_1,p_2,..,p_m]$    ， $Q=[q_1,q_2,…,q_n]$ ,   $A=[a_1,a_2,…,a_k]$</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  E = Enc(P \oplus Q \oplus A ) \end{split}\end{equation}</script><p>这个输入到预训练的方式可能会遇到点问题，一般预训练语言模型比如bert都会限制一个输入的大小，如果文章过长的话，模型看不到问题和选项可能会导致训练效果不佳。可以改为 Q、A、P的形式，因为一般Q和A都比较短。</p><p>$E = [e<em>1,e_2,…,e</em>{m+n+k}]$  </p><p>$e<em>i$ 为固定维度$d</em>{model}$ 的向量，是各自的token。</p><h3 id="Dual-Multi-head-Co-Attention"><a href="#Dual-Multi-head-Co-Attention" class="headerlink" title="Dual Multi-head Co-Attention"></a>Dual Multi-head Co-Attention</h3><p>使用双多头共同注意模型来计算文章和问答的attention表征。(可堆叠k层)</p><p>其实就是一个多头co-attention，定义一个Q、K、V (Q不是上面的问题Q)</p><p>先从E中分离出$E^P = [e^P<em>1,e^P_2,…,E^P</em>{t<em>p}]$、$E^{QA} = [e^{qA},e^{qA},…,E^{qA}</em>{t_{q_a}}]$</p><p>使用两种计算attention的方法：</p><ul><li><p>$E^P$ 做Query ，$E^{QA}$ 做 Key和Value</p></li><li><p>$E^{QA}$ 做Query ，$E^{P}$ 做 Key和Value</p></li></ul><script type="math/tex; mode=display">\begin{equation}\begin{split}  Attention(E^P,E^{QA},E^{QA}) &= softmax(\frac{E^P(E^{QA})^T}{\sqrt{d_k}})E^{QA}\\ head_i &= Attention(E^PW^Q_i,E^{QA}W^K_i)\\ MIIA(E^P, E^{QA}, E^{QA}) &= Concat(head_1,head_2,...,head_h) W^O\\ MHA_1 &= MHA(E^P, E^{QA}, E^{QA}) \\ MHA_2 &= MHA(E^{QA}, E^{P}, E^P) \\ DUMA (E^P, E^{QA}) &= Fuse(MHA_1,MHA_2)\\    \end{split}\end{equation}</script><p>其中$W<em>i^Q \in R^{d</em>{model} \times d<em>q}$ 、 $W_i^K \in R^{d</em>{model} \times d<em>k}$、  $W_i^V \in R^{d</em>{model} \times d<em>q}$ 、$W_i^O \in R^{hd_v \times d</em>{model}}$  : h 头数</p><p>$MHA$: 多头注意力</p><p>$Fuse$ 函数先使用均值池化来汇集$MHA(·)$的序列输出，然后再聚合两个池化的输出。</p><p>后文实验了三种聚合方法 元素乘法  元素相加  concat</p><p>表示在决定哪个是最佳答案选项之前，对所有关键信息进行混合。</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><script type="math/tex; mode=display">\begin{equation}\begin{split}  O_i &= DUMA(E^P, E^{QA_i}) \\ L(A_r|P,Q) &= -log\frac{exp(W^TO_r)}{\sum_{i=1}^s exp(W^TO_i)} \end{split}\end{equation}</script><p>s 是选项数量</p><h2 id="Multi-choice-MRC数据集"><a href="#Multi-choice-MRC数据集" class="headerlink" title="Multi-choice MRC数据集"></a>Multi-choice MRC数据集</h2><p>DREAM and RACE</p><p><img src="https://i.loli.net/2021/04/14/63cBOaFhfGIgd58.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://i.loli.net/2021/04/14/3tQ1BCvzHSo9bTU.png" alt=""></p><p><img src="https://i.loli.net/2021/04/14/NchfAZRWxCIuQeS.png" alt=""></p><p><img src="https://i.loli.net/2021/04/14/d6JDXcVaTEmZPLF.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;DUMA-Reading-Comprehension-with-Transposition-Thinking&quot;&gt;&lt;a href=&quot;#DUMA-Reading-Comprehension-with-Transposition-Thinking&quot; class=&quot;hea</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>FREELB: ENHANCED ADVERSARIAL TRAINING FOR NATURAL LANGUAGE UNDERSTANDING</title>
    <link href="http://example.com/2021/04/09/FREELB-ENHANCED-ADVERSARIAL-TRAINING-FOR-NATURAL-LANGUAGE-UNDERSTANDING/"/>
    <id>http://example.com/2021/04/09/FREELB-ENHANCED-ADVERSARIAL-TRAINING-FOR-NATURAL-LANGUAGE-UNDERSTANDING/</id>
    <published>2021-04-09T11:56:02.000Z</published>
    <updated>2021-04-18T16:01:34.644Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FreeLB-Enhanced-Adversarial-Training-For-Natural-Language-Understanding"><a href="#FreeLB-Enhanced-Adversarial-Training-For-Natural-Language-Understanding" class="headerlink" title="FreeLB: Enhanced Adversarial Training For Natural Language Understanding"></a>FreeLB: Enhanced Adversarial Training For Natural Language Understanding</h1><p><a href="https://coding-zuo.github.io/2021/04/07/nlp%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83-%E4%B8%8Ebert%E7%BB%93%E5%90%88/">nlp中的对抗训练</a></p><p>承接上文，上文主要讲对抗训练的原理与物理意义与发展，对抗性训练是创建健壮神经网络的一种方法。在对抗性训练期间，小批次的训练样本受到对抗性扰动的污染，然后用于更新网络参数，直到得到的模型学会抵抗此类攻击，并且对模型起到了正则化的效果，提高模型泛化能力并且防止过拟合。</p><p>这篇论文结合现在流行的预训练模型或transformer模型只能结合到下游任务的embedding中。</p><p>提出FreeLB算法在GLUE上结合Roberta达到了当时的SOTA，是基于Transformer的自然语言理解和常识推理任务模型来做对抗。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>对抗性训练可以最小化标签保留输入扰动的最大风险，已被证明对提高语言模型的泛化能力是有效的。</p><p>FreeLB (Free Large-Batch)，通过在单词嵌入中添加对抗性扰动，并最小化输入样本周围不同区域内的对抗性风险，从而提高了嵌入空间的不变性。</p><p>在GLUE基准上的实验表明，当仅应用于精调阶段时，它能够将BERT-BASE模型的整体测试分数从78.3提高到79.4，将Roberta-Large模型的测试分数从88.5提高到88.8。</p><h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><p>针对PGD算法的问题：当K较小时，基于PGD的对抗性训练仍然会导致高度卷积和非线性的损失面，在更强的对手下很容易被打破，当K大时计算开销又很大。</p><p>利用最近提出的“Free”训练策略在不同范数约束下用多样化的对抗样本来丰富训练数据，</p><p>“Free”的对抗性训练算法在一次反向传递中同时更新模型参数和对抗性扰动。</p><p>还使用将大部分对抗性更新限制在第一层，有效减少的对抗过程正反向传播总量。</p><p>比PGD计算成本小，能在大规模的预训练模型上进行对抗训练。</p><h2 id="文本的对抗样本-对手"><a href="#文本的对抗样本-对手" class="headerlink" title="文本的对抗样本(对手)"></a>文本的对抗样本(对手)</h2><ol><li>黑盒环境下对embedding进行扰动(对手不是从样本进行攻击)</li><li>在输入中添加分散注意力的句子(人工)</li><li>用GANs将输入投影到潜在空间，并搜索接近原始的文本对手</li></ol><p><img src="https://i.loli.net/2021/04/09/daKoDQkv35hwmEP.png" alt=""></p><p>第二三算是一种辅助模型，数据增强的一种形式。</p><p>如何在没有人工评估的情况下通过单词/字符替换来构建保留标签的对抗性示例仍然不清楚，因为每个单词/字符的含义取决于上下文。</p><p>所以主要还是采用第一种进行对抗训练。</p><p>因为词的输入表达有很多种，像词embedding、句子embedding和位置embedding。作者和其他对抗训练一样只干扰词embedding和拼接词的embedding。</p><p>注意，基于Embedding的对手严格来说比更传统的基于文本的对手更强大，因为对手可以在单词嵌入上进行在文本域中不可能进行的操作。因为CV都是从样本层面进行扰动，这个扰动从embedding上扰动，相当于在更高级的层面，所以更强大。</p><h2 id="FreeLB"><a href="#FreeLB" class="headerlink" title="FreeLB"></a>FreeLB</h2><p>此前预训练语言模型对于下游任务已被证实很有效。</p><p>作者的目标是通过在下游语言理解任务的精调过程中增强它们在嵌入空间中的鲁棒性，进一步提高这些预先训练的语言模型在下游语言理解任务上的泛化能力。</p><p>由于这篇论文只对对抗性训练的效果感兴趣，而不是产生实际的对抗性示例，因此使用基于梯度的方法在输入句子的嵌入中添加范数有界的对抗性扰动。</p><p>定义模型的输入One-hot向量为 $ Z=[z_1,z_2,…,z_n]$</p><p>嵌入矩阵为V</p><p>语言模型看成是一个 $y=f_{\theta}(X), X=VZ$ , y是模型输出 $\theta$是可学习参数。</p><p>定义对抗扰动为 $\delta$ </p><p>新的预测输出变为 $y’=f_{\theta}(X+\delta)$</p><p>为了保持语义，我们将δ的范数限制为较小，并假设模型的预测在扰动后不会改变。</p><p>上面的定义和其他人的研究基本都是相同的，FreeLB区别在于不要求X归一化。</p><p>FreeLB吸取了FreeAT和YOPO加速方法, 几乎不需要任何开销就可以获得参数的梯度。实现了与标准PGD训练模型相当的健壮性和泛化能力，只使用与自然训练相同或略多的正反向传播。</p><h3 id="FreeAT-Free-Adversarial-Training-NIPS2019"><a href="#FreeAT-Free-Adversarial-Training-NIPS2019" class="headerlink" title="FreeAT (Free Adversarial Training): NIPS2019"></a>FreeAT (Free Adversarial Training): NIPS2019</h3><p>从FGSM到PGD，主要是优化对抗扰动的计算，虽然取得了更好的效果，但计算量也一步步增加。对于每个样本，FGSM和FGM都只用计算两次，一次是计算x的前后向，一次是计算x+r的前后向。而PGD则计算了K+1次，消耗了更多的计算资源。因此FreeAT被提了出来，在PGD的基础上进行训练速度的优化。</p><p>FreeAT的思想是在对每个样本x连续重复m次训练，计算r时复用上一步的梯度，为了保证速度，整体epoch会除以m。r的更新公式为：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  r_{t+1} = r_t + \epsilon \cdot sign(g)    \end{split}\end{equation}</script><p>伪代码：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">初始化r=0</span><br><span class="line">对于epoch=1...N/m:</span><br><span class="line">  对于每个x:</span><br><span class="line">    对于每步m:</span><br><span class="line">      1.利用上一步的r，计算x+r的前后向，得到梯度</span><br><span class="line">      2.根据梯度更新参数</span><br><span class="line">      3.根据梯度更新r</span><br></pre></td></tr></table></figure><p>缺点：FreeLB指出，FreeAT的问题在于每次的r对于当前的参数都是次优的（无法最大化loss），因为当前r是由r(t-1)和theta(t-1)计算出来的，是对于theta(t-1)的最优。</p><p>代码：<a href="https://link.zhihu.com/?target=https%3A//github.com/mahyarnajibi/FreeAdversarialTraining/blob/d70774030871fa3207e09ce8528c1b84cd690603/main_free.py%23L160">https://github.com/mahyarnajibi…</a></p><h3 id="YOPO-You-Only-Propagate-Once-NIPS2019"><a href="#YOPO-You-Only-Propagate-Once-NIPS2019" class="headerlink" title="YOPO (You Only Propagate Once): NIPS2019"></a>YOPO (You Only Propagate Once): NIPS2019</h3><p>代码：<a href="https://link.zhihu.com/?target=https%3A//github.com/a1600012888/YOPO-You-Only-Propagate-Once">https://github.com/a1600012888/YOPO-You-Only-Propagate-Once</a></p><p>可以参考<a href="https://zhuanlan.zhihu.com/p/95904001">加速对抗训练——YOPO算法浅析</a></p><p>极大值原理PMP(Pontryagin’s maximum principle)是optimizer的一种，它将神经网络看作动力学系统。这个方法的优点是在优化网络参数时，层之间是解藕的。通过这个思想，我们可以想到，既然扰动是加在embedding层的，为什么每次还要计算完整的前后向传播呢？</p><p>基于这个想法，作者想复用后几层的梯度，假设p为定值：</p><p><img src="https://www.zhihu.com/equation?tex=p+%3D+%5Cnabla_%7Bg_%7B%5Ctilde%5Ctheta%7D%7D%28l%28g_%7B%5Ctilde%5Ctheta%7D%28f_0%28x_i%2Br_i%5E%7Bj%2C0%7D%2C+%5Ctheta_0%29%29%2Cy_i%29%29%5Ccdot%5Cnabla_%7Bf_0%7D%28g_%7B%5Ctilde%5Ctheta%7D%28f_0%28x_i%2Br_i%5E%7Bj%2C0%7D%2C+%5Ctheta_0%29%29%29+%5C%5C" alt="[公式]"></p><p>则对r的更新就可以变为</p><p><img src="https://www.zhihu.com/equation?tex=r_i%5E%7Bj%2Cs%2B1%7D+%3D+r_i%5E%7Bj%2Cs%7D%2B%5Calpha_1p%5Ccdot%5Cnabla_%7Br_i%7Df_0%28x_i%2Br_i%5E%7Bj%2Cs%7D%2C%5Ctheta_0%29+%5C%5C" alt="[公式]"></p><p>我们可以先写出YOPO的梯度下降版本：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">对于每个样本x</span><br><span class="line">初始化r(1,0)</span><br><span class="line">对于j=1,2,...,m:</span><br><span class="line">  1.根据r(j,0),计算p</span><br><span class="line">  对于s=0,1,...,n-1:</span><br><span class="line">    2.计算r(j,s+1)</span><br><span class="line">  3.另r(j+1,0)=r(j,n)</span><br></pre></td></tr></table></figure><p>作者又提出了PMP版本的YOPO，并证明SGD的YOPO是PMP版的一种特殊形式。这样每次迭代r就只用到embedding的梯度就可以了。</p><p>YOPO还主张在每次反向传播后，应将第一隐层的梯度作为常数，利用该常数与网络第一层的雅可比的乘积对对手进行多次额外更新，以获得强对手。</p><h3 id="回到FreeLB"><a href="#回到FreeLB" class="headerlink" title="回到FreeLB"></a>回到FreeLB</h3><p>与FreeAT不同的是，YOPO从每个上升步长开始累加参数的梯度，并且只在K个内上升步长之后更新一次参数。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}   min_{\theta}\mathbb{E}_{(Z,y)∼ D ,{m∼M }} [\frac {1}{K}\sum_{t=0}^{K-1} max_{\delta_t\in \Omega_t    }L(f_{\theta}(x+\delta_t),y)]  \end{split}\end{equation}</script><p>对比 PGD:</p><script type="math/tex; mode=display">\begin{equation}\begin{split}   min_{\theta}\mathbb{E}_{(x,y)∼ D} [max_{\Delta x\in \Omega    }L(x+\Delta x,y;\Theta)]     \end{split}\end{equation}</script><p>FreeLB和PGD主要有两点区别：</p><ol><li>PGD是迭代K次r后取最后一次扰动的梯度更新参数，FreeLB是取K次迭代中的平均梯度</li><li>PGD的扰动范围都在epsilon内，因为伪代码第3步将梯度归0了，每次投影都会回到以第1步x为圆心，半径是epsilon的圆内，而FreeLB每次的x都会迭代，所以r的范围更加灵活，更可能接近局部最优：</li></ol><p><img src="https://i.loli.net/2021/04/10/ZMxvfdq4FXRn69S.jpg" alt=""></p><p>它执行多次PGD迭代来构造对抗性实例，并在每次迭代中同时累积“free”参数梯度∇θL。</p><p>伪代码：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">对于每个x:</span><br><span class="line">  1.通过均匀分布初始化r，梯度g为0</span><br><span class="line">  对于每步t=1...K:</span><br><span class="line">    2.根据x+r计算前后向，累计梯度g</span><br><span class="line">    3.更新r</span><br><span class="line">  4.根据g/K更新梯度</span><br></pre></td></tr></table></figure><p>论文中还指出了很重要的一点，就是<strong>对抗训练和dropout不能同时使用</strong>，加上dropout相当于改变了网络结构，会影响r的计算。如果要用的话需要在<strong>K步中都使用同一个mask</strong>。</p><p><img src="https://i.loli.net/2021/04/10/WTh7O4Ui5YenzNM.png" alt=""></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zhuanlan.zhihu.com/p/103593948">一文搞懂NLP中的对抗训练FGSM/FGM/PGD/FreeAT/YOPO/FreeLB/SMART</a></p><p><a href="https://blog.csdn.net/weixin_41712499/article/details/110878322">对抗训练的理解，以及FGM、PGD和FreeLB的详细介绍</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;FreeLB-Enhanced-Adversarial-Training-For-Natural-Language-Understanding&quot;&gt;&lt;a href=&quot;#FreeLB-Enhanced-Adversarial-Training-For-Natural-</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>图神经网络的对抗攻击</title>
    <link href="http://example.com/2021/04/08/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/"/>
    <id>http://example.com/2021/04/08/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/</id>
    <published>2021-04-08T14:01:43.000Z</published>
    <updated>2021-04-10T06:08:47.884Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图神经网络的对抗攻击"><a href="#图神经网络的对抗攻击" class="headerlink" title="图神经网络的对抗攻击"></a>图神经网络的对抗攻击</h1><p>最近要汇报一个关于安全方面的研究。本来打算讲一些和安全擦边的关于nlp对抗训练提升模型鲁棒性的内容，正好和最近学习的阅读理解比赛相关，可以作为一个提分trick。</p><p>但老师强调要和安全相关少讲过程。而nlp中的对抗样本不可以加在原始样本中，只能在embedding中加入扰动，这样就没法攻击，多数用来提升模型鲁棒性。所以就拍马研究了一下图网络的对抗攻击。</p><p>刚开始了解，希望可以从中找出可以和我研究方向结合的地方。</p><p>如有不对的地方还希望联系我指点一下。</p><p><a href="https://coding-zuo.github.io/2021/04/07/nlp%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83-%E4%B8%8Ebert%E7%BB%93%E5%90%88/">nlp中的对抗训练&amp;与bert结合</a></p><p>在上一篇文章中主要介绍的是对抗训练，其实是一种防御的策略，对提高模型而言FGM相当于加了一个正则项。 </p><h2 id="图网络攻击难点"><a href="#图网络攻击难点" class="headerlink" title="图网络攻击难点"></a>图网络攻击难点</h2><ul><li><p>离散的结构/特征，难以直接利用现有的基于梯度的方法。</p></li><li><p>对于“无法感知”的扰动如何定义。</p></li><li><p>节点分类往往属于直推式学习，训练数据和测试数据联合使用以学习模型，这就使得攻击方法注定是与poisoning/causative attack相关，而非仅是evasion attack。</p></li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zhuanlan.zhihu.com/p/88934914">图对抗攻击 Graph Adversarial Attack</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;图神经网络的对抗攻击&quot;&gt;&lt;a href=&quot;#图神经网络的对抗攻击&quot; class=&quot;headerlink&quot; title=&quot;图神经网络的对抗攻击&quot;&gt;&lt;/a&gt;图神经网络的对抗攻击&lt;/h1&gt;&lt;p&gt;最近要汇报一个关于安全方面的研究。本来打算讲一些和安全擦边的关于nlp对抗训</summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>nlp中的对抗训练&amp;与bert结合</title>
    <link href="http://example.com/2021/04/07/nlp%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83-%E4%B8%8Ebert%E7%BB%93%E5%90%88/"/>
    <id>http://example.com/2021/04/07/nlp%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83-%E4%B8%8Ebert%E7%BB%93%E5%90%88/</id>
    <published>2021-04-07T09:45:46.000Z</published>
    <updated>2021-04-10T08:13:12.154Z</updated>
    
    <content type="html"><![CDATA[<h1 id="nlp中的对抗训练学习"><a href="#nlp中的对抗训练学习" class="headerlink" title="nlp中的对抗训练学习"></a>nlp中的对抗训练学习</h1><p>PPT : <a href="https://coding-zuo.github.io/adversary/index.html">https://coding-zuo.github.io/adversary/index.html</a></p><p>由于深度神经网络强大的表示学习能力，在许多领域都取得了很大的成功，包括计算机视觉、自然语言处理、语音识别等。然而，在其卓越性能的背后，深度神经网络作为一个黑箱，缺乏可解释性与鲁棒性，使得它易受到对抗攻击而对抗性攻击的存在可能是深度学习模型的一个固有弱点。</p><p>深度学习的对抗一般有两种含义：</p><ul><li>一是生成对抗网络(Generative Adversarial Network,GAN) 代表一大类先进的生成模型。(这方面我不是很了解)</li><li>另一个则是跟对抗攻击、对抗样本相关的领域。(主要关心模型在小扰动下的稳健性)</li></ul><h2 id="方法介绍"><a href="#方法介绍" class="headerlink" title="方法介绍"></a>方法介绍</h2><p>在CV领域，我们需要通过对模型的对抗攻击和防御来增强模型的稳健型，比如在自动驾驶系统中，要防止模型因为一些随机噪声就将红灯识别为绿灯。</p><p>在NLP领域，类似的对抗训练也是存在的，不过NLP中的对抗训练更多是作为一种正则化手段来提高模型的泛化能力！</p><p>这使得对抗训练成为了NLP刷榜的“神器”之一，前有微软通过RoBERTa+对抗训练在<a href="https://gluebenchmark.com/leaderboard">GLUE</a>上超过了原生RoBERTa。</p><h2 id="对抗样本"><a href="#对抗样本" class="headerlink" title="对抗样本"></a>对抗样本</h2><p>要认识对抗训练，首先要了解“对抗样本”，它首先出现在论文<a href="http://https//arxiv.org/abs/1312.6199">《Intriguing properties of neural networks》</a>之中。简单来说，它是指对于人类来说“看起来”几乎一样、但对于模型来说预测结果却完全不一样的样本，比如下面的经典例子：</p><p><img src="https://i.loli.net/2021/04/07/EizJ85fCHyX2drj.png" alt=""></p><p>“对抗攻击”，其实就是想办法造出更多的对抗样本。</p><p>“对抗防御”，就是想办法让模型能正确识别更多的对抗样本。</p><p>所谓对抗训练，则是属于对抗防御的一种，它构造了一些对抗样本加入到原数据集中，希望增强模型对对抗样本的鲁棒性；同时，如本文开篇所提到的，在NLP中它通常还能提高模型的表现。</p><p>用对抗训练的思路来提升NLP模型，有两个实现角度：</p><ol><li><p>因为nlp的输入通常是one-hot向量，两个one-hot向量其欧式距离恒为$\sqrt 2$ ，理论上不存在微小的扰动，不想cv图像那样可以对连续实数向量来做。比如，$\Delta x$ 是实数向量，$x+\Delta x$还是一个有意义的图。所以很多研究都是在embedding层上做扰动的，因为embedding层是我们自己训练的，所以不太可能出现认为的恶意对抗攻击。</p><p><img src="https://i.loli.net/2021/04/09/daTOFDIU3EtfyGs.png" alt=""></p></li><li><p>这种角度不知道还算不算对抗，但可以说是一种数据增强手段。如上图中下面的问题，经过缩写，添加标点，或者同义词近义词替换等等。通过辅助模型提升鲁棒性。</p></li></ol><h2 id="Min-Max"><a href="#Min-Max" class="headerlink" title="Min-Max"></a>Min-Max</h2><p>对抗训练可以统一写成如下格式：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  min_{\theta}\mathbb{E}_{(x,y)∼ D} [max_{\Delta x\in \Omega    }L(x+\Delta x,y;\Theta)]     \end{split}\end{equation}</script><p>其中$D$ 代表训练集，x代表输入，y代表标签，θ是可学习模型参数，L(x,y;θ)是单个样本的loss，Δx是对抗扰动，Ω是扰动空间。</p><p>理解为：</p><ol><li><p>$max_{\Delta x\in \Omega}L(x+\Delta x,y;\theta)$ ，往输入x中注入扰动$\Delta x$， 目的是希望 $ L(x+\Delta x,y;\theta)$ 损失越大越好，也就是让现有模型的预测出错;</p></li><li><p>当然$\Delta x$ 不能太大、无约束，否则达不到“看起来几乎一样”的效果，所以$Δx$要满足一定的约束，常规的约束是$‖Δx‖≤ϵ$，其中$ϵ$是一个常数；</p></li><li><p>构造好对抗样本后，用$x+\Delta x,y$作为数据去最小化loss，来更新参数$\theta$ (梯度下降)</p></li><li>重复执行1.2.3步。</li></ol><p>整个对抗训练优化过程是一个max和min交替执行的过程：通过注入max损失，在梯度下降让损失变min。</p><h2 id="如何计算-Delta-x-——快速梯度FGM"><a href="#如何计算-Delta-x-——快速梯度FGM" class="headerlink" title="如何计算$\Delta x$——快速梯度FGM"></a>如何计算$\Delta x$——快速梯度FGM</h2><p>$\Delta x$的目的是增大Loss，而我们知道让loss减少的方法是梯度下降，那反过来，让loss增大的方法自然就是梯度上升，因此可以简单地取</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \Delta x &= ϵ∇_xL(x,y;θ)\\ ∇_xL(x,y;θ) &= (\frac {\partial L }{\partial x})    \end{split}\end{equation}</script><p>求loss对x的梯度，然后根据梯度给Δx赋值，来实现对输入的干扰，完成干扰之后再执行常规的梯度下降。</p><p>为了防止$\Delta x$过大，通常要对 $∇xL(x,y;θ)$ 标准化，常见方式为：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}      \Delta x = ϵ \frac {∇_xL(x,y;θ)}{||∇_xL(x,y;θ)||} \text{或} \Delta x=  ϵsign(∇_xL(x,y;θ))    \end{split}\end{equation}</script><p>采用右边的取扰动值的算法叫FGSM(ICLR2015)，理解为扰动是沿着梯度方向向损失值的极大值走。</p><p>采用左边取扰动值的算法叫FGM(ICLR2017)，理解为在每个方向上都走相同的一步找到更好的对抗样本。</p><p>有了$\Delta x$，得到：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  min_{\theta}\mathbb{E}_{(x,y)∼ D} [L(x+\Delta x,y;\Theta)]     \end{split}\end{equation}</script><p>这就构成了一种对抗训练方法，被称为<strong>Fast Gradient Method（FGM）</strong>，它由GAN之父Goodfellow在论文<a href="https://arxiv.org/abs/1412.6572">《Explaining and Harnessing Adversarial Examples》</a>首先提出。</p><p>此外，对抗训练还有一种方法，叫做<strong>Projected Gradient Descent（PGD）</strong>，其实就是通过多迭代几步来达到让$L(x+Δx,y;θ)$更大的$Δx$（如果迭代过程中模长超过了$ϵ$，<a href="https://arxiv.org/abs/1706.06083">《Towards Deep Learning Models Resistant to Adversarial Attacks》</a>。在后文….</p><h3 id="梯度惩罚"><a href="#梯度惩罚" class="headerlink" title="梯度惩罚"></a>梯度惩罚</h3><p>假设已经得到对抗扰动 $\Delta x$ ,更新 $\theta$ 时，对 L 进行泰勒展开：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  min_{\theta} \mathbb{E}_{(x,y)∼ D} [L(x+\Delta x,y;\theta)] &\approx min_{\theta} \mathbb{E}_{(x,y)∼ D}[L(x,y;\theta) + <∇_xL(x,y;θ), \Delta x>] \\ &= min_{\theta} \mathbb{E}_{(x,y)∼ D}[L(x,y;\theta) + ∇_xL(x,y;θ) \cdot \Delta  x ] \\ &= min_{\theta} \mathbb{E}_{(x,y)∼ D}[L(x,y;\theta) + ∇_xL(x,y;θ)^T  \Delta  x ]     \end{split}\end{equation}</script><p>对应的 $\theta$ 的梯度为:</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  ∇_{\theta} L(x,y;\theta) + ∇_{\theta} ∇_xL(x,y;θ)^T  \Delta  x     \end{split}\end{equation}</script><p>代入 $ \Delta x = ϵ∇_xL(x,y;θ)$:</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  &∇_{\theta} L(x,y;\theta) + ϵ ∇_{\theta} ∇_xL(x,y;θ)^T  ∇_xL(x,y;θ)\\ &= ∇_{\theta}(L(x,y;θ) + \frac{1}{2} ϵ ||∇_xL(x,y;θ)||^2)    \end{split}\end{equation}</script><p>这个结果表示，对输入样本施加 $ϵ∇xL(x,y;θ)$ 的对抗扰动，一定程度上等价于往loss里边加入“梯度惩罚”</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \frac{1}{2} ϵ ||∇_xL(x,y;θ)||^2    \end{split}\end{equation}</script><p>如果对抗扰动是$ϵ‖∇xL(x,y;θ)‖$ ，那么对应的梯度惩罚项则是$ϵ‖∇xL(x,y;θ)‖$（少了个1/2，也少了个2次方）。</p><h3 id="几何图像"><a href="#几何图像" class="headerlink" title="几何图像"></a>几何图像</h3><p>事实上，关于梯度惩罚，我们有一个非常直观的几何图像。以常规的分类问题为例，假设有n个类别，那么模型相当于挖了n个坑，然后让同类的样本放到同一个坑里边去：</p><p><img src="https://i.loli.net/2021/04/09/xeOucXmabjkrS4A.png" alt=""></p><p>梯度惩罚则说“同类样本不仅要放在同一个坑内，还要放在坑底”，这就要求每个坑的内部要长这样：</p><p><img src="https://i.loli.net/2021/04/09/tHylhowkCpvP2IM.png" alt=""></p><p>为什么要在坑底呢？因为物理学告诉我们，坑底最稳定呀，所以就越不容易受干扰呀，这不就是对抗训练的目的么？</p><p>那坑底意味着什么呢？极小值点呀，导数（梯度）为零呀，所以不就是希望‖∇xL(x,y;θ)‖‖∇xL(x,y;θ)‖越小越好么？这便是梯度惩罚的几何意义了。</p><p><img src="https://kexue.fm/usr/uploads/2020/03/3963498733.gif" alt=""></p><p>苏神代码基于keras的：</p><blockquote><p><a href="https://github.com/bojone/keras_adversarial_training">https://github.com/bojone/keras_adversarial_training</a></p></blockquote><h2 id="Projected-Gradient-Descent-PGD"><a href="#Projected-Gradient-Descent-PGD" class="headerlink" title="Projected Gradient Descent (PGD)"></a>Projected Gradient Descent (PGD)</h2><p>内部max的过程，本质上是一个非凹的约束优化问题，FGM解决的思路其实就是梯度上升，<strong>那么FGM简单粗暴的“一步到位”，是不是有可能并不能走到约束内的最优点呢？</strong>当然是有可能的。于是，一个很intuitive的改进诞生了：Madry在18年的ICLR中，提出了用Projected Gradient Descent（PGD）的方法，简单的说，就是<strong>“小步走，多走几步”</strong>，如果走出了扰动半径为$\epsilon$的空间，就映射回“球面”上，以保证扰动不要过大：</p><p>其中$\mathcal{S}={r\in\mathbb{R}^d:||r||_2 \leq \epsilon}$ 为扰动的约束空间，$\alpha$为小步的步长。</p><p>作者将这一类通过一阶梯度得到的对抗样本称之为“一阶对抗”，在实验中，作者发现，经过PGD训练过的模型，对于所有的一阶对抗都能得到一个低且集中的损失值，如下图所示：</p><p><img src="https://i.loli.net/2021/04/09/SosrVAW9UGYNm6T.png" alt=""></p><p>我们可以看到，面对约束空间 $\mathcal{S}$ 内随机采样的十万个扰动，PGD模型能够得到一个<strong>非常低且集中的loss分布</strong>，因此，在论文中，作者称PGD为<strong>“一阶最强对抗”</strong>。也就是说，只要能搞定PGD对抗，别的一阶对抗就不在话下了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对于每个x:</span><br><span class="line">  1.计算x的前向loss、反向传播得到梯度并备份</span><br><span class="line">  对于每步t:</span><br><span class="line">      2.根据embedding矩阵的梯度计算出r，并加到当前embedding上，相当于x+r(超出范围则投影回epsilon内)</span><br><span class="line">      3.t不是最后一步: 将梯度归0，根据1的x+r计算前后向并得到梯度</span><br><span class="line">      4.t是最后一步: 恢复(1)的梯度，计算最后的x+r并将梯度累加到(1)上</span><br><span class="line">  5.将embedding恢复为(1)时的值</span><br><span class="line">  6.根据(4)的梯度对参数进行更新</span><br></pre></td></tr></table></figure><p>基于PGD的对抗性训练被广泛认为是最有效的，因为它在很大程度上避免了模糊的梯度问题。它将一类对抗性训练算法转化为求解交叉熵损失的极大极小问题，该问题可以通过多次投影梯度上升步骤和随后的SGD步骤可靠地实现。</p><h2 id="Virtual-Adversarial-Training"><a href="#Virtual-Adversarial-Training" class="headerlink" title="Virtual Adversarial Training"></a>Virtual Adversarial Training</h2><p>除了监督训练，对抗训练还可以用在半监督任务中，尤其对于NLP任务来说，很多时候输入的无监督文本多的很，但是很难大规模地进行标注，那么就可以参考[13]中提到的Virtual Adversarial Training进行半监督训练。</p><p>首先，我们抽取一个随机标准正态扰动（$d\sim \mathcal{N}(0, I)\in \mathbb{R}^d$），加到embedding上，并用KL散度计算梯度：</p><p>然后，用得到的梯度，计算对抗扰动，并进行对抗训练：</p><p><img src="https://i.loli.net/2021/04/09/KYX3zILWf4A1Htg.png" alt=""></p><p>实现方法跟FGM差不多</p><h2 id="FreeAT-amp-YOPO-amp-FreeLB"><a href="#FreeAT-amp-YOPO-amp-FreeLB" class="headerlink" title="FreeAT &amp; YOPO &amp; FreeLB"></a>FreeAT &amp; YOPO &amp; FreeLB</h2><p><strong>优化的主要方向有两点：得到更优的扰动 &amp; 提升训练速度</strong></p><p>其实PGD效果不错但是它迭代多步计算开销很大，所以出现了这些针对效率上的优化，并且结合预训练语言模型。</p><p>具体的就搜这些论文来看吧。</p><p>FGSM: Explaining and Harnessing Adversarial Examples</p><p>FGM: Adversarial Training Methods for Semi-Supervised Text Classification</p><p>FreeAT: Adversarial Training for Free!</p><p>YOPO: You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle</p><p>FreeLB: Enhanced Adversarial Training for Language Understanding</p><p>SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[<a href="https://kexue.fm/archives/7234">对抗训练浅谈：意义、方法和思考（附Keras实现）</a>]</p><p><a href="https://fyubang.com/2019/10/15/adversarial-train/">【炼丹技巧】功守道：NLP中的对抗训练 + PyTorch实现</a></p><p><a href="https://blog.csdn.net/chencas/article/details/103551852">NLP —- &gt;对抗学习：从FGM, PGD到FreeLB</a></p><p><a href="https://arxiv.org/pdf/2004.14543v3.pdf">TAVAT: Token-Aware Virtual Adversarial Training for Language Understanding</a></p><p><a href="https://arxiv.org/abs/1605.07725">Adversarial Training Methods for Semi-Supervised Text Classification</a><br><a href="https://github.com/tensorflow/models/blob/e97e22dfcde0805379ffa25526a53835f887a860/research/adversarial_text/adversarial_losses.py">Adversarial Text Classification原作实现</a></p><p><a href="https://blog.csdn.net/ganxiwu9686/article/details/105931668">NLP(文本)中的对抗训练</a></p><p><a href="https://blog.csdn.net/weixin_41712499/article/details/110878322">对抗训练的理解，以及FGM、PGD和FreeLB的详细介绍</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;nlp中的对抗训练学习&quot;&gt;&lt;a href=&quot;#nlp中的对抗训练学习&quot; class=&quot;headerlink&quot; title=&quot;nlp中的对抗训练学习&quot;&gt;&lt;/a&gt;nlp中的对抗训练学习&lt;/h1&gt;&lt;p&gt;PPT : &lt;a href=&quot;https://coding-zuo.</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
</feed>
