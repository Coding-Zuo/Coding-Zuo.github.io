<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Coding-Zuo</title>
  
  <subtitle>Coding And Studying</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-06-23T02:33:27.490Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Coding-Zuo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Avoiding Reasoning Shortcuts- Adversarial Evaluation, Training, and Model Development for Multi-Hop QA</title>
    <link href="http://example.com/2021/06/13/Avoiding-Reasoning-Shortcuts-Adversarial-Evaluation-Training-and-Model-Development-for-Multi-Hop-QA/"/>
    <id>http://example.com/2021/06/13/Avoiding-Reasoning-Shortcuts-Adversarial-Evaluation-Training-and-Model-Development-for-Multi-Hop-QA/</id>
    <published>2021-06-13T13:59:08.000Z</published>
    <updated>2021-06-23T02:33:27.490Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Avoiding-Reasoning-Shortcuts-Adversarial-Evaluation-Training-and-Model-Development-for-Multi-Hop-QA"><a href="#Avoiding-Reasoning-Shortcuts-Adversarial-Evaluation-Training-and-Model-Development-for-Multi-Hop-QA" class="headerlink" title="Avoiding Reasoning Shortcuts- Adversarial Evaluation, Training, and Model Development for Multi-Hop QA"></a>Avoiding Reasoning Shortcuts- Adversarial Evaluation, Training, and Model Development for Multi-Hop QA</h1><p>这篇文章作者发现在HotpotQA中经常包含Reasoning Shortcuts。也就是说模型没有真正的理解文章并进行推理，而是通过将问题与上下文中的句子进行词匹配来直接定位答案。</p><p>作者主要做了两件事：</p><ul><li>构建干扰文档数据，证明了存在推理shortcut现象。</li><li>设计一个新模型来缓解这个问题</li></ul><h2 id="对抗验证"><a href="#对抗验证" class="headerlink" title="对抗验证"></a>对抗验证</h2><p><img src="https://z3.ax1x.com/2021/06/14/27n5O1.png" alt=""></p><p>问题是What was the father of Kasper Schmeichel voted to be by the IFFHS in 1992? （卡斯珀·舒梅切尔的父亲1992年被IFFHS投票选为什么？）</p><p>模型需要从两个文档中考虑信息，找出隐含的推理链条。</p><p>Kasper Schmeichel $\rightarrow^{son}$  Peter Schemeichel $\rightarrow^{voted}$ World’s Best Goalkeeper</p><p>Kasper Schmeichel是Peter Schemeicher, Peter Schemeicher 被投票为 World’s Best Goalkeeper 世界最佳守门员。</p><p>在该示例中，也可以通过将问题中的几个关键字(“voted to be by the IFFHS in 1992——1992年投票的IFFHS”)与上下文中的相应事实相匹配来得到正确的回答，而无需通过第一跳推理来找到“Kasper Schmeichel的父亲”，因为两个distractor文档都不包含足够分散注意力的信息。 </p><p>因此，一个在现有评估上表现良好的模型并不一定表明它具有很强的复合推理能力。</p><p>随着对抗扰动文本被添加到上下文中，使用单跳shotcut方式不再可能找到正确的答案，这现在导致了两个可能的答案(“世界最佳守门员”和“世界最佳防守人”)。</p><p>添加干扰后模型预测答案为 IFFHS World’s Best Defender 最佳防守人。</p><h2 id="如何构建对抗数据"><a href="#如何构建对抗数据" class="headerlink" title="如何构建对抗数据"></a>如何构建对抗数据</h2><p><strong>要构建这种对抗的需求</strong></p><blockquote><p>作者指出HotpotQA从维基百科中选择距离目标问题最短的bigram TF-IDF的前8个文档作为干扰项，形成总共10个文档的上下文。由于在生成问题时没有向群组工作人员提供导向文档，因此不能保证在给定整个上下文的情况下，两个支持文档都是必要的来推断答案。</p><p>多跳假设可以通过两种方式由不完整的分心文档来实现。</p><ol><li><p>其中一个选定的干扰项可能包含推断答案所需的所有证据，从经验上讲，在HotpotQA中没有发现这样的情况，因为关于一个主题的Wiki文章很少讨论另一个主题的细节</p></li><li><p>整个分散注意力的文档池可能不包含真正分散读者/模型注意力的信息。</p></li></ol></blockquote><p>作者把绕过推理回答问题这种方式，叫做shortcut 。其经常出现在HotpotQA中的桥接问题中，比较问题一般不能匹配而得出。作者采样了50个桥接问题，发现其中26个有这种问题。</p><hr><p>设原内容、问题答案为 $(C,q,a)$ 是可能包含shortcut问题的数据，作者是想将其变为$(C’,q,a)$ </p><p>$q,a$ 不变，$C’$ 变成接近 $C$ 的文章。在HotpotQA中是提供两个支持文档$P$的， 其中$P\subset C$ 。</p><p> 在构建对抗文本时也就是ADDDoc。就是利用新的 $P’$ ，混合$(C,P’)$ 构成新的数据集。</p><p><strong>那么 $P’$ 如何来的？</strong></p><p>假设 $p2\in P$ 是包含答案的支持文档，$p1\in P$ 是包含线索的文档。</p><p>ADDDoc是利用词或短语级别的干扰，将 $p2$ 替换成$p’2$ ，其包含满足推理快捷方式但不与整个问题的答案相矛盾的假答案。</p><p><strong>那么词语是如何替换的？</strong></p><p>首先，对于答案中的每个非停用词，都会在GloVe100维向量空间中找到最接近的10个替代词，它的子串与原始答案的重叠子串长度不超过3个。如(Mumbai → Delhi, Goalkeeper → Defender)。如果这个过程失败，就从HotpotQA dev集合的整个答案池中随机抽样一个候选者 。 </p><p>如果原始答案有多个单词，我们将答案中的一个非停用词与相应的抽样答案单词替换，以创建假答案(“World’s Best Goalkeeper → World’s Best Defender”)。</p><p><img src="https://z3.ax1x.com/2021/06/14/27EFfA.png" alt="27EFfA.png"></p><p>问：Sachin Wamer作为软件工程师所在的公司的总部在哪里？</p><p>由此产生的段落 $p’2$ 提供了一个满足推理捷径的答案，但也与整个问题的真实答案相矛盾，因为它形成了另一个有效的推理链，将问题与假答案连接起来 (Sachin Warrier $\rightarrow^{work}$ TCS $\rightarrow^{at}$ Delhi)。</p><p>为了打破这个矛盾的推理链，我们需要用另一个实体替换连接两个证据的桥梁实体（在这种情况下为“Tata Consultancy Services”），这样生成的答案就不再作为有效答案 。</p><p>用从 HotpotQA 开发集中所有文档标题中随机采样的候选者替换 $p’2$ 的标题。 如果 $p1$ 的标题出现在$p’2$ 中，我们也将其替换为另一个采样标题，以彻底消除 $p’2$ 和 $p1$ 之间的联系。</p><p>如上图将 Tata Consultancy Services  替换为  Valencia Street Circuit </p><h2 id="模型方法"><a href="#模型方法" class="headerlink" title="模型方法"></a>模型方法</h2><h3 id="Encoding"><a href="#Encoding" class="headerlink" title="Encoding"></a>Encoding</h3><p>对于cotnext 和question 使用v 维Highway Network 合并 字符嵌入和GloVe词嵌入。</p><p>得到 $x\in R^{J\times v}$ 和 $q\in R^{S\times v}$ 其中 J 是文章长度 S 为问题长度</p><p>整体结构和BiDAF那个文章相似。</p><p><a href="https://imgtu.com/i/27xDFP"><img src="https://z3.ax1x.com/2021/06/14/27xDFP.png" alt="27xDFP.png"></a></p><h3 id="Single-Hop-Baseline"><a href="#Single-Hop-Baseline" class="headerlink" title="Single-Hop Baseline"></a>Single-Hop Baseline</h3><p>使用bi-attention + self-attention ，给定上下文和问题encoding  $h,u$ 经过 context-to-query $BiAttn(h,u)$ 计算得到一个相似矩阵 $M^{S\times J}$ :</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  M_{s,j} &= W_1u_s +W_2h_j + W_3(u_s\odot h_j) \\ p_{s,j} &= \frac {exp(M_{s,j})}{\sum_{s=1}^S exp(M_{s,j})}\\ c_{q_j} &= \sum_{s=1}^S p_{s,j} u_s    \end{split}\end{equation}</script><p>$\odot$ 对应元素相乘。</p><p>然后query-to-context 注意力：</p><script type="math/tex; mode=display">\begin{equation}\begin{split} m_j &= max_{1\le s\le S} M_{s,j} \\ p_{s,j} &= \frac {exp(m_{j})}{\sum_{j=1}^J exp(m_{j})}\\ q_c &= \sum_{j=1}^J p_{j} h_j\\ h'_j &= [h_j ;c_{q_j}; h_j\odot c_{q_j}; c_{q_j} \odot q_c]\\ h^1 &= BiLSTM(h')    \end{split}\end{equation}</script><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://i.loli.net/2021/06/23/mPiX5Rkz1o9tCSl.png" alt=""></p><p><img src="https://i.loli.net/2021/06/23/yQODEbgUcaeZApK.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Avoiding-Reasoning-Shortcuts-Adversarial-Evaluation-Training-and-Model-Development-for-Multi-Hop-QA&quot;&gt;&lt;a href=&quot;#Avoiding-Reasoning-Sh</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>TRANSFORMER-XH: MULTI-EVIDENCE REASONING WITH EXTRA HOP ATTENTION</title>
    <link href="http://example.com/2021/06/10/TRANSFORMER-XH-MULTI-EVIDENCE-REASONING-WITH-EXTRA-HOP-ATTENTION/"/>
    <id>http://example.com/2021/06/10/TRANSFORMER-XH-MULTI-EVIDENCE-REASONING-WITH-EXTRA-HOP-ATTENTION/</id>
    <published>2021-06-10T09:58:11.000Z</published>
    <updated>2021-06-11T04:18:16.018Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TRANSFORMER-XH-MULTI-EVIDENCE-REASONING-WITH-EXTRA-HOP-ATTENTION"><a href="#TRANSFORMER-XH-MULTI-EVIDENCE-REASONING-WITH-EXTRA-HOP-ATTENTION" class="headerlink" title="TRANSFORMER-XH: MULTI-EVIDENCE REASONING WITH EXTRA HOP ATTENTION"></a>TRANSFORMER-XH: MULTI-EVIDENCE REASONING WITH EXTRA HOP ATTENTION</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>提出Transformer-XH配有eXtra Hop attention以完全数据驱动的方式实现了结构化文本的内在建模。</p><p>完全数据驱动应该是指不仅可以处理序列结构的数据，还可以处理图结构。</p><p>eXtra Hop attention 除了关注每个序列内的记号外，还可以连接的文本序列之间跳跃</p><p>因此文档之间传播信息和构建全局上下文表示来更好地进行联合多证据推理。</p><p>eXtra Hop attention的作用：</p><ul><li>当每段文本与其他证据相关时能够更全局地表示每段文本所贡献的证据</li><li>以一种自然的方式，通过必要的边信息传递对证据图联合推理</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>面临的第一个挑战就是由于transformer的softmax计算在所有token对上，很难处理长文本。</p><p> Transform-XL(eXtra Long) 通过将更长的文本(多段落文档) 分解成文本段序列  $\lbrace X<em>1,…,X_r,…,X</em>{\zeta} \rbrace$ 来解决。</p><p>使用如下的公式计算在相邻文本段之间传播信息：</p><script type="math/tex; mode=display">\hat H_r^{l-1} = [cat(Freeze(H_{r-1}^{l-1}) ,H_r^{l-1})]</script><p>其中 $H_r^{l-1}$ 表示第 r 个文本段的第 $l-1$ 层Transformer表达。</p><p>新注意力机制中的 $Q,K,V$ 就表达为：</p><script type="math/tex; mode=display">\hat Q^T; \hat K^T; \hat V^T =  W^q\cdot \hat H^{l-1}_r; W^k\cdot \hat H^{l-1}_r; W^v \cdot \hat H^{l-1}_r</script><p>之后就还是送到缩放点积中。这一点处理长文本可能是参考了TransformerXL。</p><blockquote><p>Transformer-XL 的重要组件之一，<strong>Segment Recurrence Mechanism（段循环机制）</strong>想做的就是，能不能在前一段计算完后，将它计算出的隐状态都保存下来，存到一个 Memeory 中，之后在计算当前段的时候，<strong>将之前存下来的隐状态和当前段的隐状态拼起来，作为 Attention 机制的 K 和 V，从而获得更长的上下文信息</strong></p></blockquote><p><img src="https://i.loli.net/2021/06/10/5zH2Avr7IiXOl6d.png" alt=""></p><p>然而，在许多情况下，文本段被组织在线性序列之外的结构中。例如，文档由图形结构中的超链接连接，这种图形结构不容易简化为形成线性序列，从而禁止了Transformer-XL的递归方法。</p><p>下面将引出eXtra Hop attention 如下图</p><p><img src="https://i.loli.net/2021/06/10/lZicWgnPv4L1dz9.png" alt=""></p><p>图a 表示 三个链接的文档 $d_2,d_1,d_3$ . Transform-XH使用eXtra Hop attention沿图形边缘传播信息，从而在连接的文本序列之间实现信息共享。</p><p>结构化文本包含一些列节点 $ X =\lbrace X<em>1,…,X_r,…X</em>{\zeta} \rbrace$ , 对应于一个文本序列。 </p><p>目标是生产表达 $X =\lbrace \hat H<em>1,…,\hat H_r,…\hat H</em>{\zeta} \rbrace$  ，其不仅合并了每个序列 $X$ 中的本地信息，而且还合并了关于整个结构化文本 ${X，E}$的全局上下文。</p><p>Transform-XH通过两种注意机制来实现这一点：in-sequence attention 和 eXtra Hop attention。</p><p>in-sequence attention 和 Transformer一样， 在 $l$ 层，第 $i$ 个token 收集从同一文本段τ内的其他 token 的信息：</p><script type="math/tex; mode=display">h_{r,i}^l = \sum_j softmax_j (\frac {q_{r,i}^T \cdot k_{r,j}}{ \sqrt d_k}) \cdot v_{r,j}</script><p>eXtra Hop attention, 使用第 CLS token 作为 attention hub ，在 $l$ 层，第 $\tau$ 个文本序列，如果 $τ$ 文本序列与另一个文本序列η之间存在边( $e_{τη}=1 $) :</p><script type="math/tex; mode=display">\hat h_{r,0}^l = \sum_{\eta ; e_{r\eta}=1} softmax_{\eta} (\frac {\hat q_{r,0}^T \cdot \hat k_{\eta,0}}{\sqrt d_k}) \cdot \hat v_{\eta,0}</script><p>节点 $τ$  使用hop query $\hat q<em>{r,0}$ 和 key $\hat k</em>{\eta, 0}$ 计算其邻居 $η$ 上的关注度权重，然后乘以邻居的value $\hat v_{\eta,0}$ ，最后将两个注意力机制聚合起来得到新的 $l$ 层的表达:</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \hat h_{r,0}^l &= Linear(cat[h_{r,0}^l, \hat h_{r,0}^l ])\\ \hat h_{r,i}^l &= h^l_{r,i} ; \forall i \ne 0    \end{split}\end{equation}</script><p>$ i \ne 0$ 是 non-hub tokens </p><p>一层 eXtra Hop attention 可视为沿着边 E 信息传递的 single-step </p><p>例如，在图a中，文档节点 $d_3$ 通过使用hop attention ,  $d_1→d_3$ 从其邻居d1收集信息来更新其表示。当多个Transformer-xh层被堆叠时，$d_1$ 中的该信息包括来自其in-sequence attention 的 $d_1$的本地上下文，以及来自 $ l-1$ 层的 hop attention ，$d_2−d_1$ 的交叉序列信息。因此，L 层 Transformer-XH可以处理最多 L 跳以外的信息。</p><p>总之 Transformer-XH共有三个主要属性，可对原始结构化文本数据进行有效建模：</p><ul><li>信息沿边的传播</li><li>信息的重要性 (hop attention)</li><li>序列内和跨序列信息的平衡 (attention combination)</li></ul><p>在 $H$ 中学习的表示可以天生地表达结构化文本中的细微差别，这些细微差别是复杂的推理任务(如多跳QA和自然语言推理)所需的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;TRANSFORMER-XH-MULTI-EVIDENCE-REASONING-WITH-EXTRA-HOP-ATTENTION&quot;&gt;&lt;a href=&quot;#TRANSFORMER-XH-MULTI-EVIDENCE-REASONING-WITH-EXTRA-HOP-A</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>旋转数组的最小数字&amp;搜索旋转排序数组</title>
    <link href="http://example.com/2021/06/07/%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97-%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/"/>
    <id>http://example.com/2021/06/07/%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97-%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/</id>
    <published>2021-06-07T07:25:23.000Z</published>
    <updated>2021-06-07T08:44:45.385Z</updated>
    
    <content type="html"><![CDATA[<h1 id="旋转数组的最小数字-amp-搜索旋转排序数组"><a href="#旋转数组的最小数字-amp-搜索旋转排序数组" class="headerlink" title="旋转数组的最小数字&amp;搜索旋转排序数组"></a>旋转数组的最小数字&amp;搜索旋转排序数组</h1><h2 id="旋转数组最小数字"><a href="#旋转数组最小数字" class="headerlink" title="旋转数组最小数字"></a>旋转数组最小数字</h2><h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。</p><p>输入一个升序的数组的一个旋转，输出旋转数组的最小元素。</p><p>例如数组 {3,4,5,1,2}{3,4,5,1,2} 为 {1,2,3,4,5}{1,2,3,4,5} 的一个旋转，该数组的最小值为 11。</p><p>数组可能包含重复项。</p><p><strong>注意</strong>：数组内所含元素非负，若数组大小为 00，请返回 −1−1。</p><p>样例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入：nums &#x3D; [2, 2, 2, 0, 1]</span><br><span class="line"></span><br><span class="line">输出：0</span><br></pre></td></tr></table></figure><h3 id="解法：二分查找"><a href="#解法：二分查找" class="headerlink" title="解法：二分查找"></a>解法：二分查找</h3><p>题目中有排序两字，自然较优的解是涉及到二分法的</p><p>假设我们用下图表示数组，水平线代表数字相同，横坐标代表数字下标</p><p><img src="https://i.loli.net/2021/06/07/iFZJBWC4teHIxKR.png" alt=""></p><p>我们发现除了最后水平的一段（黑色水平那段）之外，其余部分满足二分性质：</p><p>竖直虚线左边的数满足 $numbers[i] ≥ numbers[0]$ ；</p><p>而竖直虚线右边的数不满足这个条件。</p><p>我们要找的便是不满足上诉性质的那段中的最小值。</p><p>所以我们先将最后水平的一段删除 , 使得右半段不满足 $numbers[i] ≥ numbers[0]$ ，而是严格满足 $numbers[i] &lt; numbers[0]$。</p><p>另外，如果处理数组完全单调的情况：</p><p>当删除最后一段后，如果剩下的最后一个大于等一第一个数，说明数组完全单调。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findMin</span><span class="params">(<span class="keyword">int</span>[] numbers)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(numbers == <span class="keyword">null</span> || numbers.length==<span class="number">0</span>) <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> left=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = numbers.length-<span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 去除第二段有序数组中最后的和第一段第一个数相同的数</span></span><br><span class="line">        <span class="comment">// 使得第二段有序数组严格满足numbers[i]&lt;numbers[0]</span></span><br><span class="line">        <span class="keyword">while</span>(right&gt;<span class="number">0</span> &amp;&amp; numbers[right]==numbers[<span class="number">0</span>])&#123;</span><br><span class="line">            right--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 如果此时整个数组都有序了，那么numbers[0]就是最小值</span></span><br><span class="line">        <span class="keyword">if</span>(numbers[<span class="number">0</span>] &lt; numbers[right])&#123;</span><br><span class="line">            <span class="keyword">return</span> numbers[<span class="number">0</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span>(left&lt;right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left+right&gt;&gt;<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span>(numbers[mid] &lt; numbers[<span class="number">0</span>])&#123; <span class="comment">// 说明mid落在了右半段，最小值在[left,mid]里</span></span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                left = mid +<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> numbers[left];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="搜索旋转数组"><a href="#搜索旋转数组" class="headerlink" title="搜索旋转数组"></a>搜索旋转数组</h2><p><img src="https://i.loli.net/2021/06/07/wMJ9hGK68eZspRy.png" alt=""></p><p>数组旋转后可画出如下图：</p><p><img src="https://i.loli.net/2021/06/07/zudQlKM7UBmpbhq.png" alt=""></p><p>橙色线表示的就是旋转后数组的左右两个部分。不难发现，如果下标落在右半部分，则一定有 $nums[mid] &lt;= nums[nums.length-1]$</p><p>判断 $nums[mid] &lt;= nums[nums.length-1]$ 是否成立</p><ul><li>成立：说明当前mid落在了数组的右半部分，而我们要找的最小值其实就是右半部分的开头，故更新区间为 $[l,mid]$ 。</li><li>否则：说明mid落在了旋转数组的左半部分，那么右半部分的起点则在 $[mid+1, r]$</li></ul><p>总之，要找满足 $nums[mid] &lt;= nums[nums.length-1]$ 的最小值</p><p>第二阶段</p><p>找到最小值后，假如最小值的下标是 min ，数组便可以分为有序的两半 $[l, min-1]$  和 $[min, r]$  此时判断 $target&lt;=nums[nums.length-1]$  。</p><p>若成立，可以再右半部分中找target，因为target如果在右半部分的话，一定大于 $nums[nums.length-1]$， 那么久应该去左半边 $[l, min-1]$ 中找 target</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">search</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(nums==<span class="keyword">null</span> || nums.length==<span class="number">0</span>) <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        <span class="comment">//1,找出数组中的最小值,即左右两边的分界点，便可以将数组分为有序的左右两边</span></span><br><span class="line">        <span class="comment">//2,判断target &lt;= nums[nums.length - 1]是否成立</span></span><br><span class="line">        <span class="comment">//  成立：target在旋转后的右半边</span></span><br><span class="line">        <span class="comment">//  不成立：target在旋转数组的左半边</span></span><br><span class="line">        <span class="keyword">int</span> l =<span class="number">0</span> ;</span><br><span class="line">        <span class="keyword">int</span> r = nums.length-<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(r&gt;<span class="number">0</span> &amp;&amp; nums[r]==nums[<span class="number">0</span>])&#123;</span><br><span class="line">            r--;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(l&lt;r)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = l+r &gt;&gt;<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &lt;= nums[nums.length-<span class="number">1</span>])&#123;</span><br><span class="line">                r=mid;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                l=mid+<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//上面while结束后，l = r，都指向旋转数组中的最小值</span></span><br><span class="line">        <span class="keyword">if</span>(target&lt;=nums[nums.length-<span class="number">1</span>])&#123;</span><br><span class="line">            <span class="comment">// target在右边， l本身就是指向右边起点的，不用更新，更新r为右边终点。</span></span><br><span class="line">            r = nums.length-<span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">//target在左半边</span></span><br><span class="line">            l = <span class="number">0</span>;<span class="comment">//左半边的起点</span></span><br><span class="line">            r--;<span class="comment">//让r指向最小值的前一个位置，即左半边的终点</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//定好了区间[l,r]后，可以在里面找target了</span></span><br><span class="line">        <span class="comment">//使用二分模板即可，找满足nums[mid] &gt;= target的最小值</span></span><br><span class="line">        <span class="keyword">while</span>(l&lt;r)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = l+r&gt;&gt;<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &gt;= target)&#123;</span><br><span class="line">                r=mid;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                l=mid+<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 判断最终找到的num[l]是否等于target</span></span><br><span class="line">        <span class="keyword">if</span>(nums[l] == target) <span class="keyword">return</span> l;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;旋转数组的最小数字-amp-搜索旋转排序数组&quot;&gt;&lt;a href=&quot;#旋转数组的最小数字-amp-搜索旋转排序数组&quot; class=&quot;headerlink&quot; title=&quot;旋转数组的最小数字&amp;amp;搜索旋转排序数组&quot;&gt;&lt;/a&gt;旋转数组的最小数字&amp;amp;搜索旋转排序</summary>
      
    
    
    
    
    <category term="刷题" scheme="http://example.com/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>GRAPH-BERT: Only Attention is Needed for Learning Graph Representations</title>
    <link href="http://example.com/2021/06/04/GRAPH-BERT-Only-Attention-is-Needed-for-Learning-Graph-Representations/"/>
    <id>http://example.com/2021/06/04/GRAPH-BERT-Only-Attention-is-Needed-for-Learning-Graph-Representations/</id>
    <published>2021-06-04T03:24:00.000Z</published>
    <updated>2021-06-05T14:19:04.521Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GRAPH-BERT-Only-Attention-is-Needed-for-Learning-Graph-Representations"><a href="#GRAPH-BERT-Only-Attention-is-Needed-for-Learning-Graph-Representations" class="headerlink" title="GRAPH-BERT: Only Attention is Needed for Learning Graph Representations"></a>GRAPH-BERT: Only Attention is Needed for Learning Graph Representations</h1><p>当前GNNs的主要方法是过度依赖图中的连接关系，这样造成了三大问题。</p><ol><li>模型假死 (<em>suspended animation problem</em>) : 随着神经网络层数的不断加深，模型对于输入的数据开始不进行反应。这个问题的原因论文没写，个人理解是由于层之间的非线性使得数据分布变换导致梯度消失。</li><li>过平滑 (<em>over-smoothing problem</em>) : 由于GNN大多依靠聚合操作 (mean,max,sum) 的信息更新方式，这样随着层的不断堆叠，每个节点都会大量收到其他信息节点的影响，从而使得每个节点的embedding预测趋同。</li><li>难以并行计算：由于内存的限制，尤其是在大型图里面，图中的关联关系难以并行计算。</li></ol><p>根据以上问题作者提出了一种新的图神经网络，即Graph-Based BERT，它完全基于注意力机制，不需要任何图卷积或聚集操作。</p><p>在模型输入部分，不会把一整个大图输入给模型，而是先采样得到大图的一些无边子图，只是抽取子节点，而不考虑这些节点之间的边关系。这样就解决了GNN不能并行的问题。</p><p>传统GNN由于图的结构多样性，不能进行跨任务的预训练工作，但Graph-Bert不考虑边之间的联系，因此并不受限于图结构，可以很好地进行预训练和迁移学习。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="符号定义"><a href="#符号定义" class="headerlink" title="符号定义"></a>符号定义</h3><p><img src="https://i.loli.net/2021/06/04/lRKbH2Aa71m3peP.png" alt=""></p><h3 id="无边子图采样"><a href="#无边子图采样" class="headerlink" title="无边子图采样"></a>无边子图采样</h3><h3 id="输入节点向量Embedding"><a href="#输入节点向量Embedding" class="headerlink" title="输入节点向量Embedding"></a>输入节点向量Embedding</h3><h4 id="原始特征embedding"><a href="#原始特征embedding" class="headerlink" title="原始特征embedding"></a>原始特征embedding</h4><h4 id="Weisfeiler-Lehman-绝对角色-Embedding"><a href="#Weisfeiler-Lehman-绝对角色-Embedding" class="headerlink" title="Weisfeiler-Lehman 绝对角色 Embedding"></a>Weisfeiler-Lehman 绝对角色 Embedding</h4><h4 id="基于亲密度的相对位置Embedding"><a href="#基于亲密度的相对位置Embedding" class="headerlink" title="基于亲密度的相对位置Embedding"></a>基于亲密度的相对位置Embedding</h4><h4 id="基于相对距离的Hop-Embedding"><a href="#基于相对距离的Hop-Embedding" class="headerlink" title="基于相对距离的Hop Embedding"></a>基于相对距离的Hop Embedding</h4><h3 id="Graph-Transformer-Encoder"><a href="#Graph-Transformer-Encoder" class="headerlink" title="Graph Transformer Encoder"></a>Graph Transformer Encoder</h3><h3 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h3><h4 id="节点原始属性重构"><a href="#节点原始属性重构" class="headerlink" title="节点原始属性重构"></a>节点原始属性重构</h4><h4 id="图结构恢复"><a href="#图结构恢复" class="headerlink" title="图结构恢复"></a>图结构恢复</h4>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;GRAPH-BERT-Only-Attention-is-Needed-for-Learning-Graph-Representations&quot;&gt;&lt;a href=&quot;#GRAPH-BERT-Only-Attention-is-Needed-for-Learning-G</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>Attention</title>
    <link href="http://example.com/2021/05/31/Attention/"/>
    <id>http://example.com/2021/05/31/Attention/</id>
    <published>2021-05-31T12:02:02.000Z</published>
    <updated>2021-05-31T14:43:04.087Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从RNN到LSTM到Transformer再到GNN"><a href="#从RNN到LSTM到Transformer再到GNN" class="headerlink" title="从RNN到LSTM到Transformer再到GNN"></a>从RNN到LSTM到Transformer再到GNN</h1><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><h3 id="单头注意力"><a href="#单头注意力" class="headerlink" title="单头注意力"></a>单头注意力</h3><p><img src="https://i.loli.net/2021/05/31/PkepyKTAUWOI3wB.png" alt=""></p><p>将句子 $S$ 中第 $i$ 个词的隐藏特征 $h$ 从 $l$ 层更新到 $l+1$ 层 :</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  h^{l+1}_i &= Attention(Q^lh_i^l, K^lh_j^l, V^lh_j^l)\\ i.e., \ h_i^{l+1} &= \sum_{j\in S} softmax_j(Q^lh^l_i,K^lh_j^l)    \end{split}\end{equation}</script><p>$Q^l, K^l , V^l$ 是可学习的线性权重（分别表示注意力计算中的Query，Key，Value）。句子中的每个单词并行执行注意力机制，从而可以一次性获得他们已更新的特征——这是Transformer相对RNNs的另一个加分点，它使得模型能够逐字更新特征。</p><p><img src="https://i.loli.net/2021/05/31/vx6helFfAStIKj4.png" alt=""></p><h3 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h3><p>事实证明，要让这种点积注意力机制起作用是很难的——如果随机初始化处理得不好会使得整个学习过程失去稳定性。我们可以通过并行执行多个注意力“头”并将结果连接起来（现在每个注意力头都有单独的可学习权重）来克服这个问题：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  h^{l+1}_i &= Concat(head_1,...,head_K) O^l\\ head_k &= Attention(Q^{k,l} h_i^l, \ K^{k,l} h_j^l, \ V^{k,l}h_j^l)    \end{split}\end{equation}</script><p>其中， $Q^{k,l} ,K^{k,l}, V^{k,l}$ 是第 $k$ 个注意力投的可学习权重，而 $O^l$ 是一个向下的投影，可以用以匹配跨层的 $h_i^{l+1}$ 和 $h^l_i$ 的尺寸。此外多头注意力形成多个子空间，可以让模型去关注不同方面的信息。</p><h2 id="Transformer架构"><a href="#Transformer架构" class="headerlink" title="Transformer架构"></a>Transformer架构</h2><p><img src="https://i.loli.net/2021/05/31/F8obteYNsUVGpPZ.png" alt=""></p><p>下面是上文的多头Attention结构，但为什么Transformer的结构为什么是这样的？</p><p>注意力机制之后的词的特征可能在不同尺度或重要性上:</p><ol><li><p>这可能是由于某些词在将其他词的特征累加时具有非常集中或非常分散的注意力权重 $w_{ij}$</p></li><li><p>在单个特征/向量输入级别，跨多个注意力头 (每个可能会以不同的比例输出值) 进行级联可以导致最终向量 $h_i^{l+1}$ 的输入具有一个大范围的值。遵循传统的机器学习思路，在上述流程中增加一个归一化层似乎是合理的选择。</p></li></ol><p>对于上面的两个问题，Transformer用LayerNorm客服了问题2，LayerNorm在特征层级上进行归一化并学习一种仿射变换。  <a href="https://zhuanlan.zhihu.com/p/113233908">batchNormalization与layerNormalization的区别</a></p><p>对于问题1，通过求特征维度的平方根，来做缩放点积。</p><p>在LayerNorm之后，是FF-MLP</p><p>是一个控制尺度问题的技巧，具有特殊结构的考虑位置的双层MLP，在多头注意力之后，他们通过一个可学习的权重将 $h_i^{l+1}$  投影到一个更高的维度，在该维度中， $h_i^{l+1}$ 经过ReLU 非线性变换，然后投影回其原始维度，然后再进行另一个归一化操作。</p><script type="math/tex; mode=display">h^{l+1}_i = LN (MLP(LN(h_l^{l+1})))</script><p>不确定超参数化前馈子层背后的确切理由是什么，似乎也没有人对此提出疑问！我认为LayerNorm和缩放的点积不能完全解决突出的问题，因此大型MLP是一种可以相互独立地重新缩放特征向量的手段。</p><p>Transformer架构也非常适合非常深的网络，使NLP界能够在模型参数和扩展数据这两方面进行延伸。每个多头注意力子层和前馈子层的输入和输出之间的残差连接是堆叠Transformer层的关键（但为了清楚起见，在上图中省略了）。</p><h2 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h2><p>图神经网络（GNNs）或图卷积网络（GCNs）在图数据中建立节点和边的表示。它们是通过邻域聚合（或消息传递）来实现的，在邻域聚合中，每个节点从其邻域收集特征，以更新其周围的局部图结构表示。通过堆叠多个GNN层使得该模型可以将每个节点的特征传播到整个图中，从其邻居传播到邻居的邻居，依此类推。</p><p><img src="https://i.loli.net/2021/05/31/VgT8NGlDXIxLB3H.png" alt=""></p><p>以这个表情符号社交网络为例：由GNN产生的节点特征可用于预测性任务，例如识别最有影响力的成员或提出潜在的联系。</p><p>在他们最基本的形式中，GNNs通过以下方法来更新节点 $i$ 在 $l$ 层的隐藏层特征 $h$ 。</p><p>也就是先将节点自身特征 $h_i^{l}$ 和每个邻居节点 $j \ \ (j\in N(i))$  特征 $h_j^{l}$ 的聚合相加，然后在整体做一个非线性变换， 如下:</p><script type="math/tex; mode=display">h_i^{l+1} = \sigma (U^{l} h_i^l + \sum_{j\in N(j)}(V^l h_j^l))</script><p>其中， $U^l, V^l$ 是GNN 层的可学习权重矩阵。</p><p>邻居节点 $j\in N(i)$ 上的求和可以被其他输入大小不变的聚合函数代替，例如简单的 均值/最大值函数或其他更强大的函数（如通过注意机制的加权和）。</p><p>如果是GAT的话其实就变成了Transformer了</p><p><img src="https://i.loli.net/2021/05/31/YRcsK5h38HoS4On.png" alt=""></p><p>如果我们要执行多个并行的邻域聚合头，并且用注意力机制（即加权和）替换领域 上的求和 ，我们将获得图注意力网络（GAT）。加上归一化和前馈MLP，瞧，我们就有了Graph Transformer！</p><h3 id="在NLP中，句子就是由词全连接而成的图"><a href="#在NLP中，句子就是由词全连接而成的图" class="headerlink" title="在NLP中，句子就是由词全连接而成的图"></a>在NLP中，句子就是由词全连接而成的图</h3><p>为了使连接更加清晰，可以将一个句子看作一个完全连接的图，其中每个单词都连接到其他每个单词。现在，我们可以使用GNN来为图（句子）中的每个节点（单词）构建特征，然后我们可以使用它来执行NLP任务。</p><p><img src="https://i.loli.net/2021/05/31/eV8aQgnZ64rsNyv.png" alt=""></p><p>广义上来讲，这就是Transformers正在做的事情：Transformers是以多头注意力作为邻聚合函数的GNNs。标准GNNs从其局部邻域节点 $j\in N(i)$ 聚合特征，而NLP的Transfors 将整个句子视为局部邻域，在每个层聚合来自每个单词 $j\in S$的特征。 而NLP的Transformers将整个句子视为局部邻域，在每个层聚合来自每个单词 $j\in S$ 的特征。</p><p>重要的是，各种特定于问题的技巧（如位置编码、因果/掩码聚合、学习率表和大量的预训练）对于Transformers的成功至关重要，但在GNN界中却很少出现。同时，从GNN的角度看Transformers可以启发我们摆脱模型结构中的许多花哨的玩意。</p><h3 id="全连接图是NLP的最佳输入格式吗？"><a href="#全连接图是NLP的最佳输入格式吗？" class="headerlink" title="全连接图是NLP的最佳输入格式吗？"></a>全连接图是NLP的最佳输入格式吗？</h3><p>在统计NLP和ML之前，Noam Chomsky等语言学家致力于发展语言结构的最新理论，如语法树/图。Tree LSTMs已经尝试过这一点，但是也许Transformers/GNNs是可以让语言理论和统计NLP的领域结合得更加紧密的更好的架构？</p><h3 id="如何学习到长期依赖？"><a href="#如何学习到长期依赖？" class="headerlink" title="如何学习到长期依赖？"></a><strong>如何学习到长期依赖？</strong></h3><p>完全连通图使得学习词与词之间非常长期的依赖关系变得非常困难，这是完全连通图的另一个问题。这仅仅是因为图中的边数与节点数成二次方关系，即在n个单词的句子中，Transformer/GNN 将在 $n^2$ 上对单词进行计算，如果n很大，那将会是一个非常棘手的问题。</p><p>NLP界对长序列和依赖性问题的看法很有意思，例如，使用注意力机制在输入大小方面稀疏或自适应，在每一层中添加递归或压缩，以及使用对局部性敏感的哈希法进行有效的注意，这些都是 优化Transformers 有希望的想法。</p><p>有趣的是，还可以看到一些GNN界的想法被混入其中，例如使用句子图稀疏化的二进制分区似乎是另一种令人兴奋的方法。</p><p><img src="https://i.loli.net/2021/05/31/k1XmCxvfYMwuNIt.png" alt=""></p><h3 id="Transformers在学习神经网络的句法吗？"><a href="#Transformers在学习神经网络的句法吗？" class="headerlink" title="Transformers在学习神经网络的句法吗？"></a>Transformers在学习神经网络的句法吗？</h3><p>NLP界有几篇关于Transformers可能学到什么的有趣论文。其基本前提是，对句子中的所有词对使用注意力机制（目的是确定哪些词对最有趣），可以让Transformers学习特定任务句法之类的东西。</p><p>多头注意力中的不同头也可能“关注”不同的句法属性。</p><p>从图的角度来看，通过在完全图上使用GNN，我们能否从GNN在每一层执行邻域聚合的方法中恢复最重要的边线及其可能带来的影响？我还不太相信这种观点。</p><h3 id="为什么要用多头注意力？为什么要用注意力机制？"><a href="#为什么要用多头注意力？为什么要用注意力机制？" class="headerlink" title="为什么要用多头注意力？为什么要用注意力机制？"></a><strong>为什么要用多头注意力？为什么要用注意力机制？</strong></h3><p>我更赞同多头机制的优化观点——拥有多个注意力可以改进学习，克服不好的随机初始化。例如，这些论文表明，Transformers头可以在训练后“修剪”或“删除”，并且不会产生重大的性能影响。</p><p>多头邻聚合机制在GNNs中也被证明是有效的，例如在GAT使用相同的多头注意力，MoNet使用多个高斯核来聚合特征。虽然多头技巧是为了稳定注意力机制而发明的，但它能否成为提炼出额外模型性能的标准？</p><p>相反，具有简单聚合函数（如sum或max）的GNNs不需要多个聚合头来维持稳定的训练。如果我们不需要计算句子中每个词对之间的成对兼容性，对Transformers来说不是很好吗？</p><p>Transformers能从抛弃注意力中获益吗？Yann Dauphin和合作者最近的工作提出了另一种ConvNet架构。Transformers也可能最终会做一些类似于ConvNets的事情。</p><p><img src="https://i.loli.net/2021/05/31/6nlhQ2GLpBSeudq.png" alt=""></p><h3 id="为什么Transformers这么难训练？"><a href="#为什么Transformers这么难训练？" class="headerlink" title="为什么Transformers这么难训练？"></a><strong>为什么Transformers这么难训练？</strong></h3><p>阅读新的Transformer论文让我觉得，在确定最佳学习率表、预热策略和衰减设置时，训练这些模型需要一些类似于黑魔法的东西。这可能仅仅是因为模型太大，而且所研究的NLP任务非常具有挑战性。</p><p>但是最近的结果表明，这也可能是由于结构中归一化和残差连接的特定组合导致的。</p><p>在这一点上我很在意，但是也让我感到怀疑：我们真的需要代价昂贵的成对的多头注意力结构，超参数化的MLP子层以及复杂的学习计划吗？</p><p>我们真的需要具有大量碳足迹的（译者注：有人提出现在训练一个模型相当于5辆汽车一天的排碳量）大规模模型吗？</p><p>具有良好归纳偏差的架构难道不容易训练吗？</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zhuanlan.zhihu.com/p/110805093">为什么说Transformer就是图神经网络？</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;从RNN到LSTM到Transformer再到GNN&quot;&gt;&lt;a href=&quot;#从RNN到LSTM到Transformer再到GNN&quot; class=&quot;headerlink&quot; title=&quot;从RNN到LSTM到Transformer再到GNN&quot;&gt;&lt;/a&gt;从RNN到LST</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Do Multi-Hop Question Answering Systems Know How to Answer the Single-Hop Sub-Questions?</title>
    <link href="http://example.com/2021/05/31/Do-Multi-Hop-Question-Answering-Systems-Know-How-to-Answer-the-Single-Hop-Sub-Questions/"/>
    <id>http://example.com/2021/05/31/Do-Multi-Hop-Question-Answering-Systems-Know-How-to-Answer-the-Single-Hop-Sub-Questions/</id>
    <published>2021-05-31T01:24:46.000Z</published>
    <updated>2021-05-31T05:15:59.425Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Do-Multi-Hop-Question-Answering-Systems-Know-How-to-Answer-the-Single-Hop-Sub-Questions"><a href="#Do-Multi-Hop-Question-Answering-Systems-Know-How-to-Answer-the-Single-Hop-Sub-Questions" class="headerlink" title="Do Multi-Hop Question Answering Systems Know How to Answer the Single-Hop Sub-Questions?"></a>Do Multi-Hop Question Answering Systems Know How to Answer the Single-Hop Sub-Questions?</h1><p>这是一篇比较有意思的工作，但是出发点是多跳阅读理解的本质问题。</p><p>多跳QA需要一个模型来检索和整合来自多个段落的信息来回答问题，作者认为现有的评估标准，EM和F1并不能证明在多大程度上学会了多跳推理能力。</p><p>所以作者根据多跳QA中的桥接实体生成了一千个相关的子问题，来测试模型的能力，并期望这样能说明一些问题。</p><h2 id="做法"><a href="#做法" class="headerlink" title="做法"></a>做法</h2><p>当设计一个多跳问题时，我们要求模型去检索一系列句子作为证据，然后对他们进行推理来回答问题。作者设计了一个HotpotQA干扰项集，的子问题集，期望模型如果具有了多跳的推理能力，多跳的问题可以回答的话，那么单跳问题也可以回答。但是这个单跳问题不是凭空出现的和原问题不相关的问题。如下图所示：</p><p><img src="https://i.loli.net/2021/05/31/YEQ2HtyLSrqU1FD.png" alt=""></p><p>这是一个典型的桥接问题，问题是：罗斯为阿诺德·施瓦辛格饰演的前纽约警探主演的一部电影做宣传是在哪一年？</p><p>想要回答问题，我们就必须先知道施瓦辛格在哪个电影里扮演了纽约警探，也就是必须找到桥梁实体 Gold Para2 中的电影《End of Days》才能回答。</p><p>那么子问题的建立就很自然的可以分为，</p><p>1、施瓦辛格正在哪个电影里扮演了纽约警探？</p><p>2、罗斯在那一年为电影《End of Days》做了宣传？</p><p>第一个问题的答案正好是桥梁实体，第二个问题的答案是最终答案。</p><p>作者认为只有模型能够完整的回答这些问题，说明模型就具备了多跳推理能力。</p><p>生成方法是半自动的：</p><ul><li>首先，我们通过预测断点将每个源问题分解成若干子串</li><li>其次，进行post processed，生成两个子问题。使用一些启发式方法从段落中提取子问题的答案。</li><li>最后，将生成的候选评价实例发送给人工验证。</li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者认为有些预测答案，虽然部分匹配EM=0但是语义上是正确的，也应该被算作预测正确。如下这种：</p><p><img src="https://i.loli.net/2021/05/31/8xBnLoPbdfKmDiI.png" alt=""></p><p>新的评估标准：给定黄金答案文本跨度 $a_g$ 和预测答案文本 $a_p$，如果满足以下两个要求之一，则它们部分匹配：</p><script type="math/tex; mode=display">f1 > 0.8\\f1>0.6 \land \{(a_g\ contains\ a_{p} ) \lor (a_p \ contains \ a_g) \}</script><p>$f1$ 值大于0.8，直接认为符合要求， 或者，$f1$ 大于0.6 ，且标准答案文本跨度包含了预测文本的答案或预测答案包含了标准答案。</p><p>Baseline 选用开源的CogQA、DFGN、DecompRC</p><p><img src="https://i.loli.net/2021/05/31/FI9gEo7fLJ3UjBO.png" alt=""></p><p>实验结果发现CogQA稍微好一些</p><p><img src="https://i.loli.net/2021/05/31/jq1aPVeZrJYBEQu.png" alt=""></p><p>模型有很高的概率没有答对其中一个问题。</p><p>作者将这些示例称为模型故障案例：模型故障案例在所有正确回答的多跳问题中所占的百分比被定义为模型故障率。</p><p>CogQA PM下的故障率: $(6.1+16.5+3.4)/(40.9+6.1+16.5+3.4) \times100\% = 38.86\%$ </p><p><img src="https://i.loli.net/2021/05/31/TfhW2lJBSLtbRP3.png" alt=""></p><p>所评估的所有三个模型都有很高的模型失败率，这表明这些模型学会了回答复杂的问题，而没有探索推理过程的多个步骤。当使用EM和PM分数进行评估时，也会出现同样的现象。</p><blockquote><p>After analyzing the model failure cases, we ob- serve a common phenomenon that there is a high similarity between the words in the second sub- question and the words near the answer in the con- text. The model has learned to answer multi-hop question by local pattern matching, instead of going through the multiple reasoning steps. For the ex- ample presented in Figure 1, the model may locate the answer “<em>1999</em>” for the multi-hop question by matching the surrounding words “ <em>Guns N Roses</em>” in the second sub-question. Despite answering the multi-hop question correctly, the model fails to identify the answer of the first sub-question which it is expected to retrieve as a multi-hop QA system.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Do-Multi-Hop-Question-Answering-Systems-Know-How-to-Answer-the-Single-Hop-Sub-Questions&quot;&gt;&lt;a href=&quot;#Do-Multi-Hop-Question-Answering-S</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch RNN之pack_padded_sequence()和pad_packed_sequence()</title>
    <link href="http://example.com/2021/05/29/Pytorch-RNN%E4%B9%8Bpack-padded-sequence-%E5%92%8Cpad-packed-sequence/"/>
    <id>http://example.com/2021/05/29/Pytorch-RNN%E4%B9%8Bpack-padded-sequence-%E5%92%8Cpad-packed-sequence/</id>
    <published>2021-05-29T02:20:02.000Z</published>
    <updated>2021-05-29T02:44:31.370Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pytorch-RNN之pack-padded-sequence-和pad-packed-sequence"><a href="#Pytorch-RNN之pack-padded-sequence-和pad-packed-sequence" class="headerlink" title="Pytorch RNN之pack_padded_sequence()和pad_packed_sequence()"></a>Pytorch RNN之pack_padded_sequence()和pad_packed_sequence()</h1><h2 id="为什么有pad和pack操作？"><a href="#为什么有pad和pack操作？" class="headerlink" title="为什么有pad和pack操作？"></a>为什么有pad和pack操作？</h2><p>先看一个例子，这个batch中有5个sample</p><p><img src="https://i.loli.net/2021/05/29/AZFV4WzLUKsgpfa.png" alt=""></p><p>如果不用pack和pad操作会有一个问题，什么问题呢？</p><p>比如上图，句子“Yes”只有一个单词，但是padding了多余的pad符号，这样会导致LSTM对它的表示通过了非常多无用的字符，这样得到的句子表示就会有误差，更直观的如下图：</p><p><img src="https://i.loli.net/2021/05/29/hT5ab7rQgunimtB.png" alt=""></p><p>那么我们正确的做法应该是怎么样呢？</p><p>在上面这个例子，我们想要得到的表示仅仅是LSTM过完单词”Yes”之后的表示，而不是通过了多个无用的“Pad”得到的表示：如下图：</p><p><img src="https://i.loli.net/2021/05/29/9efQa4sVrN8ulFd.png" alt=""></p><h2 id="torch-nn-utils-rnn-pack-padded-sequence"><a href="#torch-nn-utils-rnn-pack-padded-sequence" class="headerlink" title="torch.nn.utils.rnn.pack_padded_sequence()"></a>torch.nn.utils.rnn.pack_padded_sequence()</h2><p>这里的<code>pack</code>，理解成压紧比较好。 将一个 填充过的变长序列 压紧。（填充时候，会有冗余，所以压紧一下）</p><p>其中pack的过程为：（注意pack的形式，不是按行压，而是按列压）</p><p><img src="https://i.loli.net/2021/05/29/KTaQHmkbOIC3h8x.png" alt=""></p><p>​                                      （下面方框内为<code>PackedSequence</code>对象，由data和batch_sizes组成）</p><p>pack之后，原来填充的 PAD（一般初始化为0）占位符被删掉了。</p><p>输入的形状可以是(T×B×<em> )。<code>T</code>是最长序列长度，<code>B</code>是<code>batch size</code>，`</em><code>代表任意维度(可以是0)。如果</code>batch_first=True<code>的话，那么相应的</code>input size<code>就是</code>(B×T×*)`。</p><p><code>Variable</code>中保存的序列，应该按序列长度的长短排序，长的在前，短的在后。即<code>input[:,0]</code>代表的是最长的序列，<code>input[:, B-1]</code>保存的是最短的序列。</p><blockquote><p><code>NOTE：</code> 只要是维度大于等于2的<code>input</code>都可以作为这个函数的参数。你可以用它来打包<code>labels</code>，然后用<code>RNN</code>的输出和打包后的<code>labels</code>来计算<code>loss</code>。通过<code>PackedSequence</code>对象的<code>.data</code>属性可以获取 <code>Variable</code>。</p></blockquote><p>参数说明:</p><ul><li>input (Variable) – 变长序列 被填充后的 batch</li><li>lengths (list[int]) – <code>Variable</code> 中 每个序列的长度。</li><li>batch_first (bool, optional) – 如果是<code>True</code>，input的形状应该是<code>B*T*size</code>。</li></ul><p>返回值:</p><p>一个<code>PackedSequence</code> 对象。</p><h2 id="torch-nn-utils-rnn-pad-packed-sequence"><a href="#torch-nn-utils-rnn-pad-packed-sequence" class="headerlink" title="torch.nn.utils.rnn.pad_packed_sequence()"></a>torch.nn.utils.rnn.pad_packed_sequence()</h2><p>填充<code>packed_sequence</code>。</p><p>上面提到的函数的功能是将一个填充后的变长序列压紧。 这个操作和pack_padded_sequence()是相反的。把压紧的序列再填充回来。填充时会初始化为0。</p><p>返回的Varaible的值的<code>size</code>是 <code>T×B×*</code>, <code>T</code> 是最长序列的长度，<code>B</code> 是 batch_size,如果 <code>batch_first=True</code>,那么返回值是<code>B×T×*</code>。</p><p>Batch中的元素将会以它们长度的逆序排列。</p><p>参数说明:</p><ul><li>sequence (PackedSequence) – 将要被填充的 batch</li><li>batch_first (bool, optional) – 如果为True，返回的数据的格式为 <code>B×T×*</code>。</li></ul><p>返回值: 一个tuple，包含被填充后的序列，和batch中序列的长度列表</p><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p><img src="https://i.loli.net/2021/05/29/ok9UMKLO5RYAlWP.png" alt=""></p><p><img src="https://i.loli.net/2021/05/29/rkuqZlphAoPFyi7.png" alt=""></p><p>此时PackedSequence对象输入RNN后，输出RNN的还是PackedSequence对象 </p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/lindaxin/p/8052043.html">https://www.cnblogs.com/lindaxin/p/8052043.html</a></p><p><a href="https://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence">https://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence</a></p><p><a href="https://zhuanlan.zhihu.com/p/34418001?edition=yidianzixun&amp;utm_source=yidianzixun&amp;yidian_docid=0IVwLf60">https://zhuanlan.zhihu.com/p/34418001?edition=yidianzixun&amp;utm_source=yidianzixun&amp;yidian_docid=0IVwLf60</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Pytorch-RNN之pack-padded-sequence-和pad-packed-sequence&quot;&gt;&lt;a href=&quot;#Pytorch-RNN之pack-padded-sequence-和pad-packed-sequence&quot; class=&quot;heade</summary>
      
    
    
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Multi-hop Attention Graph Neural Networks</title>
    <link href="http://example.com/2021/05/27/Multi-hop-Attention-Graph-Neural-Networks/"/>
    <id>http://example.com/2021/05/27/Multi-hop-Attention-Graph-Neural-Networks/</id>
    <published>2021-05-27T02:27:12.000Z</published>
    <updated>2021-06-01T02:45:03.452Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Multi-hop-Attention-Graph-Neural-Networks"><a href="#Multi-hop-Attention-Graph-Neural-Networks" class="headerlink" title="Multi-hop Attention Graph Neural Networks"></a>Multi-hop Attention Graph Neural Networks</h1><p>GAT中的attention运算只能关注节点相连节点表达，这种机制不考虑不直接相连但又有很重要信息的节点表达。</p><p>所以提出了多跳注意力图神经网络(MAGNA)，这是一种将多跳上下文信息融入到注意力计算的每一层的方法。</p><p>其将注意力分数分散到整个网络，相当于增加了每一层的GNN的“感受野”。</p><p><img src="https://i.loli.net/2021/05/27/xCrtn9LhXwiZ7sK.png" alt=""></p><p>如左图，考虑A和D节点，普通的attention层只计算直接相连节点的注意力分数，如 $ \alpha<em>{A,D} $   , 但如果C的信息很重要，   $ \alpha</em>{C,D}=0 $  关注度却为0。并且，单个GAT层中A和D节点之间的运算只依赖于自己的表达，而不依赖于它们的图邻域上下文。其相当于每一层只关注了一阶邻居范围的感受野，虽然堆叠多层GNN可以扩大这个范围，但GNN层数一多就会有过平滑的问题。</p><p>再看右图，MAGNA 层的改进方法是</p><ul><li>通过扩散多跳注意力捕捉  $ \alpha<em>{D,C}’ $  表达为  $ \alpha</em>{D,C}’ = f([\alpha<em>{B,C},\alpha</em>{D,B}]) $</li><li>基于图邻接矩阵的权值，通过分散注意力来考虑节点之间的所有路径，从而增强图结构学习。MAGNA利用D的节点特征进行A和B之间的注意力计算，这意味着MAGNA中的两跳注意力是基于上下文的。</li></ul><p>总之，GAT中的一跳注意机制限制了探索更广泛的图形结构与注意权重之间关系的能力。</p><p>本文提出了多跳注意图神经网络(MAGNA)，这是一种针对图结构数据的有效的多跳自注意机制。Magna使用了一种新颖的图形注意力扩散层(图1)，其中我们首先计算边上的注意力权重(用实心箭头表示)，然后使用边上的注意力权重通过注意力扩散过程计算断开的节点对之间的自我注意力权重(虚线箭头)。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="参数定义"><a href="#参数定义" class="headerlink" title="参数定义"></a>参数定义</h3><p>图 $G =(V,E)$ , $E\in V\times V$ ,  V 节点集有 $N_n$ 个 ，E 边集有 $N_e$ 个</p><p>节点 v 到其类型的映射为 $ \phi: V \rightarrow \Tau $ , 边e 到其关系类型的映射 $\psi : E \rightarrow R$</p><p>节点的embedding : $X \in \mathbb{R}^{N_n\times d_n}$ , 边的embedding: $R\in \mathbb{R}^{N_r\times d_r}$</p><p>其中 $N_n = |V|, N_r=|R|$ , $d_n,d_r$ 是节点和边类型的embedding维度。</p><p>Embedding 的每行 $x_i = X[i:]$ 表示节点 $v_i (1\le i\le N_n)$ 的embedding ， $r_j=R[j:] ,  r_j(1\le j\le N_r)$  </p><p>首先看一下MAGNA 模块的整体结构</p><p><img src="https://i.loli.net/2021/05/27/sd32RDYOLyvK64l.png" alt=""></p><p>有点像Transformer block，现在GNN的包装越来越往Transformer based模型上靠了。</p><p>他传入节点和关系embedding，会首先经历一个对于每个节点的多头注意力层（这里和GAT一样），然后是注意力扩散、 Layer Norm、前向传播层和两个残差链接。</p><h3 id="Multi-hop-Attention-Diffusion"><a href="#Multi-hop-Attention-Diffusion" class="headerlink" title="Multi-hop Attention Diffusion"></a>Multi-hop Attention Diffusion</h3><p>Attention diffusion是每一层中用于计算MAGNA‘s的attention分数。首先第一阶段，计算每一条边上的attention分数。第二阶段，用扩散注意力计算多条邻居的注意力。</p><h4 id="Edge-Attention-Computation"><a href="#Edge-Attention-Computation" class="headerlink" title="Edge Attention Computation."></a>Edge Attention Computation.</h4><p>在每一层 $l$ 处，为每个三元组 $(v_i，r_k，v_j)$ 计算矢量消息。 为了计算在 $l+1$ 层的表示，将关联的三元组的所有消息聚合成一条消息，然后使用该消息更新 $v_j^{l+1}$。</p><p>在第一阶段， 一个边 $(v_i,r_k,v_j)$ 的注意力分数是由以下计算而来：</p><script type="math/tex; mode=display">s_{i,k,j}^{(l)} = LeakyRelu(v_a^{(l)} tanh(W_h^{(l)} h_i^{(l)} || W_t^{(l)}h_j^{(l)} || W_r^{(l)}r_k))</script><p>$ W_h^{(l)} , W_t^{(l)}\in \mathbb{R}^{d^{(l)}\times d^{(l)}} , W_r^{(l)}\in \mathbb{R}^{d^{(l)}\times d_r} , v_a^{(l)}\in \mathbb{R }^{1\times 3d^{(l)}}$   可共享的可训练参数。</p><p>$h_i^{(l)}\in \mathbb{R}^{d^{(l)}}$ 是第 $l$ 层第 $i$ 个节点的embedding。 $h_i^{(0)} = x_i$</p><p>$r_k (1\le k \le N_r)$ 是可训练的第 $k$ 个关系类型的embedding</p><p>将上式应用到graph中的每一条边后，得到 attention score matrix $ S^{(l)}$:</p><script type="math/tex; mode=display">S^{(l)}_{i,j} =\begin{cases}s_{i,j,k}^{(l)}, &\ (v_i,r_k,v_j)\ appears\ in\ G\\\infty, &otherwise\end{cases}</script><p>随后，我们通过对得分矩阵 $S^{(l)}$ 执行逐行Softmax来获得注意力矩阵 $A^{(l)}_{i,j} = Softamax(S^{(l)})$</p><p>$A^{(l)}_{i,j}$ 就定义为在第 $l$ 层中当 从节点 $j$ 和 节点 $i$ 聚合消息时的关注值。</p><p>这里其实和GAT差不多 只是多了不同种边和节点。 </p><h4 id="Attention-Diffusion-for-Multi-hop-Neighbors"><a href="#Attention-Diffusion-for-Multi-hop-Neighbors" class="headerlink" title="Attention Diffusion for Multi-hop Neighbors"></a>Attention Diffusion for Multi-hop Neighbors</h4><p>通过以下注意力扩散过程，在网络中将计算不直接连接的节点之间的注意力。</p><p>该过程基于1-hop 注意力矩阵A的幂 为：</p><script type="math/tex; mode=display">A = \sum^{\infty}_{i=0}\theta_iA^i \ \ \ where \sum_{i=0}^{\infty}\theta_i = 1 \ and \ \theta_i \gt 0</script><p>其中 $\theta<em>i$ 是 attention decay factor 并且 $\theta_i \gt \theta</em>{i+1}$ ，</p><p>注意矩阵的幂 $A^i$ 给出了从节点 $h$ 到节点 $t$ 的长度为 $i$ 的关系路径的数量，从而增加了注意的感受野。</p><p>重要的是，该机制允许两个节点之间的注意力不仅取决于它们之前的层表示，而且还考虑到节点之间的路径，从而有效地在不直接连接的节点之间创建 attention shotcuts</p><p>在实现过程中，作者使用几何分布 (geometric distribution)    $θ_i=α(1−α)^i$，其中 $α∈(0，1]$  。</p><p>该选择基于the inductive bias ，即较远的节点在消息聚合中应该被较小的权重，并且具有到目标节点的不同关系路径长度的节点以独立的方式被顺序加权。</p><p>此外，请注意，如果定义$θ<em>0=α∈(0，1] ，A_0=I$ ，则上面的公式。利用关注矩阵A和移动概率 $α$ ，给出了图上的<a href="https://blog.csdn.net/likeyou1314918273/article/details/106895794/">Personal Page Rank</a>。因此，扩散注意力权重 $A</em>{i,j}$ 可以看作是节点 $j$ 对节点 $i$ 的影响。 </p><p>同时对与目标节点关系路径长度不同的节点权重应该相互独立。因此，本文定义了基于特征聚合的graph attention diffusion：</p><script type="math/tex; mode=display">AttDiff(G,H^{(l)}, \Theta) = A H^{(l)}</script><p> 其中 $\Theta$ 为注意力参数集合。 </p><h4 id="Approximate-Computation-for-Attention-Diffusio"><a href="#Approximate-Computation-for-Attention-Diffusio" class="headerlink" title="Approximate Computation for Attention Diffusio"></a>Approximate Computation for Attention Diffusio</h4><p>对于大图，公式（3）的计算开销巨大，而DAGCN需要通过 $AH^l$进行信息聚合，本文通过定义一个数列 $Z^K$, 当 $K \rightarrow \infty$时，该数列能收敛到$AH^l$的值：</p><script type="math/tex; mode=display">Z^0 = H^L, Z^{k+1} = (1-\alpha)AZ^{(k)} + \alpha Z^0 \\lim_{K\rightarrow \infty} Z^{K} = AH^{l}</script><p>证明请参考原文。上述的近似使得attention的复杂度保持在$O(|E|)$。很多真实世界网络具有小世界（small-world ）特征，在这种情况下，较小的K值就足够。对于具有较大直径的图，选择较大的K和较小 $\alpha$ 。</p><h3 id="Multi-hop-Attention-based-GNN-Architecture"><a href="#Multi-hop-Attention-based-GNN-Architecture" class="headerlink" title="Multi-hop Attention based GNN Architecture"></a>Multi-hop Attention based GNN Architecture</h3><p>图2提供了可多次堆叠的MAGNA block 的架构概览。</p><h4 id="Multi-head-Graph-Attention-Diffusion-Layer"><a href="#Multi-head-Graph-Attention-Diffusion-Layer" class="headerlink" title="Multi-head Graph Attention Diffusion Layer"></a>Multi-head Graph Attention Diffusion Layer</h4><p>在不同的视角联合关注来自不同表示子空间的信息。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \hat H^{(l)} &= MultiHead(G, \hat H^{(l)}) =(||_{i=1}^M head_i) W_o \\head_i &=  AttDiff(G, \hat H^{(l)}, \Theta_i) \\\hat H^{(l)} &= LayerNorm(H^{(l)})    \end{split}\end{equation}</script><p>方程中以递归的方式计算注意力扩散。增加了层归一化，有助于稳定递归计算过程。</p><h4 id="Deep-Aggregation"><a href="#Deep-Aggregation" class="headerlink" title="Deep Aggregation"></a>Deep Aggregation</h4><p>此外，还包含一个完全连接的前馈子层，它由两层前馈网络组成。我们还在两个子层中添加了层标准化和残差连接，从而为每个block提供了更具表现力的聚合步骤</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \hat H^{(l+1)} &= \hat H^{(l)} + H^{(l)} \\ H^{(l+1)} &= W_2^{(l)} ReLU(W_1^{(l)} LayerNorm(\hat H^{(l+1)})) + \hat H^{(l+1)}    \end{split}\end{equation}</script><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://i.loli.net/2021/05/28/rVp3kq5IsGweYgR.png" alt=""></p><p><img src="https://i.loli.net/2021/05/28/ZzVJehsHGlPoFiY.png" alt=""></p><h2 id="Reviewer"><a href="#Reviewer" class="headerlink" title="Reviewer"></a>Reviewer</h2><blockquote><p>The central question of the reviewers’ discussion was whether the contribution of this paper was significant enough or too incremental. The discussion emphasized relevant literature which already considers multi-hop attention (e.g. <a href="https://openreview.net/forum?id=rkKvBAiiz">https://openreview.net/forum?id=rkKvBAiiz</a> [Cucurull et al.], <a href="https://ieeexplore.ieee.org/document/8683050">https://ieeexplore.ieee.org/document/8683050</a> [Feng et al.], <a href="https://arxiv.org/abs/2001.07620">https://arxiv.org/abs/2001.07620</a> [Isufi et al.]), and which should have served as baseline. In particular, the experiment suggested by R3 was in line with some of these previous works, which consider “a multi-hop adjacency matrix “ as a way to increase the GAT’s receptive field. This was as opposed to preserving the 1-hop adjacency matrix used in the original GAT and stacking multiple layers to enlarge the receptive field, which as noted by the authors, may result in over-smoothed node features. The reviewers acknowledged that there is indeed as slight difference between the formulation proposed in the paper and the one in e.g. [Cucurull et al.]. The difference consists in calculating attention and then computing the powers with a decay factor vs. increasing the receptive field first by using powers of the adjacency matrix and then computing attention. Still, the multi-hop GAT baseline of [Cucurull et al.] could be extended to use a multi-hop adjacency matrix computed with the diffusion process from [Klicpera 2019], as suggested by R3. In light of these works and the above-mentioned missing baselines, the reviewers agreed that the contribution may be viewed as rather incremental (combining multi-hop graph attention with graph diffusion). The discussion also highlighted the potential of the presented spectral analysis, which could be strengthened by developing new insights in order to become a stronger contribution (see R2’s suggestions).</p><p>Proposed methodology being more powerful than GAT is arguable:<br>When the attention scores for indirectly connected neighbors are still computed based on the immediate neighbors’ attention scores, it is not convincing enough to be argued as more powerful than GAT, which learns attention scores over contextualized immediate neighbors.Also, the approximate realization of the model described in Eqn: 5 follows a message-passing style to propagate attention scores. Suppose it is to be argued that standard message-passing-based diffusion is not powerful enough to get a good immediate neighbor representation that encodes neighbors’ information from far away. In that case, it is not immediately clear how a similar diffusion, when used for propagating attention scores from immediate neighbors to neighbors multiple hops away, will be more powerful. </p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Multi-hop-Attention-Graph-Neural-Networks&quot;&gt;&lt;a href=&quot;#Multi-hop-Attention-Graph-Neural-Networks&quot; class=&quot;headerlink&quot; title=&quot;Multi-hop </summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
    <link href="http://example.com/2021/05/24/GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training/"/>
    <id>http://example.com/2021/05/24/GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training/</id>
    <published>2021-05-24T12:17:55.000Z</published>
    <updated>2021-05-25T07:10:36.604Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training"><a href="#GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training" class="headerlink" title="GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training"></a>GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</h1><p>一个自监督的利用对比学习来学习GNN内在可迁移的先验知识的预训练框架，目的是想得到一个可迁移性强，迁移领域广的，通用的表达。为了捕捉跨多个网络的通用网络拓扑特性，预训练任务为，跨网络子图实例判别。所以预训练的重点在结构相似性层面上，并且不带节点属性。</p><h2 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h2><p>基本思想是对输入图中的实例进行采样，将每个实例视为自己的一个不同的类，并学习对这些实例进行编码和区分。</p><p>具体地说，作者认为GCC需要回答三个问题，才能学习到可转移性好的结构模式：</p><ul><li>实例是什么？</li><li>分辨规则是什么？</li><li>如何对实例进行编码？</li></ul><p>对于者三个问题作者展开研究。</p><h2 id="预训练任务——子图实例判别"><a href="#预训练任务——子图实例判别" class="headerlink" title="预训练任务——子图实例判别"></a>预训练任务——子图实例判别</h2><p>任务的目标是根据顶点的局部结构来区分它们。</p><p><img src="https://i.loli.net/2021/05/24/vH3s8Rm45liJPxh.png" alt=""></p><p>对于每个顶点，从它的多跳ego网络中抽取子图作为实例。</p><p>GCC的目的是区分从某个顶点采样的子图和从其他顶点采样的子图。并且不假设顶点和子图来自同一个图，所以图编码器被迫捕获不同输入图的通用模式。</p><h3 id="定义子图实例"><a href="#定义子图实例" class="headerlink" title="定义子图实例"></a>定义子图实例</h3><p>对比学习框架的成功很大程度上取决于数据实例的定义。CV和NLP任务可以直接将实例定义为图像或句子。</p><p> 但是，这些想法不能直接扩展到图形数据，因为图形中的实例没有明确定义。</p><p>因为节点是无属性的节点，所以要表示一个节点，就要采用以他为中心的局部结构。</p><p>具体地说，对于某个顶点v，定义一个实例为它的r-ego网络：</p><p>对于一个 r-ego 网络 $G = (V,E)$ ，$V$ 是节点集并且 $E \subseteq V\times V$</p><p>对于其中心点v， 他的 r-neighbors 定义为 $S_v={u:d(u,v)\le r}$ , 其中 $d(u,v)$ 为邻居u节点到v节点的最短路径。</p><p>顶点v的r-ego图，记为 $G_v$，是由 $S_v$ 引出的子图。 下图就是一个2-ege图，右边是预训练过程。</p><p><img src="https://i.loli.net/2021/05/25/Epkna1vfKVIU4hG.png" alt=""></p><h3 id="定义实例相似性判别准则"><a href="#定义实例相似性判别准则" class="headerlink" title="定义实例相似性判别准则"></a>定义实例相似性判别准则</h3><p>在cv中，同一图像的两个随机数据增加(例如，随机裁剪、随机调整大小、随机颜色抖动、随机翻转等)被视为相似的实例对。</p><p>在GCC中将同一r-ego网络的两个随机数据扩充看作一个相似实例对，并将数据扩充定义为图采样。</p><p>GCC的图采样遵循三个步骤</p><ul><li>重新启动的随机行走(RWR) ：从ego图的节点v出发，随机采样子图结构，并以一定概率返回到v节点。得到的采样子图可以被认为是一种数据扩增，像cv那样。</li><li>子图归纳：导出子图随机游走抽样(ISRW)。</li><li>匿名化：匿名化被采样的子图 $\hat G_v$ ,并重新排序。</li></ul><h3 id="定义图编码器"><a href="#定义图编码器" class="headerlink" title="定义图编码器"></a>定义图编码器</h3><p>给定两个采样子图 $x^q$ 和 $x^k$，GCC分别通过两个图神经网络编码器 $f_q$ 和 $f_k$ 对其进行编码。从技术上讲，任何图神经网络都可以作为这里的编码器，而GCC模型对不同的选择并不敏感。因为不考虑节点属性，而大多数GNN模型需要把节点特征/属性作为输入。为了弥补这一差距，作者建议利用每个采样子图的图结构来初始化顶点特征。</p><p>目标Loss采用对比学习经典的InfoNCE:</p><script type="math/tex; mode=display">L = -log \frac{exp(q^Tk_+/\tau)}{\sum_{i=0}^K exp(q^Tk_i/\tau)}</script><p>其中 $q = f_q(x^q) , k=f_k(x^k)$ ，$q$ 是query 对应的目录为 $K+1$ 个编码的keys: ${k_0,…,k_K}$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training&quot;&gt;&lt;a href=&quot;#GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-</summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode63_DP走方格</title>
    <link href="http://example.com/2021/05/23/Leetcode63-DP%E8%B5%B0%E6%96%B9%E6%A0%BC/"/>
    <id>http://example.com/2021/05/23/Leetcode63-DP%E8%B5%B0%E6%96%B9%E6%A0%BC/</id>
    <published>2021-05-23T03:10:04.000Z</published>
    <updated>2021-05-23T03:41:35.934Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Leetcode63-DP走方格"><a href="#Leetcode63-DP走方格" class="headerlink" title="Leetcode63_DP走方格"></a>Leetcode63_DP走方格</h1><p>题目：<a href="https://leetcode-cn.com/problems/unique-paths-ii/">https://leetcode-cn.com/problems/unique-paths-ii/</a></p><h2 id="状态定义"><a href="#状态定义" class="headerlink" title="状态定义"></a>状态定义</h2><p>$dp[i][j]$ 表示走到格子 $(i,j)$ 的路径数</p><h2 id="状态计算"><a href="#状态计算" class="headerlink" title="状态计算"></a>状态计算</h2><p>如果网格 $(i,j)$ 上有障碍物，则 $dp[i][j]$ 值为0，走到这个格子的方法数为0</p><p>否则网格 $(i,j)$ 可以从网格 $(i-1,j)$ 或者网格 $(i,j-1)$ 走过来，因此走到该格子的方法数为走到网格 $<br>(i-1,j)$ 和网格$(i,j-1)$ 的方法数之和，即: $dp[i,j] = dp[i-1,j] + dp[i,j-1]$</p><script type="math/tex; mode=display">dp[i][j] = \begin{cases}dp[i-1,j] + dp[i,j-1], & \text{(i,j) 上无障碍物}  \\0, & \text{(i,j)上有障碍物}\end{cases}</script><h2 id="初始条件"><a href="#初始条件" class="headerlink" title="初始条件"></a>初始条件</h2><p>第 1 列的格子只有从其上边格子走过去这一种走法，因此初始化 $dp[i][0]$ 值为 1，存在障碍物时为 0；</p><p>第一行的格子只有从其左边格子走过去这一种走法，因此初始化$dp[0][j]$ 值为1，存在障碍物事为0；</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[m][n];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m &amp;&amp; obstacleGrid[i][<span class="number">0</span>] == <span class="number">0</span>; i++) &#123;</span><br><span class="line">    dp[i][<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; n &amp;&amp; obstacleGrid[<span class="number">0</span>][j] == <span class="number">0</span>; j++) &#123;</span><br><span class="line">    dp[<span class="number">0</span>][j] = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">uniquePathWithObstacles</span><span class="params">(<span class="keyword">int</span>[][] grid)</span></span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (obstacleGrid == <span class="keyword">null</span> || obstacleGrid.length == <span class="number">0</span>) &#123;</span><br><span class="line">       <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">int</span> m=grid.length;</span><br><span class="line">  <span class="keyword">int</span> n=grid[<span class="number">0</span>].length;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">int</span>[][] dp=<span class="keyword">new</span> <span class="keyword">int</span>[m][n];</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m &amp;&amp; grid==<span class="number">0</span>;i++)&#123;</span><br><span class="line">       dp[i][<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n &amp;&amp; grid==<span class="number">0</span>;i++)&#123;</span><br><span class="line">      dp[<span class="number">0</span>][j] = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;m;i++)&#123;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;n;j++)&#123;</span><br><span class="line">          <span class="keyword">if</span>(grid[i][j]==<span class="number">0</span>)&#123;</span><br><span class="line">            dp[i][j] = dp[i-<span class="number">1</span>][j] + dp[i][j-<span class="number">1</span>];</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="keyword">return</span> dp[m-<span class="number">1</span>][n-<span class="number">1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>同理没有障碍物就删掉判断条件。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Leetcode63-DP走方格&quot;&gt;&lt;a href=&quot;#Leetcode63-DP走方格&quot; class=&quot;headerlink&quot; title=&quot;Leetcode63_DP走方格&quot;&gt;&lt;/a&gt;Leetcode63_DP走方格&lt;/h1&gt;&lt;p&gt;题目：&lt;a href=&quot;ht</summary>
      
    
    
    
    
    <category term="DP" scheme="http://example.com/tags/DP/"/>
    
  </entry>
  
  <entry>
    <title>Dynamically Fused Graph Network for Multi-hop Reasoning</title>
    <link href="http://example.com/2021/05/22/Dynamically-Fused-Graph-Network-for-Multi-hop-Reasoning/"/>
    <id>http://example.com/2021/05/22/Dynamically-Fused-Graph-Network-for-Multi-hop-Reasoning/</id>
    <published>2021-05-22T04:35:49.000Z</published>
    <updated>2021-05-22T08:34:06.616Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Dynamically-Fused-Graph-Network-for-Multi-hop-Reasoning"><a href="#Dynamically-Fused-Graph-Network-for-Multi-hop-Reasoning" class="headerlink" title="Dynamically Fused Graph Network for Multi-hop Reasoning"></a>Dynamically Fused Graph Network for Multi-hop Reasoning</h1><p>受人类分步推理行为的启发，提出了动态融合图网络(DFGN)，回答那些需要多个分散证据并在这些证据上进行推理的问题。不依赖于任何额外的预定义知识基础，能回答开放领域中的问题。</p><h2 id="大体过程"><a href="#大体过程" class="headerlink" title="大体过程"></a>大体过程</h2><p><img src="https://i.loli.net/2021/05/22/OzWcZmVIGBq9yut.png" alt=""></p><p>给出了一个问题和三个段落。DFGN通过从多个段落构造实体图，预测动态掩码选择子图，沿着图传播信息，最后将图中的信息传回文本来定位答案，从而对事实进行多步推理。节点是实体引用，带颜色节点表示潜在实体。边由共现关系构造而成，每一步都由DFGN选择灰色圆圈内的子图来处理。</p><h2 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h2><ol><li>由于并不是每个文档都包含相关信息，基于多跳文本的问答需要从多个段落中滤除噪声并提取有用信息。</li></ol><blockquote><p>作者思路: 通过DFGN这种多轮迭代的动态实体图来解决，如上图DFGN每一轮都通过掩码预测模块在动态图上生成和推理，其中不相关的实体被屏蔽，只有推理远被保留也就是灰色圆圈内的子图，这种做法缓解了误差的传播问题。</p><p>此外，DFGN的预测mask的过程可隐含地导出推理链，可以解释推理结果。针对开放域语料库基本真值推理链难以定义和标注的问题，提出了一种可行的弱监督掩码学习方法。提出了一种新的度量来评估预测推理链和构建的实体图的质量。</p></blockquote><p>但这样做有用的信息被mask了怎么办？怎么确定的mask范围？其实是用了注意力机制计算实体的权重，后文写。</p><ol><li>不能直接从实体图中提取出答案，现实中，答案可能不在所提取的实体图的实体中。</li></ol><blockquote><p>作者思路：在DFGN中设计了一个fusion处理模块，不仅将信息从文档聚合到实体图(Doc2graph)，还将实体图的信息传播回文档表示(Raph2doc)。通过文档token和实体在每一跳迭代地执行融合过程，然后从文档令牌获得最终结果答案。Doc2graph和Graph2doc的融合过程以及动态实体图共同改善了文档信息和实体图之间的交互性，从而减少了噪声，从而提高了答案的准确性。</p></blockquote><p>相当于在tokens的表达中加入了推理图中的推理信息，这个思想还是挺不错的。</p><h2 id="具体过程"><a href="#具体过程" class="headerlink" title="具体过程"></a>具体过程</h2><p>模仿人类对QA的推理过程。从查询感兴趣的实体开始，聚焦于开始实体周围的单词，连接到在邻居中发现的或由相同表面信息链接的某些相关实体，重复该步骤以形成推理链，并且落在可能是答案的某个实体或片段上。</p><p>DFGN包含五个组件：</p><p><img src="https://i.loli.net/2021/05/22/DMpXHmNs6PYVbad.png" alt=""></p><ul><li>段落选择子网络</li><li>实体图构建模块</li><li>编码层</li><li>用于多跳推理的融合模块</li><li>最终预测层</li></ul><h3 id="Fusion-Block"><a href="#Fusion-Block" class="headerlink" title="Fusion Block"></a>Fusion Block</h3><p>这里只着重写一下Fusion Block</p><p>在为查询Q和上下文C计算嵌入后，剩下的挑战是如何识别支持实体和潜在答案的文本跨度。</p><p>Fusion Block从 $Q_0$和 $C_0$ 开始，寻找一步支持实体。</p><ol><li>通过计算实体嵌入从tokens将信息传递到实体(Doc2Graph Flow)</li><li>在实体图上进行信息传递</li><li>传递信息从实体图到文本tokens(Graph2Doc flow)</li></ol><p><img src="https://i.loli.net/2021/05/22/zlremMsBCHVwILJ.png" alt=""></p><h4 id="Doc2Graph-Flow"><a href="#Doc2Graph-Flow" class="headerlink" title="Doc2Graph Flow"></a>Doc2Graph Flow</h4><p>由于通过NER工具识别每个实体，因此利用与实体相关联的文本跨度来计算实体嵌入(Doc2Graph)。</p><p>为此，作者定义了一个 M 是01矩阵，$M<em>{i,j}$ 的值意思是，如果第 $i$ 个token在第j个实体的范围内，则 $M</em>{i,j}=1$</p><p>这个 M 用于选择与实体相关联的文本范围。这其实是一个池化操作，将上下文C 的嵌入变成了实体E的嵌入 。</p><p>$E<em>{t-1} = [e</em>{t-1,1},…,e_{t-1,N}] \in R^{2d_2\times N}$  这个模块作者定位 Tok2Ent，就是上图的左边部分。</p><h4 id="Dynamic-Graph-Attention"><a href="#Dynamic-Graph-Attention" class="headerlink" title="Dynamic Graph Attention"></a>Dynamic Graph Attention</h4><p>然后是图中的中间部分，动态图注意力部分。</p><p>在从输入上下文Ct−1获得实体嵌入后，我们应用图神经网络将节点信息传播给它们的邻居。我们提出了一种动态图注意机制来模仿人类的循序渐进的探索和推理行为。与q越相关，邻居节点从附近接收的信息越多。</p><p>首先通过在实体上创建 Soft Mask 来识别与查询相关的节点。它充当信息看门人，即只允许与查询有关的那些实体节点传播信息。</p><p>使用查询嵌入和实体嵌入之间的注意力网络来预测 Soft Mask $m_t$，其目的是表示第 t 个推理步骤中的开始实体：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \hat q^{(t-1)} &= MeanPooling(Q^{t-1}) \\ \gamma^{(t)}_i &= \frac {\hat q^{(t-1)} V^{(t)} e_i^{(t-1)} }{\sqrt{d_2}}\\ m^{(t)} &= \sigma([\gamma^{(t)}_1,...,\gamma_{N}^{(t)}]) \\ \hat E^{(t-1)} &= [m_1^{(t)}e_1^{(t-1)} ,..., m^{(t)}_N e_{N}^{(t-1)}]    \end{split}\end{equation}</script><p>其实就是用注意力机制计算每个实体嵌入的权重。$V_t$ 是线性映射矩阵。</p><p>总之就是通过Soft Mask，得到想要的开始推理的实体，并将它送入图中初始化。噪声信息不放入图中，相当于过滤掉。</p><blockquote><p>此外，作者引入一个弱监督信号来诱导每个 Fusion Block 处的软掩码来匹配启发式掩码。对于每个训练案例，启发式掩码包含从查询中检测到的开始掩码，并且通过对相邻矩阵应用广度优先搜索(BFS)获得的附加BFS掩码给出开始掩码。然后，将预测的软掩码和启发式之间的二进制交叉熵损失添加到目标。（跳过那些无法从查询中检测到起始掩码的情况）。</p></blockquote><p>在送入图后的信息聚合方式是使用的GAT，但有一点作者和以前的GAT不同，</p><p>在Dynamic Graph Attention中，每个节点隐层的列进行求和，形成一个新的实体状态，其中包含它从邻居收到的全部信息：</p><script type="math/tex; mode=display">e_i^{(t)} =ReLU(\sum_{j\in B_i} \alpha_{j,i}^{(t)} h_j^{(t)})</script><p>其中 $B_i$ 是邻居实体集合中的第 i 个实体，所以一次更新后的实体表达为 $E^{(t)} =[e_1^{(t)},…,e_N^{(t)}] $</p><h4 id="Updating-Query"><a href="#Updating-Query" class="headerlink" title="Updating Query"></a>Updating Query</h4><p>一条推理链包含多个步骤，每一步新访问的实体就是下一步的起始实体。</p><p>为了预测下一步期望的起始实体，引入了一种Updating Query机制，通过当前步骤的实体嵌入来更新查询嵌入。</p><script type="math/tex; mode=display">Q^{(t)} = Bi-Attention(Q^{(t-1)},E^{(t)})</script><h4 id="Graph-to-Document-Flow"><a href="#Graph-to-Document-Flow" class="headerlink" title="Graph to Document Flow"></a>Graph to Document Flow</h4><p>利用Tok2Ent和动态图关注度，实现了实体级的推理步骤。然而，不受限制的答案仍然无法追溯。</p><p>为了解决这个问题，开发了一个Graph2Doc模块来保持信息从实体回流到上下文中的tokens。因此，与答案有关的文本跨度可以在上下文中本地化。</p><p>使用Doc2Graph Flow中一样的M矩阵，将$C_{t-1}$ 中的先前tokens嵌入和 对应于该令牌的关联实体嵌入对应回来。</p><p>M中的每一行对应一个令牌，因此如果该令牌出现在实体的提及中，就使用它从 $E_t$ 中选择一个实体的嵌入。利用LSTM层进一步处理该信息，以产生下一级上下文表示： $C^{(t)} = LSTM([C^{(t-1)}, ME^{(t)T}])$</p><h3 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h3><p>有四个输出维度，1.支持句，2.答案的开始位置，3.答案的结束位置，4.答案的类型。</p><p>使用四个同构的LSTM $F_i$ 是逐层堆叠的。最后Fusion Block的上下文表示被发送到第一个LSTM $F_0$。每个$F_i$输出的logits为$  O∈R^{m×d2}$   ，并计算这些logit上的交叉熵损失。</p><p><img src="https://i.loli.net/2021/05/22/6ov3shgtbaNZWDP.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://i.loli.net/2021/05/22/udk43YL7pFQDCZX.png" alt=""></p><h3 id="作者提出的推理链质量的衡量标准"><a href="#作者提出的推理链质量的衡量标准" class="headerlink" title="作者提出的推理链质量的衡量标准"></a>作者提出的推理链质量的衡量标准</h3><p>ESP(实体级支持)分数</p><blockquote><p>推理链是实体图上的一条有向路径，因此高质量的实体图是良好推理的基础。由于NER模型的精度有限和图结构的不完备性，31.3%的发展集中的情况不能进行完整的推理过程，其中至少有一个支持语句不能通过实体图到达，即在这个句子中没有实体被NER模型识别。我们将这类情况命名为“缺失支撑实体”，这种情况的比率可以用来评价图的构造质量。<br>下面，在给出ESP(实体级支持)分数之前，我们首先给出几个定义。</p></blockquote><p><img src="https://i.loli.net/2021/05/22/Eft6vqQFXSKu7ek.png" alt=""></p><h3 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h3><p><img src="https://i.loli.net/2021/05/22/ryUW4c5s7VbQwKx.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Dynamically-Fused-Graph-Network-for-Multi-hop-Reasoning&quot;&gt;&lt;a href=&quot;#Dynamically-Fused-Graph-Network-for-Multi-hop-Reasoning&quot; class=&quot;h</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>120三角形最小路径和</title>
    <link href="http://example.com/2021/05/22/120%E4%B8%89%E8%A7%92%E5%BD%A2%E6%9C%80%E5%B0%8F%E8%B7%AF%E5%BE%84%E5%92%8C/"/>
    <id>http://example.com/2021/05/22/120%E4%B8%89%E8%A7%92%E5%BD%A2%E6%9C%80%E5%B0%8F%E8%B7%AF%E5%BE%84%E5%92%8C/</id>
    <published>2021-05-22T03:33:49.000Z</published>
    <updated>2021-05-23T03:10:13.228Z</updated>
    
    <content type="html"><![CDATA[<h1 id="120三角形最小路径和"><a href="#120三角形最小路径和" class="headerlink" title="120三角形最小路径和"></a>120三角形最小路径和</h1><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">给定一个三角形，找出自顶向下的最小路径和。</span><br><span class="line">每一步只能移动到下一行中相邻的结点上。</span><br><span class="line"></span><br><span class="line">相邻的结点 </span><br><span class="line">在这里指的是 下标 与 上一层结点下标 相同或者等于 上一层结点下标 + 1 的两个结点。</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">例如，给定三角形：</span><br><span class="line"></span><br><span class="line">[</span><br><span class="line">     [2],</span><br><span class="line">    [3,4],</span><br><span class="line">   [6,5,7],</span><br><span class="line">  [4,1,8,3]</span><br><span class="line">]</span><br><span class="line">自顶向下的最小路径和为 11（即，2 + 3 + 5 + 1 &#x3D; 11）。</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">说明：</span><br><span class="line"></span><br><span class="line">如果你可以只使用 O(n) 的额外空间（n 为三角形的总行数）来解决这个问题，</span><br></pre></td></tr></table></figure><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>若定义 f(i,j) 为 (i, j)点到底边的最小路径和，则递归求解式为：</p><script type="math/tex; mode=display">f(i,j) = mian(f(i+1,j) , f(i+1,j+1)) + triangle[i][j]</script><p>由此，将任一点到底边的最小路径和，转化为了与该点相邻两点到底边的最小路径和中的较小值，再加上该点本身的值。</p><p>得出(解一) 递归解法。</p><h3 id="解一（递归）"><a href="#解一（递归）" class="headerlink" title="解一（递归）"></a>解一（递归）</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">minimumTotal</span><span class="params">(List&lt;List&lt;Integer&gt;&gt; tri)</span></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> dfs(tri,<span class="number">0</span>,<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">dfs</span><span class="params">(List&lt;List&lt;Integer&gt;&gt; tri, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span></span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (i==tri.size()) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">return</span> Math.min(dfs(tri,i+<span class="number">1</span>,j), dfs(tri,i+<span class="number">1</span>, j+<span class="number">1</span>)) + tri.get(i).get(j);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>缺点：暴力搜索会有大量重复计算，引出解法二，结合记忆化数组进行优化。</p><h3 id="解法二：-递归-记忆化"><a href="#解法二：-递归-记忆化" class="headerlink" title="解法二：(递归+记忆化)"></a>解法二：(递归+记忆化)</h3><p>定义一个二位数组用来记忆化</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Integer[][] memo;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">minimumTotal</span><span class="params">(List&lt;List&lt;Integer&gt;&gt; tri)</span></span>&#123;</span><br><span class="line">  memo = <span class="keyword">new</span> Integer[tri.size()][tri.size()];</span><br><span class="line">  <span class="keyword">return</span> dfs(tri,<span class="number">0</span>,<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">dfs</span><span class="params">(List&lt;List&lt;Integer&gt;&gt; tri, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(i==tri.size()) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span>(memo[i][j]!=<span class="keyword">null</span>) <span class="keyword">return</span> memo[i][j];</span><br><span class="line">  <span class="keyword">return</span> memo[i][j] = Math.min(dfs(tri, i+<span class="number">1</span>, j), dfs(tri, i+<span class="number">1</span>, j+<span class="number">1</span>)) + tri.get(i).get(j);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>时间复杂度：$O(N^2)$，N为三角形的行数。<br>空间复杂度：$O(N^2)$，N为三角形的行数。</p><h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>定义二维 dp 数组，将解法二中「自顶向下的递归」改为「自底向上的递推」。</p><p>状态表示：$dp[i][j]$ 表示从点 $(i,j)$ 到底边的最小路径和</p><p>状态计算：$dp[i][j] = min(dp[i+1][j] , dp[i+1][j+1]]) + tri[i][j]$</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">minmumTotal</span><span class="params">(List&lt;List&lt;Integer&gt;&gt; tri)</span></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> n = tri.size();</span><br><span class="line">  <span class="comment">// dp[i][j] 表示从点(i,j)到底边的最小路径和</span></span><br><span class="line">  <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[n+<span class="number">1</span>][n+<span class="number">1</span>];</span><br><span class="line">  <span class="comment">// 从三角形的最后一行开始递推</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=n-<span class="number">1</span>;i&gt;=<span class="number">0</span>;i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;=i;j++)&#123;</span><br><span class="line">      dp[i][j]=Math.min(dp[i+<span class="number">1</span>][j], dp[i+<span class="number">1</span>][j+<span class="number">1</span>]) + tri.get(i).get(j);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> dp[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>时间复杂度：$O(N^2)$，N为三角形的行数。<br>空间复杂度：$O(N^2)$，N为三角形的行数。</p><h3 id="空间优化"><a href="#空间优化" class="headerlink" title="空间优化"></a>空间优化</h3><p>上一个解定义了一个N行N列的 dp数组</p><p>但实际递推中发现，计算 $dp[i][j]$ 时，只用到了下一行的 $dp[i+1][j]  和 dp[i+1][j+1]$</p><p>因此dp 数组不需要定义N行，只需定义1行</p><p>把 $i$ 所在维度去掉，将就可以将 $O(N^2)$ 的空间复杂度优化成$ O(N)$</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">minimumTotal</span><span class="params">(List&lt;List&lt;Integer&gt;&gt; tri)</span> </span>&#123;</span><br><span class="line"> <span class="keyword">int</span> n = tri.size();</span><br><span class="line">  <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[n+<span class="number">1</span>];</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=n-<span class="number">1</span>; j&gt;=<span class="number">0</span>; i--)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;=i; j++)&#123;</span><br><span class="line">      dp[j] = Math.min(dp[j], dp[j+<span class="number">1</span>]) + tri.get(i).get(j);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> dp[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;120三角形最小路径和&quot;&gt;&lt;a href=&quot;#120三角形最小路径和&quot; class=&quot;headerlink&quot; title=&quot;120三角形最小路径和&quot;&gt;&lt;/a&gt;120三角形最小路径和&lt;/h1&gt;&lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;</summary>
      
    
    
    
    
    <category term="DP" scheme="http://example.com/tags/DP/"/>
    
  </entry>
  
  <entry>
    <title>Unsupervised Multi-hop Question Answering by Question Generation</title>
    <link href="http://example.com/2021/05/21/Unsupervised-Multi-hop-Question-Answering-by-Question-Generation/"/>
    <id>http://example.com/2021/05/21/Unsupervised-Multi-hop-Question-Answering-by-Question-Generation/</id>
    <published>2021-05-21T13:30:26.000Z</published>
    <updated>2021-05-21T17:01:32.274Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Unsupervised-Multi-hop-Question-Answering-by-Question-Generation"><a href="#Unsupervised-Multi-hop-Question-Answering-by-Question-Generation" class="headerlink" title="Unsupervised Multi-hop Question Answering by Question Generation"></a>Unsupervised Multi-hop Question Answering by Question Generation</h1><p>第一个研究无监督多跳QA的。</p><p>MQA-QG致力于探索在不参考任何人工标记的多跳问答对的情况下训练性能良好的多跳QA模型的可能性。</p><p>无标签数据源分为同构和异构，即作者考虑了两种数据源，一种是结构化的表格文本数据，一种是纯文本数据。</p><p>如果推理链中只有一种数据源的叫同构，两种数据源的叫异构。</p><p>仅使用生成的训练数据，和有监督的性能做对比，对于Hybridge QA和HotpotQA数据集分别有61%和83%的有监督学习性能。其中hotpotQA(同构数据)，Hybridge QA(异构数据)。</p><h2 id="大体过程"><a href="#大体过程" class="headerlink" title="大体过程"></a>大体过程</h2><p>从每个数据源中选择或生成相关信息，将多个信息整合成一个问题。</p><ul><li><p>首先定义一系列operators去检索或生成相关信息。</p></li><li><p>然后定义六个推理图每个对应于一种类型的多跳问题，且是建立在operators之上的计算图。</p></li></ul><p><img src="https://i.loli.net/2021/05/21/dz9iH1rRSjg68ox.png" alt=""></p><p>如上图，是生成table2text的问题。给出输入$(table, text)$ ，桥梁实体Jenson Button被operators找出来，他链接了文本数据和表格数据。</p><p>然后再用一个叫 (QGwithEnt operator)的操作生成了右边的一个简单的问题：简森巴顿是什么时候出生？</p><p>左边被 (DescribeEnt operator) 生成了一个描述桥梁实体的句子：简森巴顿是2004年美国大奖赛排名第4位的车手。</p><p>最后再由 BridgeBlend operator 混合成一个多跳问题：2004年美国大奖赛排名第四的车手是什么时候出生的？</p><h2 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h2><p>给一个问题 q 和一系列文本 $C = {C_1,…,C_n}$ 其中 $C_i$ 可能是文章、table，如果推广到多模态还可能是image。</p><p>QA model 表示为 $p_{\theta}(a|q,C)$</p><p>在本文中，作者只考虑两跳问题，并将所需的上下文表示为 $C_i$ 和 $C_j$。</p><p>主要有三个成分分别是 </p><ul><li>operators：由规则或现成的预训练模型实现的原子操作，用于从输入上下文 $(C_i、C_j)$ 检索、生成或融合相关信息。</li><li>reasoning graphs：不同的推理图定义了不同类型的以operators构建的多跳QA推理链。通过执行推理图生成训练 (q，a) 对。</li><li>question filtration：去除不相关和不自然的(q，a)对，给出多跳问答的最终训练集D。</li></ul><p><img src="https://i.loli.net/2021/05/21/zQhscxV4MLRoWwn.png" alt=""></p><h3 id="Operators"><a href="#Operators" class="headerlink" title="Operators"></a>Operators</h3><p>定义了8个基本operator ，分为三种类型：</p><ul><li>选择：从单个上下文中检索相关信息</li><li>生成：从单个上下文中生成信息</li><li>融合：将多个检索或生成的信息进行融合，以构造多跳问题。</li></ul><h4 id="FindBridge"><a href="#FindBridge" class="headerlink" title="FindBridge"></a>FindBridge</h4><p>大多数多跳问题依赖于连接不同输入上下文的实体整合多条信息，即桥梁实体。</p><p>FindBridge将两个上下文 $(C_i、C_j)$ 作为输入，并提取出现在 $C_i$ 和 $C_j$ 中的实体作为桥实体。如在第一个图中，提取“Jenson Button”作为桥实体。</p><h3 id="FindComEnt"><a href="#FindComEnt" class="headerlink" title="FindComEnt"></a>FindComEnt</h3><p>在生成比较类型的多跳问题时，我们需要决定为桥实体比较什么属性。</p><p>Find-Coment提取潜在的可比性，从文本中提取具有NER类型的实体作为比较属性(国籍、位置、日期时间和数字)。</p><p><img src="https://i.loli.net/2021/05/21/desLTyfBGOnzgqR.png" alt=""></p><p>如上图的 #1 步骤</p><h4 id="QGwithAns和QGwithEnt"><a href="#QGwithAns和QGwithEnt" class="headerlink" title="QGwithAns和QGwithEnt"></a>QGwithAns和QGwithEnt</h4><p>这两个都是生成简单的单跳问题的， 随后会被用来合成多跳问题。</p><p>作者使用预培训好的Google T5模型微调在SQuAD来实现这两个操作员。给定上下文-问题-答案三元组训练集D={(c，q，a)}，我们在两个任务上联合微调模型。给定上下文-问题-答案三元组训练集 $D={(c，q，a)}$ ，在两个任务上联合微调模型。</p><ul><li>QGwithAns的目标是生成一个问题Q，其中a为答案，给定(c，a)为输入。</li><li>QGwithEnt旨在生成包含特定实体e的问题Q，给定(c，e)作为输入。</li></ul><p><img src="https://i.loli.net/2021/05/21/IR6M35PrYKt91ba.png" alt=""></p><h4 id="CompBlend"><a href="#CompBlend" class="headerlink" title="CompBlend"></a>CompBlend</h4><p>基于两个单跳问题Q1和Q2组成比较型多跳问题。这两个问题询问两个不同实体e1和e2的相同比较属性p。我们通过将p、e1和e2填入预定义模板来形成多跳问题。</p><p><img src="https://i.loli.net/2021/05/21/yiafYuGkUSeWl4B.png" alt=""></p><h4 id="DescribeEnt"><a href="#DescribeEnt" class="headerlink" title="DescribeEnt"></a>DescribeEnt</h4><p>给定table $T$ 和表中的目标实体 e，DescribeEnt operator 基于T中的信息生成描述实体 e 的语句。使用GPT-TabGen模型，该模型首先使用模板将T 变成文档 $P_T$，然后将 $P_T$ 送到GPT-2生成输出句子 Y</p><p><img src="https://i.loli.net/2021/05/22/W9gHwV2PKpRtulr.png" alt=""></p><p>为了避免 $P_T$中存在不相关信息，应用了一个仅描述目标实体所在行的模板。然后，在 ToTTo 数据集上通过最大化 $p(Y|P_T;β)$ 的来微调模型，β表示模型参数。ToTTo 数据集是一种受控的表格到文本生成的大规模数据集。</p><h4 id="QuesToSent"><a href="#QuesToSent" class="headerlink" title="QuesToSent"></a>QuesToSent</h4><p>通过应用《Transforming question answering datasets into natural language inference datasets》定义的语言规则将问题Q转换成其陈述形式s。</p><h4 id="BridgeBlend"><a href="#BridgeBlend" class="headerlink" title="BridgeBlend"></a>BridgeBlend</h4><p>基于1)桥接实体e。2)包含e的单跳问题q。3)描述e的句子s。组成桥接型多跳问题。</p><p><img src="https://i.loli.net/2021/05/22/Rqj6PJW5kCOnS7x.png" alt=""></p><p>作者通过应用简单但有效的规则来实现这一点，该规则将 Q中的桥接实体 e替换为 “the [MASK] that s”，并采用预训练的Bert-Large来填充[MASK]。</p><h3 id="Reasoning-Graphs"><a href="#Reasoning-Graphs" class="headerlink" title="Reasoning Graphs"></a>Reasoning Graphs</h3><p>基于以上的operators ，作者定义了6种类型的推理图，生成不同类型的问题。</p><p> 每个问题都是有向无环图 G，每个G对应一个operator。</p><p>一共有四类：</p><ul><li>Table-to-Text：表格和文本之间的桥接式问题，答案来自文本。</li><li>Text-to-Table ：表格和文本之间的桥接式问题，答案来自表格。</li><li>Text-to-Text : 桥接类型，两边都是文本。</li><li>Comparison：基于两篇文章的比较型问题。</li></ul><p>通过执行每个推理图来生成QA对。通过定义新的算子和推理图，可以很容易地扩展到其他模态和推理链。</p><p><img src="https://i.loli.net/2021/05/22/9fWtj7U2vu4lbIi.png" alt=""></p><p><img src="https://i.loli.net/2021/05/22/aAs6NjF2TlPYiQd.png" alt=""></p><p><img src="https://i.loli.net/2021/05/22/mnT1JuCsK25ZrVx.png" alt=""></p><h3 id="Question-Filtration"><a href="#Question-Filtration" class="headerlink" title="Question Filtration"></a>Question Filtration</h3><p>使用了两种方法来提炼生成的QA对的质量</p><ul><li>Filtration：使用预先训练的gpt-2模型来过滤解决那些不流利或不自然的问题。选择困惑度最低的前N个样本作为生成的数据集来训练多跳QA模型。</li><li>Paraphrasing：基于BART模型训练一个问题解释模型来解释每个生成的问题。</li></ul><p>实验表明，过滤给QA模型带来了明显的改进。然而，在实验中展示了释义产生了更多类似人类的问题，但是引入了语义漂移问题，从而损害了QA性能。</p><p>为了生成更自然的问题，我们试图训练一个基于BART的问题释义模型，以对每个生成的问题进行语法分析。观察到，通过将原问题的冗余部分改写成更简洁的表达，释义确实产生了更多的流行性问题。然而，释义引入了“语义漂移”问题，即释义后的疑问句改变了原疑问句的语义。我们认为这会影响QA性能，因为它会产生问答不一致的嘈杂样本。</p><p>作者认为在无监督多跳问答中，对于生成的问题，语义的忠实性比流利性更重要。这就解释了为什么设计手工制作的推理图来保证语义的忠实性。然而，如何在保持语义忠实性的同时生成流畅的类人问题是未来的一个重要方向。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>对于Hybridge QA，问题按其答案是来自表格(56%)还是来自段落(44%)进行划分。大约80%的Hybridge QA问题需要桥接式推理。</p><p><img src="https://i.loli.net/2021/05/22/TpgkmquVjYtrPMf.png" alt=""></p><p>在Hybridge QA和HotpotQA上的QA性能。</p><p><img src="https://i.loli.net/2021/05/22/IUMvlYxrGH4j8WC.png" alt=""></p><p><img src="https://i.loli.net/2021/05/22/AKh2cWJMs9UYrn3.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Unsupervised-Multi-hop-Question-Answering-by-Question-Generation&quot;&gt;&lt;a href=&quot;#Unsupervised-Multi-hop-Question-Answering-by-Question-Ge</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>Is Graph Structure Necessary for Multi-hop Question Answering?</title>
    <link href="http://example.com/2021/05/19/Is-Graph-Structure-Necessary-for-Multi-hop-Question-Answering/"/>
    <id>http://example.com/2021/05/19/Is-Graph-Structure-Necessary-for-Multi-hop-Question-Answering/</id>
    <published>2021-05-19T05:04:14.000Z</published>
    <updated>2021-05-19T07:02:21.658Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Is-Graph-Structure-Necessary-for-Multi-hop-Question-Answering"><a href="#Is-Graph-Structure-Necessary-for-Multi-hop-Question-Answering" class="headerlink" title="Is Graph Structure Necessary for Multi-hop Question Answering?"></a>Is Graph Structure Necessary for Multi-hop Question Answering?</h1><p>图结构对多跳问答有多大贡献？</p><p>这是一篇只有6页的类似实验报告的论文。是在作者在做实验时发现，图结构和邻接矩阵都是与任务相关的先验知识，graph-attention可以看作是self-attention的特例。实验和可视化分析表明，self-attention和transformer可以代替graph-attention或整个图形结构，并且效果无明显变化，从而对图网络在自然语言处理任务上的应用能力提出了质疑。并希望未来引入图结构纳入自然语言处理任务的工作，应说明其必要性和优越性。</p><p>作者使用 <strong>Dynamically Fused Graph Network </strong> 那篇文章作为baseline开展了研究。</p><h2 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h2><p>首先描述baseline模型，证明了只有当预先训练的模型以feature-based的方式使用时，图结构才能发挥重要作用。虽然在fine-tuning方法中使用预先训练的模型，但图结构可能没有帮助。</p><p>作者复现了DFGN，并修改了预训练模型的使用。该模型首先利用检索器从候选集合中选择相关段落，并将其提供给基于图形的阅读器。实体图中的所有实体都由独立的NER模型抽取而来。</p><ul><li><p>检索器：在HotpotQA任务中使用Roberta Large模型来计算查询与每个候选段落之间的相关得分。我们对得分小于0.1%的段落进行过滤，最大入选段落数为3，入选段落拼接为Context C</p></li><li><p>编码层：我们将查询Q和上下文C连接起来，并将序列提供给另一个Roberta，结果被进一步送到Bi-attention layer 以从编码层获得表示。</p></li><li><p>图形融合块：给定第 t-1 跳的上下文表达 $C<em>{t-1}$ , 将token送入到mean-max pooling得到实体图$H</em>{t-1}\in R^{2d\times N}$, 其中N是实体的数量。之后采用的是图注意力层更新节点表达:</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \beta_{i,j}^{(t)} &= LeakyReLU(W_t^T[h_i^{(t-1)},h_j^{(t-1)}])\\ \alpha_{i,j}^{(t)} &= \frac {exp(\beta^{(t)}_{i,j})}{\sum_{k\in N_i}\alpha^{(t)}_{i,k}h_k^{(t-1)}}\\ h_i^{(t)} &= ReLU(\sum_{k\in N_i} \alpha^{(t)}_{i,k}h_k^{(t-1)})    \end{split}\end{equation}</script><p>还有查询实体关注、查询更新机制、弱监督等模块。</p></li><li><p>构建实体图：上下文中具有相同提及文本的实体被连接、同一句中出现的实体是相连的。</p></li><li><p>最终预测层</p></li></ul><p>作者将将预训练语言模型的输出直接送到预测层，由于基线模型与DFGN的主要区别在于我们在fine-tuning方法中使用了large的预训练模型，而不是基于feature-based的方法，因此在两种不同的设置下进行了实验。在HotpotQA上提交的结果，并进行了消融实验对比如下：</p><p><img src="https://i.loli.net/2021/05/19/Kmq726TuL9aU1wF.png" alt=""></p><p>结果表明在fine-tuning下有没有图结构效果不明显，在feature-based下图结构是有明显作用的。</p><h2 id="graph-attention是self-attention的一种特例"><a href="#graph-attention是self-attention的一种特例" class="headerlink" title="graph-attention是self-attention的一种特例"></a>graph-attention是self-attention的一种特例</h2><p>基于人工定义的规则和图结构的邻接矩阵可以看作是先验知识边，可以通过self-attention或Transformers来学习</p><p>基于以上的实验结果表明，自我注意或变形金刚在多跳问题回答中可能具有优势。</p><blockquote><p>解决多跳问题的关键是通过查询在原文中找到对应的实体。然后，构建从这些起始实体到其他相同或共现实体的一条或多条推理路径。如图1所示，以前的工作通常从多个段落中提取实体，并将这些实体建模为实体图。邻接矩阵是由人工定义的规则构建的，这些规则通常是实体之间的共现关系。从这个角度看，图的结构和邻接矩阵都可以看作是与任务相关的先验知识。实体图结构限制了模型只能基于实体进行推理，邻接矩阵辅助模型忽略一跳中的非相邻节点。然而，可能是在没有任何先验知识的情况下，模型仍然可以学习实体到实体的注意模式。</p><p>此外从上文图注意力的公式来看，不难发现图注意力与自我注意具有相似的形式。在前向传播中，实体图中的每个节点都会计算与其他连接节点的关注度得分。如图1所示，当图中的节点完全连接时，图注意力将退化为普通的自我关注层。因此，图形注意可以看作是自我注意的一种特例。</p></blockquote><p>基于以上的想法，作者将图结构做成全连接的实体和self-attention做了一次实验比较，为了验证整个图结构能否被transformer取代。</p><p><img src="https://i.loli.net/2021/05/19/TAMlO8FU5KeLpma.png" alt=""></p><p>实验结果如图，与自我注意相比，图形注意并没有显示出明显的优势。</p><p><img src="https://i.loli.net/2021/05/19/RGcaHXL1o9QOUzI.png" alt=""></p><p>对于图形注意和自我注意在不同密度区间的结果。尽管邻接矩阵的密度不同，但图形注意与自我注意的结果是一致的。这意味着自我关注可以学会忽略不相关的实体。</p><p>此外，transformer显示出强大的推理能力。只有叠加两层变压器才能获得与DFGN相当的效果。</p><p>并且作者分布从Entity2Entity、Attribute2Entity、Coreference2Entity、Entity2Sentence角度可实话了注意力权重在预训练语言模型中效果：</p><p><img src="https://i.loli.net/2021/05/19/sDHM6uNOE4tIX5S.png" alt=""></p><p>认为基于实体的图网络忽略了后三种链接的信息，我认为可能异质图更多的解决这个问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Is-Graph-Structure-Necessary-for-Multi-hop-Question-Answering&quot;&gt;&lt;a href=&quot;#Is-Graph-Structure-Necessary-for-Multi-hop-Question-Answeri</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>Navicat配置远程连接sqlite</title>
    <link href="http://example.com/2021/05/19/Navicat%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5sqlite/"/>
    <id>http://example.com/2021/05/19/Navicat%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5sqlite/</id>
    <published>2021-05-18T16:17:13.000Z</published>
    <updated>2021-05-20T10:52:58.097Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Navicat配置远程连接sqlite"><a href="#Navicat配置远程连接sqlite" class="headerlink" title="Navicat配置远程连接sqlite"></a>Navicat配置远程连接sqlite</h1><p><img src="https://i.loli.net/2021/05/20/qdk2IJAea8BhGxf.png" alt=""></p><p>远程sqlite其实是需要php环境的</p><p>因为配置远程需要一个php文件当做通道，而且要能在浏览器上可访问这个文件</p><p>所以首先找到这个文件，这个文件一般在Navicat的目录下，把它放在db的同目录下</p><p><img src="https://i.loli.net/2021/05/19/YCiGenoyAQvwIDE.png" alt=""></p><p>然后就是安装apache2 和php</p><p><a href="https://blog.csdn.net/qq_37264323/article/details/90586239">https://blog.csdn.net/qq_37264323/article/details/90586239</a></p><p>我是按这个装的很简单</p><p>按步骤按完 apache2和php就可以了其余的不用装</p><p>然后就是配置可访问，先打开80端口的访问网址看看Apache启动成功否</p><p>之后就是配置虚拟路径，因为一般我们的db都不是放在Apache默认的www目录下</p><p>我的服务器系统是ubantu</p><p>Apache安装目录在：/etc/apache2</p><p>我要配置的位置是 :vim /etc/apache2/sites-available/000-default.conf  </p><p><img src="https://i.loli.net/2021/05/19/PsukcIXBlH6U9pS.png" alt=""></p><p>在virtualhost内放置上面 alias 和directory 配置好路径</p><p>service apache2 restart</p><p>但是访问<a href="http://10.12.1.150/data1/ntunnel_sqlite.php">http://10.12.1.150/data1/ntunnel_sqlite.php</a> </p><p>会出现forbidden</p><p>我是参考这个解决的 </p><p><img src="https://i.loli.net/2021/05/19/vMgWrR8ayShqTHu.png" alt=""></p><p><a href="https://www.cnblogs.com/starof/p/4685999.html">https://www.cnblogs.com/starof/p/4685999.html</a></p><p>最终可访问：</p><p><img src="https://i.loli.net/2021/05/19/MHCveaGhg59qc3y.png" alt=""></p><p>但目前存在一个问题 SQLite3 class available No 链接不成功</p><p><img src="https://i.loli.net/2021/05/19/DrnsE9yZ8PT6jvC.png" alt=""></p><p>我怀疑是php对sqlite3的某个配置没有配好</p><p>vim /etc/php/7.2/apache2/php.ini</p><p><img src="https://i.loli.net/2021/05/20/bci5VXT3GdEpmZW.png" alt=""></p><p>找到这两个地方把注释去掉 </p><blockquote><p>apt-get install php7.2-sqlite3</p></blockquote><p><img src="https://i.loli.net/2021/05/20/ISnMmaUVirpyYG8.png" alt=""></p><p><img src="https://i.loli.net/2021/05/20/FKb1XUYksjNRinw.png" alt=""></p><p><img src="https://i.loli.net/2021/05/20/BCE6i4lpxnStk8I.png" alt=""></p><p>另参考：<a href="https://ya2.top/articles/navicat%E4%BD%BF%E7%94%A8http%E9%80%9A%E9%81%93%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5sqlite/">Navicat使用HTTP通道远程连接SQLite</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Navicat配置远程连接sqlite&quot;&gt;&lt;a href=&quot;#Navicat配置远程连接sqlite&quot; class=&quot;headerlink&quot; title=&quot;Navicat配置远程连接sqlite&quot;&gt;&lt;/a&gt;Navicat配置远程连接sqlite&lt;/h1&gt;&lt;p&gt;&lt;i</summary>
      
    
    
    
    
    <category term="配置记录" scheme="http://example.com/tags/%E9%85%8D%E7%BD%AE%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>从01背包问题一维优化到多重背包问题二进制、单调队列优化总结</title>
    <link href="http://example.com/2021/05/17/%E4%BB%8E01%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%E4%B8%80%E7%BB%B4%E4%BC%98%E5%8C%96%E5%88%B0%E5%A4%9A%E9%87%8D%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%E4%BA%8C%E8%BF%9B%E5%88%B6%E3%80%81%E5%8D%95%E8%B0%83%E9%98%9F%E5%88%97%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/"/>
    <id>http://example.com/2021/05/17/%E4%BB%8E01%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%E4%B8%80%E7%BB%B4%E4%BC%98%E5%8C%96%E5%88%B0%E5%A4%9A%E9%87%8D%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%E4%BA%8C%E8%BF%9B%E5%88%B6%E3%80%81%E5%8D%95%E8%B0%83%E9%98%9F%E5%88%97%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/</id>
    <published>2021-05-17T03:45:22.000Z</published>
    <updated>2021-05-23T03:10:37.437Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从01背包问题一维优化到多重背包问题二进制、单调队列优化总结"><a href="#从01背包问题一维优化到多重背包问题二进制、单调队列优化总结" class="headerlink" title="从01背包问题一维优化到多重背包问题二进制、单调队列优化总结"></a>从01背包问题一维优化到多重背包问题二进制、单调队列优化总结</h1><p>背包问题很经典，但从来都没有从头到尾总结过。</p><p>01背包问题，是给一个容量大小为V的背包和N件物品，每件物品有各自的价值w，且每个物品只能被选择1次。要求在有限的背包容量下，装入物品总价值最大。</p><p>多重背包问题的变动是，每个物品不止可以选择1次了，但要求还是在有限容量下装入最大的价值。</p><p>相当于问题除了给出背包容量V，每种物品的价值W，之外，还给了每种物品的可选数量S</p><p>多重背包问题的做法有</p><ul><li><p>将多重背包问题拆分为01背包问题，每种物品的每个我都选择一下0或1选与不选，这种做法时间复杂度较高。</p><p>适用数据范围为：</p><p>$0&lt;N,V≤1000$<br>$0&lt;v_i,w_i≤1000$ (因为题目一般的可解计算量为$10^7$ )</p></li><li><p>范围超了有，二进制优化方法</p><p>适用数据范围为：</p><p>$0&lt;N \le 1000$</p><p>$0&lt;V \le 2000$</p><p>$0&lt;v_i,w_i,s_i≤2000$</p></li><li><p>再大还有单调队列优化方法</p><p>适用数据范围为：</p><p>$0&lt;N \le 1000$</p><p>$0&lt;V \le 20000$</p><p>$0&lt;v_i,w_i,s_i≤20000$</p></li></ul><h2 id="01背包问题"><a href="#01背包问题" class="headerlink" title="01背包问题"></a>01背包问题</h2><p>题目：<a href="https://www.acwing.com/problem/content/2/">https://www.acwing.com/problem/content/2/</a></p><p>不断对第i个物品做出决策，[0-1] 代表选与不选两种抉择</p><p><img src="https://i.loli.net/2021/05/17/xM8coeUv3Guh1LX.png" alt=""></p><p>将状态$f[i][j]$优化到一维$f[j]$，实际上只需要做一个等价变形。</p><p>为什么可以这样变形呢？我们定义的状态$f[i][j]$可以求得任意合法的 $i$ 与 $j$ 最优解，但题目只需要求得最终状态$f[n][m]$，因此我们只需要一维的空间来更新状态。</p><ol><li>状态$f[j]$ 定义：N件物品，背包容量 $j$ 下的最优解</li><li>注意枚举背包容量 $j$ 必须从 $V$ 开始</li><li>为什么一维情况下枚举背包容量需要逆序？ 在2维情况下，状态 $f[i][j]$ 是由上一轮 $i-1$ 的状态得来的， $f[i][j]$ 与 $f[i-1][j]$ 是相互独立的。而优化到1维后，如果还是正序遍历，则有 $f[较小体积]$ 更新到 $f[较大体积]$， 则有可能本应该用第 $i-1$ 轮的状态却用的是第 $i$ 轮的状态</li><li>例如，一维状态第$i$ 轮对体积为3的物品进行决策，则$f[7]$ 由 $f[4]$ 更新而来，这里的$f[4]$ 正确应该是 $f[i-1][4]$，但从后小岛大枚举 $j$ 这里的 $f[4]$ 在第 $i$ 轮却成了 $f[i][4]$。 当逆序枚举背包容量 $j$ 时， 我们求$f[7]$ 同样由 $f[4]$ 更新。这里的 $f[4]$ 还没有在第 $i$ 轮计算，所以实际计算的 $f[4]$ 仍是 $f[i-1][4]$</li><li>简单来说，一维情况下正序更新状态 $f[j]$ 需要用到前面计算的状态已经被污染，逆序则不会有这样的问题</li><li>状态转移方程为 $f[j] = max(f[j], f[j-v[i]]+ w[i])$</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner sc = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="keyword">int</span> N = sc.nextInt();</span><br><span class="line">        <span class="keyword">int</span> V = sc.nextInt();</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span>[] v=<span class="keyword">new</span> <span class="keyword">int</span>[N+<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span>[] w=<span class="keyword">new</span> <span class="keyword">int</span>[N+<span class="number">1</span>];</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)&#123;</span><br><span class="line">            v[i] = sc.nextInt();</span><br><span class="line">            w[i] = sc.nextInt();</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">         <span class="comment">//dp1(v,w, N, V);// 无优化数组</span></span><br><span class="line">       dp2(v,w, N, V);<span class="comment">// 优化为1维数组</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dp1</span><span class="params">(<span class="keyword">int</span>[] v,<span class="keyword">int</span> [] w, <span class="keyword">int</span> N, <span class="keyword">int</span> V)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[N+<span class="number">1</span>][V+<span class="number">1</span>];</span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>]= <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;=V;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(j&lt;v[i]) dp[i][j]=dp[i-<span class="number">1</span>][j];</span><br><span class="line">                <span class="keyword">else</span>&#123;</span><br><span class="line">                    dp[i][j] = Math.max(dp[i-<span class="number">1</span>][j], dp[i-<span class="number">1</span>][j-v[i]]+w[i]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(dp[N][V]);</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dp2</span><span class="params">(<span class="keyword">int</span>[] v,<span class="keyword">int</span> [] w, <span class="keyword">int</span> N, <span class="keyword">int</span> V)</span></span>&#123;</span><br><span class="line">          <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[V+<span class="number">1</span>];</span><br><span class="line">          dp[<span class="number">0</span>]= <span class="number">0</span>;</span><br><span class="line">          <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)&#123;</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> j=V;j&gt;=v[i];j--)&#123;</span><br><span class="line">                  <span class="comment">// if(j&lt;v[i]) dp[j]=dp[j];</span></span><br><span class="line">                  <span class="comment">// else&#123;</span></span><br><span class="line">                  dp[j] = Math.max(dp[j], dp[j-v[i]]+w[i]);</span><br><span class="line">                  <span class="comment">// &#125;</span></span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          System.out.println(dp[V]);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>还可以优化输入</p><p>处理数据时，我们是一个物品一个物品，一个体积一个体积的去枚举</p><p>因此可以不必开两个数组去记录体积和价值，而是边输入边处理。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner sc = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="keyword">int</span> N = sc.nextInt();</span><br><span class="line">        <span class="keyword">int</span> V = sc.nextInt();</span><br><span class="line">        <span class="keyword">int</span>[] f=<span class="keyword">new</span> <span class="keyword">int</span>[V+<span class="number">1</span>];</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> v = sc.nextInt();</span><br><span class="line">            <span class="keyword">int</span> w = sc.nextInt();</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = V;j &gt;= v;j--)&#123;</span><br><span class="line">                f[j] = Math.max(f[j], f[j-v]+w);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(f[V]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="多重背包问题1"><a href="#多重背包问题1" class="headerlink" title="多重背包问题1"></a>多重背包问题1</h2><p>题目：<a href="https://www.acwing.com/problem/content/4/">https://www.acwing.com/problem/content/4/</a></p><p>多重背包问题，在给出每个物品的体积V和价值W的基础上，让每个物品不只可选1次</p><p>完全背包和01背包的区别是完全背包中每个物品可以用无限次，而多重背包不是无限次用。</p><p>最直接也最耗时的思路是，所有的可选的物品种类和次数都询问一次选或不选，也就是当成01背包问题来做。</p><p>但也比01背包问题多了一个数量级, 相对暴力</p><p><img src="https://i.loli.net/2021/05/18/UWfLp2TGBw4jPvn.png" alt=""></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="keyword">int</span> N = scanner.nextInt();</span><br><span class="line">        <span class="keyword">int</span> V = scanner.nextInt();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span>[] f = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">110</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> v = scanner.nextInt();</span><br><span class="line">            <span class="keyword">int</span> w = scanner.nextInt();</span><br><span class="line">            <span class="keyword">int</span> s = scanner.nextInt();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = V; j &gt;= <span class="number">0</span>; j--) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">1</span>; k &lt;= s &amp;&amp; k * v &lt;= j; k++) &#123;</span><br><span class="line">                    f[j] = Math.max(f[j], f[j - k * v] + k * w);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        System.out.println(f[V]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="多重背包问题2"><a href="#多重背包问题2" class="headerlink" title="多重背包问题2"></a>多重背包问题2</h2><p>题目：<a href="https://www.acwing.com/problem/content/5/">https://www.acwing.com/problem/content/5/</a></p><p>这道题和多重背包问题1其实是一样的，只不过数量级有变化，要求你用二进制优化的方法来解。</p><p>那么什么是二进制优化法？</p><p>上一道题是将每种物品拆成单份的01背包去求解的</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"> 即 v,w,s = v,w,7 时：</span><br><span class="line"> 正常拆分：-&gt; (v,w)，(v,w)，(v,w)，(v,w)，(v,w)，(v,w)，(v,w)</span><br><span class="line"> 二进制拆分：-&gt; (v,w),(v&lt;&lt;1,w&lt;&lt;1),(v&lt;&lt;2,w&lt;&lt;2)</span><br><span class="line"></span><br><span class="line"> 7 : 1 ,2, 4</span><br><span class="line"> 0</span><br><span class="line"> 1</span><br><span class="line"> 2</span><br><span class="line"> 3 = 1+2</span><br><span class="line"> 4</span><br><span class="line"> 5= 1+4</span><br><span class="line"> 6 =2+4</span><br><span class="line"> 7=1+2+4</span><br><span class="line"> &lt;p&gt;</span><br><span class="line"> s - 1 -2 - 4 - 8  ....</span><br><span class="line"> 减 2的幂  减到不能减为止 用s - (1+2+4+8...)</span><br><span class="line"> 就可以把物品分成log(s)份 而不是s份</span><br><span class="line"> &lt;p&gt;</span><br><span class="line"> log(2000)=11</span><br><span class="line"> 1000*11*2000 = 2*10^7</span><br><span class="line"> 所以将每个物品拆成log份</span><br><span class="line"> </span><br><span class="line"> 模拟</span><br><span class="line">1 2 4 8 16 32 64 128 256 512 1024</span><br><span class="line">这十一个数可以拼凑出0-2047间的所有整数</span><br><span class="line">1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16................</span><br><span class="line">1 2 1+2 4 4+1 4+2 4+2+1 8 8+1 8+2 8+2+1 8+4 8+4+1 8+4+2 8+4+2+1 16................</span><br><span class="line">所以在使用二进制将si个i物品拆包组装成一个个大包之后我们总归可以通过01背包的枚举方式来得到一个正确的i物品选用数量，比如说应该选67件i物品，那么体现成我们选取了 价值为64w的物品一件 + 价值为2w的物品一件 + 价值为1*w的物品一件</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>为什么这么拆有用？</p><p>上一题的状态转移方程是</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  f[j] &= max(f[j-1], f[j-v[i]]+w[i], f[j-2*v[i]]+2*w[i],... ) \\    \end{split}\end{equation}</script><p>我们首先确认三点：</p><p>（1）我们知道转化成01背包的基本思路就是：判断每件物品我是取了你好呢还是不取你好。</p><p>（2）我们知道任意一个实数可以由二进制数来表示，也就是$2^0 - 2^k$其中一项或几项的和。</p><p>（3）这里多重背包问的就是每件物品取多少件可以获得最大价值。</p><p>如果仍然不是很能理解的话，取这样一个例子:要求在一堆苹果选出n个苹果。我们传统的思维是一个一个地去选，选够n个苹果就停止。这样选择的次数就是n次</p><p>二进制优化思维就是：现在给出一堆苹果和10个箱子，选出n个苹果。将这一堆苹果分别按照1,2,4,8,16,…..512分到10个箱子里，那么由于任何一个数字x ∈[1,1024]<br>都可以从这10个箱子里的苹果数量表示出来，但是这样选择的次数就是 ≤10次 。</p><p>这样利用二进制优化，时间复杂度就从$O(n^3)降到O(n^2logS)$,从$4<em>10^9$降到了$2</em>10^7$。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span></span>&#123;</span><br><span class="line">   Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">run1</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> N = scanner.nextInt();</span><br><span class="line">        <span class="keyword">int</span> V = scanner.nextInt();</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span>[] v_arr = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">12010</span>];</span><br><span class="line">        <span class="keyword">int</span>[] w_arr = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">12010</span>];</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span> cnt = <span class="number">0</span>; <span class="comment">// 分组的组别</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> v = scanner.nextInt();</span><br><span class="line">            <span class="keyword">int</span> w = scanner.nextInt();</span><br><span class="line">            <span class="keyword">int</span> s = scanner.nextInt();</span><br><span class="line">            <span class="keyword">int</span> k =<span class="number">1</span>; <span class="comment">//组别里的类别个数</span></span><br><span class="line">            <span class="keyword">while</span>(k&lt;=s)&#123;</span><br><span class="line">                cnt++; <span class="comment">//组别先增加</span></span><br><span class="line">                v_arr[cnt] = v*k; <span class="comment">//整体体积</span></span><br><span class="line">                w_arr[cnt] = w*k; <span class="comment">//整体价值</span></span><br><span class="line">                s-=k;   </span><br><span class="line">                k*=<span class="number">2</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//剩余一组</span></span><br><span class="line">            <span class="keyword">if</span>(s&gt;<span class="number">0</span>)&#123;</span><br><span class="line">                cnt++;</span><br><span class="line">                v_arr[cnt]= v*s;</span><br><span class="line">                w_arr[cnt]= w*s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span>[] f = <span class="keyword">new</span> <span class="keyword">int</span>[V+<span class="number">1</span>];</span><br><span class="line">        N = cnt;</span><br><span class="line">        <span class="comment">// 01背包</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=V;j&gt;=v_arr[i];j--)&#123;</span><br><span class="line">                f[j] = Math.max(f[j] , f[j-v_arr[i]]+w_arr[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(f[V]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">new</span> Main().run1();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="多重背包问题3"><a href="#多重背包问题3" class="headerlink" title="多重背包问题3"></a>多重背包问题3</h2><p>题目：<a href="https://www.acwing.com/problem/content/description/6/">https://www.acwing.com/problem/content/description/6/</a></p><p>$0&lt;N \le 1000$</p><p>$0&lt;V \le 20000$</p><p>$0&lt;v_i,w_i,s_i≤20000$</p><p>如果还以上面的二进制优化来做，复杂度为 $1000 <em> log(20000) </em> 20000 = 3*10^8$  会超时。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  原: f[j]=max(f[j],f[[j-kv[i]]+kw[i]);\\    \end{split}\end{equation}</script><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)  <span class="comment">//第一重循环</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;=V;j++)  <span class="comment">//第二重循环</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">0</span>; k &lt;= s[i] &amp;&amp; k * v[i] &lt;= j; k ++ )  <span class="comment">//第三重循环</span></span><br></pre></td></tr></table></figure><p>考虑到对于每次层 $i，j$  只与 j % v+kv 有关，k 的范围 $[0,s]$</p><p>优化二三重循环,将每一层 j 按 j%v 分成v组，节省了第二重循环中 $ j+v…j+kv$ 的时间，将两重循环优化为遍历一次 m；</p><p>$f[i][j]=max(f[i][j],f[i-1][j-kv[i]]+k*w[i]) $相当于求每一组在s个范围内的最大值，单调队列O（1）时间即可；</p><p>时间复杂度应该是O(NV)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span></span>&#123;</span><br><span class="line">   Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">run1</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> N = scanner.nextInt();</span><br><span class="line">        <span class="keyword">int</span> V = scanner.nextInt();</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span>[] v_arr = <span class="keyword">new</span> <span class="keyword">int</span>[V+<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span>[] w_arr = <span class="keyword">new</span> <span class="keyword">int</span>[V+<span class="number">1</span>];</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span>[] f = <span class="keyword">new</span> <span class="keyword">int</span>[V+<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span>[] g = <span class="keyword">new</span> <span class="keyword">int</span>[V+<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span>[] q = <span class="keyword">new</span> <span class="keyword">int</span>[V+<span class="number">1</span>];</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> v = scanner.nextInt();</span><br><span class="line">            <span class="keyword">int</span> w = scanner.nextInt();</span><br><span class="line">            <span class="keyword">int</span> s = scanner.nextInt();</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;v;j++)&#123;</span><br><span class="line">                <span class="keyword">int</span> hh,tt;</span><br><span class="line">                hh=<span class="number">0</span>,tt=-<span class="number">1</span>;</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> k=j;k&lt;=m;k+=vi)&#123;</span><br><span class="line">                    g[k] = f[k];<span class="comment">//每次f[k]都可能会更新， 预先保存f[i-1, k]的值 </span></span><br><span class="line">                    <span class="keyword">if</span>(hh&lt;=tt&amp;&amp;(k-q[hh])/vi&gt;si) hh++;<span class="comment">//保证保证不超前si个</span></span><br><span class="line">                    <span class="keyword">while</span>(hh&lt;=tt&amp;&amp;g[q[tt]]+(k-q[tt])/vi*wi &lt;f[k]) tt--;<span class="comment">//单调队列入队方法</span></span><br><span class="line">                    q[++tt] = k;</span><br><span class="line">                    f[k] = g[q[hh]]+(k-q[hh])/vi*wi;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">new</span> Main().run1();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;从01背包问题一维优化到多重背包问题二进制、单调队列优化总结&quot;&gt;&lt;a href=&quot;#从01背包问题一维优化到多重背包问题二进制、单调队列优化总结&quot; class=&quot;headerlink&quot; title=&quot;从01背包问题一维优化到多重背包问题二进制、单调队列优化总结&quot;&gt;</summary>
      
    
    
    
    
    <category term="DP" scheme="http://example.com/tags/DP/"/>
    
  </entry>
  
  <entry>
    <title>Adam &amp; AdamW 原论文</title>
    <link href="http://example.com/2021/05/16/Adam-AdamW-%E5%8E%9F%E8%AE%BA%E6%96%87/"/>
    <id>http://example.com/2021/05/16/Adam-AdamW-%E5%8E%9F%E8%AE%BA%E6%96%87/</id>
    <published>2021-05-16T11:00:29.000Z</published>
    <updated>2021-05-16T11:18:13.019Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Adam-amp-AdamW-原论文"><a href="#Adam-amp-AdamW-原论文" class="headerlink" title="Adam &amp; AdamW 原论文"></a>Adam &amp; AdamW 原论文</h1><hr><h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>一种基于低阶矩估计的随机目标函数一阶梯度优化算法。该方法也适用于非平稳目标和具有非常强噪声和/或稀疏梯度的问题。特点有：实现简单、计算高效、低内存要求、对梯度的对角重新缩放不变，并且很适合于数据和/或参数较大的问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Adam-amp-AdamW-原论文&quot;&gt;&lt;a href=&quot;#Adam-amp-AdamW-原论文&quot; class=&quot;headerlink&quot; title=&quot;Adam &amp;amp; AdamW 原论文&quot;&gt;&lt;/a&gt;Adam &amp;amp; AdamW 原论文&lt;/h1&gt;&lt;hr&gt;
</summary>
      
    
    
    
    
    <category term="ML&amp;DL" scheme="http://example.com/tags/ML-DL/"/>
    
  </entry>
  
  <entry>
    <title>GPT-GNN: Generative Pre-Training of Graph Neural Networks</title>
    <link href="http://example.com/2021/05/15/GPT-GNN-Generative-Pre-Training-of-Graph-Neural-Networks/"/>
    <id>http://example.com/2021/05/15/GPT-GNN-Generative-Pre-Training-of-Graph-Neural-Networks/</id>
    <published>2021-05-15T14:24:59.000Z</published>
    <updated>2021-05-16T07:25:28.520Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GPT-GNN-Generative-Pre-Training-of-Graph-Neural-Networks"><a href="#GPT-GNN-Generative-Pre-Training-of-Graph-Neural-Networks" class="headerlink" title="GPT-GNN: Generative Pre-Training of Graph Neural Networks"></a>GPT-GNN: Generative Pre-Training of Graph Neural Networks</h1><p>Self-Supervised Learning分成两种方法:一种是生成式模型，一种是判别式模型(对比学习)。</p><p>以输入图片信号为例，生成式模型，输入一张图片，通过Encoder编码和Decoder解码还原输入图片信息，监督信号是输入输出尽可能相似。判别式模型，输入两张图片，通过Encoder编码，监督信号是判断两张图是否相似(例如，输入同一个人的两张照片，判断输入相似，输出1；输入两个人的照片，判断输入不相似，输出0)。</p><h2 id="文章贡献"><a href="#文章贡献" class="headerlink" title="文章贡献"></a>文章贡献</h2><p>继上一文 <a href="https://coding-zuo.github.io/2021/05/12/Strategies-for-Pre-training-Graph-Neural-Networks/">Strategies for Pre-training Graph Neural Networks</a> 对预训练GNN做了大规模的实验，并提出提出了一种结合节点级和图级表示的预训练方法，优化了单单使用一种级别做预训练后产生的负迁移效果。</p><p>又以生成式自监督的方式，来在预训练阶段捕捉图数据的结构信息和语义信息。分别是边生成任务和属性生成任务。</p><p>它们联合优化等价于最大化整个属性图的概率似然，这样预训练模型可以捕捉到节点属性与图结构之间的内在依赖关系。</p><p>预训练的GNN网络目标主要是异质单个(大规模)图上预训练，并进行节点级迁移。</p><p>然后优化了预训练模型可以让其处理大规模的图采样子图，采用的是通过自适应的嵌入队列，减轻负采样带来的不准确损失。</p><p>接下来主要介绍两种自监督任务和这个优化方法。</p><h2 id="行文逻辑"><a href="#行文逻辑" class="headerlink" title="行文逻辑"></a>行文逻辑</h2><p>通过行文逻辑，学习怎么写论文。</p><p>首先作者先是说GNN有用，预训练GNN刚刚被证明有用！接下来从充分利用无标签数据做无监督任务说，大规模的图数据标记成本昂贵。NLP的数据也一样标注昂贵，所以有了bert那样的预训练语言模型，并且提高了下游任务性能。同样在cv领域也是。</p><p>列举了GAE、GraphRNN、半监督GCN等图生成技术，但他们不适合用于预训练GNN。因为：首先，它们大多只关注于生成无属性的图结构，而没有定义节点属性与图结构之间的底层模式，图结构是GNNs中卷积聚合的核心。其次，它们被设计用来处理迄今为止的小图形，限制了它们在大规模图形上进行预训练的潜力。</p><p>然后介绍了下预训练和finetuning的流程，就不多说了。</p><p>然后切入正题介绍他的贡献，上文介绍过。</p><hr><p>然后是准备工作和相关工作，介绍GNN的传统机制，信息传递和信息聚合的基本原理，不多介绍。</p><p>和GNN发展历史，其中有一个Graph Infomax 最近可能要学习一下，最大化了从GNNs获得的节点表示和图pooling表示之间的互信息，也就是节点级和图级。作者认为其，在纯监督学习环境下表现出改进，但学习任务通过强迫附近节点具有相似的嵌入来实现，而忽略了图的丰富语义和高阶结构。</p><p>介绍预训练在cv和nlp的成功。不过我最近听说cv圈有一篇文章，最近2021的有一篇预训练CNN其效果并不比基于transformer的模型差。</p><p>介绍生成预训练任务的数学定义，之后是具体细节和模型方法，再到实验结论等等。</p><h2 id="关于生成式预训练任务的框架流程"><a href="#关于生成式预训练任务的框架流程" class="headerlink" title="关于生成式预训练任务的框架流程"></a>关于生成式预训练任务的框架流程</h2><p>形式上给出图数据 $G = (V,E,X)$  和GNN模型 $f_{\theta}$</p><p>我们通过这个GNN将此图上的可能性建模为 $p(G;θ)$ ——-表示G中的节点是如何属性化和连接其他节点的(可以理解为先验知识)。</p><p>其目的是通过最大化图的似然，得到参数 $θ^∗=max_{θ}p(G;θ)$ 来预先训练广义神经网络模型。</p><p>那么问题变成了如何对 $p(G;\theta)$ 进行适当的建模。</p><p>现在大多的现有图生成方法都是遵循自回归方式来分解目标概率分布，也就是图中的节点是按顺序来的，并且边是通过将每个新到达的节点连接到现有节点来生成的。什么是自回归？</p><script type="math/tex; mode=display">X_t = c+\sum_{i=1}^p\phi_iX_{t-i}+\epsilon_t</script><p>如上式，c 为常数项，$\epsilon$ 为随机误差，概况来说就是X的当前期值等于一个或数个前期值的线性组合加常数项和睡觉误差。</p><p>类似的作者也通过一个排列向量 $\pi$ 来确定节点顺序，其中 $i^{\pi}$ 表示向量中第i个位置的节点id。因此，图的分布$p(G;\theta)$ 等价于所有可能排列上的期望似然：</p><script type="math/tex; mode=display">p(G;\theta) = \mathbb{E}_{\pi} [p_{\theta}(X^{\pi},E^{\pi})]</script><p>其中$X^{\pi} \in R^{|V|\times d}$ ，$E$ 是边集 ，$E_{i}^{\pi}$ 表示所有连接节点$i^{\pi}$ 的边。</p><p>为简单起见，假设观察到任何节点排序 $π$ 的概率相等，并且在下面的章节中说明一个排列的生成过程时也省略了下标 $π$。给定一个排列顺序，我们可以将对数似然率自动回归分解-每次迭代生成一个节点，如下所示：</p><script type="math/tex; mode=display">logp_{\theta}(X,E) = \sum_{i=1}^{|V|}logp_{\theta}(X_i,E_i|X_{\lt i},E_{\lt i})</script><p>在第i步，使用所有 i 之前已生成的节点，他们的属性和边分别是 $X<em>{\lt i}$ ，$E</em>{\lt i}$ ，给定 $X<em>{\lt i}$ $E</em>{\lt i}$ 生成节点 i 的概率log加和。</p><p>从本质上讲，等式中的目标。描述了属性图的自回归生成过程。问题变成：如何对条件概率 $p<em>θ(X_i，E_i|X</em>{&lt;i}，E_{&lt;i})$ 建模？</p><h3 id="因式分解属性图生成"><a href="#因式分解属性图生成" class="headerlink" title="因式分解属性图生成"></a>因式分解属性图生成</h3><p>为了计算 $p<em>{\theta}(X_i,E_i|X</em>{\lt i},E_{\lt i})$ ，一种天真的解决方案可以是简单地假设 $X_i$ 和 $E_i$是独立的，即 :</p><script type="math/tex; mode=display">p_{\theta}(X_i,E_i|X_{\lt i},E_{\lt i}) = p_{\theta}(X_i|X_{\lt i},E_{\lt i}) \cdot p_{\theta}(E_i|X_{\lt i},E_{\lt i})</script><p>然而通过这样的分解，对于每个节点，其属性和连接之间的依赖性被完全忽略。</p><p>然而，被忽略的依赖性是属性图的核心性质，也是GNNs中卷积聚集的基础。因此，这种天真的分解不能为训练前的GNN提供信息指导。</p><p>就比如，物以类聚人以群分，我和相似的人右边是因为我们有相似的属性。</p><p>为了解决这个问题，作者提出了属性图生成过程的依赖感知分解机制。具体地说，当估计一个新节点的属性时，我们会得到它的结构信息，反之当估计一个新的结构边信息时，我们会考虑到它的属性信息。在该过程中，可以将生成分解为两个耦合部分：</p><ul><li>1.给出观测边的边，生成节点属性</li><li>2.给出观测边和1中已经生成的节点属性，生成剩余的边</li></ul><p>通过这种方式，模型可以捕获每个节点的属性和结构之间的依赖关系。</p><p>正式的定义如何建模，定义一个变量 $o$ , 表示$E_i$内所有观测边的索引向量。</p><p>$E_{i,o}$ 是已观测的边，$\lnot o$表示要生成的所有mask边的索引。通过所有的已观测边来重写条件概率作为一个期望似然如下：</p><p><img src="https://i.loli.net/2021/05/16/qIjAo2HyNkF5WcS.png" alt=""></p><p>这里的理解非常重要，第一个等式中，把 $E<em>i$ 拆成了$E</em>{i,¬o}$和 $E_{i,o}$ ，也就是说指定了哪些边是观测边，哪些边是masked边。需要注意的是，当o确定下来了，$\lnot o$ 也是确定的。因此等式外面加上了对o的累加，这里可以理解为类似于全概率公司去对所有可能的o求和。</p><p>此外，这里要注意  $E<em>i, E</em>{&lt;i},E<em>{i,o},E</em>{i,\lnot o}$  四个符号分别表示什么：</p><ul><li>现在位于step i，$E_{&lt;i}$ 是指在step i 之前生成的边</li><li>$E_i$ 指在step i 将会生成的边 (与节点i 相连，有好多边)</li><li>将 $E<em>i$ 的边生成过程拆分成一件生成的和将要生成的两部分，即 $E</em>{i,o},E_{i,\lnot o}$</li></ul><p>在第二个等式中，把p 看成是概率分布，写作对于o 期望的形式。</p><p>最后把 $X<em>i$ 和 $E</em>{i,\lnot o}$ 看做独立的过程，拆成两个概率分布。</p><p>这种分解的优势在于，没有忽略 $X<em>i$ 和 $E</em>{i,o}$ 的联系。第一项表示given观测边，聚合目标节点i的邻居信息来生成其属性$X<em>i$ 。第二项表示given观测边和刚生成的属性$X_i$，预测$E</em>{i,¬o}$中的边是否存在。</p><p><img src="https://i.loli.net/2021/05/16/hTuqivsBcRC1fon.png" alt=""></p><p>如上图所示，给出一个例子。对于academic图，我们要去生成一个paper node，它的属性为title。我们要去生成一个paper node，它的属性为title，并且其和author，publish venue，reference相连。上图中的实线部分为已经观测到的边，首先生成节点的属性，即title。然后基于author1，author2，author3和刚生成的节点属性title，预测剩下的边，即虚线部分。</p><h3 id="高效的属性和边生成"><a href="#高效的属性和边生成" class="headerlink" title="高效的属性和边生成"></a>高效的属性和边生成</h3><p>出于效率考虑希望：</p><ul><li>对于输入图只跑一次GNN就能计算节点属性生成和边生成过程的loss</li><li>希望节点属性生成和边生成能同时运行</li></ul><p>然而边生成需要用到节点属性信息，如果两个生成过程同时进行，会导致信息泄露。为避免这个问题，将节点分成两种类型：</p><ul><li>属性生成节点，mask住这些节点的属性，用一个公用的dummy token，并学习一个共享向量$X^{init}$来代替 和$X_i$ 维度相同。</li><li>边生成节点，对于这些节点，保留他们的属性。</li></ul><p>需要注意的是，同一个节点在不同阶段扮演不同的角色，可能是属性生成节点也可能是边生成节点。只在某一阶段，一个节点有一个确定的角色。</p><p>在graph上训练GNN 来生成各个节点的embedding，用$h<em>{attr}$ 和 $h</em>{edge}$ 来分别表示属性生成节点和边生成节点的embedding。由于属性生成节点的属性被mask了，因此$h<em>{attr}$中包含的信息通畅会少于 $h</em>{edge}$。</p><p>因此，在GNN的message passing过程中，只使用$h_{edge}$ 作为向其他节点发送的信息。 也就是说，对于每个节点，其聚合邻居 $h_edge$ 的信息和自身信息来生成新的embedding。之后使用不同的decoder来生成节点属性和边。（注意，节点的embedding和节点属性不是一回事。通俗理解，在GNN中节点的属性是input，节点的embedding是hidden layer。）</p><p>对于属性生成，用$Dec^{Attr}(\cdot)$ 来表示decoder，输入$h_{attr}$ 来生成节点属性。decoder的选择依赖于节点属性的类型，如果是text类型的节点属性，可以使用LSTM等。如果节点属性是vector，可以使用MLP。</p><p>定义一个距离函数来度量生成属性和真实属性之间的差异，对于text类型属性，可以使用perplexity困惑度，对于vector属性，可以使用L2距离。因此，可以计算属性生成过程中的loss</p><script type="math/tex; mode=display">L_i^{Attr} = Distance(Dec^{Attr}(h_i^{Attr}, X_i))</script><p>最小化生成属性和真实属性之间的差异，等价于对generate attribute做MLE，也就是最大化 $p<em>{\theta}(X_i|E</em>{i,o},X<em>{&lt;i},E</em>{&lt;i})$ 从而捕捉了图中的节点属性信息。</p><p>对于边生成过程，假设每条边的生成过程和其他边是独立的，由此对likelihood分解：</p><script type="math/tex; mode=display">p_{\theta} (E_{i,\lnot o}|E_{i,o},X_{\le i},E_{\le i}) = \prod_{j^+\in E_{i,\lnot o}} p_{\theta}(j^+|E_{i,o},X_{\le i},E_{\le i})</script><p>得到$h_{edge}$ 后，如果节点i和节点j相连，则使用</p><script type="math/tex; mode=display">Dec^{Edge} (h_i^{Edge},h_j^{Edge})</script><p>进行建模，$Dec^{Edge}$ 是一个pairwise score function</p><p>loss定义为：</p><script type="math/tex; mode=display">L_i^{Edge} = - \sum_{j^+\in E_{i,\lnot o}} log \frac{exp(Dec^{Edge}(h_i^{Edge},h_{j^+}^{Edge}))}{\sum_{j\in S_i^-\bigcup{j^+} }exp(Dec^{Edge}(h_i^{Edge},h_j^{Edge}))}</script><p>$S_i^-$ 是指没有和节点i相连的节点</p><p>下面是作者给出的属性图生成过程的说明性示例。</p><p><img src="https://i.loli.net/2021/05/16/i8IYhQ2NbSfEAKe.png" alt=""></p><ul><li>a) 对于input graph 确定排列 $\pi$</li><li>b) 随机挑选一部分与节点i相连的边作为已观测的$E<em>{i,o}$ ,剩下的作为masked edges $E</em>{i,\lnot o}$ 并删除masked edges</li><li>c) 把节点分为属性生成节点和边生成节点</li><li>d) 计算节点 3，4，5的embedding，包括他们的属性生成节点和边生成节点。</li><li>d)-e) 通过对每个节点并行进行节点属性预测和masked预测来训练一个GNN模型</li></ul><p>具体算法流程：</p><p><img src="https://i.loli.net/2021/05/16/tGbrz7QJfmKEhdw.png" alt=""></p><p>输入一个属性图，每次采样一个子图 $\hat G$作为训练的实例进行训练。首先决定permutation order π。同时，我们希望能够并行化训练，只做一次前向传播，就能得到整个图的embedding，由此可以同时计算所有节点的loss。因此，根据permutation order π来移除边，也就是使每个节点只能从更低order的节点处获得信息。<br> 之后，需要决定哪些边被mask。对于每个节点，获得其所有的出边，随机挑选一部分边被mask住，这一过程对应上述line4。<br> 之后，对节点进行划分，得到整个图中节点的embedding，用于之后loss的计算，对应line5。<br> line 7-9进行loss的计算。<br> line 8中，通过整合采样图中未连接的节点和Q中以前计算的节点embedding来选择负样本，这种方式能够减轻对于采样图优化和对于整个图优化的差距。<br> 在line11-12中，优化模型并更新Q。</p><h2 id="GPT-GNN-对于异质的大图"><a href="#GPT-GNN-对于异质的大图" class="headerlink" title="GPT-GNN 对于异质的大图"></a>GPT-GNN 对于异质的大图</h2><p>对于异构图，即包含不同类型的点和边的图，唯一的不同在于不同类型的点和边采用不同的decoder。<br> 对于大规模的图，可以采样子图来进行训练，即上述算法流程中Sampler的作用。为了计算 $L_{edge}$ 这一loss，需要遍历输入图的所有节点。然而，我们只能在采样的子图上计算这个loss。为了缓解这一差异，提出了adaptive queue，其中存储了之前采样的子图的节点embedding作为负样本。每次采样一个新的子图时，逐步更新这个队列，增加新的节点embedding，移除旧的节点embedding。通过引入adaptive queue，不同采样子图中的节点也能为全局的结构提供信息。</p><h2 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h2><p><img src="https://i.loli.net/2021/05/16/xER1ftIsSWcaK72.png" alt=""></p><p><img src="https://i.loli.net/2021/05/16/ZNcLJsHUqRGOhCk.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;GPT-GNN-Generative-Pre-Training-of-Graph-Neural-Networks&quot;&gt;&lt;a href=&quot;#GPT-GNN-Generative-Pre-Training-of-Graph-Neural-Networks&quot; class=</summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Strategies for Pre-training Graph Neural Networks</title>
    <link href="http://example.com/2021/05/12/Strategies-for-Pre-training-Graph-Neural-Networks/"/>
    <id>http://example.com/2021/05/12/Strategies-for-Pre-training-Graph-Neural-Networks/</id>
    <published>2021-05-12T09:10:47.000Z</published>
    <updated>2021-05-12T15:56:43.849Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Strategies-for-Pre-training-Graph-Neural-Networks"><a href="#Strategies-for-Pre-training-Graph-Neural-Networks" class="headerlink" title="Strategies for Pre-training Graph Neural Networks"></a>Strategies for Pre-training Graph Neural Networks</h1><p>目前深度学习各个领域的预训练都搞的热火朝天，GNN也是肯定要搞的。那么预训练之后下一个热潮会是什么呢？</p><p>ICLR2020 首次系统的探索了大规模GNN预训练</p><p>提出了一种结合节点级和图级表示的预训练方法来训练模型。</p><p>在节点级，使用了两种自监督方法，即上下文预测和属性预测。</p><p>在图形级，使用有监督的图级属性预测和结构相似性预测</p><p>同时作者建立了两个新的预训练数据集，2M graph的化学数据集和一个有395K graph的生物数据集。</p><p>接下来介绍作者这么做的理由</p><h2 id="发现"><a href="#发现" class="headerlink" title="发现"></a>发现</h2><p>因为对于特定任务的有标签数据是很稀少的，但无标签数据却有很多，所以为了充分利用无标签数据，各种自监督方法开始兴起。</p><p>所以作者分别在图级和节点级层面上提出了两大类预测方法</p><ul><li>属性预测：属性mask(节点)、有监督的属性预测(图级)</li><li>结构预测：上下文预测(节点)、结构相似性预测(图级)</li></ul><p>以往的一些研究表明(Xu et al., 2017; Ching et al., 2018; Wang et al., 2019),一个成功的迁移学习不仅仅是增加与下游任务来自同一领域的标注好的预训练数据集的数量。相反，它需要大量的领域专业知识来仔细选择与感兴趣的下游任务相关的示例和目标标签。否则，知识从相关的预训练任务转移到新的下游任务可能会损害泛化，这被称为负迁移(Rosenstein等人，2005年)，并极大地限制了预训练模型的适用性和可靠性。</p><p>作者研究发现朴素的策略要么在整个图的层面上预先训练GNN，要么在单个节点层面上预先训练GNN，所给出的改进有限，甚至可能导致许多下游任务的负迁移。在只有图级的预训练下大约有1/4的任务出现了负迁移。</p><p><img src="https://i.loli.net/2021/05/12/z5CEtxbX9Tj1WwN.png" alt=""></p><p>图(a.i)当仅使用节点级预训练时，可以很好地分离不同形状的节点(语义上不同的节点)，但汇集节点级嵌入创建的结果，图嵌入是不可分离的(图嵌入由+和−表示)</p><p>图(a.ii)仅在图级预训练的情况下，图嵌入可以很好地分离，但是单个节点的嵌入并不一定捕获它们特定于领域的语义。</p><p>图(a.iii) 高质量的节点嵌入使得不同类型的节点能够很好地分开，同时嵌入空间也是可组合的。这允许对整个图形进行准确和健壮的表示，并允许将预先训练的模型健壮地传输到各种下游任务。</p><h2 id="预训练策略"><a href="#预训练策略" class="headerlink" title="预训练策略"></a>预训练策略</h2><p>在预训练策略的技术核心是在单个节点以及整个图的级别预先训练。这一概念鼓励GNN在两个级别捕获特定域的语义。</p><h3 id="节点级预训练"><a href="#节点级预训练" class="headerlink" title="节点级预训练"></a>节点级预训练</h3><p>两种自监督方法，上下文预测和属性mask。</p><p><img src="https://i.loli.net/2021/05/12/RFS46a2tozyNGkp.png" alt=""></p><p>图(a)在上下文预测中，子图是所选中心节点周围的K跳邻域，其中K是GNN层的数量，上图中设置为K=2。环境定义为中心节点r1-和r2-Hop之间的周围图结构，上图中使用r1=1和r2=4。</p><p>图(b) 在属性mask中，输入节点/边属性(例如，分子图中的原子类型)被随机mask，并且要求GNN预测它们。</p><h4 id="上下文预测：利用图结构的分布性"><a href="#上下文预测：利用图结构的分布性" class="headerlink" title="上下文预测：利用图结构的分布性"></a>上下文预测：利用图结构的分布性</h4><p>使用子图来预测其周围的图结构。目标是预先训练GNN，以便它将出现在类似结构上下文中的节点映射到附近的嵌入。</p><p>通过三个步骤：</p><ul><li><p>邻居节点和上下文图</p><p>对于每个节点v，定义v的邻居和上下文图。因为GNN信息聚合的是K层邻居，所以节点v的嵌入$h_v$ 依赖于距离v至多k跳节点。上下文图由两个超参数r1和r2来描述，并且它表示远离v的r1跳和r2跳之间的子图(即它是宽度为r2−r1的环)。并且r1&lt;K，以便在邻域和上下文图之间共享一些节点，我们将这些节点称为上下文锚节点。这些锚节点提供关于邻居图和上下文图如何彼此连接的信息。</p></li><li><p>使用一个辅助GNN把上下文编码成固定向量</p><p>由于图的组合性，直接预测上下文图是很困难的。这与自然语言处理不同，在自然语言处理中，单词来自固定和有限的词汇表。为了实现上下文预测，将上下文图编码为固定长度的向量。为此，引入一个上下文GNN作为辅助编码，就是图中的GNN‘。首先用其获得上下文图中的节点嵌入，然后对上下文锚点的嵌入进行平均，得到固定长度的上下文嵌入。对于图G中的节点v，将其对应的上下文嵌入表示为$c^G_v$</p></li><li><p>负采样</p><p>主要的GNN编码邻居节点获取节点的embedding—— $h_v^{(K)}$ ，上下文GNN编码上下文图获取上下文embedding——$c^G_v$。学习目标是一个二分类：是否特定邻域和特定上下文图是否属于同一节点。</p><script type="math/tex; mode=display">\sigma(h^{(k)T}_v c_{v'}^{G'}) \approx 1 \{\text{v and v' are the same nodes}\}</script></li></ul><p>  让v‘=v并且G’=G(即正例)，或者我们从随机选择的图G‘中随机抽样v’(即负例)。</p><h4 id="属性mask-利用图属性的分布性"><a href="#属性mask-利用图属性的分布性" class="headerlink" title="属性mask:利用图属性的分布性"></a>属性mask:利用图属性的分布性</h4><p>目标是通过学习图结构上节点/边属性的分布规律来获取领域知识。</p><p>属性mask有节点mask和属性mask两类</p><p>工作原理：掩蔽节点/边缘属性，然后让GNN基于相邻结构预测这些属性，这参考了bert的mask。</p><p>具体地说，通过用特殊的屏蔽指示符替换输入节点/边属性(例如分子图中的原子类型)来随机屏蔽它们。然后应用GNNs来获得相应的节点/边嵌入(边嵌入:为边的端点的节点嵌入之和来获得)。</p><p>最后，在嵌入的基础上应用线性模型来预测被mask的节点/边属性。有趣的是bert的mask其实相当于在全连通的token图上应用了消息传递。</p><p>在图结构数据中是对非全连通图进行操作，目的是捕捉节点/边属性在不同图结构上的分布规律。</p><h3 id="图级别预训练"><a href="#图级别预训练" class="headerlink" title="图级别预训练"></a>图级别预训练</h3><p>我们的目标是确保节点和图嵌入都是高质量的，以便图嵌入是健壮的，并且可以跨下游任务传输。</p><p>有两个用于图级预训练的选项：预测整个图的特定于域的属性(监督标签)，或者预测图结构。</p><h4 id="有监督的图级属性预测"><a href="#有监督的图级属性预测" class="headerlink" title="有监督的图级属性预测"></a>有监督的图级属性预测</h4><p>由于图形级表示 $h_G$ 直接用于对下游预测任务进行微调，希望将特定于域的信息直接编码成 $h_G$。</p><p>考虑了一种对图表示进行预训练的实用方法：图级多任务监督预训练，用于联合预测单个图的不同监督标签集。例如，在分子性质预测中，我们可以预先训练GNN来预测到目前为止实验测量的分子的所有性质。在蛋白质功能预测中，目标是预测给定的蛋白质是否具有给定的功能，我们可以预先训练GNN来预测到目前为止已经验证的各种蛋白质功能的存在。</p><p>重要的是，单独进行大量的多任务图级预训练可能无法给出可转移的图级表示。(问题来了)</p><p>这是因为一些有监督的预训练任务可能与下游感兴趣的任务无关，甚至会损害下游的绩效（负迁移）。一种解决办法是选择“真正相关的”有监督的训练前任务，只对这些任务进行训练前GNN训练。然而，这样的解决方案成本极高，因为选择相关任务需要大量的领域专业知识，并且需要针对不同的下游任务分别进行预训练。</p><p>为了缓解这个问题，作者的见解是，多任务监督的预训练只提供图形级的监督；因此，创建图形级嵌入的本地节点嵌入可能没有意义。这种无用的节点嵌入可能会加剧负迁移问题，因为许多不同的预训练任务在节点嵌入空间中更容易相互干扰。受此启发，在执行图级预训练之前，先通过上文描述的节点级预训练方法在单个节点级别对GNN进行正则化。正如作者所料，组合策略产生了更多可转移的图形表示。并且在没有专家选择监督的预训练任务的情况下稳健地改善了下游性能。</p><h4 id="结构相似性预测"><a href="#结构相似性预测" class="headerlink" title="结构相似性预测"></a>结构相似性预测</h4><p>目标是对两个图的结构相似性进行建模</p><p>此类任务的示例包括对图形编辑距离进行建模(Bai等人，2019年)或预测图形结构相似性(Navarin等人，2018年)。</p><p>这里好像作者感觉比较难没有全部实现，留到了以后的工作中</p><h3 id="总体预训练策略"><a href="#总体预训练策略" class="headerlink" title="总体预训练策略"></a>总体预训练策略</h3><p>预训练策略是首先进行节点级的自监督预训练，然后进行图级多任务监督的预训练。当GNN预训练完成后，我们对下游任务的预训练GNN模型进行微调。具体地说，我们在图级表示的基础上添加线性分类器来预测下游的图标签。随后以端到端的方式微调整个模型，即预先训练的GNN和下游线性分类器。</p><h2 id="进一步相关工作"><a href="#进一步相关工作" class="headerlink" title="进一步相关工作"></a>进一步相关工作</h2><p>关于图中单个节点的无监督表示学习的文献非常丰富，大致分为两类。</p><p>第一类是使用基于局部随机行走的目标的方法(Grover&amp;Leskovec，2016；Perozzi等人，2014；Don等人，2015)以及例如通过预测边的存在来重建图的邻接矩阵的方法。</p><p>在第二类中是诸如Deep Graph Infomax的方法，其训练最大化局部节点表示和聚集的全局图表示之间的互信息的节点编码器。(基于对比学习互信息的最近也要研究研究)</p><p>这两种方法都鼓励附近的节点具有相似的嵌入表示，最初是针对节点分类和链路预测提出和评估的。然而，这对于图级预测任务来说可能是次优的，在图级预测任务中，捕捉局部邻域的结构相似性通常比捕捉图中节点的位置信息更重要</p><p>所以该预训练策略既考虑了节点级的预训练任务，也考虑了图级的预训练任务，并且正如在实验中所显示的，为了使预训练模型获得良好的性能，必须同时使用这两种类型的任务。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://i.loli.net/2021/05/12/cFBosvWCfURYdhx.png" alt=""></p><p>阴影单元格表示负迁移，即预训练模型的ROC-AUC比未预训练模型的ROC-AUC差。借此说明两个级别共用的重要性。</p><p><img src="https://i.loli.net/2021/05/12/HvFtBiY5RadqGMw.png" alt=""></p><p>在有无预培训的情况下测试不同GNN架构的ROC-AUC(%)性能。</p><p>这里表达能力越强的结构预训练效果越好，表达能力较弱的GNN收益较小，甚至有时未负。这一发现证实了先前的观察结果(例如，Erhan等人)。(2010))，使用富有表现力的模型对于充分利用预培训至关重要，当用于表达能力有限的模型(如GCN、GraphSAGE和GAT)时，预培训甚至会影响性能。</p><p>并且GAT的表现反而下降了不少。作者认为GAT属于表达能力有限的模型，还有人认为GAT attention的参数比较多，模型结构比较复杂导致。</p><p><img src="https://i.loli.net/2021/05/12/kFYCKXIvcU2iLeZ.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Strategies-for-Pre-training-Graph-Neural-Networks&quot;&gt;&lt;a href=&quot;#Strategies-for-Pre-training-Graph-Neural-Networks&quot; class=&quot;headerlink&quot; t</summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Meta Learning(李宏毅)</title>
    <link href="http://example.com/2021/05/09/Meta-Learning-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    <id>http://example.com/2021/05/09/Meta-Learning-%E6%9D%8E%E5%AE%8F%E6%AF%85/</id>
    <published>2021-05-09T15:15:57.000Z</published>
    <updated>2021-05-16T11:03:17.552Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Meta-Learning"><a href="#Meta-Learning" class="headerlink" title="Meta Learning"></a>Meta Learning</h1><p>李宏毅：<a href="https://www.bilibili.com/video/BV15b411g7Wd?p=57&amp;spm_id_from=pageDriver">https://www.bilibili.com/video/BV15b411g7Wd?p=57&amp;spm_id_from=pageDriver</a></p><p>一个不错的科普：<a href="https://www.bilibili.com/video/BV1KB4y1c7gg?from=search&amp;seid=2922012165894972973">https://www.bilibili.com/video/BV1KB4y1c7gg?from=search&amp;seid=2922012165894972973</a></p><h2 id="什么是元学习"><a href="#什么是元学习" class="headerlink" title="什么是元学习"></a>什么是元学习</h2><p>Meta Learning = Learn to Learn (学习如何去做学习这件事)</p><p>机器在学习了很多task后，在获得过去的任务下所汲取的经验后，学习到了更多的学习技巧，成为了一个更厉害的学习者。</p><p>从而有一个新任务，他可以学的更快更好。</p><p>比如：task1你教机器去学语音识别，task2你教他去做图片识别，那么task3你让他去学习文字识别，那么他可能学的会更好。</p><p>元学习的输入是训练数据，输出的是可以用于下一个任务的function，function也就是万能函数模拟器神经网络的模型参数</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  f^* = F(D_{train})    \end{split}\end{equation}</script><p>其中F 代表元学习算法，D是数据，f就是function。理解下图：</p><p><img src="https://i.loli.net/2021/05/09/rMLgmoSywHJcq5u.png" alt=""></p><h3 id="和机器学习的区别"><a href="#和机器学习的区别" class="headerlink" title="和机器学习的区别"></a>和机器学习的区别</h3><p>机器学习：定义一系列function—-&gt;定一个function好坏的指标——-&gt; 用gradient decent找到一个最好的function</p><p>元学习(也是找一个function)：定义一系列大Function——-&gt;定一个评价大Function好坏的指标——-&gt;找到一个最好的大Function</p><h3 id="和终身学习-Life-long-learning-有些像？"><a href="#和终身学习-Life-long-learning-有些像？" class="headerlink" title="和终身学习(Life-long learning)有些像？"></a>和终身学习(Life-long learning)有些像？</h3><p><a href="https://blog.csdn.net/zyy617532750/article/details/104217399">持续/终身学习</a>：是让同一个模型可以同时学会很多任务技能</p><p>而元学习是不同的任务仍然有不同的模型，我们期待的是模型通过以前的学习经历可以让他在未来别的任务上学的好。</p><h2 id="元学习过程"><a href="#元学习过程" class="headerlink" title="元学习过程"></a>元学习过程</h2><h3 id="定义一系列学习算法"><a href="#定义一系列学习算法" class="headerlink" title="定义一系列学习算法"></a>定义一系列学习算法</h3><p>为什么是一系列学习算法，其实不同的模型参数、不同的结构、不同的学习参数的组合都是不同的学习算法。</p><p><img src="https://i.loli.net/2021/05/09/P6iANVszETHWB37.png" alt=""></p><p>以梯度下降法为例，首先定义一个网络结构，初始化一个参数，通过训练数据计算一个梯度g，再通过学习率更新参数。</p><p>迭代多次最后得到最终参数$\hat \theta$</p><p>但上图中红色框框内的都是人为定义的。元学习就是想让这红框内的东西，不让人来设计，让机器根据先验知识来自己学习设计。</p><h3 id="评估function参数好坏"><a href="#评估function参数好坏" class="headerlink" title="评估function参数好坏"></a>评估function参数好坏</h3><p>让模型先学一些任务，去解一些问题看看。</p><p>比如Task1：用一些$D<em>{train}$ 数据去训练模型得到$f_1$ ,再用Task1的$D</em>{test}$ 去衡量 $f_1$ 得到一个loss $l_1$</p><p>一个任务不够，再多找些任务来</p><p>Task2：用一些$D<em>{train}$ 数据去训练模型得到$f_2$ ,再用Task2的$D</em>{test}$ 去衡量 $f_2$得到一个loss $l_2$</p><p>最后得到评价F好坏的Loss：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  L(F) &= \sum_{n=1}^Nl_n\\ F^* &= argmin_FL(F)    \end{split}\end{equation}</script><p>N 为任务数</p><p>meta learning 通常会把task的Train叫做Suppot set，Test叫做Query set</p><h2 id="MAML-Model-Agnostic-Meta-Learning"><a href="#MAML-Model-Agnostic-Meta-Learning" class="headerlink" title="MAML(Model Agnostic Meta-Learning)"></a>MAML(Model Agnostic Meta-Learning)</h2><p>学一个初始化的参数</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  L(\phi) = \sum_{n=1}^N l^n(\hat \theta^n)    \end{split}\end{equation}</script><p>$\phi$ 输入的初始化参数，$\hat \theta^n$ 在第n个task上学出来的model，$\hat \theta^n$ 取决于$\phi$ </p><p>$l^n(\hat \theta^n)$: 把$\hat \theta^n$这组参数拿到第n个task的测试集中去看看效果怎么样</p><p>怎么确定初始化的参数好不好，就用初始化参数到不同task上去做训练</p><p>最小化$L(\phi)$ : $\phi \gets \phi-\alpha ▽_{\phi}L(\phi)$</p><h3 id="和迁移学习-Transfer-learning-预训练有些像？"><a href="#和迁移学习-Transfer-learning-预训练有些像？" class="headerlink" title="和迁移学习(Transfer learning) 预训练有些像？"></a>和迁移学习(Transfer learning) 预训练有些像？</h3><p>迁移学习：某一个任务的数据很少，但另外一个任务的数据多。就把model预训练在多的数据上，再fine-tuning在少的数据上。</p><p>他的loss function：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}   L(\phi) = \sum_{n=1}^N l^n(\phi)    \end{split}\end{equation}</script><p>在MAML里面loss是用$\phi$ 训练完后的model计算出来的，是训练过后的model</p><p>在pretrain里是用现在这个model直接去下游任务中衡量表现怎么样。</p><p>有的文章把预训练改成MAML的形式，以缓解预训练任务和下游任务直接目标不同产生的gap。</p><p>在MAML中，我们不在意$\phi$ 在training task上的表现，在意的是用$\phi$ 训练出来的$\hat \theta^n$的表现如何</p><p>（面向的是<strong>学习的过程</strong>，并不是<strong>学习的结果</strong>）</p><p><img src="https://i.loli.net/2021/05/10/7V2Uua4g8e9R1tk.png" alt=""></p><p><img src="https://i.loli.net/2021/05/10/epRfZzxlFTgSjVI.png" alt=""></p><p>如上图虽然$\phi$ 本身表现不够好，但$\phi$经过训练以后可以变得很强 (潜力如何)</p><p>而pretrain在意的是现在这个$\phi$表现的怎么样，是在找寻在所有task都最好的$\phi$, 并不保证训练以后会得到好的 $ \hat \theta^n$ （现在表现如何）</p><p>并且MAML只训练很少的步数，因为</p><ul><li>为了快速</li><li>希望在训练一步就得到很好的结果</li><li>在使用算法模型时可以多update</li><li>为了适应Few-shot learning </li></ul><h3 id="Toy-Example"><a href="#Toy-Example" class="headerlink" title="Toy Example"></a>Toy Example</h3><p>每一个任务：</p><ul><li>给一个目标sin函数 $y = a sin(x+b)$ 其中 a、b 都是随机数，每一组 a、b 对应一条正弦曲线</li><li>从目标函数中采样k个点</li><li>使用采样点去估计目标函数</li></ul><p>希望拟合的y越好越好。随机采样不同的a和b就可以得到不同的任务。</p><p><img src="https://i.loli.net/2021/05/10/9YnTfrxqoBDgVCU.png" alt=""></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zhuanlan.zhihu.com/p/367684934">元学习-总结</a></p><p><a href="https://zhuanlan.zhihu.com/p/108503451">元学习（Meta-learning）——李宏毅老师教学视频笔记</a></p><p><a href="https://zhuanlan.zhihu.com/p/181709693">[meta-learning] 对MAML的深度解析</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Meta-Learning&quot;&gt;&lt;a href=&quot;#Meta-Learning&quot; class=&quot;headerlink&quot; title=&quot;Meta Learning&quot;&gt;&lt;/a&gt;Meta Learning&lt;/h1&gt;&lt;p&gt;李宏毅：&lt;a href=&quot;https://www.b</summary>
      
    
    
    
    
    <category term="ML&amp;DL" scheme="http://example.com/tags/ML-DL/"/>
    
  </entry>
  
</feed>
