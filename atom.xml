<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Coding-Zuo</title>
  
  <subtitle>Coding And Studying</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-09-05T03:43:34.505Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Coding-Zuo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Rumor Detection on Social Media with Bi-Directional Graph Convolutional Networks</title>
    <link href="http://example.com/2021/09/04/Rumor-Detection-on-Social-Media-with-Bi-Directional-Graph-Convolutional-Networks/"/>
    <id>http://example.com/2021/09/04/Rumor-Detection-on-Social-Media-with-Bi-Directional-Graph-Convolutional-Networks/</id>
    <published>2021-09-04T10:54:37.000Z</published>
    <updated>2021-09-05T03:43:34.505Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Rumor-Detection-on-Social-Media-with-Bi-Directional-Graph-Convolutional-Networks"><a href="#Rumor-Detection-on-Social-Media-with-Bi-Directional-Graph-Convolutional-Networks" class="headerlink" title="Rumor Detection on Social Media with Bi-Directional Graph Convolutional Networks"></a>Rumor Detection on Social Media with Bi-Directional Graph Convolutional Networks</h1><p><a href="https://github.com/TianBian95/BiGCN">https://github.com/TianBian95/BiGCN</a></p><p>从社交媒体上如此海量的信息中识别谣言正成为一项艰巨的挑战。</p><p>一些深度学习方法被应用于通过谣言传播的方式来发现谣言，如递归神经网络(RvNN)等。然而，这些深度学习方法只考虑了深度传播的特点，而忽略了谣言检测中广泛分散的结构。</p><p>实际上，propagation(传播)和dispersion(扩散)是谣言的两个关键特征。</p><p>作者提出一种新的双向图模型，称为双向图卷积网络(Bi-GCN)，通过对谣言的自上而下和自下而上的传播进行操作，来探索这两个特性。</p><ul><li>利用具有自上而下谣言传播有向图的GCN来学习谣言传播模式</li><li>具有相反方向的谣言扩散图，以捕获谣言扩散的结构</li></ul><p>此外，来自消息来源的信息涉及到GCN的每一层，以增强谣言根源的影响力。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Rumor-Detection-on-Social-Media-with-Bi-Directional-Graph-Convolutional-Networks&quot;&gt;&lt;a href=&quot;#Rumor-Detection-on-Social-Media-with-Bi-</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>Interpretable Rumor Detection in Microblogs by Attending to User Interactions</title>
    <link href="http://example.com/2021/09/04/Interpretable-Rumor-Detection-in-Microblogs-by-Attending-to-User-Interactions/"/>
    <id>http://example.com/2021/09/04/Interpretable-Rumor-Detection-in-Microblogs-by-Attending-to-User-Interactions/</id>
    <published>2021-09-04T10:52:02.000Z</published>
    <updated>2021-09-05T03:10:18.157Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Interpretable-Rumor-Detection-in-Microblogs-by-Attending-to-User-Interactions"><a href="#Interpretable-Rumor-Detection-in-Microblogs-by-Attending-to-User-Interactions" class="headerlink" title="Interpretable Rumor Detection in Microblogs by Attending to User Interactions"></a>Interpretable Rumor Detection in Microblogs by Attending to User Interactions</h1><p><a href="https://github.com/serenaklm/rumor_detection">https://github.com/serenaklm/rumor_detection</a></p><p>通过学习区分社区对微博中真假claim的响应来解决谣言检测问题。</p><p>现有最先进的模型是基于对会话树建模的树模型。然而，在社交媒体中，发布回复的用户可能是对整个thread的回复，而不是对特定用户的回复。</p><p>提出Multi-head post-level attention模型(PLAN)来构建推文之间的远距离交互。并提出几个变体：</p><ul><li>结构感知自注意模型(StA-PLAN)，将树形结构信息合并到Transformer中</li><li>分层token和post-leve attention(StA-HiTPLAN), 通过token-level 自注意力学习句子表征</li></ul><p>这篇工作重点是利用社区对虚假claim的响应来检测虚假索赔。这一研究领域通过将自然语言处理应用于针对claim的评论来利用社区的集体智慧。这些工作背后的关键原则是，社交媒体上的用户会分享对不准确信息的看法、猜测和证据。</p><h2 id="样本"><a href="#样本" class="headerlink" title="样本"></a>样本</h2><p><img src="https://i.loli.net/2021/09/04/FwWU2TKkf1Ji5B3.png" alt=""></p><p>源贴：“沃尔玛捐赠1万美元支持达伦·威尔逊和正在进行的种族主义警察谋杀案#弗格森#抵制沃尔玛URL”。</p><p>推特R_1及其回复推文R_1_1对消息来源的真实性表示怀疑。</p><p>推特R_2_1和R_3_1提供了确凿的证据，揭穿了消息来源的说法是假的。</p><p>虽然R_2_1和R_3_1分别是R_2和R_3的子节点，但它们可以为树上的所有其他节点(如R_1_1和R_1)提供重要信息。</p><p>因此，应该考虑所有tweet之间的互动，而不仅仅是父节点和他们的孩子节点之间的互动。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>两篇使用树形结构进行建模的sota对社交媒体中谣言检测有限制。</p><p>Rumor detection on twitter with tree-structured recursive neural networks(2018) ，将来源claim及其回复推文组织成树形结构，使用递归神经网络对传播树中的信息传播进行建模。来自不同节点的信号以自下而上或自上而下的方式进行粗略地重新聚合。在自下而上的模型中，信息从子节点传播到父节点，在自上而下的模型中，信息从父节点传播到子节点，反之亦然。</p><p>Tree lstms with convolution units to predict stance and rumor veracity in social media conver- sations.(2019) ，组织了树状结构的对话线程，并探索了用于谣言检测的branch和tree LSTM的几种变体。</p><p>这两篇论文都使用了树模型，目的是对会话线索中存在的结构信息进行建模。在树模型中，信息从父级传播到子级，反之亦然。然而，社交媒体对话中的线索结构有所不同，每个用户通常能够观察到对话的不同分支中的所有回复。揭穿假新闻的用户不能只针对他回复的人创建的内容也可能适用于该帖子中的其他推文。树模型不会对来自其他分支的节点之间的交互进行显式建模，这是对社交媒体会话建模时的一个关键限制。</p><p>自动区分真假Claims的现有方法利用了各种特征：</p><ul><li>Claims的内容</li><li>Claims来源的重点考虑和社交网络</li><li>使用可信来源(例如，维基百科)进行事实核查</li><li>社区对Claims的反应。</li></ul><p>这篇重点在社区响应，接下来展开介绍介绍。</p><h3 id="Content-Information"><a href="#Content-Information" class="headerlink" title="Content Information"></a>Content Information</h3><p>早期关于欺骗性内容检测的工作研究了语言线索的使用，例如代词的百分比、词长、动词数量和词类。也有工作对关于虚假的评论，目击者的陈述，和讽刺。利用语言特征对假新闻的检测也进行了研究。这种对概念内容的分析依赖于可能是领域或主题所独有的语言特征。</p><h3 id="Source-and-Social-Network"><a href="#Source-and-Social-Network" class="headerlink" title="Source and Social Network"></a>Source and Social Network</h3><p>研究假新闻的来源及其社交网络，在内容中加入来源信息提高了假新闻分类准确率。为传播假新闻而创建的账户往往具有不同的社交网络特征。</p><h3 id="Fact-Checking"><a href="#Fact-Checking" class="headerlink" title="Fact Checking"></a>Fact Checking</h3><p>事实核查网站，如PolitiFact。com和snopes.com依靠人工验证来揭穿假新闻，但无法匹配假新闻的生成速度(Funke 2019)。自动事实核查旨在对照诸如维基百科(Ciampaglia et al.2015年)。</p><p>最近，索恩等人(2018)提出了FEVER共享任务，针对包含500万维基百科文档的数据库验证输入Claim，并将每个Claim分为三类：支持、驳斥或信息不足。</p><p>事实核查是一种更有原则的假新闻检测方法。然而，它也需要建立一个经过核实的事实语料库，而且可能不适用于证据很少的新Claim。</p><h3 id="Community-Response"><a href="#Community-Response" class="headerlink" title="Community Response"></a>Community Response</h3><p>研究人员致力于通过构建分类器来自动预测claim的有效性，分类器利用对社交媒体帖子的评论和回复，以及传播模式。</p><p>Ma(2018a)采用多任务学习方法构建了一个学习立场感知特征的分类器用于谣言检测。</p><p>Li (2019)对他的模型采用了多任务学习方法，并在他的模型中包括了用户信息。</p><p>Chen(2017)提出汇集截然不同的特征，以捕捉帖子随时间的上下文变化。</p><p>除了语言特征，其他研究人员也关注了用户的人口统计或交互预测来确定用户的可信度。</p><p>Yang(2012)收集了从事传播假新闻的用户特征，通过对传播路径进行分类，仅利用用户特征构建了假新闻检测器</p><p>Li(2019)使用用户信息和内容特征相结合的方式训练具有多任务学习目标的LSTM。</p><p>在本文中，仅从帖子和评论两个方面来检测谣言和假新闻。提出了一种用于谣言检测的Transformer，而不是递归树模型。</p><h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>问题陈述，将每个线程thread定义为：</p><script type="math/tex; mode=display">X = \{x_1,x_2,...,x_n\}</script><p>其中$x_1$是源tweet，$x_i$是按时间顺序排列的第 $i$ 条tweet，$n$是线程中的tweet数量。</p><p>会话树中除了文本信息外，还有可以利用的结构信息。在树形结构模型中，如果$x_i$对$x_j$进行应答，反之亦然，则只对Twitter $x_i$和$x_j$进行关联。</p><p>本文模型允许任何帖子关注同一主题中的任何其他帖子。</p><p>在提出的结构感知模型中，用关系标签来标记任何一对推文 $x_i$ 和 $x_j$之间的关系 $R(i,j)\in {\text{parent, child, before, after, self}}$ 。$R(i，j)$ 的值是通过依次应用以下规则集来获得的：</p><ul><li>parent：如果 $x_i$ 直接回复 $x_j$</li><li>child：如果 $x_j$ 直接回复 $x_i$</li><li>before：如果 $x_i$ 在 $x_j$之前到来</li><li>after：如果 $x_i$ 在 $x_j$之后</li><li>self：如果i=j</li></ul><p>谣言检测任务简化为学习预测每个$(X，R)$到其谣言类别 $y$。</p><p>在两个谣言检测数据集上进行了实验，即Twitter15和Twitter16数据，以及PHEME 5数据。</p><p>对于我们正在处理的数据集，分类标签是不同的：</p><ul><li>Twitter15 and Twitter16：$y\in {\text{non-rumor, false-rumor, true-rumor, unverified}}$</li><li>PHEME: $y\in {\text{false-rumor, true-rumor, unverified}}$</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>然而，正如我们将在下表中的数据统计中看到的那样，数据集中的树非常浅，大部分评论直接回复源tweet，而不是回复其他tweet。</p><p><img src="https://i.loli.net/2021/09/05/2RAGI9fpbJ5D387.png" alt=""></p><p>我们发现，在社交媒体中，由于整个帖子通常都是可见的，回复根帖子的用户可能会继续与更活跃的用户进行对话，而不是专门为根帖子撰写回复。因此，在对社交媒体对话进行建模时，没有对推文之间的每一种可能的成对交互进行显式建模的树模型是次优的，所以用Transformer-based模型。</p><h3 id="Post-Level-Attention-Network-PLAN"><a href="#Post-Level-Attention-Network-PLAN" class="headerlink" title="Post-Level Attention Network (PLAN)"></a>Post-Level Attention Network (PLAN)</h3><p><img src="https://i.loli.net/2021/09/05/aq5MoQL1iACXujd.png" alt=""></p><p>首先将对话树的结构展平，并将推文按时间顺序排列成直线结构，源推文作为第一条推文。对于我们的计划模型，我们在线性结构中对每个推文 $x_i$ 应用最大池化来获得它的句子表示 $x_i$。</p><p>然后传递一个句子嵌入序列 $X’ = (x_1’,x_2’,…,x_n’)$  通过s个数的多头注意力(MHA)层来模拟推文之间的交互。</p><p>我们将这些MHA层称为post-level attention层。因此，这将改变 $X’ =(x_1’,x_2’,…,x_n’)$ 为 $U=(u_1,u_2,…,u_n)$</p><p>最后，使用注意力机制对推文进行插值，然后通过一个全连接层进行预测。</p><h3 id="Structure-Aware-Post-Level-Attention-Network-StA-PLAN"><a href="#Structure-Aware-Post-Level-Attention-Network-StA-PLAN" class="headerlink" title="Structure Aware Post-Level Attention Network (StA-PLAN)"></a>Structure Aware Post-Level Attention Network (StA-PLAN)</h3><p>模型的一个可能的局限性是，我们通过以线性结构组织推文来丢失结构信息。转换树中固有存在的结构化信息对于假新闻检测可能仍然有用。</p><p>树模型在这方面更优越，因为结构信息是显式建模的。为了将树模型的优点和自我注意机制结合起来，对计划模型进行了扩展，使其显式地包含了结构信息。</p><script type="math/tex; mode=display">\alpha_{ij} = softmax(\frac{q_ik_j^T+a_{ij}^K}{\sqrt{d_k}})</script><script type="math/tex; mode=display">z_i = \sum_{j=1}^n\alpha_{ij}(v_j+a_{ij}^V)</script><p>$a^V<em>{ij}$ 和 $a^K</em>{ij}$ 都是代表tweet对之间五种可能的结构关系(即parent, child, before, after, self)之一的向量</p><h3 id="Structure-Aware-Hierarchical-Token-and-Post-Level-Attention-Network-StA-HiTPLAN"><a href="#Structure-Aware-Hierarchical-Token-and-Post-Level-Attention-Network-StA-HiTPLAN" class="headerlink" title="Structure Aware Hierarchical Token and Post-Level Attention Network (StA-HiTPLAN)"></a>Structure Aware Hierarchical Token and Post-Level Attention Network (StA-HiTPLAN)</h3><p><img src="https://i.loli.net/2021/09/05/Cx2RNBdOwAUcMI1.png" alt=""></p><p>PLAN模型执行最大池化，以获得每条推文的句子表示。</p><p>然而，让模型学习单词向量的重要性可能会更理想。因此，提出了一种分层的注意模型—token-level的注意和post-level的注意力。分层模型的概述如图所示。</p><p>在使用注意机制插入输出之前，我们执行token-level自注意力，而不是使用最大池化来获得句子表示。</p><p>每条推文可以表示为一系列单词记号$x<em>i=(x</em>{i,1}，x<em>{i,2}，…，x</em>{i,|xi|})$。我们在一条推文中通过MHA层传递了单词token的序列。这允许tweet中的token之间进行交互，将这些层称为token-level关注层。</p><h3 id="Time-Delay-Embedding"><a href="#Time-Delay-Embedding" class="headerlink" title="Time Delay Embedding"></a>Time Delay Embedding</h3><p>在不同的时间间隔创建的推文可以有不同的解释。首次创建源claim时表示不相信的推文可能很常见，因为claim可能尚未经过验证。然而，在传播的后期阶段，可疑的推文可能表明消息来源的说法是假的倾向很高。</p><p>因此，提出的三个模型PLAN、STA-PLAN和STA-HiTPLAN研究了带有时延信息的Tweet编码的实用性。</p><p>为了包括每个tweet的时间延迟信息，根据从源tweet创建时起的延迟将tweet绑定。</p><p>将时间箱的总数设置为100，每个箱代表10分钟的间隔。延迟超过1000分钟的推文将落入最后一个时间段。</p><script type="math/tex; mode=display">TDE_{pos,2i} = sin\frac{pos}{10000^{2i/d_model}}</script><script type="math/tex; mode=display">TDE_{pos,2i+1} = cos\frac{pos}{10000^{2i/d_model}}</script><p>其中pos表达为时间bin，$pos\in[0,100)$</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p><img src="https://i.loli.net/2021/09/05/2RAGI9fpbJ5D387.png" alt=""></p><p><img src="https://i.loli.net/2021/09/05/MUWvDm5j36NYksw.png" alt=""></p><ul><li>Twitter15 and Twitter16 ：对于Twitter15和Twitter16的数据集，每个声明中都有很大比例的转发：Twit-15和Twitter16分别为89%和90%。因为作者假设转发不会给模型带来新信息，所以删除了Twitter15和Twitter16的所有转发。在删除转发后，观察到少数索赔将只剩下来源Claim。既然作者的方法背后的原则是，我们可以利用人群的信号来侦测谣言，那么没有任何回复的说法就应该是“未经核实的(unverified)”。因此，在训练数据中修改了这类说法的标签为“未经证实”</li></ul><p><img src="https://i.loli.net/2021/09/05/vaNkVL2UIAhD4n1.png" alt=""></p><p><img src="https://i.loli.net/2021/09/05/7riuNwZBWUvxjE6.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Interpretable-Rumor-Detection-in-Microblogs-by-Attending-to-User-Interactions&quot;&gt;&lt;a href=&quot;#Interpretable-Rumor-Detection-in-Microblogs</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>Dice Loss for Data-imbalanced NLP Tasks</title>
    <link href="http://example.com/2021/09/01/Dice-Loss-for-Data-imbalanced-NLP-Tasks/"/>
    <id>http://example.com/2021/09/01/Dice-Loss-for-Data-imbalanced-NLP-Tasks/</id>
    <published>2021-09-01T07:37:09.000Z</published>
    <updated>2021-09-01T12:54:41.077Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Dice-Loss-for-Data-imbalanced-NLP-Tasks"><a href="#Dice-Loss-for-Data-imbalanced-NLP-Tasks" class="headerlink" title="Dice Loss for Data-imbalanced NLP Tasks"></a>Dice Loss for Data-imbalanced NLP Tasks</h1><p>许多自然语言处理任务，如序列标注和机器阅读理解(MRC)，都面临着严重的数据失衡问题：</p><ul><li>负样本明显多于正样本，占据绝大多数的负例会支配模型的训练过程，导致模型倾向于负例，而测试时使用的F1指标需要每个类都能准确预测；</li><li>大量简单负例（easy-negative）使训练不堪重负。负例占绝大多数也意味着其中包含了很多简单样本，这些简单样本对于模型学习困难样本几乎没有帮助，反而会在交叉熵的作用下推动模型遗忘对困难样本的知识。</li></ul><p>loss中最常用的交叉熵实际上是以精度为导向的，这造成了训练和测试之间的差异。在训练时，每个训练样本对目标函数的贡献相等，而在测试时，F1 score更关注正例。</p><p>本文认为这种问题是交叉熵本身的特点带来的：交叉熵“平等”地看待每一个样本，无论正负，都尽力把它们推向1（正例）或0（负例）。但实际上，对分类而言，将一个样本分类为负只需要它的概率＜0.5即可，完全没有必要将它推向0。</p><p>基于这个观察，作者使用现有的Dice Loss，并提出一个基于Dice Loss的自适应损失——DSC，在训练时推动模型更加关注困难的样本，降低简单负例的学习度，从而在整体上提高基于F1值的效果。</p><h2 id="从Cross-Entropy-到-Dice-Losses"><a href="#从Cross-Entropy-到-Dice-Losses" class="headerlink" title="从Cross Entropy 到 Dice Losses"></a>从Cross Entropy 到 Dice Losses</h2><h3 id="交叉熵损失-CE"><a href="#交叉熵损失-CE" class="headerlink" title="交叉熵损失(CE)"></a>交叉熵损失(CE)</h3><p>以二分类作为说明，记输入为 $x$, 输出为一个二值概率 $p = [p_0,p_1]$, 并且有一个二元真值 $y = [y_0,y_1]$</p><p>首先交叉熵损失是：</p><script type="math/tex; mode=display">CE =  -(y_0log\ p_0 + y_1log \ p_1)</script><p>显然，对每个样本，CE都对它们一视同仁，不管当前样本是简单还是复杂。当简单样本有很多时，模型训练就会被这些简单的样本占据，使得模型难以从复杂样本中学习。于是，一种简单的改进方法是，降低模型在简单样本上的学习速率，从而得到下述加权交叉损失：</p><script type="math/tex; mode=display">Weighted \ CE = -\alpha(y_0log \ p_0+y_1log \ p_1)</script><p>对不同样本，我们可以设置不同的权重，从而控制模型在该样本上学习的程度。但是此时，权重的选择又变得比较困难。因为我们的目标是缓解数据集的不平衡问题，从而提高基于F1评测标准的效果，我们希望有一种损失函数能够直接作用于F1。</p><h3 id="Sorensen–Dice系数（DSC）"><a href="#Sorensen–Dice系数（DSC）" class="headerlink" title="Sørensen–Dice系数（DSC）"></a>Sørensen–Dice系数（DSC）</h3><p>一种现有的方法——Sørensen–Dice系数（简称DSC）——去衡量F1。</p><p>DSC是一种用于衡量两个集合之间相似度的指标：</p><script type="math/tex; mode=display">DSC(A,B) = \frac{2|A\cap B|}{|A|+|B|}</script><script type="math/tex; mode=display">F1 = \frac{2(precision*recall)}{precision+recall}</script><script type="math/tex; mode=display">A = precision = \frac{TP}{TP+FP} ,  B = recall =\frac{TP}{TP+FN}</script><p>如果我们令A是所有模型预测为正的样本的集合，令B是所有实际上为正的样本集合，那么DSC就可以重写为：</p><script type="math/tex; mode=display">DSC(D,f) = \frac{2TP}{2TP+FN+FP}=F1</script><p>其中D数据集，f是一个分类模型。于是在这个意义上DSC与F1是等价的。</p><p>既然如此，就直接优化DSC，然而上述表达式是离散的，为此，需要把上述DSC表达式转化为连续的版本，从而可以视作一种soft F1。</p><p>对于单个样本x，直接定义它的DSC：</p><script type="math/tex; mode=display">DSC(x,f) = \frac{2p_1y_1}{p_1+y_1}</script><p>可以看到如果x是父类，那么它的DSC就为0，从而不会对训练有贡献。为了让父类也能有所贡献，所以增加一个平滑项：</p><script type="math/tex; mode=display">DSC_s(x,f) = \frac{2p_1y_1 + \epsilon}{p_1+y_1+\epsilon}</script><p>但这样一来，又需要我们根据不同的数据集手动地调整平滑项。而且当easy-negative样本很多的时候，即便使用上述平滑项，整个模型训练过程仍然会被它们主导。基于此，我们使用一种“自调节”的DSC（这里就和focal loss很像）：</p><script type="math/tex; mode=display">DSC(x,f) = \frac{2(1-p_1)p_1\cdot y_1 + \epsilon}{(1-p_1)p_1 + y_1 + \epsilon}</script><p>比较上面两个DSC，可以发现，$1-p_1$ 实际上充当了缩放系数，对于简单样本($p_1$ 趋向于1或0)，$(1-p_1)p_1$ 使得模型更少地去关注他们。</p><p>从导数上看，一旦模型正确分类当前样本（刚刚经过0.5），DSC就会使模型更少关注它，而不是像交叉熵那样，鼓励模型迫近0或1这两个点。这就能有效避免因简单样本过多导致模型训练受到简单样本的支配。</p><p>事实上，这比较类似Focal Loss(FL)，降低已分好类的样本的学习权重：</p><script type="math/tex; mode=display">FL = -(y_0(1-p_0)^\gamma log p_0 + y_1(1-p_1)^\gamma log p_1)</script><p>不过，FL即使能对简单样本降低学习权重，但它本质上仍然是在鼓励简单样本趋向于0或1，这就和DSC有了本质上的区别。因此，说DSC通过“平衡”简单样本和困难样本的学习过程，从而提高了最终的F1值（因为F1要求各类都有比较好的结果）</p><h3 id="Dice-Loss-DL-与Tversky-Loss-TL"><a href="#Dice-Loss-DL-与Tversky-Loss-TL" class="headerlink" title="Dice Loss(DL)与Tversky Loss(TL)"></a>Dice Loss(DL)与Tversky Loss(TL)</h3><p>除了上述DSC外，还比较了两种$DSC_s(x,f)$的变体，分别是Dice Loss（DL）和Tversky Loss（TL）：</p><script type="math/tex; mode=display">DL = 1 - \frac{2p_1y_1+\epsilon}{p_1^2+y_1^2+\epsilon}</script><script type="math/tex; mode=display">TL = 1-\frac{p_1y_1 + \epsilon}{p_1y_1+\alpha p_1y_0+\beta p_0y_1 + \epsilon}</script><p>在$\alpha=\beta=0.5$时，TL就退化为DSC。           </p><h3 id="损失总结"><a href="#损失总结" class="headerlink" title="损失总结"></a>损失总结</h3><p><img src="https://i.loli.net/2021/09/01/YkHOMIlVSPjG5aw.png" alt=""></p><p>后三个统称为Dice loss</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Dice-Loss-for-Data-imbalanced-NLP-Tasks&quot;&gt;&lt;a href=&quot;#Dice-Loss-for-Data-imbalanced-NLP-Tasks&quot; class=&quot;headerlink&quot; title=&quot;Dice Loss for </summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs</title>
    <link href="http://example.com/2021/08/29/Edge-augmented-Graph-Transformers-Global-Self-attention-is-Enough-for-Graphs/"/>
    <id>http://example.com/2021/08/29/Edge-augmented-Graph-Transformers-Global-Self-attention-is-Enough-for-Graphs/</id>
    <published>2021-08-29T08:32:53.000Z</published>
    <updated>2021-09-01T13:41:21.763Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Edge-augmented-Graph-Transformers-Global-Self-attention-is-Enough-for-Graphs"><a href="#Edge-augmented-Graph-Transformers-Global-Self-attention-is-Enough-for-Graphs" class="headerlink" title="Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs"></a>Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs</h1><p>—do_train —do_eval —train_batch_size 64 —num_train_epochs 50 —embeddings_learning_rate 0.7e-4 —encoder_learning_rate 0.7e-4 —classifier_learning_rate 7e-4 —warmup_steps 200 —max_seq_len 132 —dropout_rate 0.15 —metric_key_for_early_stop “macro avg<strong>f1-score</strong>level_2” —logging_steps 200 —patience 6 —label2freq_level_1_dir /data2/code/DaguanFengxian/bert_model/data/label2freq_level_1.json —label2freq_level_2_dir /data2/code/DaguanFengxian/bert_model/data/label2freq_level_2.json —processor_sep “\t” —loss_fct_name dice </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Edge-augmented-Graph-Transformers-Global-Self-attention-is-Enough-for-Graphs&quot;&gt;&lt;a href=&quot;#Edge-augmented-Graph-Transformers-Global-Sel</summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Breadth First Reasoning Graph for Multi-hop Question Answering</title>
    <link href="http://example.com/2021/08/17/Breadth-First-Reasoning-Graph-for-Multi-hop-Question-Answering/"/>
    <id>http://example.com/2021/08/17/Breadth-First-Reasoning-Graph-for-Multi-hop-Question-Answering/</id>
    <published>2021-08-17T09:48:20.000Z</published>
    <updated>2021-08-19T12:23:50.481Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Breadth-First-Reasoning-Graph-for-Multi-hop-Question-Answering"><a href="#Breadth-First-Reasoning-Graph-for-Multi-hop-Question-Answering" class="headerlink" title="Breadth First Reasoning Graph for Multi-hop Question Answering"></a>Breadth First Reasoning Graph for Multi-hop Question Answering</h1><p>为了解决GNNs不必要的更新和简单的边结构阻碍直接提取准确的答案跨度，和更可解释性。</p><p>作者提出了一种新的广度优先推理图(BFR-Graph)模型，它提供了一种新的更符合推理过程的消息传递方式。</p><p>在BFR-Graph中，推理信息要求从问题结点开始，逐跳传递到下一个句子结点，直到所有的边都经过，可以有效地防止每个结点的过度平滑或不必要的多次更新。</p><p>为了引入更多的语义，我们还将推理图定义为考虑了共现关系数和句子间距离的加权图。</p><p>然后基于GNN提出了一种更直接、更易解释的方法来聚合不同粒度级别的分数。</p><h2 id="现有GNN方法的几个问题"><a href="#现有GNN方法的几个问题" class="headerlink" title="现有GNN方法的几个问题"></a>现有GNN方法的几个问题</h2><ul><li>首先，当前的方法将所有节点（包括一些不必要的节点）一起更新到每一层中，这可能导致节点收敛到相似的值，并失去对具有更多层的GNN的识别能力。</li><li>第二，虽然GNN设计了不同类型的边，但是在没有考虑句子之间的其他关系信息的情况下，相同类型的边之间没有更细粒度的区别。</li><li>第三，现有的方法只是潜在地融合了GNN和上下文编码器的隐藏表示，而没有以直接和可解释的方式进行答案广度提取。</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="https://i.loli.net/2021/08/19/NlBA3uLTydzhVwi.png" alt=""></p><p>与现有的基于GNN的方法不同，BFR-Graph对消息传递引入了新的限制：消息只从问题开始，然后逐跳传递到后面的句子节点。此外，考虑到共现实体和句子之间的距离，图被构造成一个加权图。此外，利用BFR图的推理结果，设计了多分数答案预测。</p><p>简言之，我们在加权图上提出了广度优先推理，然后在多任务联合训练的框架下，结合多级分数进行回答预测。</p><h3 id="Paragraph-Selection"><a href="#Paragraph-Selection" class="headerlink" title="Paragraph Selection"></a>Paragraph Selection</h3><p>训练一个二分类bert，对每个段落进行打分，选择得分在前N位的段落作为有用的段落，然后将这些段落连接在一起作为上下文C。</p><h3 id="Context-Encoding"><a href="#Context-Encoding" class="headerlink" title="Context Encoding"></a>Context Encoding</h3><p>bert输出+bi-attention后获得问题和上下文的编码表达</p><script type="math/tex; mode=display">H = \{h_0, ..., h_{L-1}\}</script><p>其中L是输入序列的长度(连接问题和上下文)，d是双关注层的输出维度(也是BERT的维度)。</p><p>为了实现句子级表示，首先获得每个句子的标记级表示：</p><script type="math/tex; mode=display">S_i^{seq} = H[s_i^{start}:s_i^{end}]\in \mathbb{R}^{L_{s_i}\times d}</script><p>获得每个句子的表示是用了Bi-LSTM的方法</p><script type="math/tex; mode=display">s_i = \sum_{k=0}^{L_s}\alpha_k^iS_i^{seq}[k, :]\in \mathbb{R}^d</script><p>$\alpha_k^i$ 是第i个句子中第k个token的权重，通过两层MLP output size=1获得</p><h3 id="Weighted-Graph-Construction"><a href="#Weighted-Graph-Construction" class="headerlink" title="Weighted Graph Construction"></a>Weighted Graph Construction</h3><p>为了更好地挖掘句子之间复杂的关系信息，定义了正相关和负相关两种类型的相关性：</p><ul><li>正相关：如果表示句子 $i$ 和 $j$ 的节点具有 n(n≥1) 个相同命名实体，则添加一条边，该边的权重为：</li></ul><script type="math/tex; mode=display">w_{ij} = \frac{1}{1+e^{-n+K_1}}</script><ul><li>负相关：否则，如果两个节点最初来自同一段落，则添加一条边，该边的权重为：</li></ul><script type="math/tex; mode=display">w_{ij} = \frac{1}{1+e^{d+K_2}}</script><p>其中d是两个句子的距离(例如，如果该句子紧跟在段落中的另一个句子之后，则d=1，如果它们之间有句子，则d=2，依此类推)。K1和K2是超参数。</p><p>是同质图，它包含单一类型的节点和边。</p><h3 id="Breadth-First-Reasoning"><a href="#Breadth-First-Reasoning" class="headerlink" title="Breadth First Reasoning"></a>Breadth First Reasoning</h3><p>下图直观地显示了BFR-Graph和典型GNN之间的区别。</p><p><img src="https://i.loli.net/2021/08/19/oCzcsTeIHXZGW3f.png" alt=""></p><p>当我们在段落上推理来回答一个问题时，我们从问题开始，一跳一跳地找到下一个句子。</p><p>对于节点表示句子的GNN，以下消息传递是不必要的，可能会抑制无用节点的干扰：</p><ul><li>从后一个节点到前一个节点</li><li>某个节点尚未收到来自问题的消息，但它会更新其他节点。</li></ul><p>具体地说，当同时满足以下条件时，节点i由节点j更新：</p><ul><li>节点 $i$ 和节点 $j$ 是邻居</li><li>节点 $j$ 是Active的</li><li>节点 $i$ 和节点 $j$ 之间的边以前没有经过</li></ul><p>BFR-Graph的整个消息传递过程:</p><p><img src="https://z3.ax1x.com/2021/08/19/fqlv7j.png" alt=""></p><p>消息更新传递的函数还是GAT</p><h3 id="Multi-score-Answer-Prediction"><a href="#Multi-score-Answer-Prediction" class="headerlink" title="Multi-score Answer Prediction"></a>Multi-score Answer Prediction</h3><p>HotpotQA数据集中的答案是上下文的span。现有工作仅计算编码器输出（如BERT）上的跨度概率，或额外连接GNN的隐藏输出。不同的是，我们通过计算从GNN获得的句子分数和段落分数来使用更易于解释的方法。如下图：</p><p><img src="https://z3.ax1x.com/2021/08/19/fq3Cad.png" alt=""></p><p>通常，作为答案跨度的开始/结束的上下文中的第y个单词的分数通过以下方式计算：</p><script type="math/tex; mode=display">\phi_{start}(y) = MLP_1(H[y,:])</script><script type="math/tex; mode=display">\phi_{end}(y) = MLP_2(H[y,:])</script><p>然后，计算GNN中每个节点对应的句子得分：</p><script type="math/tex; mode=display">\phi_{sent}(s_i) =MLP_3(s_i)</script><p>计算段落分数, 通过全局最大池：</p><script type="math/tex; mode=display">\phi_{para}(p_j) = MLP_4(Max(\{s_0^{p_j},...,s_{L_{p_j}-1}^{P_j}\}))</script><p>$s_i^{P_j}$是第$i$句话在第Pj段中的表达。这也可以通过在所有语句节点上取每个维度上的最大隐藏值来实现。</p><p>最后，上下文中第y个单词作为答案范围开始的概率由以下公式确定：</p><script type="math/tex; mode=display">p_{start}(y) = softmax(\phi'_{start}(y))</script><script type="math/tex; mode=display">\phi'_{start}(y) = \phi_{start}(y) + \phi_{sent}(s_i) + \phi_{para}(p_j)</script><p>并且可以类似地计算上下文中的第y个单词作为答案跨度结束的概率。</p><p>如果一个句子或段落的得分较高，则位于其中的单词更有可能是答案。</p><p>最后是一个多任务预测</p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p><img src="https://z3.ax1x.com/2021/08/19/fqGFjf.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Breadth-First-Reasoning-Graph-for-Multi-hop-Question-Answering&quot;&gt;&lt;a href=&quot;#Breadth-First-Reasoning-Graph-for-Multi-hop-Question-Answe</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>RealFormer: Transformer Likes Residual Attention</title>
    <link href="http://example.com/2021/08/15/RealFormer-Transformer-Likes-Residual-Attention/"/>
    <id>http://example.com/2021/08/15/RealFormer-Transformer-Likes-Residual-Attention/</id>
    <published>2021-08-15T03:16:37.000Z</published>
    <updated>2021-08-15T15:43:15.910Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RealFormer-Transformer-Likes-Residual-Attention"><a href="#RealFormer-Transformer-Likes-Residual-Attention" class="headerlink" title="RealFormer: Transformer Likes Residual Attention"></a>RealFormer: Transformer Likes Residual Attention</h1><p>提出了一个简单的基于Transformer的体系结构，创建一条“直接”路径在整个网络中传播原始注意力分数</p><p>如下图(c), 每个RealFormer层都获取前一层中所有注意力头部的原始注意力分数，并在顶部添加“残差分数”(计算方式与常规Transformers中的注意力分数相同)。</p><p>换句话说，RealFormer可以被视为向Post-LN Transformer添加一个简单的跳跃连接。不会向计算图中添加任何乘法运算，因此预期性能与之相当。</p><p>RealFormer中的AT往往更稀疏，跨层相关性更强，我们认为这可能具有一些正则化效应，可以稳定训练，有利于微调。</p><p><img src="https://i.loli.net/2021/08/15/jZrDYmkL5EzFqi6.png" alt="">t</p><ul><li>(a) 传统transformer的PostLN</li><li>(b) PreLN 论文：ON LAYER NORMALIZATION IN THE TRANSFORMER ARCHITECTURE，这种设计为每个子层增加了LN作为“预处理”步骤。</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>标准Transformer Encoder</p><script type="math/tex; mode=display">\text{MultiHead}(Q,K,V) = Concat(head_1,...,head_h)W^O</script><script type="math/tex; mode=display">head_i = Attention(QW_i^Q, KW^K_i,VW^V_i)</script><script type="math/tex; mode=display">Attention(Q',K',V') = Softmax(\frac{Q'K'^T}{\sqrt{d_k}})V'</script><script type="math/tex; mode=display">FFN(x) = \sigma(xW_1+b_1)W_2+b_2</script><p>Post-LN是Vaswani等人提出的原创体系结构。对每个子层末尾的输出进行标准化。</p><p>相反，Pre-LN规格化子层输入，并创建直接路径(没有LN)来传播序列中的令牌嵌入。</p><h3 id="Residual-Attention-Layer-Transformer"><a href="#Residual-Attention-Layer-Transformer" class="headerlink" title="Residual Attention Layer Transformer"></a>Residual Attention Layer Transformer</h3><p>RealFormer紧跟Post-LN设计，简单地增加了一个skip edge来连接相邻层中的多头注意力，如上图c所示。</p><p>形式上添加一个$Prev$，是上一个softmax的注意力分数也就是pre-softmax，形状为$(heads,\text{from_seq_len},\text{to_seq_len})^2$</p><script type="math/tex; mode=display">\text{ResidualMultiHead}(Q,K,V,Prev) = Concat(head1,...,head_h)W^O</script><script type="math/tex; mode=display">head_i = \text{ResidualAttention}(QW_i^Q,KW_i^K,VW_i^V,Prev_i)</script><p>$Prev_i$ 的形状为$(\text{from_seq_len,to_seq_len})$ 对应于每个$head_i$</p><script type="math/tex; mode=display">\text{ResidualAttention}(Q',K',V',Prev') = \text{Softmax}(\frac{Q'K'^T}{\sqrt{d_k}}+Prev')V'</script><p>新的注意力分数$\frac{Q’K’^T}{\sqrt{d_k}}+Prev’$</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://i.loli.net/2021/08/15/K86nvAXLs954SMf.png" alt=""></p><p>值得特别指出的是第一张图和第四张图。从第一张图我们可以看到，对于RealFormer结构，加大模型规模（large到xlarge）可以带来性能的明显提升，而ALBERT论文曾经提到加大BERT的模型规模并不能带来明显受益，结合两者说明这可能是PostLN的毛病而不是BERT的固有毛病，换成RealFormer可以改善这一点。从第四张图我们可以看到，RealFormer结构训练50万步，效果就相当于PostLN训练100万步，这表明RealFormer有着很高的训练效率。</p><p>除了上述实验外，论文还对比了不同学习率、不同Dropout比例的效果，表明RealFormer确实对这些参数是比较鲁棒的。原论文还分析了RealFormer的Attention值分布，表明RealFormer的Attention结果更加合理。</p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>RealFormer对梯度下降更加友好，这不难理解，因为$A<em>n = \frac{Q_nK_n^T}{\sqrt{d_k}} + A</em>{n-1}$的设计确实提供了一条直通路，使得第一层的Attention能够直通最后一层，自然就没有什么梯度消失的风险了。相比之下，PostLN是 $LayerNorm(x+f(x))$ 的结构，看上去$x+f(x)$防止了梯度消失，但是LayerNorm这一步会重新增加了梯度消失的风险，造成的后果是初始阶段前面的层梯度很小，后面的层梯度很大，如果用大学习率，后面的层容易崩，如果用小学习率，前面的层学不好，因此PostLN更难训练，需要小的学习率加warmup慢慢训。</p><p>还有一个就是叠加的问题PreLN每一步都是$x+f(x)$的形式，到了最后一层变成了$x+f_1(x)+f_2(x)+…++f_n(x)$的形式，一层层累加，可能导致数值和方差都很大，最后迫不得已强制加一层Layer Norm让输出稳定下来。这样，尽管PreLN改善了梯度状况，但它本身设计上就存在一些不稳定因素。</p><p>Realformer的$A<em>n = \frac{Q_nK_n^T}{\sqrt{d_k}} + A</em>{n-1}$存在叠加问题吗？如果只看A，那么确实有这样的问题，但A后面还要做个softmax归一化后才参与运行，也就是说，模型对矩阵A是自带归一化功能的，所以它不会有数值发散的风险。而且刚刚相反，随着层数的增加，A的叠加会使得A的元素绝对值可能越来越大，Attention趋近于onehot形式，造成后面的层梯度消失，但是别忘了，我们刚才说PostLN前面的层梯度小后面的大，而现在也进一步缩小了后面层的梯度，反而使得两者更同步，从而更好优化了；</p><p>另一方面Attention的概率值可能会有趋同的趋势，也就是说Attention的模式可能越来越稳定了。带来类似ALBERT参数共享的正则化效应，这对模型效果来说可能是有利的。同时，直觉上来想，用RealFormer结构去做FastBert之类的自适应层数的改进，效果会更好，因为RealFormer的Attention本身会有趋同趋势，更加符合FastBert设计的出发点。</p><p>此外，我们也可以将RealFormer理解为还是使用了常规的残差结构，但是残差结构只用在<strong>Q</strong>,<strong>K</strong>而没有用在<strong>V</strong>上。</p><p>为啥<strong>V</strong>“不值得”一个残差呢？从近来的一些相对位置编码的改进中，笔者发现似乎有一个共同的趋势，那就是去掉了<strong>V</strong>的偏置，比如像NEZHA的相对位置编码，是同时在Attention矩阵（即<strong>Q</strong>,<strong>K</strong>）和<strong>V</strong>上施加的，而较新的XLNET和T5的相对位置编码则只施加在Attention矩阵上，所以，似乎去掉<strong>V</strong>的不必要的偏置是一个比较好的选择，而RealFormer再次体现了这一点。</p><h3 id="RealFormer与Baseline-Transformers在本质上有什么不同？"><a href="#RealFormer与Baseline-Transformers在本质上有什么不同？" class="headerlink" title="RealFormer与Baseline Transformers在本质上有什么不同？"></a>RealFormer与Baseline Transformers在本质上有什么不同？</h3><p>dev set中随机抽样了8,192个示例，并可视化了这些示例中每个token(不包括padding)在表2中的三个预先训练的BERT-Base模型中的所有层和所有头部的注意概率分布。</p><p>特别地，对于每个(token、layer、head)三元组，我们计算关注权重(概率)的熵作为关注度的“稀疏度量”。直观地说，熵越低，注意力权重分布就越偏斜，因此注意力就越稀疏。</p><p><img src="https://i.loli.net/2021/08/15/av6cdqPg5HxWShY.png" alt=""></p><p>用RealFormer训练好的BERT-BASE对8192个突出例子的标记注意概率的熵分布</p><p>为了更好地辨认，每一层中的注意力都是按照熵的中位数排序的。根据熵的中位数对分布重新进行颜色编码：红色(中位数&gt;4.5)、黄色(1.5≤中位数≤4.5)、蓝色(中位数&lt;1.5)。也就是说，颜色越冷意味着注意力越稀疏。有一个明显的趋势是，较高的层往往具有较稀疏的注意力。</p><p>下面的是post-LN和pre-LN的熵分布</p><p><img src="https://i.loli.net/2021/08/15/MIfAtTSK6QV1a4h.png" alt=""></p><p><img src="https://i.loli.net/2021/08/15/yIdzx7tDBTnNuUM.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;RealFormer-Transformer-Likes-Residual-Attention&quot;&gt;&lt;a href=&quot;#RealFormer-Transformer-Likes-Residual-Attention&quot; class=&quot;headerlink&quot; title</summary>
      
    
    
    
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Centos 6无法使用yum解决办法</title>
    <link href="http://example.com/2021/08/12/Centos-6%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8yum%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <id>http://example.com/2021/08/12/Centos-6%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8yum%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</id>
    <published>2021-08-12T08:54:16.000Z</published>
    <updated>2021-08-12T08:58:01.013Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Centos-6无法使用yum解决办法"><a href="#Centos-6无法使用yum解决办法" class="headerlink" title="Centos 6无法使用yum解决办法"></a>Centos 6无法使用yum解决办法</h1><p>12月后Centos 6 系统无法使用yum出现错误(文章底部看)</p><p>相信已经有一部分朋友今天连接到CentOS 6的服务器后执行yum后发现报错，那么发生了什么？</p><p>CentOS 6已经随着2020年11月的结束进入了EOL（Reaches End of Life），不过有一些老设备依然需要支持，CentOS官方也给这些还不想把CentOS 6扔进垃圾堆的用户保留了最后一个版本的镜像，只是这个镜像不会再有更新了</p><p>官方便在12月2日正式将CentOS 6相关的软件源移出了官方源，随之而来逐级镜像也会陆续将其删除。</p><p>不过有一些老设备依然需要维持在当前系统，CentOS官方也给这些还不想把CentOS 6扔进垃圾堆的用户保留了各个版本软件源的镜像，只是这个软件源不会再有更新了。</p><h2 id="错误详情"><a href="#错误详情" class="headerlink" title="错误详情"></a>错误详情</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@c8-20 ~]<span class="comment"># yum makecache</span></span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line">YumRepo Error: All mirror URLs are not using ftp, http[s] or file.</span><br><span class="line"> Eg. Invalid release/repo/arch combination/</span><br><span class="line">removing mirrorlist with no valid mirrors: /var/cache/yum/x86_64/6/base/mirrorlist.txt</span><br><span class="line">Error: Cannot find a valid baseurl <span class="keyword">for</span> repo: base</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@li496-237 ~]<span class="comment"># yum -y install unzip zip</span></span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">Setting up Install Process</span><br><span class="line">Determining fastest mirrors</span><br><span class="line">YumRepo Error: All mirror URLs are not using ftp, http[s] or file.</span><br><span class="line">Eg. Invalid release/repo/arch combination/</span><br><span class="line">YumRepo Error: All mirror URLs are not using ftp, http[s] or file.</span><br><span class="line">Eg. Invalid release/repo/arch combination/</span><br><span class="line">YumRepo Error: All mirror URLs are not using ftp, http[s] or file.</span><br><span class="line">Eg. Invalid release/repo/arch combination/</span><br><span class="line">http://mirrors.linode.com/centos/6/os/x86_64/repodata/repomd.xml: [Errno 14] PYCURL ERROR 22 - <span class="string">&quot;The requested URL returned error: 404 Not Found&quot;</span></span><br><span class="line">Trying other mirror.</span><br><span class="line">To address this issue please refer to the below knowledge base article </span><br><span class="line"></span><br><span class="line">https://access.redhat.com/articles/1320623</span><br><span class="line"></span><br><span class="line">If above article doesn<span class="string">&#x27;t help to resolve this issue please open a ticket with Red Hat Support.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Error: Cannot retrieve repository metadata (repomd.xml) for repository: base. Please verify its path and try again</span></span><br></pre></td></tr></table></figure><p>一键修复</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;s|enabled&#x3D;1|enabled&#x3D;0|g&quot; &#x2F;etc&#x2F;yum&#x2F;pluginconf.d&#x2F;fastestmirror.conf</span><br><span class="line">mv &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo.backup</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo https:&#x2F;&#x2F;www.xmpan.com&#x2F;Centos-6-Vault-Aliyun.repo </span><br><span class="line">yum clean all</span><br><span class="line">yum makecache</span><br></pre></td></tr></table></figure><p>手动修复教程:</p><p>首先把fastestmirrors关了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#编辑</span><br><span class="line">vi &#x2F;etc&#x2F;yum&#x2F;pluginconf.d&#x2F;fastestmirror.conf</span><br><span class="line">#修改</span><br><span class="line">enable&#x3D;0</span><br><span class="line">#或者执行以下命令</span><br><span class="line">sed -i &quot;s|enabled&#x3D;1|enabled&#x3D;0|g&quot; &#x2F;etc&#x2F;yum&#x2F;pluginconf.d&#x2F;fastestmirror.conf</span><br></pre></td></tr></table></figure><p>先把之前的repo挪到备份，然后下面两个二选一</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo.bak</span><br></pre></td></tr></table></figure><p>替换为官方Vault源(海外服务器用)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo https:&#x2F;&#x2F;www.xmpan.com&#x2F;Centos-6-Vault-Official.repo</span><br></pre></td></tr></table></figure><p>或者替换为阿里云Vault镜像(国内服务器用)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo https:&#x2F;&#x2F;www.xmpan.com&#x2F;Centos-6-Vault-Aliyun.repo</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Centos-6无法使用yum解决办法&quot;&gt;&lt;a href=&quot;#Centos-6无法使用yum解决办法&quot; class=&quot;headerlink&quot; title=&quot;Centos 6无法使用yum解决办法&quot;&gt;&lt;/a&gt;Centos 6无法使用yum解决办法&lt;/h1&gt;&lt;p&gt;12</summary>
      
    
    
    
    
    <category term="配置记录" scheme="http://example.com/tags/%E9%85%8D%E7%BD%AE%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>从 SGD 到 AdamW 原理和代码解读</title>
    <link href="http://example.com/2021/08/12/%E4%BB%8E-SGD-%E5%88%B0-AdamW-%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/"/>
    <id>http://example.com/2021/08/12/%E4%BB%8E-SGD-%E5%88%B0-AdamW-%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/</id>
    <published>2021-08-12T05:00:18.000Z</published>
    <updated>2021-08-13T08:34:00.830Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从-SGD-到-AdamW-原理和代码解读"><a href="#从-SGD-到-AdamW-原理和代码解读" class="headerlink" title="从 SGD 到 AdamW 原理和代码解读"></a>从 SGD 到 AdamW 原理和代码解读</h1><p>深度学习优化算法经历了 SGD -&gt; SGDM -&gt; NAG -&gt;AdaGrad -&gt; AdaDelta -&gt; Adam -&gt; Nadam -&gt; AdamW 这样的发展历程。</p><p>接下来用一个框架来梳理所有的优化算法。</p><p>首先定义：待优化参数：$w$ , 目标函数：$f(x)$ , 初始学习率 $\alpha$</p><p>然后，开始进行迭代优化。在每个epoch $t$:</p><ol><li>计算目标函数关于当前参数的梯度: $g_t = ∇f(w_t)$</li><li>根据历史梯度计算一阶动量和二阶动量: $m_t = \phi(g_1,g_2,…,g_t); V_t =\psi(g_1,g_2,…,g_t)$</li><li>计算当前时刻的下降梯度: $\eta = \alpha \cdot m_t/\sqrt V_t$</li><li>根据下降梯度进行更新: $w_{t+1} = w_t -\eta_t$</li></ol><p>步骤3、4对于各个算法几乎都是一致的，主要的差别就体现在1和2上。</p><p>也就是计算一阶动量$m_t$ 和 二阶动量$V_t$时采用不同的套路。</p><p>此外在所有优化器代码里有一些函数作用是相通的：</p><blockquote><p>共性的方法有：</p></blockquote><ul><li>add_param_group(param_group) : 把参数放进优化器中，这在Fine-tune预训练时可以使冻结层可训练，并随着训练的进行添加到优化器中。</li><li>load_state_dict(state_dict): 把优化器的状态加载进去。</li><li>state_dict():返回优化器状态，以dict形式</li><li>step(closure=None):优化一步参数</li><li>zero_grad(set_to_none=False):把所有的梯度值设为0</li></ul><blockquote><p>使用方法：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> dataset:</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">closure</span>():</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br><span class="line">    loss = loss_fn(output, target)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line">  optimizer.step(closure)</span><br></pre></td></tr></table></figure><h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p>SGD没有动量的概念，也就是说：</p><script type="math/tex; mode=display">m_t = g_t ; V_t=I^2</script><p>代入步骤3，可以看到下降梯度就是最简单的</p><script type="math/tex; mode=display">\eta_t = \alpha \cdot g_t</script><p>SGD最大的缺点就是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点。</p><h2 id="SGD-with-Momentum"><a href="#SGD-with-Momentum" class="headerlink" title="SGD with Momentum"></a>SGD with Momentum</h2><p>为了抑制震荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一点。</p><p>在SGD的基础上引入了一阶动量：</p><script type="math/tex; mode=display">m_t = \beta_1\cdot m_{t-1} +(1-\beta_1)\cdot g_t</script><p>一阶动量就是各个时刻梯度方向的指数移动平均，约等于最近$1/(1-\beta_1)$个时刻的梯度向量和的平均值。</p><p>也就是说，t 时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。</p><p>$\beta_1$的经验值为0.9，这意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。想象高速公路上汽车转弯，在高速向前的同时略微偏向，急转弯可是要出事的。</p><h2 id="SGD-with-Nesterov-Acceleration"><a href="#SGD-with-Nesterov-Acceleration" class="headerlink" title="SGD with Nesterov Acceleration"></a>SGD with Nesterov Acceleration</h2><p>SGD还有一个问题是困在局部最优的沟壑里震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能呆在这里了。可是你如果爬上高地，就会方向外卖的世界还很广阔。</p><p>因此外卖不能停留在当前位置去观察未来的方向，而是要向前一步，多看一步，看远一些。</p><p>NAG全称Nesterov Accelerated Gradient，是在SGD、SGDM的基础上的进一步改进，改进点在于步骤1。</p><p>我们知道在时刻 t 的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算。那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。</p><p>因此NAG在步骤1，不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向：</p><script type="math/tex; mode=display">g_t =∇ f(w_t-\beta_1\cdot m_{t-1}/\sqrt{V_{t-1}})</script><p>然后用下一个点的梯度方向，与历史累积动量结合，计算步骤2中当前时刻的累加动量。</p><blockquote><p>定义优化器：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.optim.SGD(params, lr&#x3D;&lt;required parameter&gt;, momentum&#x3D;0, dampening&#x3D;0, weight_decay&#x3D;0, nesterov&#x3D;False)</span><br></pre></td></tr></table></figure><blockquote><p>参数：</p></blockquote><ul><li><strong>params</strong> (iterable) – 优化器作用的模型参数。</li><li><strong>lr</strong> (float) – learning rate，相当于是统一框架中的 $\alpha$</li><li><strong>momentum</strong> (float, optional) – 动量参数。(默认值：0)</li><li><strong>weight_decay</strong> (float, optional) – 权重衰减系数 weight decay (L2 penalty) (默认值：0)</li><li><strong>dampening</strong> (float, optional) – dampening for momentum (默认值：0)</li><li><strong>nesterov</strong> (bool, optional) – 允许 Nesterov momentum (默认值：False)</li></ul><blockquote><p>源码解读：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer, required</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[docs]<span class="class"><span class="keyword">class</span> <span class="title">SGD</span>(<span class="params">Optimizer</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Implements stochastic gradient descent (optionally with momentum).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Nesterov momentum is based on the formula from</span></span><br><span class="line"><span class="string">    `On the importance of initialization and momentum in deep learning`__.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float): learning rate</span></span><br><span class="line"><span class="string">        momentum (float, optional): momentum factor (default: 0)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string">        dampening (float, optional): dampening for momentum (default: 0)</span></span><br><span class="line"><span class="string">        nesterov (bool, optional): enables Nesterov momentum (default: False)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer.zero_grad()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; loss_fn(model(input), target).backward()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer.step()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. note::</span></span><br><span class="line"><span class="string">        The implementation of SGD with Momentum/Nesterov subtly differs from</span></span><br><span class="line"><span class="string">        Sutskever et. al. and implementations in some other frameworks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Considering the specific case of Momentum, the update can be written as</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. math::</span></span><br><span class="line"><span class="string">            \begin&#123;aligned&#125;</span></span><br><span class="line"><span class="string">                v_&#123;t+1&#125; &amp; = \mu * v_&#123;t&#125; + g_&#123;t+1&#125;, \\</span></span><br><span class="line"><span class="string">                p_&#123;t+1&#125; &amp; = p_&#123;t&#125; - \text&#123;lr&#125; * v_&#123;t+1&#125;,</span></span><br><span class="line"><span class="string">            \end&#123;aligned&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        where :math:`p`, :math:`g`, :math:`v` and :math:`\mu` denote the </span></span><br><span class="line"><span class="string">        parameters, gradient, velocity, and momentum respectively.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        This is in contrast to Sutskever et. al. and</span></span><br><span class="line"><span class="string">        other frameworks which employ an update of the form</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. math::</span></span><br><span class="line"><span class="string">            \begin&#123;aligned&#125;</span></span><br><span class="line"><span class="string">                v_&#123;t+1&#125; &amp; = \mu * v_&#123;t&#125; + \text&#123;lr&#125; * g_&#123;t+1&#125;, \\</span></span><br><span class="line"><span class="string">                p_&#123;t+1&#125; &amp; = p_&#123;t&#125; - v_&#123;t+1&#125;.</span></span><br><span class="line"><span class="string">            \end&#123;aligned&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The Nesterov version is analogously modified.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, params, lr=required, momentum=<span class="number">0</span>, dampening=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">0</span>, nesterov=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> lr <span class="keyword">is</span> <span class="keyword">not</span> required <span class="keyword">and</span> lr &lt; <span class="number">0.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid learning rate: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr))</span><br><span class="line">        <span class="keyword">if</span> momentum &lt; <span class="number">0.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid momentum value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(momentum))</span><br><span class="line">        <span class="keyword">if</span> weight_decay &lt; <span class="number">0.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid weight_decay value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight_decay))</span><br><span class="line"></span><br><span class="line">        defaults = <span class="built_in">dict</span>(lr=lr, momentum=momentum, dampening=dampening,</span><br><span class="line">                        weight_decay=weight_decay, nesterov=nesterov)</span><br><span class="line">        <span class="keyword">if</span> nesterov <span class="keyword">and</span> (momentum &lt;= <span class="number">0</span> <span class="keyword">or</span> dampening != <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Nesterov momentum requires a momentum and zero dampening&quot;</span>)</span><br><span class="line">        <span class="built_in">super</span>(SGD, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SGD, self).__setstate__(state)</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            group.setdefault(<span class="string">&#x27;nesterov&#x27;</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">[docs]    @torch.no_grad()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, closure=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">                loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            weight_decay = group[<span class="string">&#x27;weight_decay&#x27;</span>]</span><br><span class="line">            momentum = group[<span class="string">&#x27;momentum&#x27;</span>]</span><br><span class="line">            dampening = group[<span class="string">&#x27;dampening&#x27;</span>]</span><br><span class="line">            nesterov = group[<span class="string">&#x27;nesterov&#x27;</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                d_p = p.grad  <span class="comment"># 得到每个参数的梯度，也就是g_t</span></span><br><span class="line">                <span class="keyword">if</span> weight_decay != <span class="number">0</span>: <span class="comment"># 如果使用weight_decay的话，相当于目标函数上加上 L2正则</span></span><br><span class="line">                    d_p = d_p.add(p, alpha=weight_decay)</span><br><span class="line">                <span class="keyword">if</span> momentum != <span class="number">0</span>:</span><br><span class="line">                    param_state = self.state[p]</span><br><span class="line">                    <span class="keyword">if</span> <span class="string">&#x27;momentum_buffer&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> param_state:</span><br><span class="line">                        buf = param_state[<span class="string">&#x27;momentum_buffer&#x27;</span>] = torch.clone(d_p).detach()</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        buf = param_state[<span class="string">&#x27;momentum_buffer&#x27;</span>]</span><br><span class="line">                        <span class="comment"># 计算动量，momentum参数beta_1一般取0.9，相当于之前的动量buf乘以0.9再加上此次梯度</span></span><br><span class="line">                        <span class="comment"># d_p乘以(1-beta_1)=0.1</span></span><br><span class="line">                        buf.mul_(momentum).add_(d_p, alpha=<span class="number">1</span> - dampening)</span><br><span class="line">                    <span class="keyword">if</span> nesterov:</span><br><span class="line">                      <span class="comment"># 如果通过nesterov方式更新参数，那么eta_t就相当于g_t+m_t*beta_1</span></span><br><span class="line">                        d_p = d_p.add(buf, alpha=momentum)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                      <span class="comment"># 如果不通过nesterov方式更新参数，那么\eta_t就是相当于是上一步计算出的动量m_t</span></span><br><span class="line">                        d_p = buf</span><br><span class="line"></span><br><span class="line">                p.add_(d_p, alpha=-group[<span class="string">&#x27;lr&#x27;</span>]) <span class="comment">#最后用学习率更新梯度</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>此前我们都没有用到二阶动量。二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。</p><p>SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到(想想大规模的embedding)。</p><p>对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学习一些，即学习率大一些。</p><p>怎么样去度量历史更新频率呢？那就是二阶动量——该维度上，迄今为止所有梯度值的平方和：</p><script type="math/tex; mode=display">V_t = \sum_{\tau=1}^t g_{\tau}^2</script><p>我们在回顾一些步骤3中的下降梯度：</p><script type="math/tex; mode=display">\eta_t = \alpha\cdot m_t/\sqrt{V_t}</script><p>可以看出，此时实质上的学习率由$\alpha$ 变成了，$\alpha/\sqrt{V_t}$。</p><p>一般为了避免分母为0，会在分母上加一个小的平滑项。因此$\sqrt{V_t}$是恒大于0的，而且参数更新越频繁，二阶动量越大，学习率就越小。</p><p>这一方法在稀疏数据场景下表现非常好，但也存在一些问题：因为$\sqrt{V_t}$ 是单调递增的，会使学习率单调递减至0，可能会使训练过程提前结束，即便后续还有数据也无法学到必要的知识。</p><blockquote><p>定义优化器：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.optim.Adagrad(params,lr&#x3D;0.01,lr_decay&#x3D;0,weight_decay&#x3D;0,initial_accumulator_value&#x3D;0,eps&#x3D;1e-10)</span><br></pre></td></tr></table></figure><blockquote><p>参数：</p></blockquote><ul><li><strong>params</strong> (iterable) – 优化器作用的模型参数。</li><li><strong>lr</strong> (float) – learning rate – 相当于是统一框架中的 。</li><li><strong>lr_decay</strong>(float,optional) – 学习率衰减 (默认值：0)</li><li><strong>weight_decay</strong> (float, optional) – 权重衰减系数 weight decay (L2 penalty) (默认值：0)</li><li><strong>eps</strong>(float,optional)：防止分母为0的一个小数 (默认值：1e-10)</li></ul><blockquote><p>源码解读：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">[docs]<span class="class"><span class="keyword">class</span> <span class="title">Adagrad</span>(<span class="params">Optimizer</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Implements Adagrad algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has been proposed in `Adaptive Subgradient Methods for Online Learning</span></span><br><span class="line"><span class="string">    and Stochastic Optimization`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-2)</span></span><br><span class="line"><span class="string">        lr_decay (float, optional): learning rate decay (default: 0)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-10)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Adaptive Subgradient Methods for Online Learning and Stochastic</span></span><br><span class="line"><span class="string">        Optimization: http://jmlr.org/papers/v12/duchi11a.html</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, params, lr=<span class="number">1e-2</span>, lr_decay=<span class="number">0</span>, weight_decay=<span class="number">0</span>, initial_accumulator_value=<span class="number">0</span>, eps=<span class="number">1e-10</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= lr:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid learning rate: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= lr_decay:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid lr_decay value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr_decay))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= weight_decay:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid weight_decay value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight_decay))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= initial_accumulator_value:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid initial_accumulator_value value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(initial_accumulator_value))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= eps:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid epsilon value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(eps))</span><br><span class="line"></span><br><span class="line">        defaults = <span class="built_in">dict</span>(lr=lr, lr_decay=lr_decay, eps=eps, weight_decay=weight_decay,</span><br><span class="line">                        initial_accumulator_value=initial_accumulator_value)</span><br><span class="line">        <span class="built_in">super</span>(Adagrad, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">                state = self.state[p]</span><br><span class="line">                state[<span class="string">&#x27;step&#x27;</span>] = <span class="number">0</span></span><br><span class="line">                state[<span class="string">&#x27;sum&#x27;</span>] = torch.full_like(p, initial_accumulator_value, memory_format=torch.preserve_format)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">share_memory</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">                state = self.state[p]</span><br><span class="line">                state[<span class="string">&#x27;sum&#x27;</span>].share_memory_()</span><br><span class="line"></span><br><span class="line">[docs]    @torch.no_grad()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, closure=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">                loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            params_with_grad = []</span><br><span class="line">            grads = []</span><br><span class="line">            state_sums = []</span><br><span class="line">            state_steps = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    params_with_grad.append(p)</span><br><span class="line">                    grads.append(p.grad)</span><br><span class="line">                    state = self.state[p]</span><br><span class="line">                    state_sums.append(state[<span class="string">&#x27;sum&#x27;</span>])</span><br><span class="line">                    <span class="comment"># update the steps for each param group update</span></span><br><span class="line">                    state[<span class="string">&#x27;step&#x27;</span>] += <span class="number">1</span></span><br><span class="line">                    <span class="comment"># record the step after step update</span></span><br><span class="line">                    state_steps.append(state[<span class="string">&#x27;step&#x27;</span>])</span><br><span class="line"></span><br><span class="line">            F.adagrad(params_with_grad,</span><br><span class="line">                      grads,</span><br><span class="line">                      state_sums,</span><br><span class="line">                      state_steps,</span><br><span class="line">                      group[<span class="string">&#x27;lr&#x27;</span>],</span><br><span class="line">                      group[<span class="string">&#x27;weight_decay&#x27;</span>],</span><br><span class="line">                      group[<span class="string">&#x27;lr_decay&#x27;</span>],</span><br><span class="line">                      group[<span class="string">&#x27;eps&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h2 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h2><p>由于AdaGrad 单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也是就是AdaDelta名称中Delta的来历。</p><p>修改思路很简单，前面讲到，指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量：</p><script type="math/tex; mode=display">V_t = \beta_2 * V_{t-1} + (1-\beta_2)g_t^2</script><p>接下来还是步骤3：</p><script type="math/tex; mode=display">\eta_t = \alpha \cdot g_t/\sqrt{V_t}</script><p>这就避免了二阶动量持续累积、导致训练过程提前结束的问题了。</p><h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><blockquote><p>定义优化器：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.optim.RMSprop(params, lr&#x3D;0.01, alpha&#x3D;0.99, eps&#x3D;1e-08, weight_decay&#x3D;0, momentum&#x3D;0, centered&#x3D;False)</span><br></pre></td></tr></table></figure><blockquote><p>参数：</p></blockquote><ul><li><strong>params</strong> (iterable) – 优化器作用的模型参数。</li><li><strong>lr</strong> (float) – learning rate – 相当于是统一框架中的 $\alpha$。</li><li><strong>momentum</strong> (float, optional) – 动量参数。(默认值：0)。</li><li><strong>alpha</strong>(<em>float,optional</em>) – 平滑常数 (默认值：0.99)。</li><li><strong>centered</strong>(bool,optional) – if<code>True</code>, compute the centered RMSProp, the gradient is normalized by an estimation of its variance，就是这一项是 True 的话就把方差使用梯度作归一化。</li><li><strong>weight_decay</strong> (float, optional) – 权重衰减系数 weight decay (L2 penalty) (默认值：0)</li><li><strong>eps</strong>(float,optional)：防止分母为0的一个小数 (默认值：1e-10)</li></ul><blockquote><p><strong>源码解读：</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[docs]<span class="class"><span class="keyword">class</span> <span class="title">RMSprop</span>(<span class="params">Optimizer</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Implements RMSprop algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Proposed by G. Hinton in his</span></span><br><span class="line"><span class="string">    `course &lt;https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The centered version first appears in `Generating Sequences</span></span><br><span class="line"><span class="string">    With Recurrent Neural Networks &lt;https://arxiv.org/pdf/1308.0850v5.pdf&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The implementation here takes the square root of the gradient average before</span></span><br><span class="line"><span class="string">    adding epsilon (note that TensorFlow interchanges these two operations). The effective</span></span><br><span class="line"><span class="string">    learning rate is thus :math:`\alpha/(\sqrt&#123;v&#125; + \epsilon)` where :math:`\alpha`</span></span><br><span class="line"><span class="string">    is the scheduled learning rate and :math:`v` is the weighted moving average</span></span><br><span class="line"><span class="string">    of the squared gradient.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-2)</span></span><br><span class="line"><span class="string">        momentum (float, optional): momentum factor (default: 0)</span></span><br><span class="line"><span class="string">        alpha (float, optional): smoothing constant (default: 0.99)</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        centered (bool, optional) : if ``True``, compute the centered RMSProp,</span></span><br><span class="line"><span class="string">            the gradient is normalized by an estimation of its variance</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, params, lr=<span class="number">1e-2</span>, alpha=<span class="number">0.99</span>, eps=<span class="number">1e-8</span>, weight_decay=<span class="number">0</span>, momentum=<span class="number">0</span>, centered=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= lr:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid learning rate: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= eps:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid epsilon value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(eps))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= momentum:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid momentum value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(momentum))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= weight_decay:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid weight_decay value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight_decay))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= alpha:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid alpha value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(alpha))</span><br><span class="line"></span><br><span class="line">        defaults = <span class="built_in">dict</span>(lr=lr, momentum=momentum, alpha=alpha, eps=eps, centered=centered, weight_decay=weight_decay)</span><br><span class="line">        <span class="built_in">super</span>(RMSprop, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(RMSprop, self).__setstate__(state)</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            group.setdefault(<span class="string">&#x27;momentum&#x27;</span>, <span class="number">0</span>)</span><br><span class="line">            group.setdefault(<span class="string">&#x27;centered&#x27;</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">[docs]    @torch.no_grad()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, closure=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">                loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                grad = p.grad</span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    <span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;RMSprop does not support sparse gradients&#x27;</span>)</span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># State initialization</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">&#x27;step&#x27;</span>] = <span class="number">0</span></span><br><span class="line">                    state[<span class="string">&#x27;square_avg&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line">                    <span class="keyword">if</span> group[<span class="string">&#x27;momentum&#x27;</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                        state[<span class="string">&#x27;momentum_buffer&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line">                    <span class="keyword">if</span> group[<span class="string">&#x27;centered&#x27;</span>]:</span><br><span class="line">                        state[<span class="string">&#x27;grad_avg&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line"></span><br><span class="line">                square_avg = state[<span class="string">&#x27;square_avg&#x27;</span>]</span><br><span class="line">                alpha = group[<span class="string">&#x27;alpha&#x27;</span>]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">&#x27;step&#x27;</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">&#x27;weight_decay&#x27;</span>] != <span class="number">0</span>:</span><br><span class="line">                    grad = grad.add(p, alpha=group[<span class="string">&#x27;weight_decay&#x27;</span>])</span><br><span class="line"></span><br><span class="line">                square_avg.mul_(alpha).addcmul_(grad, grad, value=<span class="number">1</span> - alpha)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">&#x27;centered&#x27;</span>]:</span><br><span class="line">                    grad_avg = state[<span class="string">&#x27;grad_avg&#x27;</span>]</span><br><span class="line">                    grad_avg.mul_(alpha).add_(grad, alpha=<span class="number">1</span> - alpha)</span><br><span class="line">                    avg = square_avg.addcmul(grad_avg, grad_avg, value=-<span class="number">1</span>).sqrt_().add_(group[<span class="string">&#x27;eps&#x27;</span>]) <span class="comment"># 计算当前步的动量</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    avg = square_avg.sqrt().add_(group[<span class="string">&#x27;eps&#x27;</span>])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">&#x27;momentum&#x27;</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                    buf = state[<span class="string">&#x27;momentum_buffer&#x27;</span>]</span><br><span class="line">                    buf.mul_(group[<span class="string">&#x27;momentum&#x27;</span>]).addcdiv_(grad, avg)</span><br><span class="line">                    p.add_(buf, alpha=-group[<span class="string">&#x27;lr&#x27;</span>])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    p.addcdiv_(grad, avg, value=-group[<span class="string">&#x27;lr&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间</p><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>谈到这里，Adam和Nadam的出现就很自然而然了——他们是前述方法的集大成者。</p><p>SGDM在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加的二阶动量。</p><p>把一阶动量和二阶动量都用起来就是Adam了——Adaptive + Momentum</p><p>SGD的一阶动量：</p><script type="math/tex; mode=display">m_t = \beta_1 \cdot m_{t-1} +(1-\beta_1)\cdot g_t</script><p>加上AdaDelta的二阶动量：</p><script type="math/tex; mode=display">\hat m_t =\frac{m_t}{1-\beta_1^t}</script><script type="math/tex; mode=display">\hat V_t = \frac{V_t}{1-\beta_2^t}</script><p>优化算法里最常见的两个超参数$\beta_1,\beta_2$就都在这里了，前者是控制一阶动量，后者控制二阶动量。</p><h2 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h2><p>都说Adam是集大成者，但它遗漏了Nesterov，安装NAG的步骤1：</p><script type="math/tex; mode=display">g_t = ∇f(w_t-\alpha\cdot m_{t-1}/\sqrt{V_t})</script><p>Nesterov+Adam = Nadam</p><blockquote><p>定义优化器：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.optim.Adam(params, lr&#x3D;0.001, betas&#x3D;(0.9, 0.999), eps&#x3D;1e-08, weight_decay&#x3D;0, amsgrad&#x3D;False)</span><br></pre></td></tr></table></figure><blockquote><p>参数：</p></blockquote><ul><li><strong>params</strong> (iterable) – 优化器作用的模型参数。</li><li><strong>lr</strong> (float) – learning rate – 相当于是统一框架中的 。</li><li><strong>betas</strong>(Tuple[float,float],optional) – coefficients used for computing running averages of gradient and its square ((默认值：(0.9, 0.999))</li><li><strong>weight_decay</strong> (float, optional) – 权重衰减系数 weight decay (L2 penalty) (默认值：0)</li><li><strong>eps</strong>(float,optional)：防止分母为0的一个小数 (默认值：1e-10)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[docs]<span class="class"><span class="keyword">class</span> <span class="title">Adam</span>(<span class="params">Optimizer</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Implements Adam algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has been proposed in `Adam: A Method for Stochastic Optimization`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-3)</span></span><br><span class="line"><span class="string">        betas (Tuple[float, float], optional): coefficients used for computing</span></span><br><span class="line"><span class="string">            running averages of gradient and its square (default: (0.9, 0.999))</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string">        amsgrad (boolean, optional): whether to use the AMSGrad variant of this</span></span><br><span class="line"><span class="string">            algorithm from the paper `On the Convergence of Adam and Beyond`_</span></span><br><span class="line"><span class="string">            (default: False)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Adam\: A Method for Stochastic Optimization:</span></span><br><span class="line"><span class="string">        https://arxiv.org/abs/1412.6980</span></span><br><span class="line"><span class="string">    .. _On the Convergence of Adam and Beyond:</span></span><br><span class="line"><span class="string">        https://openreview.net/forum?id=ryQu7f-RZ</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, params, lr=<span class="number">1e-3</span>, betas=(<span class="params"><span class="number">0.9</span>, <span class="number">0.999</span></span>), eps=<span class="number">1e-8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">0</span>, amsgrad=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= lr:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid learning rate: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= eps:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid epsilon value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(eps))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">0</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid beta parameter at index 0: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(betas[<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">1</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid beta parameter at index 1: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(betas[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= weight_decay:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid weight_decay value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight_decay))</span><br><span class="line">        defaults = <span class="built_in">dict</span>(lr=lr, betas=betas, eps=eps,</span><br><span class="line">                        weight_decay=weight_decay, amsgrad=amsgrad)</span><br><span class="line">        <span class="built_in">super</span>(Adam, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Adam, self).__setstate__(state)</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            group.setdefault(<span class="string">&#x27;amsgrad&#x27;</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">[docs]    @torch.no_grad()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, closure=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">                loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                grad = p.grad</span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    <span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;Adam does not support sparse gradients, please consider SparseAdam instead&#x27;</span>)</span><br><span class="line">                amsgrad = group[<span class="string">&#x27;amsgrad&#x27;</span>]</span><br><span class="line"></span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># State initialization</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">&#x27;step&#x27;</span>] = <span class="number">0</span></span><br><span class="line">                    <span class="comment"># Exponential moving average of gradient values</span></span><br><span class="line">                    state[<span class="string">&#x27;exp_avg&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line">                    <span class="comment"># Exponential moving average of squared gradient values</span></span><br><span class="line">                    state[<span class="string">&#x27;exp_avg_sq&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line">                    <span class="keyword">if</span> amsgrad:</span><br><span class="line">                        <span class="comment"># Maintains max of all exp. moving avg. of sq. grad. values</span></span><br><span class="line">                        state[<span class="string">&#x27;max_exp_avg_sq&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line"></span><br><span class="line">                exp_avg, exp_avg_sq = state[<span class="string">&#x27;exp_avg&#x27;</span>], state[<span class="string">&#x27;exp_avg_sq&#x27;</span>]</span><br><span class="line">                <span class="keyword">if</span> amsgrad:</span><br><span class="line">                    max_exp_avg_sq = state[<span class="string">&#x27;max_exp_avg_sq&#x27;</span>]</span><br><span class="line">                beta1, beta2 = group[<span class="string">&#x27;betas&#x27;</span>]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">&#x27;step&#x27;</span>] += <span class="number">1</span></span><br><span class="line">                bias_correction1 = <span class="number">1</span> - beta1 ** state[<span class="string">&#x27;step&#x27;</span>]</span><br><span class="line">                bias_correction2 = <span class="number">1</span> - beta2 ** state[<span class="string">&#x27;step&#x27;</span>]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">&#x27;weight_decay&#x27;</span>] != <span class="number">0</span>:</span><br><span class="line">                    grad = grad.add(p, alpha=group[<span class="string">&#x27;weight_decay&#x27;</span>])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Decay the first and second moment running average coefficient</span></span><br><span class="line">                exp_avg.mul_(beta1).add_(grad, alpha=<span class="number">1</span> - beta1)</span><br><span class="line">                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=<span class="number">1</span> - beta2)</span><br><span class="line">                <span class="keyword">if</span> amsgrad:</span><br><span class="line">                    <span class="comment"># Maintains the maximum of all 2nd moment running avg. till now</span></span><br><span class="line">                    torch.<span class="built_in">max</span>(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)</span><br><span class="line">                    <span class="comment"># Use the max. for normalizing running avg. of gradient</span></span><br><span class="line">                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group[<span class="string">&#x27;eps&#x27;</span>])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group[<span class="string">&#x27;eps&#x27;</span>])</span><br><span class="line"></span><br><span class="line">                step_size = group[<span class="string">&#x27;lr&#x27;</span>] / bias_correction1</span><br><span class="line"></span><br><span class="line">                p.addcdiv_(exp_avg, denom, value=-step_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h2 id="AdamW"><a href="#AdamW" class="headerlink" title="AdamW"></a>AdamW</h2><p>Adam的另一个改进版AdamW</p><p>AdamW就是Adam优化器加上L2正则，来限制参数值不可太大。</p><p>以往的L2正则是直接加在损失函数上，比如这样：加入正则， 损失函数就会变成：</p><script type="math/tex; mode=display">L_{l_2}(\theta) = L(\theta) + 1/2\gamma||\theta||^2</script><p>所以在计算梯度$g_t$时要加上粉色的这一项。</p><p>但AdamW稍有不同，如下图所示，将正则加在了绿色位置。</p><p><img src="https://i.loli.net/2021/08/13/C37Bz8jYiwRI4fF.png" alt=""></p><p>至于为何这么做？直接摘录BERT里面的原话看看：</p><blockquote><p><strong>Just</strong> adding the square of the weights to the loss function is <em>not</em> the correct way of using L2 regularization/weight decay with Adam, since that will interact with the m and v parameters in strange ways. Instead we want to decay the weights in a manner that doesn’t interact with the m/v parameters. This is equivalent to adding the square of the weights to the loss with plain (non-momentum) SGD. Add weight decay at the end (fixed version).</p></blockquote><p>意思s 如果直接将L2正则加到loss上去，由于Adam优化器的后续操作，该正则项将会与$m_t$和$v_t$产生奇怪的作用。因而，AdamW选择将L2正则项加在了Adam的$m_t$和$v_t$等参数被计算完之后，在于学习率$\eta$相乘之前，所以这也表明了weight_decay和L2正则虽目的一致、公式一致，但用法还是不同，二者有明显的区别。</p><p>以 PyTorch1.7.0 中的AdamW代码为例：</p><blockquote><p>定义优化器：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.optim.AdamW(params, lr&#x3D;0.001, betas&#x3D;(0.9, 0.999), eps&#x3D;1e-08, weight_decay&#x3D;0.01, amsgrad&#x3D;False)</span><br></pre></td></tr></table></figure><ul><li><strong>params</strong> (iterable) – 优化器作用的模型参数。</li><li><strong>lr</strong> (float) – learning rate – 相当于是统一框架中的 。</li><li><strong>betas</strong>(Tuple[float,float],optional) – coefficients used for computing running averages of gradient and its square ((默认值：(0.9, 0.999))</li><li><strong>weight_decay</strong> (float, optional) – 权重衰减系数 weight decay (L2 penalty) (默认值：0)</li><li><strong>eps</strong>(float,optional)：防止分母为0的一个小数 (默认值：1e-10)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[docs]<span class="class"><span class="keyword">class</span> <span class="title">AdamW</span>(<span class="params">Optimizer</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Implements AdamW algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.</span></span><br><span class="line"><span class="string">    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-3)</span></span><br><span class="line"><span class="string">        betas (Tuple[float, float], optional): coefficients used for computing</span></span><br><span class="line"><span class="string">            running averages of gradient and its square (default: (0.9, 0.999))</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay coefficient (default: 1e-2)</span></span><br><span class="line"><span class="string">        amsgrad (boolean, optional): whether to use the AMSGrad variant of this</span></span><br><span class="line"><span class="string">            algorithm from the paper `On the Convergence of Adam and Beyond`_</span></span><br><span class="line"><span class="string">            (default: False)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Adam\: A Method for Stochastic Optimization:</span></span><br><span class="line"><span class="string">        https://arxiv.org/abs/1412.6980</span></span><br><span class="line"><span class="string">    .. _Decoupled Weight Decay Regularization:</span></span><br><span class="line"><span class="string">        https://arxiv.org/abs/1711.05101</span></span><br><span class="line"><span class="string">    .. _On the Convergence of Adam and Beyond:</span></span><br><span class="line"><span class="string">        https://openreview.net/forum?id=ryQu7f-RZ</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, params, lr=<span class="number">1e-3</span>, betas=(<span class="params"><span class="number">0.9</span>, <span class="number">0.999</span></span>), eps=<span class="number">1e-8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">1e-2</span>, amsgrad=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= lr:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid learning rate: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= eps:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid epsilon value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(eps))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">0</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid beta parameter at index 0: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(betas[<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">1</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid beta parameter at index 1: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(betas[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= weight_decay:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid weight_decay value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight_decay))</span><br><span class="line">        defaults = <span class="built_in">dict</span>(lr=lr, betas=betas, eps=eps,</span><br><span class="line">                        weight_decay=weight_decay, amsgrad=amsgrad)</span><br><span class="line">        <span class="built_in">super</span>(AdamW, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AdamW, self).__setstate__(state)</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            group.setdefault(<span class="string">&#x27;amsgrad&#x27;</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">[docs]    @torch.no_grad()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, closure=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">                loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Perform stepweight decay</span></span><br><span class="line">                p.mul_(<span class="number">1</span> - group[<span class="string">&#x27;lr&#x27;</span>] * group[<span class="string">&#x27;weight_decay&#x27;</span>])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Perform optimization step</span></span><br><span class="line">                grad = p.grad</span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    <span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;Adam does not support sparse gradients, please consider SparseAdam instead&#x27;</span>)</span><br><span class="line">                amsgrad = group[<span class="string">&#x27;amsgrad&#x27;</span>]</span><br><span class="line"></span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># State initialization</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">&#x27;step&#x27;</span>] = <span class="number">0</span></span><br><span class="line">                    <span class="comment"># Exponential moving average of gradient values</span></span><br><span class="line">                    state[<span class="string">&#x27;exp_avg&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line">                    <span class="comment"># Exponential moving average of squared gradient values</span></span><br><span class="line">                    state[<span class="string">&#x27;exp_avg_sq&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line">                    <span class="keyword">if</span> amsgrad:</span><br><span class="line">                        <span class="comment"># Maintains max of all exp. moving avg. of sq. grad. values</span></span><br><span class="line">                        state[<span class="string">&#x27;max_exp_avg_sq&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line"></span><br><span class="line">                exp_avg, exp_avg_sq = state[<span class="string">&#x27;exp_avg&#x27;</span>], state[<span class="string">&#x27;exp_avg_sq&#x27;</span>]</span><br><span class="line">                <span class="keyword">if</span> amsgrad:</span><br><span class="line">                    max_exp_avg_sq = state[<span class="string">&#x27;max_exp_avg_sq&#x27;</span>]</span><br><span class="line">                beta1, beta2 = group[<span class="string">&#x27;betas&#x27;</span>]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">&#x27;step&#x27;</span>] += <span class="number">1</span></span><br><span class="line">                bias_correction1 = <span class="number">1</span> - beta1 ** state[<span class="string">&#x27;step&#x27;</span>]</span><br><span class="line">                bias_correction2 = <span class="number">1</span> - beta2 ** state[<span class="string">&#x27;step&#x27;</span>]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Decay the first and second moment running average coefficient</span></span><br><span class="line">                exp_avg.mul_(beta1).add_(grad, alpha=<span class="number">1</span> - beta1)</span><br><span class="line">                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=<span class="number">1</span> - beta2)</span><br><span class="line">                <span class="keyword">if</span> amsgrad:</span><br><span class="line">                    <span class="comment"># Maintains the maximum of all 2nd moment running avg. till now</span></span><br><span class="line">                    torch.<span class="built_in">max</span>(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)</span><br><span class="line">                    <span class="comment"># Use the max. for normalizing running avg. of gradient</span></span><br><span class="line">                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group[<span class="string">&#x27;eps&#x27;</span>])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group[<span class="string">&#x27;eps&#x27;</span>])</span><br><span class="line"></span><br><span class="line">                step_size = group[<span class="string">&#x27;lr&#x27;</span>] / bias_correction1</span><br><span class="line"></span><br><span class="line">                p.addcdiv_(exp_avg, denom, value=-step_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>与Adam不一样的地方是：</p><p>Adam如果使用weight_decay的话，那么相当于目标函数上加了$1/2\gamma||\theta||^2$，所以相当于是梯度加上$\gamma\theta$故Adam使用了</p><p>grad = grad.add(p, alpha=group[‘weight_decay’])</p><p>而 AdamW 是 p.mul_(1 - group[‘lr’] * group[‘weight_decay’]) 直接让参数：</p><p>$\theta<em>t =\theta</em>{t-1}-\alpha\cdot\lambda\cdot\theta_{t-1}-\alpha\cdot\eta_t$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;从-SGD-到-AdamW-原理和代码解读&quot;&gt;&lt;a href=&quot;#从-SGD-到-AdamW-原理和代码解读&quot; class=&quot;headerlink&quot; title=&quot;从 SGD 到 AdamW 原理和代码解读&quot;&gt;&lt;/a&gt;从 SGD 到 AdamW 原理和代码解读&lt;/</summary>
      
    
    
    
    
    <category term="ML&amp;DL" scheme="http://example.com/tags/ML-DL/"/>
    
  </entry>
  
  <entry>
    <title>TreeMap红黑树</title>
    <link href="http://example.com/2021/08/10/TreeMap%E7%BA%A2%E9%BB%91%E6%A0%91/"/>
    <id>http://example.com/2021/08/10/TreeMap%E7%BA%A2%E9%BB%91%E6%A0%91/</id>
    <published>2021-08-10T12:10:27.000Z</published>
    <updated>2021-08-30T06:58:27.655Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TreeMap红黑树"><a href="#TreeMap红黑树" class="headerlink" title="TreeMap红黑树"></a>TreeMap红黑树</h1><ul><li>前言</li><li>二叉查找树BST</li><li>BST存在的问题</li><li>2-3-4树</li><li>红黑树</li></ul><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>除了要学会着色、旋转规则，还要了解为什么变色，为什么旋转。</p><p>五大性质的缘由</p><p>jdk的TreeMap在红黑树上做了一些优化，原版红黑树删除操作它是找的前驱节点替代原删除节点，而TreeMap源码里是用的后继节点替代原删除节点，这两种方案实际效果一样，只不过树的结构不一样，但是对应的红黑树都是平衡的。</p><h2 id="二叉查找树-BST"><a href="#二叉查找树-BST" class="headerlink" title="二叉查找树(BST)"></a>二叉查找树(BST)</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>二叉查找树，就是一颗二叉树，他的左节点比父节点小，右节点比父节点大，他的高度决定查找效率。</p><p><img src="https://i.loli.net/2021/08/10/K6rNGpl3tUue714.png" alt=""></p><h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><p>查找(红黑树通用)：查找每个节点从根开始</p><ul><li>查找值比当前大，搜右子树</li><li>查找值等于当前值，停止查找，返回</li><li>查找值比当前值小，则搜右子树</li></ul><p>插入：要插入节点，必须先找到插入节点位置，按照搜索的流程，找到左子树或右子树为空的位置插入。</p><p>遍历(红黑树通用)：前中后序遍历。</p><p>查找最小值(红黑树通用)：沿着根节点的左子树一路查找，直到最后一个不为空的节点</p><p>查找最大值(红黑树通用)：沿着根节点的右子树一路查找，直到最后一个不为空的节点</p><p>查找前驱节点(红黑树通用)：小于当前节点的最大值</p><p>查找后继节点(红黑树通用)：大于当前节点的最小值</p><p><img src="https://z3.ax1x.com/2021/08/20/fX2DBT.png" alt=""></p><p>删除 : 本质上是找前驱或后继节点来替代</p><ul><li>叶子节点直接删除(没有前驱或或后继)</li><li>只有一个子节点的用子节点替代(本质上就是找的前驱节点或者后继节点，左节点就是前驱节点，右节点就是后继节点)</li><li>有两个子节点的，需要找到替代节点(替代节点也是前驱或者后继)</li></ul><p>删除操作和红黑树一样，只不过红黑树多了着色和旋转过程。</p><h3 id="BST存在的问题"><a href="#BST存在的问题" class="headerlink" title="BST存在的问题"></a>BST存在的问题</h3><p>BST存在的问题是，树在插入的时候会导倾斜，不同的插入顺序会导致高度不一样，而树的高度直接影响了树的查找效率。</p><p>基于这个问题平衡二叉树产生了，平衡树的插入和删除时，会通过旋转操作将高度保持在LogN。</p><p>其中两款有代表性的平衡树分别为</p><ul><li>AVL树（高度平衡树，具备二叉搜索树的全部特性，而且左右子树高度差不超过1）</li><li>红黑树</li></ul><p>面试题：有了AVL树为什么还要红黑树？</p><p>AVL树由于实现比较复杂，而且插入和删除性能差。AVL很多性能耗在旋转操作上</p><p>在实际环境下的应用不如红黑树。</p><p>红黑树的实际应用范围广，如java中的HashMap和TreeSet，java8中HashMap的实现因为用RBTree代替链表(链表长度大于8时)，性能有提升。</p><h2 id="2-3-4树"><a href="#2-3-4树" class="headerlink" title="2-3-4树"></a>2-3-4树</h2><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><p>2-3-4树是四阶B树(Balance Tree)，他属于一种多路查找树，他的结构有以下限制：</p><ul><li>所有叶子节点都拥有相同的深度。</li><li>节点只能是2-节点、3-节点、4-节点之一。</li><li><ul><li>2-节点：包含1个元素的节点，有2个子节点；</li><li>3-节点：包含2个元素的节点，有3个子节点；</li><li>4-节点：包含3个元素的节点，有4个子节点；</li><li>所有节点必须至少包含1个元素</li></ul></li><li>元素始终保持排序顺序，整体上保持二叉查找树的性质，即父结点大于左子节点，小于右子结点；而且结点有多个元素时，每个元素必须大于它左边的和它的右子树中元素。</li></ul><p><img src="https://i.loli.net/2021/08/14/dVxDmrkuKgUv6M9.png" alt=""></p><h3 id="结点插入"><a href="#结点插入" class="headerlink" title="结点插入"></a>结点插入</h3><p>2-3-4树中结点添加需要遵循以下规则：</p><ul><li>插入都是向最下面一层插入</li><li>升元：将插入结点由2-节点升级成3-节点，或由3-结点升级成4-结点</li><li>向4-结点插入元素后，需要将中间元素提到父节点升元，原节点变成两个2-节点，再把元素插入2-结点中，如果父节点也是4-结点，则递归向上层升元，直到根节点后将树高加1。</li></ul><p>而将这些规则对应到红黑树里，就是：</p><ul><li>新插入的结点颜色为<strong>红色</strong>，这样才可能不会对红黑树的高度产生影响。</li><li>2-结点对应红黑树中的单个黑色结点，插入时直接成功(对应2-结点升元)</li><li>3-结点对应红黑树中的<strong>黑+红</strong>子树，插入后将其修复成<strong>红+黑+红</strong>子树</li><li>4-结点对应红黑树中的<strong>红+黑+红</strong>子树，插入后将其修复成<strong>红色祖父+黑色父叔+红色孩子</strong>子树，然后再把祖父结点当成新插入的红色结点递归向上层修复，直至修复成功或遇到root结点。</li></ul><p>公式：<strong>红黑树+新增一个节点(红色) = 对等的2-3-4树+新增一个节点</strong></p><h3 id="删除结点"><a href="#删除结点" class="headerlink" title="删除结点"></a>删除结点</h3><p>2-3-4树的删除可以全部转换为叶子节点的删除</p><p>删除原则是先看能不能和下面的叶子节点合并，能合并的直接合并完后删除，不能合并的就要找个元素替换上去，最终都是要保持平衡。</p><p>合并—&gt;删除</p><p>合并—&gt;替换—&gt;删除</p><p>合并—&gt;无法替换—&gt;再合并—&gt;删除</p><p><strong>红色结点一定全部都在多元素节点中</strong></p><p>红黑树的删除要比插入复杂，还是类比2-3-4树：</p><ul><li>查找最近的叶子结点的元素替代被删除元素，删除替代元素后，从替代元素所处叶子结点开始处理</li><li>降元：4-结点变3-结点，3-结点变2-结点。</li><li>2-结点中只有一个元素，所以借兄弟结点中的元素来补充删除后的造成的空结点。</li><li>当兄弟结点中也没有多个元素可以补充时，尝试将父节点降元，失败时向上递归，直到子树降元成功或root结点树高减一</li></ul><p>将这些规则对应到红黑树中即：</p><ul><li>查找离当前结点最近的叶子结点作为替代节点，(左子树的最右结点或右子树的最左结点都能保证替换后二叉树的节点排序性质，叶子节点的替代结点是自身) 替换掉被删除结点，从替代的叶子结点向上递归修复。</li><li>替代结点颜色为红色(对应2-3-4树中 4-节点或3-结点) 时删除子结点直接成功</li><li>替代节点为黑色(对应2-3-4树中 2-节点)时， 意味着替代结点所在的子树会降一层，需要依次检验以下三项，以恢复子树高度：</li><li><ul><li>兄弟结点的子结点中有红色节点(兄弟结点对应3-结点或4-结点) 能够“借用”，旋转过来后修正颜色。</li><li>父结点是红色结点（父结点对应3-结点或4-结点，可以降元）时，将父结点变为黑色，自身和兄弟结点变红色后删除。</li><li>父结点和兄弟结点都是黑色时，将子树降一层后把 <strong>父结点当做替代结点</strong> 递归向上处理。</li></ul></li></ul><h3 id="红黑树对应一颗2-3-4数，一颗2-3-4树对应多颗红黑树"><a href="#红黑树对应一颗2-3-4数，一颗2-3-4树对应多颗红黑树" class="headerlink" title="红黑树对应一颗2-3-4数，一颗2-3-4树对应多颗红黑树"></a>红黑树对应一颗2-3-4数，一颗2-3-4树对应多颗红黑树</h3><p>红黑树和2-3-4树的结点添加和删除都有一共基本规则：避免子树高度变化，因为无论是2-3-4树还是红黑树，一旦子树高度有变动，势必会影响其他子树进行调整。所以我们在插入和删除节点时尽量通过子树内部调整来达到平衡。</p><p>2-3-4树实现平衡是通过结点的旋转和结点元素变化，红黑树是通过结点旋转和变色。</p><p><img src="https://i.loli.net/2021/08/14/kgC4yNQ5frcoXBm.png" alt=""></p><p>2节点全是黑色，3节点有左倾右倾两种情况，4节点上黑下红</p><p>2-3-4树的裂变状态: 红黑树新增都是以红色节点进来的，11裂变上去变成红色，下面两个变成黑色</p><p><img src="https://i.loli.net/2021/08/14/ykrG7CEatD3hULF.png" alt=""></p><p>整体对比2-3-4树和红黑树</p><p><img src="https://i.loli.net/2021/08/14/2OrLtl73KcwIunk.png" alt=""></p><p><img src="https://i.loli.net/2021/08/14/5DzbNJyHqQRv8dx.png" alt=""></p><h2 id="红黑树"><a href="#红黑树" class="headerlink" title="红黑树"></a>红黑树</h2><p><img src="https://i.loli.net/2021/08/14/CmA461TBsgPtdjG.png" style="zoom:50%;" /></p><h3 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h3><p>红黑树是一种结点带有颜色属性的二叉查找树，但它在二叉查找树之外还有以下五大性质：</p><ul><li>结点是红色或黑色</li><li>根是黑色</li><li>所有叶子都是黑色(叶子是NIL节点，这类节点不可忽视，否则代码会看不懂)</li><li>每个红色节点必须有两个黑色子节点（从每个叶子到根的所有路径上不能有两个连续的红色结点）</li><li>从任意一节点到其每个叶子的所有简单路径都包含相同数目的黑色结点（黑色平衡）</li></ul><h3 id="常见操作"><a href="#常见操作" class="headerlink" title="常见操作"></a>常见操作</h3><p><strong>变色、左旋、右旋</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> RBNode root;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 围绕p左旋</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> p</span></span><br><span class="line"><span class="comment"> *              pf                  pf</span></span><br><span class="line"><span class="comment"> *            /                   /</span></span><br><span class="line"><span class="comment"> *           p                   pr(r)</span></span><br><span class="line"><span class="comment"> *          / \                 /  \</span></span><br><span class="line"><span class="comment"> *         pl  pr(r)    -&gt;     p   rr</span></span><br><span class="line"><span class="comment"> *             / \            / \</span></span><br><span class="line"><span class="comment"> *            rl rr          pl  rl</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">leftRotate</span><span class="params">(RBNode p)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (p != <span class="keyword">null</span>) &#123;</span><br><span class="line">        RBNode r = p.right;</span><br><span class="line">        p.right = r.left;</span><br><span class="line">        <span class="keyword">if</span> (r.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">            r.left.parent = p;</span><br><span class="line">        &#125;</span><br><span class="line">        r.parent = p.parent;</span><br><span class="line">        <span class="keyword">if</span> (p.parent == <span class="keyword">null</span>) &#123;</span><br><span class="line">            root = r;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (p.parent.left == p) &#123;</span><br><span class="line">            p.parent.left = r;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            p.parent.right = r;</span><br><span class="line">        &#125;</span><br><span class="line">        r.left = p;</span><br><span class="line">        p.parent = r;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 围绕p右旋</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> p</span></span><br><span class="line"><span class="comment"> *          pf                        pf</span></span><br><span class="line"><span class="comment"> *           \                         \</span></span><br><span class="line"><span class="comment"> *            p                         pl(l)</span></span><br><span class="line"><span class="comment"> *           / \           -&gt;           /  \</span></span><br><span class="line"><span class="comment"> *       pl(l)  pr                    ll    p</span></span><br><span class="line"><span class="comment"> *       / \                               / \</span></span><br><span class="line"><span class="comment"> *     ll   lr                            lr  pr</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">rightRotate</span><span class="params">(RBNode p)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (p != <span class="keyword">null</span>) &#123;</span><br><span class="line">        RBNode l = p.left;</span><br><span class="line">        p.left = l.right;</span><br><span class="line">        <span class="keyword">if</span> (l.right != <span class="keyword">null</span>) &#123;</span><br><span class="line">            l.right.parent = p;</span><br><span class="line">        &#125;</span><br><span class="line">        l.parent = p.parent;</span><br><span class="line">        <span class="keyword">if</span> (p.parent != <span class="keyword">null</span>) &#123;</span><br><span class="line">            root = l;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (p.parent.right == p) &#123;</span><br><span class="line">            p.parent.right = l;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            p.parent.left = l;</span><br><span class="line">        &#125;</span><br><span class="line">        l.right = p;</span><br><span class="line">        p.parent = l;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>新增</strong> （七种情况，五种情况需要考虑自平衡）</p><p><img src="https://i.loli.net/2021/08/30/1zoZ7HYrsJDvkUy.png" alt=""></p><p>分情况讨论，主要是找到插入位置，然后自平衡(左旋或者右旋) 且插入结点是红色节点(如果是黑色的话那么当前分支上就会多出一个黑色结点出来，从而破坏了黑色平衡)，以下分析全部以左子树为例，右子树的情况则相反。</p><ul><li>情况1、如果插入的是第一个节点(根节点)，红色变黑色</li><li>情况2、如果父节点为黑色，则直接插入，不需要变色</li><li>如果父节点为红色，叔叔节点也是红色（此种情况爷爷节点一定是黑色），则父节点和叔叔节点变黑色，爷爷节点变红色（如果爷爷节点是根节点，则再变成黑色），爷爷节点此时需要递归（把爷爷节点当做新插入的节点再次进行比较）</li><li>如果父节点是红色，没有叔叔节点或者叔叔节点是黑色（此时只能是NIL节点），则以爷爷节点为支点右旋，旋转之后原来的爷爷节点变红色，原来的父节点变黑色。</li></ul><p>还是与2-3-4树对比，新增一定在叶子节点上</p><p>情况1</p><p><img src="https://i.loli.net/2021/08/14/cNnouZS7wDbf48r.png" alt=""></p><p>情况2——右边相当于左右旋了</p><p><img src="https://i.loli.net/2021/08/14/rWCDmgQLfAUb7s3.png" alt=""></p><p>与3节点合并情况</p><p><img src="https://i.loli.net/2021/08/14/rWPIiuGDKaRx8UQ.png" alt=""></p><p><img src="https://i.loli.net/2021/08/14/yDrf4qQhVX2t65a.png" alt=""></p><p><img src="https://i.loli.net/2021/08/14/8D9eqhpGnMKFwTZ.png" alt=""></p><p>裂变情况</p><p><img src="https://i.loli.net/2021/08/14/ahr8mFQO14gBfCY.png" alt=""></p><p><img src="https://z3.ax1x.com/2021/08/20/fXRvW9.png" alt=""></p><p><img src="https://z3.ax1x.com/2021/08/20/fXfK39.png" alt=""></p><p><strong>删除</strong> (重点) 五种情况、两种需要考虑自平衡，又细分八种情况，其中四种为镜像情况</p><p>先看二叉搜索树的删除：</p><ul><li>删除叶子节点，直接删除</li><li>删除的节点有一个子节点，那么用子节点来替代</li><li>如果删除的节点有2个子节点，此时需要找到前驱节点或者后继节点来替代</li></ul><p>写代码时，删除方案：</p><ul><li>找到前驱节点，复制前驱节点值覆盖准备删除的节点值，然后删除前驱节点</li><li>找到后继节点，复制后继节点的值覆盖准备删除的节点值，然后删除后继节点</li><li>被删除的前驱节点或者后继节点只有两种情况：1、被删除节点是叶子节点。2、被删除节点只有一个孩子。</li></ul><p>找前驱后继代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 找到指定节点的前驱节点，即找小于node节点的最大值</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> node</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> RBNode <span class="title">preedecessor</span><span class="params">(RBNode node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (node.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">            RBNode p = node.left;</span><br><span class="line">            <span class="keyword">while</span> (p.right != <span class="keyword">null</span>) &#123;</span><br><span class="line">                p = p.right;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> p;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 删除时不一定用到，但找前驱时，左子树就得向上找了</span></span><br><span class="line">            <span class="comment">// 找到第一个左拐的地方</span></span><br><span class="line">            RBNode p = node.parent;</span><br><span class="line">            RBNode ch = node;</span><br><span class="line">            <span class="keyword">while</span> (p != <span class="keyword">null</span> &amp;&amp; p.left == ch) &#123;</span><br><span class="line">                ch = p;</span><br><span class="line">                p = p.parent;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> p;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 找到指定节点的后继节点，大于节点的最小值</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> node</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> RBNode <span class="title">sucessor</span><span class="params">(RBNode node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (node.right != <span class="keyword">null</span>) &#123;</span><br><span class="line">            RBNode p = node.right;</span><br><span class="line">            <span class="keyword">while</span> (p.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">                p = p.left;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> p;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 删除时不一定用到，但找前驱时，左子树就得向上找了</span></span><br><span class="line">            <span class="comment">// 找到第一个左拐的地方</span></span><br><span class="line">            RBNode p = node.parent;</span><br><span class="line">            RBNode ch = node;</span><br><span class="line">            <span class="keyword">while</span> (p != <span class="keyword">null</span> &amp;&amp; p.right == ch) &#123;</span><br><span class="line">                ch = p;</span><br><span class="line">                p = p.parent;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> p;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>红黑树的删除：1先找到节点，2删除</p><p>红黑树上面的删除节点一定是2-3-4树上的叶子节点</p><p><img src="https://z3.ax1x.com/2021/08/20/fXVpDA.png" alt=""></p><p>三局话：</p><ul><li><strong>自己能搞定的自己搞定</strong>：</li><li><ul><li>如果删除的节点对应于2-3-4树的3节点或者4节点，则直接删除，不用和兄弟或父亲借。</li><li>如果删除的是红色节点，则直接删除；如果是黑色节点，则红色节点上来替代，变黑即可</li></ul></li><li><strong>搞不定的找兄弟和父亲帮忙</strong>:</li><li><ul><li>前提是找到真正的可用的兄弟结点 (真正的兄弟节点是对应于2-3-4树中的兄弟节点，如上图左，5的兄弟节点是8，图右中5的兄弟节点应该是7和7.5，可以通过旋转来找)</li><li>兄弟节点有的借(此时兄弟节点一定是黑色，如果是红色那说明这个节点不是真正的兄弟节点，需要回到上一步找真正的兄弟节点)</li><li>兄弟节点有两个子节点的情况(两个子节点肯定是红色，如果是黑色的话相当于此时兄弟节点对应2-3-4树是2节点，不可能有多余的元素可以借)，此时需要旋转变色</li><li>兄弟节点只有一个子节点的情况，此时需要旋转变色</li></ul></li><li><strong>父亲和兄弟帮不了那有福同享，有难同当(父亲和兄弟自损)</strong>：</li><li><ul><li>前提还是找到真正的兄弟节点</li><li>兄弟节点没有多余的元素可借（此时兄弟节点一定为黑色的2节点），此时兄弟节点所在分支也要自损一个黑色节点以达到黑色平衡，最快的方式就是兄弟节点直接变红(相当于减少一个黑色节点)，此时以父节点为root的子树又达到了平衡(两边都比之前少了一个黑色)。但是以祖父结点为root的树依然是不平衡的，此时需要递归处理。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;TreeMap红黑树&quot;&gt;&lt;a href=&quot;#TreeMap红黑树&quot; class=&quot;headerlink&quot; title=&quot;TreeMap红黑树&quot;&gt;&lt;/a&gt;TreeMap红黑树&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;前言&lt;/li&gt;
&lt;li&gt;二叉查找树BST&lt;/li&gt;
&lt;li&gt;BS</summary>
      
    
    
    
    
    <category term="数据结构" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>并查集</title>
    <link href="http://example.com/2021/08/10/%E5%B9%B6%E6%9F%A5%E9%9B%86/"/>
    <id>http://example.com/2021/08/10/%E5%B9%B6%E6%9F%A5%E9%9B%86/</id>
    <published>2021-08-10T07:16:29.000Z</published>
    <updated>2021-08-10T10:01:05.205Z</updated>
    
    <content type="html"><![CDATA[<h1 id="并查集"><a href="#并查集" class="headerlink" title="并查集"></a>并查集</h1><h2 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h2><p>解决元素分组问题，管理一系列不相交的集合</p><h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><ul><li>合并(Union)：把两个不相交的集合合并为一个集合</li><li>查询(Find)：查询两个元素是否在同一个集合</li></ul><h3 id="以一个应用场景为例应用场景"><a href="#以一个应用场景为例应用场景" class="headerlink" title="以一个应用场景为例应用场景"></a>以一个应用场景为例应用场景</h3><p>(洛谷P1551) 亲戚</p><blockquote><p><strong>题目背景</strong><br>若某个家族人员过于庞大，要判断两个是否是亲戚，确实还很不容易，现在给出某个亲戚关系图，求任意给出的两个人是否具有亲戚关系。<br><strong>题目描述</strong><br>规定：x和y是亲戚，y和z是亲戚，那么x和z也是亲戚。如果x,y是亲戚，那么x的亲戚都是y的亲戚，y的亲戚也都是x的亲戚。<br><strong>输入格式</strong><br>第一行：三个整数n,m,p，（n&lt;=5000,m&lt;=5000,p&lt;=5000），分别表示有n个人，m个亲戚关系，询问p对亲戚关系。<br>以下m行：每行两个数Mi，Mj，1&lt;=Mi，Mj&lt;=N，表示Mi和Mj具有亲戚关系。<br>接下来p行：每行两个数Pi，Pj，询问Pi和Pj是否具有亲戚关系。<br><strong>输出格式</strong><br>P行，每行一个’Yes’或’No’。表示第i个询问的答案为“具有”或“不具有”亲戚关系。</p></blockquote><p>把所有人划分到若干个不相交的集合中，每个集合里的人彼此是亲戚。为了判断两个人是否为亲戚，只需看它们是否属于同一个集合即可。因此，这里就可以考虑用并查集进行维护。</p><h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p><strong>用集合中的一个元素代表集合</strong>。我曾看过一个有趣的比喻，把集合比喻成<strong>帮派</strong>，而代表元素则是<strong>帮主</strong>。接下来我们利用这个比喻，看看并查集是如何运作的。</p><p><img src="https://i.loli.net/2021/08/10/2xK7tBoWcLDlghy.png" alt=""></p><p>最开始，每个元素的代表元素是自己。每个元素的最顶端是自己，是一个自环节点。</p><p>在比较两人是不是一个帮派的时候，就找自己的帮主，看看是不是一个帮主。每个元素向上找，找到不能再找了就是了。</p><p>然而帮派规模大了，肯定会造成等级(树的深度)变深。</p><p>有两个操作可以优化一个是路径压缩，一个是按秩压缩。</p><p>之前说找帮主来判断两个元素是否在同一集合内。找到帮主一样说明是一个集合里的，不一样把较小集合的帮主指向较大集合的帮主，这样做就是按秩压缩：</p><p><img src="https://i.loli.net/2021/08/10/QYl1ZJRFnekUhCB.png" alt=""></p><p>按秩压缩了之后其实还是可以发现树是深度是越来越深的，那么再采取路径压缩。</p><p>在每次向上查找的过程中，将路过的节点之间指向帮主，这是通过栈来实现的。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> 并查集 </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 样本进来会包一层，叫做元素</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Element</span>&lt;<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">public</span> V value;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">Element</span><span class="params">(V value)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.value = value;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UnionFindSet</span>&lt;<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> HashMap&lt;V, Element&lt;V&gt;&gt; elementMap;</span><br><span class="line">        <span class="comment">// key 某个元素value 该元素的父</span></span><br><span class="line">        <span class="keyword">public</span> HashMap&lt;Element&lt;V&gt;, Element&lt;V&gt;&gt; fatherMap;</span><br><span class="line">        <span class="comment">// key 某个集合的代表元素， value该集合的大小</span></span><br><span class="line">        <span class="keyword">public</span> HashMap&lt;Element&lt;V&gt;, Integer&gt; sizeMap;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">UnionFindSet</span><span class="params">(List&lt;V&gt; list)</span> </span>&#123;</span><br><span class="line">            elementMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            fatherMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            sizeMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (V value : list) &#123;</span><br><span class="line">                Element element = <span class="keyword">new</span> Element(value);</span><br><span class="line">                elementMap.put(value, element);</span><br><span class="line">                fatherMap.put(element, element);</span><br><span class="line">                sizeMap.put(element, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 给定一个element，网上找，把代表元素返回</span></span><br><span class="line">        <span class="function"><span class="keyword">private</span> Element&lt;V&gt; <span class="title">findHead</span><span class="params">(Element&lt;V&gt; element)</span> </span>&#123;</span><br><span class="line">            Stack&lt;Element&lt;V&gt;&gt; path = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">            <span class="keyword">while</span> (element != fatherMap.get(element)) &#123;</span><br><span class="line">                path.push(element);</span><br><span class="line">                element = fatherMap.get(element);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">while</span> (!path.isEmpty()) &#123;  <span class="comment">// 路径铺平</span></span><br><span class="line">                fatherMap.put(path.pop(), element);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> element;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isSameSet</span><span class="params">(V a, V b)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (elementMap.containsKey(a) &amp;&amp; elementMap.containsKey(b)) &#123;</span><br><span class="line">                <span class="keyword">return</span> findHead(elementMap.get(a)) == findHead(elementMap.get(b));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">union</span><span class="params">(V a, V b)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (elementMap.containsKey(a) &amp;&amp; elementMap.containsKey(b)) &#123;</span><br><span class="line">                Element&lt;V&gt; aF = findHead(elementMap.get(a));</span><br><span class="line">                Element&lt;V&gt; bF = findHead(elementMap.get(b));</span><br><span class="line">                <span class="keyword">if</span> (aF != bF) &#123;</span><br><span class="line">                    Element&lt;V&gt; big = sizeMap.get(aF) &gt;= sizeMap.get(bF) ? aF : bF;</span><br><span class="line">                    Element&lt;V&gt; small = big == aF ? bF : aF;</span><br><span class="line">                    fatherMap.put(small, big);</span><br><span class="line">                    sizeMap.put(big, sizeMap.get(aF) + sizeMap.get(bF));</span><br><span class="line">                    sizeMap.remove(small);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>洛谷P1551亲戚</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="keyword">int</span> n = scanner.nextInt(); <span class="comment">// 人数</span></span><br><span class="line">        <span class="keyword">int</span> m = scanner.nextInt(); <span class="comment">// 关系数</span></span><br><span class="line">        <span class="keyword">int</span> p = scanner.nextInt(); <span class="comment">// 询问多少个关系</span></span><br><span class="line"></span><br><span class="line">        List&lt;Integer&gt; personList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">int</span>[][] relation = <span class="keyword">new</span> <span class="keyword">int</span>[m][<span class="number">2</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> p1 = scanner.nextInt();</span><br><span class="line">            <span class="keyword">int</span> p2 = scanner.nextInt();</span><br><span class="line">            <span class="keyword">if</span> (!personList.contains(p1)) personList.add(p1);</span><br><span class="line">            <span class="keyword">if</span> (!personList.contains(p2)) personList.add(p2);</span><br><span class="line">            relation[i][<span class="number">0</span>] = p1;</span><br><span class="line">            relation[i][<span class="number">1</span>] = p2;</span><br><span class="line"><span class="comment">//            relationMap.put(p2, p1);</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        UnionFindSet&lt;Integer&gt; unionSet = <span class="keyword">new</span> UnionFindSet&lt;&gt;(personList);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//        for (Map.Entry&lt;Integer, Integer&gt; entry : relationMap.entrySet()) &#123;</span></span><br><span class="line"><span class="comment">//            unionSet.union(entry.getKey(), entry.getValue());</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">            unionSet.union(relation[i][<span class="number">0</span>], relation[i][<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; p; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> p1 = scanner.nextInt();</span><br><span class="line">            <span class="keyword">int</span> p2 = scanner.nextInt();</span><br><span class="line">            <span class="keyword">if</span> (unionSet.isSameSet(p1, p2)) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;Yes&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;No&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;并查集&quot;&gt;&lt;a href=&quot;#并查集&quot; class=&quot;headerlink&quot; title=&quot;并查集&quot;&gt;&lt;/a&gt;并查集&lt;/h1&gt;&lt;h2 id=&quot;用途&quot;&gt;&lt;a href=&quot;#用途&quot; class=&quot;headerlink&quot; title=&quot;用途&quot;&gt;&lt;/a&gt;用途&lt;/h2&gt;&lt;p</summary>
      
    
    
    
    
    <category term="数据结构" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Docker学习(四) 网络</title>
    <link href="http://example.com/2021/08/06/Docker%E5%AD%A6%E4%B9%A0-%E5%9B%9B-%E7%BD%91%E7%BB%9C/"/>
    <id>http://example.com/2021/08/06/Docker%E5%AD%A6%E4%B9%A0-%E5%9B%9B-%E7%BD%91%E7%BB%9C/</id>
    <published>2021-08-06T09:14:03.000Z</published>
    <updated>2021-08-06T13:24:05.990Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker学习-四-网络"><a href="#Docker学习-四-网络" class="headerlink" title="Docker学习(四) 网络"></a>Docker学习(四) 网络</h1><h2 id="理解docker0"><a href="#理解docker0" class="headerlink" title="理解docker0"></a>理解docker0</h2><p><img src="https://i.loli.net/2021/08/06/Aln7NGbIr8FKL2h.png" alt=""></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker 是如何处理容器网络访问的</span></span><br><span class="line"></span><br><span class="line">(base) root@linux:/home/cpss# docker run -d -P --name tomcat01 tomcat</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看容器的内部网络地址 ip addr</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 发现容器启动的时候会得到一个 eth0@if34 ip地址，docker分配的</span></span><br><span class="line">(base) root@linux:/home/cpss# docker exec -it tomcat01 ip addr</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">33: eth0@if34: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default </span><br><span class="line">    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 思考：linux能不能ping通容器内部</span></span><br><span class="line">(base) root@linux:/home/cpss# ping 172.17.0.2</span><br><span class="line">PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.107 ms</span><br><span class="line">64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.055 ms</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> linux 可以ping通docker容器内部</span></span><br></pre></td></tr></table></figure><blockquote><p>原理</p></blockquote><ul><li><p>我们每启动一个docker容器，docker就会给docker容器分配一个ip，只要安装了docker，就会有一个网卡docker0 桥接模式，使用的技术是 evth-pair技术</p><p>现在再在宿主机执行ip addr，发现多了一个</p><p><img src="https://i.loli.net/2021/08/06/nkoUMcDPLG6KRax.png" alt=""></p></li></ul><p>再启动一个容器测试</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss<span class="comment"># docker run -d -P --name tomcat02 tomcat</span></span><br><span class="line"><span class="comment"># 每启动一个就多一个网卡</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 发现这个容器带来的网卡，都是一对一对的</span></span><br><span class="line"><span class="comment"># evth-pair 就是一对的虚拟设备端口，他们都是成对出现的，一段连着协议，一段彼此相连</span></span><br><span class="line"><span class="comment"># 正因为有这个特性，evth-pair 充当一个桥梁,连接各种虚拟网络设备的</span></span><br><span class="line"><span class="comment"># Openstac，Docker容器之间的链接，ovs的链接，都是使用这个技术</span></span><br></pre></td></tr></table></figure><p>测试一下tomcat01和tomcat02是否能ping通</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:&#x2F;home&#x2F;cpss# docker exec -it tomcat02 ping 172.17.0.2</span><br><span class="line">PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from 172.17.0.2: icmp_seq&#x3D;1 ttl&#x3D;64 time&#x3D;0.135 ms</span><br><span class="line">64 bytes from 172.17.0.2: icmp_seq&#x3D;2 ttl&#x3D;64 time&#x3D;0.086 ms</span><br><span class="line">64 bytes from 172.17.0.2: icmp_seq&#x3D;3 ttl&#x3D;64 time&#x3D;0.067 ms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 结论 容器之间可以互相ping通的</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/06/8x9nU6PVTZepR7b.png" alt=""></p><p>Tomcat01 和tomcat02 是公用的一个路由器，docker0</p><p>所有的容器不指定网络的情况下，都是使用docker0路由的，docker会给我们的容器分配一个默认的可用ip</p><blockquote><p>小结</p></blockquote><p>docker 使用的是桥接，宿主机中是一个docker容器的网桥 docker0</p><p><img src="https://i.loli.net/2021/08/06/bTBl5Qsh7qVCXI4.png" alt=""></p><p>docker 中所有的网络端口都是虚拟的，虚拟的转发效率高(内网传递)</p><blockquote><p>思考一个场景，编写了一个微服务，database url=ip:, 项目不重启，数据库ip换掉了，我们希望可以处理这个问题，可以通过名字来进行访问容器</p></blockquote><h3 id="—link"><a href="#—link" class="headerlink" title="—link"></a>—link</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss# docker exec -it tomcat02 ping tomcat01</span><br><span class="line">ping: tomcat01: Name or service not known</span><br><span class="line"></span><br><span class="line">(base) root@linux:/home/cpss# docker run -d -P --name tomcat03 --link tomcat02 tomcat</span><br><span class="line">6f62526ba5484b2c542bc31b0891a1f06c1baedbdb8667322b9b051a1f443e06</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过--link 可以解决</span></span><br><span class="line">(base) root@linux:/home/cpss# docker exec -it tomcat03 ping tomcat02</span><br><span class="line">PING tomcat02 (172.17.0.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from tomcat02 (172.17.0.3): icmp_seq=1 ttl=64 time=0.108 ms</span><br><span class="line">64 bytes from tomcat02 (172.17.0.3): icmp_seq=2 ttl=64 time=0.066 ms</span><br><span class="line">64 bytes from tomcat02 (172.17.0.3): icmp_seq=3 ttl=64 time=0.069 ms</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 反向 ping不通</span></span><br><span class="line">(base) root@linux:/home/cpss# docker exec -it tomcat02 ping tomcat03</span><br><span class="line">ping: tomcat03: Name or service not known</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(base) root@linux:/home/cpss# docker network ls</span><br><span class="line">NETWORK ID     NAME      DRIVER    SCOPE</span><br><span class="line">27abb1e4b0d3   bridge    bridge    local</span><br><span class="line">e69e785a705e   host      host      local</span><br><span class="line">2412989a4eb3   none      null      local</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 其实这个tomcat03就是在本地配置了tomcat02的配置</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --link 就是我们在hosts配置中增加了一个 映射</span></span><br><span class="line">(base) root@linux:/home/cpss# docker exec -it tomcat03 cat /etc/hosts</span><br><span class="line">127.0.0.1localhost</span><br><span class="line">::1localhost ip6-localhost ip6-loopback</span><br><span class="line">fe00::0ip6-localnet</span><br><span class="line">ff00::0ip6-mcastprefix</span><br><span class="line">ff02::1ip6-allnodes</span><br><span class="line">ff02::2ip6-allrouters</span><br><span class="line">172.17.0.3tomcat02 9de7a985a568</span><br><span class="line">172.17.0.46f62526ba548</span><br></pre></td></tr></table></figure><p>现在玩docker已经不建议使用 —link了</p><p>自定义网络! 不适用docker0</p><p>docker0问题：他不支持容器名链接访问</p><h3 id="自定义网络-（容器互联）"><a href="#自定义网络-（容器互联）" class="headerlink" title="自定义网络 （容器互联）"></a>自定义网络 （容器互联）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss# docker network --help</span><br><span class="line">Commands:</span><br><span class="line">  connect     Connect a container to a network</span><br><span class="line">  create      Create a network</span><br><span class="line">  disconnect  Disconnect a container from a network</span><br><span class="line">  inspect     Display detailed information on one or more networks</span><br><span class="line">  ls          List networks</span><br><span class="line">  prune       Remove all unused networks</span><br><span class="line">  rm          Remove one or more networks</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:&#x2F;home&#x2F;cpss# docker network ls</span><br><span class="line">NETWORK ID     NAME      DRIVER    SCOPE</span><br><span class="line">27abb1e4b0d3   bridge    bridge    local</span><br><span class="line">e69e785a705e   host      host      local</span><br><span class="line">2412989a4eb3   none      null      local</span><br></pre></td></tr></table></figure><p>网络模式</p><ul><li>bridge：桥接docker (默认)</li><li>none: 不配置网络</li><li>host: 和宿主机共享网络</li><li>container：容器内网络连通 (用的少，局限性大)</li></ul><p>测试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 我们之间启动的命令 --net bridge 而这个就是我们的docker0</span></span><br><span class="line">docker run -d -P --name tomcat01 tomcat</span><br><span class="line">docker run -d -P --name tomcat01 --net bridge tomcat</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker0 特点。默认，域名不能访问， --link可以打通</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 我们可以自定义个网络</span></span><br><span class="line">(base) root@linux:/home/cpss# docker network create --help</span><br><span class="line">Options:</span><br><span class="line">      --attachable           Enable manual container attachment</span><br><span class="line">      --aux-address map      Auxiliary IPv4 or IPv6 addresses used by Network driver (default map[])</span><br><span class="line">      --config-from string   The network from which to copy the configuration</span><br><span class="line">      --config-only          Create a configuration only network</span><br><span class="line">  -d, --driver string        Driver to manage the Network (default &quot;bridge&quot;)</span><br><span class="line">      --gateway strings      IPv4 or IPv6 Gateway for the master subnet</span><br><span class="line">      --ingress              Create swarm routing-mesh network</span><br><span class="line">      --internal             Restrict external access to the network</span><br><span class="line">      --ip-range strings     Allocate container ip from a sub-range</span><br><span class="line">      --ipam-driver string   IP Address Management Driver (default &quot;default&quot;)</span><br><span class="line">      --ipam-opt map         Set IPAM driver specific options (default map[])</span><br><span class="line">      --ipv6                 Enable IPv6 networking</span><br><span class="line">      --label list           Set metadata on a network</span><br><span class="line">  -o, --opt map              Set driver specific options (default map[])</span><br><span class="line">      --scope string         Control the network&#x27;s scope</span><br><span class="line">      --subnet strings       Subnet in CIDR format that represents a network segment</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">docker network create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 mynet</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/06/vcuabfSy6KpP9rl.png" alt=""></p><p>不使用—link 也能连接了</p><p>我们自定义的网络docker都已经帮我们维护好了对应的关系</p><p>好处：</p><p>不同的集群使用不同的网络，保证集群是安全健康的。</p><h3 id="网络连通"><a href="#网络连通" class="headerlink" title="网络连通"></a>网络连通</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss<span class="comment"># docker network --help</span></span><br><span class="line">  connect     Connect a container to a network</span><br><span class="line">  </span><br><span class="line">(base) root@linux:/home/cpss<span class="comment"># docker network connect --help</span></span><br><span class="line">Options:</span><br><span class="line">      --<span class="built_in">alias</span> strings           Add network-scoped <span class="built_in">alias</span> <span class="keyword">for</span> the container</span><br><span class="line">      --driver-opt strings      driver options <span class="keyword">for</span> the network</span><br><span class="line">      --ip string               IPv4 address (e.g., 172.30.100.104)</span><br><span class="line">      --ip6 string              IPv6 address (e.g., 2001:db8::33)</span><br><span class="line">      --link list               Add link to another container</span><br><span class="line">      --link-local-ip strings   Add a link-local address <span class="keyword">for</span> the container</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试打通 tomcat01 - mynet</span></span><br><span class="line"><span class="comment"># 连通之后就是将tomcat01 放到了mynet网络下</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>结论：假设要跨网络操作别人，就需要使用docker network connect</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Docker学习-四-网络&quot;&gt;&lt;a href=&quot;#Docker学习-四-网络&quot; class=&quot;headerlink&quot; title=&quot;Docker学习(四) 网络&quot;&gt;&lt;/a&gt;Docker学习(四) 网络&lt;/h1&gt;&lt;h2 id=&quot;理解docker0&quot;&gt;&lt;a href=</summary>
      
    
    
    
    
    <category term="Docker" scheme="http://example.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker学习(三)</title>
    <link href="http://example.com/2021/08/05/Docker%E5%AD%A6%E4%B9%A0-%E4%B8%89/"/>
    <id>http://example.com/2021/08/05/Docker%E5%AD%A6%E4%B9%A0-%E4%B8%89/</id>
    <published>2021-08-05T11:33:59.000Z</published>
    <updated>2021-08-06T09:13:20.525Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker学习-三"><a href="#Docker学习-三" class="headerlink" title="Docker学习(三)"></a>Docker学习(三)</h1><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><h3 id="portainer-不常用"><a href="#portainer-不常用" class="headerlink" title="portainer (不常用)"></a>portainer (不常用)</h3><p>Docker的图形化界面管理工具</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 外部8088 内部9000 </span><br><span class="line"># -v 挂载</span><br><span class="line">docker run -d -p 8088:9000\</span><br><span class="line">--restart&#x3D;always -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock --privileged&#x3D;true portainer&#x2F;portainer</span><br></pre></td></tr></table></figure><h3 id="Rancher-CI-CD再用"><a href="#Rancher-CI-CD再用" class="headerlink" title="Rancher(CI/CD再用)"></a>Rancher(CI/CD再用)</h3><hr><h2 id="镜像是什么"><a href="#镜像是什么" class="headerlink" title="镜像是什么"></a>镜像是什么</h2><p>是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件。它包含某个软件所需的所有内容，包括代码、运行时、库、环境变量和配置文件。</p><p>如何得到：</p><ul><li>远程仓库下载</li><li>拷贝</li><li>自己制作一个镜像DockerFile</li></ul><h3 id="镜像加载原理"><a href="#镜像加载原理" class="headerlink" title="镜像加载原理"></a>镜像加载原理</h3><blockquote><p>UnionFS(联合文件系统)</p></blockquote><p>我们下载的时候看到的一层层就是这个</p><p>UnionFS是一种分层、轻量级且高性能的文件系统，它支持文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下.</p><p>特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。</p><p>docker的镜像实际上由一层一层的文件系统组成，这种层级文件系统就是UnionFS</p><p>bootfs(boot file system)，在Docker镜像的最底层是bootfs，这一层与典型的Linux、Unix系统是一样的，它主要包含bootloader和kernel，bootloader主要是引导加载kernel，Linux刚启动时会加载bootfs文件系统。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载掉bootfs</p><p>rootfs(root file system)，在bootfs之上，包含的是Linux系统中的/dev /proc /bin /etc 等标准目录和文件，rootfs就是各种不同的操作系统发行版，如ubantu，centos</p><p>对于一个精简的OS，rootfs可以很小，只包含最基本的命令，因为底层直接用host的kernel。</p><h2 id="分层理解"><a href="#分层理解" class="headerlink" title="分层理解"></a>分层理解</h2><blockquote><p>分层的镜像</p></blockquote><p>下载的日志输出，可以看到是一层一层的在下载</p><p><img src="https://i.loli.net/2021/08/05/SmgOoLaUYiwG6sr.png" alt=""></p><p>为什么采用这种分层结构 ？</p><p>最大的好处就是资源共享，比如有多个镜像都从相同的Base镜像构建而来，那么宿主机只需在磁盘上保留一份base镜像，同时内存中也只需要加载一份base镜像，这样就可以为所有的容器服务了，而且镜像的每一层都可以被共享。</p><p>查看镜像分层方式可以通过docker image inspect命令</p><blockquote><p>特点</p></blockquote><p>Docker镜像都是只读的，当容器启动时，一个新的可写层被加载到镜像的顶部。</p><p>这一层就是通常说的容器层，容器之下的都叫镜像层。</p><p><img src="https://i.loli.net/2021/08/05/Bo3v1qXJnadp5Kh.png" alt=""></p><h2 id="如何commit镜像"><a href="#如何commit镜像" class="headerlink" title="如何commit镜像"></a>如何commit镜像</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker commit 提交容器成为一个新的副本</span><br><span class="line"># 和git类似</span><br><span class="line">docker commit -m&#x3D;&quot;提交的描述信息&quot; -a&#x3D;&quot;作者&quot; 容器id 容器镜像名:[TAG]</span><br></pre></td></tr></table></figure><h2 id="容器数据卷"><a href="#容器数据卷" class="headerlink" title="容器数据卷"></a>容器数据卷</h2><p>数据？如果数据都在容器中，那么容器删除，数据就会丢失！需求：数据可以持久</p><p>容器之间可以有一个数据共享的技术，docker容器中产生的数据，同步到本地</p><p>目录挂载，将容器内的目录挂载到linux上。</p><p><img src="https://i.loli.net/2021/08/06/cCQPLRghqlwpUZr.png" alt=""></p><p>总结一句话，容器的持久化和同步操作，容器间也是可以数据共享的。</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><blockquote><p>方式1：直接使用命令挂载 -v</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-v 主机目录:容器内目录</span><br><span class="line">-p 主机端口:容器内端口</span><br><span class="line"></span><br><span class="line">docker run -it -v &#x2F;home&#x2F;ceshi:&#x2F;home centos &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/06/TpZcrXgRi3SV9hB.png" alt=""></p><p>是双向的同步，哪怕容器已经停止。</p><p>好处：以后修改只需在本地修改即可。</p><h3 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h3><p>思考：mysql的数据持久化问题 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss# docker pull mysql:5.7</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行，需要数据挂载</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装启动mysql时，需要配置密码的！</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -d 后台运行</span></span><br><span class="line">(base) root@linux:/home/cpss# docker run -d -p 3310:3306 -v /data2/mysql/conf:/etc/mysql/conf.d -v /data2/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root --name mysql01 mysql:5.7</span><br><span class="line">72180bd20207e871aebdc0a06fddfe10e30d39561620c78558328b9ac0a30b9c</span><br></pre></td></tr></table></figure><h3 id="具名和匿名挂载"><a href="#具名和匿名挂载" class="headerlink" title="具名和匿名挂载"></a>具名和匿名挂载</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 匿名挂载</span></span><br><span class="line">-v 容器内路径</span><br><span class="line">docker run -d -P --name nginx01 -v /etc/nginx nginx</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看所有的volume情况，匿名卷挂载</span></span><br><span class="line">(base) root@linux:/data2/mysql# docker volume ls</span><br><span class="line"><span class="meta">#</span><span class="bash"> 这种就是匿名挂载，在-v只写了容器内的路径，没有写容器外的路径！</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 具名挂载</span></span><br><span class="line">docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx nginx</span><br><span class="line">(base) root@linux:/data2/mysql# docker volume ls</span><br><span class="line">DRIVER    VOLUME NAME</span><br><span class="line">local     juming-nginx</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过-v 卷名：容器内路径</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看一下这个卷</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/06/HStUJkLaIY9sXm7.png" alt=""></p><p>所有的docker容器内的卷，没有指定目录的情况下都是在 /var/lib/docker/volumes/xxxxx/_data</p><p>通过具名挂载可以方便的找到一个卷，大多数情况在使用的是具名挂载。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如何确定是具名挂载还是匿名挂载， 还是指定路径挂载</span></span><br><span class="line">-v 容器内路径 <span class="comment"># 匿名挂载</span></span><br><span class="line">-v 卷名：容器内路径 <span class="comment"># 具名挂载</span></span><br><span class="line">-v /宿主机路径:容器内路径 <span class="comment"># 指定路径挂载</span></span><br></pre></td></tr></table></figure><p>扩展</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:ro nginx</span><br><span class="line">docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:rw nginx</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过 -v 容器内路径:ro rw改变读写权限</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  容器对我们挂载出来的内容就有限定了</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ro <span class="built_in">readonly</span> 只要看到ro就说明这个路径只能通过宿主机来操作，容器内部是无法操作的。</span></span><br></pre></td></tr></table></figure><blockquote><p>方式二、dockerfile 创建镜像时就挂载出来</p></blockquote><p>Dockerfile 就是用来构建docker镜像的构建文件</p><p>通过脚本可以生成镜像，镜像是一层一层的，脚本一个个的命令，每个命令都是一层</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/data2/docker-volume# pwd</span><br><span class="line">/data2/docker-volume</span><br><span class="line">(base) root@linux:/data2/docker-volume# vim dockerfile1</span><br><span class="line">(base) root@linux:/data2/docker-volume# cat dockerfile1 </span><br><span class="line">FROM centos</span><br><span class="line"></span><br><span class="line">VOLUME [&quot;volume01&quot;,&quot;volume02&quot;]</span><br><span class="line"></span><br><span class="line">CMD echo &quot;---end---&quot;</span><br><span class="line"></span><br><span class="line">CMD /bin/bash</span><br><span class="line">(base) root@linux:/data2/docker-volume# docker build -f dockerfile1 -t zuo/centos:1.0 .</span><br><span class="line">Sending build context to Docker daemon  2.048kB</span><br><span class="line">Step 1/4 : FROM centos</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 300e315adb2f</span></span><br><span class="line">Step 2/4 : VOLUME [&quot;volume01&quot;,&quot;volume02&quot;]</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> 26da05b75834</span></span><br><span class="line">Removing intermediate container 26da05b75834</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 5ae4812f35a4</span></span><br><span class="line">Step 3/4 : CMD echo &quot;---end---&quot;</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> 29c52fec2f47</span></span><br><span class="line">Removing intermediate container 29c52fec2f47</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> cb1793533f3d</span></span><br><span class="line">Step 4/4 : CMD /bin/bash</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> c4bc1543fe44</span></span><br><span class="line">Removing intermediate container c4bc1543fe44</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> c635584bb2a8</span></span><br><span class="line">Successfully built c635584bb2a8</span><br><span class="line">Successfully tagged zuo/centos:1.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(base) root@linux:/data2/docker-volume# docker images</span><br><span class="line">REPOSITORY      TAG       IMAGE ID       CREATED          SIZE</span><br><span class="line">zuo/centos      1.0       c635584bb2a8   59 seconds ago   209MB</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建dockerfile文件，名字可以随机</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 文件中的内容 指令(大写) 参数</span></span><br><span class="line">FROM centos</span><br><span class="line">每个命令就是镜像的一层</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/06/9LG7RVIYzOsK4Sc.png" alt=""></p><p>这个卷是匿名挂载，一定有外部的目录</p><p><img src="https://i.loli.net/2021/08/06/g9H4moazDAIJUTj.png" alt=""></p><h3 id="数据卷容器"><a href="#数据卷容器" class="headerlink" title="数据卷容器"></a>数据卷容器</h3><p>两个mysql同步数据 —volumes-from</p><p><img src="https://i.loli.net/2021/08/06/nZx8FVqjUbBS1I2.png" alt=""></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动3个容器，通过我们刚才自己的写镜像启动</span></span><br><span class="line">(base) root@linux:/data2/docker-volume# docker run -it --name docker01 zuo/centos:1.0</span><br><span class="line"></span><br><span class="line">(base) root@linux:/data2/docker-volume# docker run -it --name docker02 --volumes-from docker01 zuo/centos:1.0</span><br></pre></td></tr></table></figure><p>容器之间配置信息的传递，数据卷容器的生命周期一直持续到没有容器为止</p><p>一旦持久化到了本地，本地的数据是不会删除的。</p><h2 id="DockerFile"><a href="#DockerFile" class="headerlink" title="DockerFile"></a>DockerFile</h2><p>dockerfile 是用来构建docker镜像的文件，命令参数脚本</p><p>构建步骤：</p><ul><li>编写一个dockerfile文件</li><li>docker build构建成为一个镜像</li><li>docker run 运行镜像</li><li>docker push 发布镜像(docker hub，阿里云镜像仓库)</li></ul><h3 id="构建过程"><a href="#构建过程" class="headerlink" title="构建过程"></a>构建过程</h3><p>基础知识</p><ul><li>每个保留关键字指令都是大写字母</li><li>执行从上到下顺序执行</li><li>每个指令都会创建提交一个新的镜像层</li></ul><p><img src="https://i.loli.net/2021/08/06/3JdmShcx4r6znqw.png" alt=""></p><p>dockerfile是面向开发的，我们以后要发布项目做镜像，要写。</p><h3 id="指令"><a href="#指令" class="headerlink" title="指令"></a>指令</h3><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span>          <span class="comment"># 基础镜像，一切从这里开始构建</span></span><br><span class="line"><span class="keyword">MAINTAINER</span>    <span class="comment"># 镜像是谁写的，姓名+邮箱</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash">           <span class="comment"># 镜像构建时要运行的命令</span></span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash">           <span class="comment"># 步骤，添加内容</span></span></span><br><span class="line">WORKERDIR     <span class="comment"># 镜像的工作目录</span></span><br><span class="line"><span class="keyword">VOLUME</span><span class="bash">         <span class="comment"># 挂载的目录</span></span></span><br><span class="line"><span class="keyword">EXPOSE</span>         <span class="comment"># 暴露端口</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash">            <span class="comment"># 指定容器启动时需要运行的命令,只有最后一个会生效，可被替代</span></span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash">     <span class="comment"># 指定容器启动时需要运行的命令，可以追加命令</span></span></span><br><span class="line"><span class="keyword">ONBUILD</span>        <span class="comment"># 当构建一个被继承 Dockerfile </span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash">           <span class="comment">#  类似ADD 将文件拷贝到镜像中</span></span></span><br><span class="line"><span class="keyword">ENV</span>            <span class="comment"># 构建的时候设置环境变量</span></span><br></pre></td></tr></table></figure><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ubuntu:<span class="number">18.04</span>  <span class="comment"># 指定基础镜像 如果为scratch代表从下一行开始是镜像的第一层</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&#x27;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&#x27;</span> &gt; /usr/share/nginx/html/index.html <span class="comment"># RUN指令用来执行命令，每一行代表新建docker的一个layer</span></span></span><br><span class="line"><span class="comment">#能在一个layer内执行的指令就通过&amp;&amp; 进行联接，并可应用shell中的换行符\</span></span><br><span class="line"><span class="comment">#在dockerfile每层都要检查，下载，展开的多余文件，以及缓存等能删除的尽量都去掉</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> <span class="comment">#COPY 指令将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置。</span></span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> package.json /usr/src/app/ <span class="comment"># 将当前上下文路径的json文件复制到image的指定路径下</span></span></span><br><span class="line"></span><br><span class="line">AND <span class="comment">#丰富了COPY的功能，但是会降低构件image速度，如果不需要自动解压缩，则不推荐使用该指令</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> <span class="comment"># ？？？？？？？？？ 还没理解</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> <span class="comment"># 当存在 ENTRYPOINT 后，CMD 的内容将会作为参数传给ENTRYPOINT，从而达到了我们预期的效果。</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> <span class="comment">#用来设置环境变量  ENV &lt;key&gt; &lt;value&gt; 或 ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;...</span></span><br><span class="line"><span class="keyword">ENV</span> VERSION=<span class="number">1.0</span> DEBUG=on \</span><br><span class="line">    NAME=<span class="string">&quot;Happy ONE&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> LD_LIBRARY_PATH=\</span><br><span class="line">$LD_LIBRARY_PATH:\</span><br><span class="line">$NAME/alpha</span><br><span class="line"></span><br><span class="line"><span class="keyword">ARG</span> <span class="comment"># ARG &lt;参数名&gt;[=&lt;默认值&gt;] Dockerfile 中的 ARG 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令 docker build 中用 --build-arg &lt;参数名&gt;=&lt;值&gt; 来覆盖</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ARG</span> DOCKER_USERNAME=library <span class="comment"># 注意：在FROM之前定义的ARG参数，会消失，在FROM后需要重新定义</span></span><br><span class="line"><span class="comment"># ARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。但是不要因此就使用 ARG 保存密码之类的信息，因为 docker history 还是可以看到所有值的。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">VOLUME</span><span class="bash"> <span class="comment"># 用于指定image启动时挂载到容器中的默认卷，而不是写入容器存储层</span></span></span><br><span class="line"><span class="keyword">VOLUME</span><span class="bash"> /data <span class="comment"># VOLUME [&quot;&lt;路径1&gt;&quot;, &quot;&lt;路径2&gt;&quot;...] 或 VOLUME &lt;路径&gt;</span></span></span><br><span class="line">在image启动时可替换</span><br><span class="line">docker <span class="keyword">run</span><span class="bash"> -d -v mydata:/data xxxx <span class="comment">#其中的 -v mydata:/data 就是挂载宿主机的卷到容器内</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="comment"># EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...] EXPOSE 指令是声明容器运行时提供服务的端口，这只是一个声明，在容器运行时并不会因为这个声明应用就会开启这个端口的服务</span></span><br><span class="line"><span class="comment"># 在 Dockerfile 中写入这样的声明有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> <span class="comment"># WORKDIR &lt;工作目录路径&gt; 使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">USER</span>  <span class="comment"># USER &lt;用户名&gt;[:&lt;用户组&gt;] 指定当前用户</span></span><br><span class="line"><span class="keyword">HEALTHCHECK</span></span><br><span class="line"><span class="bash">ONBUILD</span></span><br><span class="line">LEBEL</span><br><span class="line"><span class="keyword">SHELL</span><span class="bash"> <span class="comment">#SHELL 指令可以指定 RUN ENTRYPOINT CMD 指令的 shell，Linux 中默认为 [&quot;/bin/sh&quot;, &quot;-c&quot;]   </span></span></span><br><span class="line">Dockerfile 多阶段构建</span><br></pre></td></tr></table></figure><blockquote><p>创建一个字节的centos</p></blockquote><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1 编写dockerfile文件</span></span><br><span class="line"><span class="keyword">FROM</span> centos</span><br><span class="line"><span class="keyword">MAINTAINER</span> zuo&lt;com&gt;</span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> MYPATH /usr/local</span><br><span class="line">WORKERDIR $MYPATH</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> yum -y install vim  <span class="comment"># 你想让他干啥</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">80</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> <span class="built_in">echo</span> <span class="variable">$MYPATH</span></span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&quot;----end----&quot;</span></span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> /bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2通过这个文件构建镜像</span></span><br><span class="line">docker build -f mydockerfile-centos -t mycentos:<span class="number">0.1</span> .</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3测试运行</span></span><br><span class="line">docker <span class="keyword">run</span><span class="bash"> -it mycentos</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看docker镜像构建历史</p><p><img src="https://i.loli.net/2021/08/06/VuB78orZcqWGS5v.png" alt=""></p><blockquote><p>CMD 和 ENTRYPOINT 的区别</p></blockquote><p><img src="https://i.loli.net/2021/08/06/rpFZ8xUYwROiauC.png" alt=""></p><p>ENTRYPOINT 是可以追加命令的</p><h3 id="做一个tomcat镜像"><a href="#做一个tomcat镜像" class="headerlink" title="做一个tomcat镜像"></a>做一个tomcat镜像</h3><ul><li>准备镜像文件，tomcat压缩包，jdk的压缩包</li><li>编写dockerfile文件，官方命名 Dockerfile，build会自动寻找这个文件，就不需要-f 指定了</li></ul><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> centos</span><br><span class="line"><span class="keyword">MAINTAINER</span> zuo&lt;com&gt;</span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> readme.txt /usr/<span class="built_in">local</span>/readme.txt</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> jdk-8ull-linux-x64.tar.gz /usr/<span class="built_in">local</span>/   <span class="comment"># 会自动解压</span></span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> apache-tomcat-9.0.22.tar.gz /usr/<span class="built_in">local</span>/ </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash">  yum -y install vim</span></span><br><span class="line"><span class="keyword">ENV</span> MYPATH /usr/local</span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> <span class="variable">$MYPATH</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> JAVA_HOME /<span class="keyword">user</span>/local/jdk1.<span class="number">8.0</span>_11</span><br><span class="line"><span class="keyword">ENV</span> CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line"><span class="keyword">ENV</span> CLASSPATH_HOME /usr/local/apache-tomcat-<span class="number">9.0</span>.<span class="number">22</span></span><br><span class="line"><span class="keyword">ENV</span> CLASSPATH_BASH /usr/local/apache-tomcat-<span class="number">9.0</span>.<span class="number">22</span></span><br><span class="line"><span class="keyword">ENV</span> PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin</span><br><span class="line"></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">8080</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> /usr/<span class="built_in">local</span>/apache-tomcat-9.0.22/bin/startup.sh &amp;&amp; tail -F /usr/<span class="built_in">local</span>/apache-tomcat-9.0.22/bin/logs</span></span><br></pre></td></tr></table></figure><p>构建镜像</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker build -t diytomcat .</span><br><span class="line"></span><br><span class="line">docker <span class="keyword">run</span><span class="bash"> -d -p 9090:8080 --name zuotomcat -v /home/zuo/build/tomcat/<span class="built_in">test</span>:/usr/<span class="built_in">local</span>/apache-tomcat-9.0.22/webapps/<span class="built_in">test</span> -v /home/zuo/build/tomcat/tomcatlogs/:/usr/<span class="built_in">local</span>/apache-tomcat-9.0.22/logs diytomcat</span></span><br></pre></td></tr></table></figure><h2 id="发布自己的镜像"><a href="#发布自己的镜像" class="headerlink" title="发布自己的镜像"></a>发布自己的镜像</h2><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss<span class="comment"># docker login --help</span></span><br><span class="line"></span><br><span class="line">Usage:  docker login [OPTIONS] [SERVER]</span><br><span class="line"></span><br><span class="line">Log in to a Docker registry.</span><br><span class="line">If no server is specified, the default is defined by the daemon.</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -p, --password string   Password</span><br><span class="line">      --password-stdin    Take the password <span class="keyword">from</span> stdin</span><br><span class="line">  -u, --username string   Username</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">(base) root@linux:/home/cpss<span class="comment"># docker login -u zzuuoo666</span></span><br><span class="line">Password: </span><br><span class="line">WARNING! Your password will be stored unencrypted in /root/.docker/config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/login/<span class="comment">#credentials-store</span></span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker push zzuuoo666/diytomcat:<span class="number">1.0</span> <span class="comment"># 就可以了 不加信息可能会被拒绝</span></span><br><span class="line"><span class="comment"># 如果镜像上传过被拒绝，可以添加一个tag</span></span><br><span class="line">docker tag ID zzuuoo666/tomcat:<span class="number">1.0</span></span><br><span class="line">docker push zzuuoo666/tomcat:<span class="number">1.0</span></span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><img src="https://i.loli.net/2021/08/06/VmDSgJkyvzeHXtZ.png" alt=""></p><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Docker学习-三&quot;&gt;&lt;a href=&quot;#Docker学习-三&quot; class=&quot;headerlink&quot; title=&quot;Docker学习(三)&quot;&gt;&lt;/a&gt;Docker学习(三)&lt;/h1&gt;&lt;h2 id=&quot;可视化&quot;&gt;&lt;a href=&quot;#可视化&quot; class=&quot;head</summary>
      
    
    
    
    
    <category term="Docker" scheme="http://example.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker学习(二)例子练习</title>
    <link href="http://example.com/2021/08/01/Docker%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E4%BE%8B%E5%AD%90%E7%BB%83%E4%B9%A0/"/>
    <id>http://example.com/2021/08/01/Docker%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E4%BE%8B%E5%AD%90%E7%BB%83%E4%B9%A0/</id>
    <published>2021-08-01T10:37:05.000Z</published>
    <updated>2021-08-05T11:34:55.748Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker学习-二-例子练习"><a href="#Docker学习-二-例子练习" class="headerlink" title="Docker学习(二)例子练习"></a>Docker学习(二)例子练习</h1><h2 id="部署Nginx"><a href="#部署Nginx" class="headerlink" title="部署Nginx"></a>部署Nginx</h2><ul><li>搜索镜像去docker hub上</li><li>下载镜像 docker pull nginx</li><li>docker run -d 后台运行 —name nginx01 -p 10024:80</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;home&#x2F;cpss# docker run -d --name nginx01 -p 10024:80 nginx</span><br><span class="line">84960293d8409dc9f7e70be88027c2149ece57d7cf02dc4d71eb81fe1651fc96</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;home&#x2F;cpss# docker ps</span><br><span class="line">CONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS                                     NAMES</span><br><span class="line">84960293d840   nginx     &quot;&#x2F;docker-entrypoint.…&quot;   21 seconds ago   Up 20 seconds   0.0.0.0:10024-&gt;80&#x2F;tcp, :::10024-&gt;80&#x2F;tcp   nginx01</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;home&#x2F;cpss# curl localhost:10024</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;&#x2F;title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;&#x2F;style&gt;</span><br><span class="line">&lt;&#x2F;head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;&#x2F;h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;&#x2F;p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;nginx.org&#x2F;&quot;&gt;nginx.org&lt;&#x2F;a&gt;.&lt;br&#x2F;&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;nginx.com&#x2F;&quot;&gt;nginx.com&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;</span><br><span class="line">&lt;&#x2F;body&gt;</span><br><span class="line">&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure><p>-p 暴露端口的概念</p><p><img src="https://i.loli.net/2021/08/04/4Bx8PzGlrD1T6vA.png" alt=""></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it nginx01 /bin/bash 进入容器</span><br><span class="line">root@84960293d840:/# whereis nginx</span><br><span class="line">nginx: /usr/sbin/nginx /usr/lib/nginx /etc/nginx /usr/share/nginx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(base) root@localhost:/home/cpss# docker ps</span><br><span class="line">CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS                                     NAMES</span><br><span class="line">84960293d840   nginx     &quot;/docker-entrypoint.…&quot;   8 minutes ago   Up 8 minutes   0.0.0.0:10024-&gt;80/tcp, :::10024-&gt;80/tcp   nginx01</span><br><span class="line">(base) root@localhost:/home/cpss# docker stop 84960293d840</span><br><span class="line">84960293d840</span><br></pre></td></tr></table></figure><p>思考：每次改动nginx配置文件，都需要进入容器内部，十分麻烦</p><p>可以在容器外部提供一个映射路径，达到在容器修改文件名，内部容器就可以自动修改。</p><p>这个技术是  -v 数据卷技术  </p><h2 id="部署-ES-Kibana"><a href="#部署-ES-Kibana" class="headerlink" title="部署 ES+Kibana"></a>部署 ES+Kibana</h2><p>ES暴露端口很多，也耗内存，数据一般需要放到安全目录，挂载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> --net somenetwork 网络配置</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --rm 用完就删掉</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 elasticsearch 比较耗内存</span></span><br><span class="line">docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; elasticsearch:7.14.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CONTAINER ID   NAME            CPU %     MEM USAGE / LIMIT     MEM %     NET I/O           BLOCK I/O       PIDS</span><br><span class="line">a9057d9c6e50   elasticsearch   2.54%     32.49GiB / 125.8GiB   25.83%    11.1kB / 1.94kB   100MB / 292MB   98</span><br><span class="line"></span><br><span class="line">(base) root@linux:/home/cpss# curl localhost:9200</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot; : &quot;a9057d9c6e50&quot;,</span><br><span class="line">  &quot;cluster_name&quot; : &quot;docker-cluster&quot;,</span><br><span class="line">  &quot;cluster_uuid&quot; : &quot;lkLPT_ssQ2CV30B55gn4bg&quot;,</span><br><span class="line">  &quot;version&quot; : &#123;</span><br><span class="line">    &quot;number&quot; : &quot;7.14.0&quot;,</span><br><span class="line">    &quot;build_flavor&quot; : &quot;default&quot;,</span><br><span class="line">    &quot;build_type&quot; : &quot;docker&quot;,</span><br><span class="line">    &quot;build_hash&quot; : &quot;dd5a0a2acaa2045ff9624f3729fc8a6f40835aa1&quot;,</span><br><span class="line">    &quot;build_date&quot; : &quot;2021-07-29T20:49:32.864135063Z&quot;,</span><br><span class="line">    &quot;build_snapshot&quot; : false,</span><br><span class="line">    &quot;lucene_version&quot; : &quot;8.9.0&quot;,</span><br><span class="line">    &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;,</span><br><span class="line">    &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;tagline&quot; : &quot;You Know, for Search&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 增加内存限制，修改卑职文件 -e 环境修改</span></span><br><span class="line">docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; -e ES_JAVA_OPTS=&quot;-Xms64m -Xmx512m&quot;   elasticsearch:7.14.0</span><br><span class="line"></span><br><span class="line">CONTAINER ID   NAME             CPU %     MEM USAGE / LIMIT     MEM %     NET I/O       BLOCK I/O         PIDS</span><br><span class="line">e40105c39e81   elasticsearch1   281.21%   676.5MiB / 125.8GiB   0.53%     2.84kB / 0B   26.9MB / 1.22MB   103</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/05/PiIaTA4C1DsMZz3.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Docker学习-二-例子练习&quot;&gt;&lt;a href=&quot;#Docker学习-二-例子练习&quot; class=&quot;headerlink&quot; title=&quot;Docker学习(二)例子练习&quot;&gt;&lt;/a&gt;Docker学习(二)例子练习&lt;/h1&gt;&lt;h2 id=&quot;部署Nginx&quot;&gt;&lt;a h</summary>
      
    
    
    
    
    <category term="Docker" scheme="http://example.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker学习(一)</title>
    <link href="http://example.com/2021/07/30/Docker%E5%AD%A6%E4%B9%A0-%E4%B8%80/"/>
    <id>http://example.com/2021/07/30/Docker%E5%AD%A6%E4%B9%A0-%E4%B8%80/</id>
    <published>2021-07-30T01:27:56.000Z</published>
    <updated>2021-08-01T10:36:24.367Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker学习-一"><a href="#Docker学习-一" class="headerlink" title="Docker学习(一)"></a>Docker学习(一)</h1><p>文档：<a href="https://docs.docker.com/">https://docs.docker.com/</a></p><p>Hub : <a href="https://hub.docker.com/">https://hub.docker.com/</a></p><h2 id="路线"><a href="#路线" class="headerlink" title="路线"></a>路线</h2><ul><li>Docker概述</li><li>Docker安装</li><li>Docker命令</li><li><ul><li>镜像命令</li><li>容器命令</li><li>操作命令</li><li>……</li></ul></li><li>Docker镜像</li><li>容器数据卷</li><li>DockerFile</li><li>Docker网络原理</li><li>Docker Compose</li><li>Docker Swarm</li><li>CI\CD Jenkins</li></ul><h2 id="Docker概述"><a href="#Docker概述" class="headerlink" title="Docker概述"></a>Docker概述</h2><p>Docker为什么会出现？</p><blockquote><p>环境配置十分麻烦，每个机器都要部署环境，很难跨平台，集群环境更浪费时间。项目能不能带上环境打包(镜像)。</p></blockquote><p>能干嘛？</p><blockquote><p>之前的虚拟机技术,浪费资源比较多</p></blockquote><p><img src="https://i.loli.net/2021/07/30/cG1EzWbCX5ArsTf.png" alt=""></p><p>缺点：</p><ul><li>资源占用多</li><li>冗余步骤多</li><li>启动很慢</li></ul><blockquote><p>容器化技术</p></blockquote><p>不是模拟一个完整的操作系统</p><p><img src="https://i.loli.net/2021/07/30/MojI6CL2lqRapYW.png" alt=""></p><p>不同之处：</p><ul><li>传统虚拟机，虚拟出一套硬件，运行一个完整的操作系统，然后在这个系统上运行安装软件</li><li>容器的应用直接运行在宿主机上的内核中，容器是没有自己的内核的，没有虚拟硬件，比较轻便</li><li>每个容器间是互相隔离的，每个容器内都有一共自己的文件系统，互不影响。</li></ul><blockquote><p>DevOps（开发 运维）</p></blockquote><p>更快速的交付和部署</p><p>传统：一堆帮助文档，安装程序</p><p>Docker: 打包镜像发布测试，一键运行</p><p>更便捷的升级和扩容缩容，更高效的计算资源利用，测试环境都高度一致。</p><p>Docker是内核级别的虚拟化，可以再一个物理机上运行很多的容器实例。</p><h2 id="Docker-基本组成"><a href="#Docker-基本组成" class="headerlink" title="Docker 基本组成"></a>Docker 基本组成</h2><p><img src="https://i.loli.net/2021/07/30/XMPGFCdjvi96tey.png" alt=""></p><p>从左到右，依次是客户端、服务器和仓库。</p><ul><li><p>镜像（Image）：docker镜像就好比是一个模板，可以通过这个模板来创建容器服务。如：tomcat镜像—-&gt;run—-&gt;tomcat01容器。通过这个镜像可以创建多个容器，最终服务运行或者项目运行就是在容器中的</p></li><li><p>容器（Containers）：Docker利用容器技术，可以独立运行一个或者一组应用，通过镜像来创建。启动，</p><p>停止，删除基本命令。目前可把这个容器简单理解为就是一个简易的linux系统。</p></li><li><p>仓库（Repository）：存放镜像的地方。分为公有仓库和私有仓库，和GitHub差不多。Docker hub默认是国外的，可以配阿里云镜像加速。</p></li></ul><h2 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h2><p><img src="https://i.loli.net/2021/07/30/oyDtNArhC1qmlXV.png" alt=""></p><p>如何查看hello world镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;home&#x2F;cpss# docker images</span><br><span class="line">REPOSITORY      TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">hello-world     latest    d1165f221234   4 months ago    13.3kB</span><br><span class="line">studyfang&#x2F;hgn   latest    37553493935b   10 months ago   8.88GB</span><br></pre></td></tr></table></figure><p>docker默认工作路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;home&#x2F;cpss# ls &#x2F;var&#x2F;lib&#x2F;docker&#x2F;</span><br><span class="line">buildkit  containers  image  network  overlay2  plugins  runtimes  swarm  tmp  trust  volumes</span><br></pre></td></tr></table></figure><h2 id="镜像加速"><a href="#镜像加速" class="headerlink" title="镜像加速"></a>镜像加速</h2><p>创建或修改 /etc/docker/daemon.json 文件，修改为如下形式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [</span><br><span class="line">    &quot;https:&#x2F;&#x2F;registry.docker-cn.com&quot;,</span><br><span class="line">    &quot;http:&#x2F;&#x2F;hub-mirror.c.163.com&quot;,</span><br><span class="line">    &quot;https:&#x2F;&#x2F;docker.mirrors.ustc.edu.cn&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;etc&#x2F;docker# vim daemon.json</span><br><span class="line">(base) root@localhost:&#x2F;etc&#x2F;docker# systemctl daemon-reload</span><br><span class="line">(base) root@localhost:&#x2F;etc&#x2F;docker# systemctl restart docker</span><br><span class="line">(base) root@localhost:&#x2F;etc&#x2F;docker# systemctl status docker</span><br></pre></td></tr></table></figure><p>使用docker info 查看镜像改变。</p><p><img src="https://i.loli.net/2021/07/30/AJiXGsvMfug9jc5.png" alt=""></p><h2 id="run流程原理"><a href="#run流程原理" class="headerlink" title="run流程原理"></a>run流程原理</h2><p> <img src="https://i.loli.net/2021/07/30/6jbSgOziPhAYNkd.png" alt=""></p><p>docker是怎么工作的？</p><blockquote><p>docker是一个client-server结构的系统，docker的守护进程运行在主机上，通过socket从客户端访问。</p><p>docker server 接收到docker client的指令就会执行这个命令</p></blockquote><p><img src="https://i.loli.net/2021/07/30/vFZk7yGAHVgfad1.png" alt=""></p><p>docker为什么比VM快？</p><blockquote><p>docker有比虚拟机更少的抽象层。</p><p>docker利用的是宿主机的内核，vm需要Guest OS</p></blockquote><p><img src="https://i.loli.net/2021/07/30/wormWUzExPMNBsh.png" style="zoom:150%;" /></p><p>所以新建一个容器的时候，docker不需要向虚拟机一样重新加载一个操作系统内核。避免引导操作，虚拟机是加载GuestOS，docker是利用宿主机的操作系统，省略了这个复杂的过程。</p><p><img src="https://i.loli.net/2021/07/30/ode6RsjDH1Y5yF3.png" style="zoom:150%;" /></p><h2 id="Docker的常用命令"><a href="#Docker的常用命令" class="headerlink" title="Docker的常用命令"></a>Docker的常用命令</h2><h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker version     <span class="comment"># docker的版本信息</span></span><br><span class="line">docker info <span class="comment"># 显示docker的系统信息，包括镜像和容器的数量</span></span><br><span class="line">docker 命令 --<span class="built_in">help</span>  <span class="comment"># 帮助命令</span></span><br></pre></td></tr></table></figure></h2><h3 id="镜像命令"><a href="#镜像命令" class="headerlink" title="镜像命令"></a>镜像命令</h3><p>docker images</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:/home/cpss<span class="comment"># docker images</span></span><br><span class="line">REPOSITORY      TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">hello-world     latest    d1165f221234   4 months ago    13.3kB</span><br><span class="line">studyfang/hgn   latest    37553493935b   10 months ago   8.88GB</span><br></pre></td></tr></table></figure><ul><li><p>REPOSITORY 镜像的仓库源</p></li><li><p>TAG 镜像的标签</p></li><li><p>IMAGE ID 镜像的id</p></li><li><p>CREATED 镜像的创建时间</p></li><li><p>SIZE 镜像大小</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Options:</span><br><span class="line">  -a, --all             Show all images (default hides intermediate images)</span><br><span class="line">  -q, --quiet           Only show image IDs</span><br></pre></td></tr></table></figure><p>docker search 搜索镜像</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:/home/cpss# docker search hotpotqa</span><br><span class="line">NAME                                  DESCRIPTION                     STARS     OFFICIAL   AUTOMATED</span><br><span class="line">qipeng/hotpotqa-eval                                                  0                    </span><br><span class="line">studyfang/hotpotqa                                                    0                    </span><br><span class="line">qipeng/hotpotqa-base                                                  0                    </span><br><span class="line">tuming1990/hotpotqa-docker                                            0                    </span><br><span class="line">hamishivi/hotpotqa-base               Hotpotqa with extra packages.   0                    </span><br><span class="line">qipeng/hotpotqa_submission_cuda10.2                                   0                    </span><br><span class="line">tswings2018/hotpotqa                  by deng                         0        </span><br></pre></td></tr></table></figure><p>docker pull</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># 下载镜像 docker pull 镜像名[:tag]</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/07/30/r9qWyoDSOTk7QKh.png" alt=""></p><p>docker rmi 删除镜像</p><p>可通过id 或者 名称来删</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi -f 镜像id</span><br></pre></td></tr></table></figure><h3 id="容器命令"><a href="#容器命令" class="headerlink" title="容器命令"></a>容器命令</h3><p>有了镜像才可以创建容器</p><p>这里下载一个centos镜像来测试学习</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull centos</span><br></pre></td></tr></table></figure><p>新建容器并启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">docker run [可选参数] image</span><br><span class="line"><span class="meta">#</span><span class="bash"> 参数说明</span></span><br><span class="line">--name=&quot;Name&quot; 容器名字  tomcat01 tomcat02 用来区分容器</span><br><span class="line">-d            后台方式运行 nohup</span><br><span class="line">-it           使用交互方式运行，进入容器查看内容</span><br><span class="line">-p            指定容器的端口  ip:主机端口:容器端口 主机端口:容器端口(常用)   容器端口</span><br><span class="line">-P随机指定端口</span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试 启动并进入容器</span></span><br><span class="line">(base) root@linux:/home/cpss# docker run -it centos /bin/bash</span><br><span class="line">[root@ef41db25d696 /]# 容器内就是自己的服务器环境</span><br><span class="line"></span><br><span class="line">docker ps # 查看正在运行的容器</span><br><span class="line">docker ps -a # 查看曾经运行过的容器</span><br><span class="line">docker ps -a -n=1 # 显示个数</span><br><span class="line">docker ps -aq # 只显示编号</span><br></pre></td></tr></table></figure><p>退出容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">exit # 直接退出容器并停止</span><br><span class="line">ctrl +p +q # 容器不停止退出</span><br></pre></td></tr></table></figure><p>删除容器</p><p>删除容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker rm 容器id                # 删除指定的容器 不能删除正在运行的容器 -f强制删除</span><br><span class="line">docker rm -f $(docker ps -aq)  # 删除所有的容器</span><br><span class="line"></span><br><span class="line">docker ps -a -q|xargs docker rm # 删除所有的容器</span><br></pre></td></tr></table></figure><p>启动和停止容器的操作</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker start 容器id</span><br><span class="line">docker restart 容器id</span><br><span class="line">docker stop 容器id</span><br><span class="line">docker <span class="built_in">kill</span> 容器</span><br></pre></td></tr></table></figure><h3 id="常用其他命令"><a href="#常用其他命令" class="headerlink" title="常用其他命令"></a>常用其他命令</h3><p>后台启动容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -d centos</span><br><span class="line"><span class="meta">#</span><span class="bash"> 问题 docker ps时发现centos停止了</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 常见的坑，docker 容器使用后台运行，就必须要有一个前台进程。docker发现没有应用就会自动停止。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 容器启动后，发现自己没有提供服务，就会立即停止</span></span><br></pre></td></tr></table></figure><p>查看日志</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss# docker logs --help</span><br><span class="line">Options:</span><br><span class="line">      --details        Show extra details provided to logs</span><br><span class="line">  -f, --follow         Follow log output</span><br><span class="line">      --since string   Show logs since timestamp (e.g. 2013-01-02T13:23:37Z) or relative (e.g. 42m for 42 minutes)</span><br><span class="line">  -n, --tail string    Number of lines to show from the end of the logs (default &quot;all&quot;)</span><br><span class="line">  -t, --timestamps     Show timestamps</span><br><span class="line">      --until string   Show logs before a timestamp (e.g. 2013-01-02T13:23:37Z) or relative (e.g. 42m for 42 minutes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> -tf 显示日志</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --tail number 要显示日志条数</span></span><br><span class="line">docker logs -tf --tail 10 f3c59b35b738</span><br><span class="line"><span class="meta">#</span><span class="bash"> 容器没有日志</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 自己写一段shell</span></span><br><span class="line">(base) root@linux:/home/cpss# docker run -d centos /bin/sh -c &quot;while true;do echo 111;sleep 1;done&quot;</span><br><span class="line">ba0ae87cb0949d44e179f03e2bb3e25a38b394bb98b7aa0f4a1a2b9ad68ca86d</span><br><span class="line">(base) root@linux:/home/cpss# docker ps</span><br><span class="line">CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS        PORTS     NAMES</span><br><span class="line">ba0ae87cb094   centos    &quot;/bin/sh -c &#x27;while t…&quot;   3 seconds ago   Up 1 second             determined_bouman</span><br><span class="line">(base) root@linux:/home/cpss# docker logs -tf --tail 10 ba0ae87cb094</span><br></pre></td></tr></table></figure><p>查看容器中的进程信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker top 容器id</span><br></pre></td></tr></table></figure><p>查看镜像的元数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">docker inspect 容器id</span><br><span class="line"></span><br><span class="line">(base) root@linux:/home/cpss# docker inspect ba0ae87cb094</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;Id&quot;: &quot;ba0ae87cb0949d44e179f03e2bb3e25a38b394bb98b7aa0f4a1a2b9ad68ca86d&quot;,</span><br><span class="line">        &quot;Created&quot;: &quot;2021-08-01T03:10:14.298411164Z&quot;,</span><br><span class="line">        &quot;Path&quot;: &quot;/bin/sh&quot;,</span><br><span class="line">        &quot;Args&quot;: [</span><br><span class="line">            &quot;-c&quot;,</span><br><span class="line">            &quot;while true;do echo 111;sleep 1;done&quot;</span><br><span class="line">        ],</span><br><span class="line">        &quot;State&quot;: &#123;</span><br><span class="line">            &quot;Status&quot;: &quot;exited&quot;,</span><br><span class="line">            &quot;Running&quot;: false,</span><br><span class="line">            &quot;Paused&quot;: false,</span><br><span class="line">            &quot;Restarting&quot;: false,</span><br><span class="line">            &quot;OOMKilled&quot;: false,</span><br><span class="line">            &quot;Dead&quot;: false,</span><br><span class="line">            &quot;Pid&quot;: 0,</span><br><span class="line">            &quot;ExitCode&quot;: 137,</span><br><span class="line">            &quot;Error&quot;: &quot;&quot;,</span><br><span class="line">            &quot;StartedAt&quot;: &quot;2021-08-01T03:10:15.270494437Z&quot;,</span><br><span class="line">            &quot;FinishedAt&quot;: &quot;2021-08-01T03:12:01.287526932Z&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;Image&quot;: &quot;sha256:300e315adb2f96afe5f0b2780b87f28ae95231fe3bdd1e16b9ba606307728f55&quot;,</span><br><span class="line">        .....</span><br><span class="line">]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>进入当前正在运行的容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 我们通常容器都是使用后台方式运行的，需要进入容器，修改一些配置</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 方式1</span></span><br><span class="line">docker exec -it 容器id bashShell</span><br><span class="line"><span class="meta">#</span><span class="bash"> 方式2</span></span><br><span class="line">docker attach 容器id </span><br><span class="line"><span class="meta">#</span><span class="bash"> 区别</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> attach 正在执行的代码 进入正在执行的终端，不会启动新的进程</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">exec</span> 进入容器后开启一个新的终端，可以在里面操作</span></span><br></pre></td></tr></table></figure><p>从容器内拷贝文件到主机上</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 容器停止也可以拷贝，容器在数据就在</span></span><br><span class="line">docker cp 容器id:容器内路径 目的的主机路径</span><br><span class="line"><span class="comment"># 拷贝是一个手动过程，以后可以使用 -v 卷的技术 可以实现自动同步</span></span><br></pre></td></tr></table></figure><h3 id="命令小结"><a href="#命令小结" class="headerlink" title="命令小结"></a>命令小结</h3><p><img src="https://i.loli.net/2021/08/01/ha7fdJZE2jnNIOU.png" alt=""></p><p>现在学的是Images 和 Cotainer里的命令，其他的还没学</p><p><img src="https://i.loli.net/2021/08/01/FH1ZyqXoS3wx2Dg.png" alt=""></p><p><img src="https://i.loli.net/2021/08/01/YTrjW8M1c3I5sQ9.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Docker学习-一&quot;&gt;&lt;a href=&quot;#Docker学习-一&quot; class=&quot;headerlink&quot; title=&quot;Docker学习(一)&quot;&gt;&lt;/a&gt;Docker学习(一)&lt;/h1&gt;&lt;p&gt;文档：&lt;a href=&quot;https://docs.docker.com/</summary>
      
    
    
    
    
    <category term="Docker" scheme="http://example.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>HotpotQA Submission Guide</title>
    <link href="http://example.com/2021/07/28/HotpotQA-Submission-Guide/"/>
    <id>http://example.com/2021/07/28/HotpotQA-Submission-Guide/</id>
    <published>2021-07-28T09:12:56.000Z</published>
    <updated>2021-08-06T07:43:46.495Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HotpotQA-Submission-Guide"><a href="#HotpotQA-Submission-Guide" class="headerlink" title="HotpotQA Submission Guide"></a>HotpotQA Submission Guide</h1><p>记录如何提交模型在HotpotQA test</p><h2 id="codalab安装与注册"><a href="#codalab安装与注册" class="headerlink" title="codalab安装与注册"></a>codalab安装与注册</h2><p>先去注册 <a href="https://worksheets.codalab.org/">https://worksheets.codalab.org/</a></p><p>首先安装codalab</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install codalab -U </span><br></pre></td></tr></table></figure><p>如果ERROR: Cannot uninstall ‘PyYAML’. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.</p><p>使用 pip install codalab -U —ignore-installed PyYAML</p><p>Codalab wiki ： <a href="https://github.com/codalab/codalab-worksheets/wiki">https://github.com/codalab/codalab-worksheets/wiki</a></p><p>注册安装完成后可以再命令行登录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cl work</span><br><span class="line">Requesting access at https://worksheets.codalab.org</span><br><span class="line">Username: guest1</span><br><span class="line">Password:</span><br><span class="line">Currently on worksheet https://worksheets.codalab.org::home-guest1(0x39729afdca6140869a11e055e4cc0649).</span><br></pre></td></tr></table></figure><p><code>cl work</code>命令的意思就是切换工作表（worksheet），默认的工作表指向主页工作表 （<code>home-&lt;username&gt;</code>）。</p><h2 id="先一个例子提交Hotpot-QA的baseline"><a href="#先一个例子提交Hotpot-QA的baseline" class="headerlink" title="先一个例子提交Hotpot QA的baseline"></a>先一个例子提交Hotpot QA的baseline</h2><p>distractor setting 是需要提交代码的，full wiki不需要。先主要攻克 distractor setting吧</p><p>尝试完baseline再上传我自己的模型。</p><blockquote><p>你的分数想要在排行榜上出现，需要预留最多一个月的时间。</p></blockquote><p>在干扰项设置中，要求您将代码提交给Codalab，并根据隐藏的测试集对其进行评估。您应该首先确保您的代码能够正确生成dev的输出和评估结果，以便可以更容易地将您的设置转移到测试集。下面，提供一个提交baseline模型的示例。</p><h3 id="Step-1-Preparing-code-and-data"><a href="#Step-1-Preparing-code-and-data" class="headerlink" title="Step 1: Preparing code and data"></a>Step 1: Preparing code and data</h3><p>首先将基线模型的GitHub存储库克隆到Codalab包中（在codalab上代码是公开的，想要不公开。。）</p><p>在命令行中运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cl run --request-network <span class="string">&#x27;git clone https://github.com/hotpotqa/hotpot.git&#x27;</span> -n repo</span><br></pre></td></tr></table></figure><p>—request-network：需要网络环境， -n是添加别名为repo，以后可以更容易地引用它(而不是每次都使用长UUID)。</p><p>注意这里git克隆的要是https的ssh的路径会失败。</p><p>成功后刷新网页控制台：</p><p><img src="https://i.loli.net/2021/07/28/mo35I4yMLEDawFH.png" alt=""></p><p>然后，我们上传对训练集进行预处理后生成的词汇映射文件。</p><p>创建包含所有必要预处理文件的mappings.zip文件，即idx2char.json、idx2word.json、char2idx.json、word2idx.json、char_emb.json和word_emb.json。这个是baseline运行所需要的。</p><p>作者提供了下载mappings.zip的下载地址：<a href="http://curtis.ml.cmu.edu/datasets/hotpot/mappings.zip">http://curtis.ml.cmu.edu/datasets/hotpot/mappings.zip</a></p><p>要上传数据到Codalab CLI，只需运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cl upload mappings.zip -n mappings</span><br></pre></td></tr></table></figure><p>完全上传后，Codalab会为您解压zip文件。</p><p>当然，还需要上传预先训练好的模型文件。我们已经准备好了预先训练好的文件model.pt。</p><p>下载地址：<a href="http://curtis.ml.cmu.edu/datasets/hotpot/model.pt">http://curtis.ml.cmu.edu/datasets/hotpot/model.pt</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cl upload model.pt -n model</span><br></pre></td></tr></table></figure><p>如果超时就重新执行</p><p><img src="https://i.loli.net/2021/07/28/eDSWJn71hkAIlGr.png" alt=""></p><h3 id="Step-2-Preparing-the-environment"><a href="#Step-2-Preparing-the-environment" class="headerlink" title="Step 2: Preparing the environment"></a>Step 2: Preparing the environment</h3><p>现在基本已经准备好对新的输入进行预测。我们只需要设置代码需要在其中运行的适当环境。</p><blockquote><p>要做到这一点，最简单的方法是使用Docker镜像，我们在<a href="https://hub.docker.com/r/qipeng/hotpotqa-base">qipeng/hotpotqa-base</a>上提供了一个镜像，其中预装了nvidia GPU相关库和Anaconda 3。我们还安装了运行此docker映像中的基线模型所需的所有软件包，这样我们就不必在Codalab包中安装所有东西。</p></blockquote><p>如果确实忘记了环境中的某些内容，也可以在Codalab中轻松设置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cl run -n download_spacy_model --request-docker-image qipeng/hotpotqa-base:gpu --request-network :repo <span class="string">&#x27;cp -r repo/hotpot .; python -m spacy download en&#x27;</span></span><br></pre></td></tr></table></figure><p>注意：由于评估期间禁止使用网络，此捆绑包仅用于演示目的。对于需要下载的软件依赖项，强烈建议下载到您准备的Docker镜像中。</p><p><img src="https://i.loli.net/2021/07/28/JzasC3nmrFNPI1U.png" alt=""></p><h3 id="Step-3-Running-evaluation"><a href="#Step-3-Running-evaluation" class="headerlink" title="Step 3: Running evaluation"></a>Step 3: Running evaluation</h3><p>现在，继续根据刚刚上传的模型进行预测，并评估输出。</p><p>要在dev集上运行上传的基线模型的预测，我们运行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cl run -n predict --request-docker-image qipeng/hotpotqa-base:gpu --request-gpus 1 --request-cpus 4 --request-memory 32g repo:download_spacy_model :mappings input.json:0xbdd8f3 :model <span class="string">&#x27;cp -r repo/hotpot .; cp mappings/* hotpot; mkdir hotpot/model; cp model hotpot/model/model.pt; cp input.json hotpot; cd hotpot; python main.py --mode prepro --data_file input.json --para_limit 2250 --data_split dev; python main.py --mode test --data_split dev --para_limit 2250 --batch_size 24 --init_lr 0.1 --keep_prob 1.0 --sp_lambda 1.0 --save model --prediction_file pred.json; cp pred.json ../;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>让我们看看上面的命令中发生了什么。在第一部分中，我们使用-n predic命名包，并使用指定所需的资源</p><p>—request-docker-image qipeng/hotpotqa-base:gpu</p><p>—request-gpus 1</p><p>—request-cpus 4</p><p>—request-memory 32g</p><p>请注意，您不能在此捆绑包中使用—request-network</p><p>然后指定对其他包的依赖关系。repo:download_spacy_model表示将包download_spacy_model别名为repo</p><p>:mappings指定对捆绑包mapping的依赖关系，而不使用别名。</p><p>input.json:hotpotqa-data//dev_distractor_input_v1.0将输入json文件重命名为input.json，其中包id指向dev json文件。0xbdd8f3</p><p>请不要上传您自己版本的开发集文件并使用它，因为我们依赖官方的开发文件UUID来确定在评估期间用测试集替换什么(如果您使用自己的开发集文件，评估将失败)。</p></blockquote><p>然后，使用一系列cp命令从不同的包复制文件，并以我们的预测脚本可以处理的方式组织它们。</p><p>请注意，可以将每个引用的捆绑包视为当前捆绑包中的一个目录。例如，通过cp -r repo/hotpot。我们将repo捆绑包中的hotot子目录复制到当前捆绑包的“根”目录(开始运行捆绑包中的代码时所在的目录，而不是/root！)。</p><p>然后，我们调用main.py两次，第一次使用—mode prepro预处理dev集，第二次使用—mode test进行预测。如果您使用代码，则可以相应地更改此设置。</p><p>请注意，如果您的代码涉及预训练的特征提取(例如，Elmo或BERT)，则应该将其合并为此处命令的一部分，而不是作为包上传，因为您事先没有访问测试集的权限。（也就是说要预处理测试集的话要在一个捆绑包里进行多次运行python文件吧）</p><p>还要注意，您的模型不应该依赖于键类型和级别来进行预测，因为这些键没有出现在测试集中。</p><p>之后，我们将文件pred.json复制到当前包的“根”目录。请注意，文件名必须命名为pred.json，并且该文件必须放在包的“根”目录下，评估命令才能正常工作。</p><p><img src="https://i.loli.net/2021/07/28/cvhExjYFiz8bka9.png" alt=""></p><p>使用以下命令（将predict替换为您自己的prediction bundle的名称），确保您能够在dev set上评估您的模型而不会遇到任何问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cl macro hotpotqa-utils//dev-eval-distractor-v1.0 predict -n evaluate</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/07/28/EJ1SVlTsnHdZ9rq.png" alt=""></p><p><img src="https://i.loli.net/2021/07/28/jkswd5B198ZbqDi.png" alt=""></p><h3 id="Step-4-Describe-and-tag-your-submission-描述并标记您的提交"><a href="#Step-4-Describe-and-tag-your-submission-描述并标记您的提交" class="headerlink" title="Step 4: Describe and tag your submission 描述并标记您的提交"></a>Step 4: Describe and tag your submission 描述并标记您的提交</h3><p>准备好后，编辑预测捆绑包的说明，以反映在排行榜上显示所需的信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Model name (Affiliation) (single model &#x2F; ensemble) [paper name](paper link) (code link)</span><br></pre></td></tr></table></figure><p>如果您在使用深渊翻滚时愿意，可以使用匿名Anonymous作为您的从属关系，之后可以通过编辑您的深渊翻滚捆绑包的描述来修改它。</p><p>[paper名称] 和(代码链接)部分是可选的 </p><p>请注意，虽然[paper名称]和(paper链接)之间没有空格，因为这会造成代码链接的歧义。以下是一些示例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Baseline Model (Carnegie Mellon University, Stanford University, &amp; Universite de Montreal) (single model) [(Yang, Qi, Zhang, et al. 2018)](https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1809.09600.pdf) (https:&#x2F;&#x2F;github.com&#x2F;hotpotqa&#x2F;hotpot)</span><br><span class="line"></span><br><span class="line">My Awesome Model (Awesome Institute) (ensemble) [(Awesome et al., 2018)](https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1812.12345.pdf)</span><br><span class="line"></span><br><span class="line">PotLuck (Culinary University) (single model) [](https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1901.12345.pdf) (https:&#x2F;&#x2F;github.com&#x2F;potluck&#x2F;potluck)</span><br></pre></td></tr></table></figure><p>第一个示例正是我们用来描述基线模型的。第二个没有代码链接，第三个没有指定论文名称。</p><p>请注意，Codalab在捆绑描述中使用非ASCII字符有问题，因此请避免使用它们。</p><p>要提交您的捆绑包，请使用hotpotqa-diditor-test-submit标记您的预测捆绑包(这可以在Web UI上通过选择捆绑包并修改右侧面板上的标签来完成)，然后向彭琪(pengqi@cs.stanford.edu)发送一封简短的电子邮件，其中包含您的捆绑包UUID(0x后跟32个字符)或指向您的捆绑包的链接(不是您的工作表或工作表UUID！)。</p><p>请确保您的预测所依赖的所有捆绑包都是公开可读的(这是Codalab中的默认可见性)。</p><blockquote><p>重要信息：</p><p>1.请仅在dev集合上执行改善模型性能所需的任何模型选择或消融。不能在测试集上支持同一模型的多个提交。</p><p>2.请避免删除您的Submission捆绑包，即使在填写排行榜条目之后也是如此。这是您更新与您的Submission相关的信息的最佳方式，包括但不限于其名称、隶属关系、纸质链接、代码链接等。</p><p>3.如果你提交了多份报告(单一模型和集成模型)，请确保你的预测捆绑包有不同的名称。例如，predict-single和<code>predict-ensemble</code>。这是唯一一种我们在30天内容纳的多次提交。</p></blockquote><h2 id="1总结"><a href="#1总结" class="headerlink" title="1总结"></a>1总结</h2><ul><li>从github下载代码</li><li>上传模型和需要的数据</li><li>设置环境用docker</li><li>run</li></ul><p><img src="https://i.loli.net/2021/08/01/S6ik5vh4ImFxqCt.png" alt=""></p><p><a href="https://zhuanlan.zhihu.com/p/196343938">https://zhuanlan.zhihu.com/p/196343938</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;HotpotQA-Submission-Guide&quot;&gt;&lt;a href=&quot;#HotpotQA-Submission-Guide&quot; class=&quot;headerlink&quot; title=&quot;HotpotQA Submission Guide&quot;&gt;&lt;/a&gt;HotpotQA Su</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Heterogeneous Graph Transformer for Graph-to-Sequence Learning</title>
    <link href="http://example.com/2021/07/23/Heterogeneous-Graph-Transformer-for-Graph-to-Sequence-Learning/"/>
    <id>http://example.com/2021/07/23/Heterogeneous-Graph-Transformer-for-Graph-to-Sequence-Learning/</id>
    <published>2021-07-23T10:44:21.000Z</published>
    <updated>2021-07-23T13:48:47.720Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Heterogeneous-Graph-Transformer-for-Graph-to-Sequence-Learning"><a href="#Heterogeneous-Graph-Transformer-for-Graph-to-Sequence-Learning" class="headerlink" title="Heterogeneous Graph Transformer for Graph-to-Sequence Learning"></a>Heterogeneous Graph Transformer for Graph-to-Sequence Learning</h1><p>Graph2Seq学习的目的是将图结构的表示转换为单词序列，以便生成文本。</p><p>AMR-to-text是从抽象意义表示(AMR)图中生成文本的任务，其中节点表示语义概念，边表示概念之间的关系。</p><p>传统GNN只考虑了直接相连节点之间的关系，而忽略了远距离节点之间的间接关系。</p><p>Graph2Seq的其他两个和Graph Transformer的论文</p><ul><li><p>Graph transformer for graph-to-sequence learning AAAI 2020</p></li><li><p>Modeling graph structure in transformer for better AMR-to-text gen- eration  EMNLP 2019</p></li></ul><p>使用节点之间的最短关系路径来编码语义关系。但是，它们忽略了关系路径中节点的信息，对直接关系和间接关系没有区别地进行编码。当从直接邻居那里聚集信息时，可能会干扰信息的传播过程。</p><p>作者使用Heterogeneous Graph Transformer来独立地建模原始图的各个子图中的不同关系，包括节点之间的直接关系、间接关系和多种可能的关系。</p><h2 id="Input-Graph-Transformer"><a href="#Input-Graph-Transformer" class="headerlink" title="Input Graph Transformer"></a>Input Graph Transformer</h2><p>为了缓解语料库中的数据稀疏问题，作者将进一步将字节对编码(BPE)引入Levi图。</p><p>将原始节点拆分成多个子词节点。除了添加缺省连接外，我们还在子词之间添加了反向边和自循环边。</p><p>如下图：</p><p><img src="https://i.loli.net/2021/07/23/fswuO1n4J7bptHG.png" alt=""></p><p>例如，图中的单词Country被分割为co@@、un@@、try  它们之间有三种类型的边。</p><p><img src="https://i.loli.net/2021/07/23/LUmRx6udzNeJyAr.png" style="zoom:67%;" /></p><p>该任务一般先将抽象概念图(上图a)，转换成Levi图(上图b)。将AMR图转换为扩展的Levi图，该图可以看作是一个异构图，因为它具有不同类型的边。</p><h2 id="Heterogeneous-Graph-Transformer"><a href="#Heterogeneous-Graph-Transformer" class="headerlink" title="Heterogeneous Graph Transformer"></a>Heterogeneous Graph Transformer</h2><p><img src="https://i.loli.net/2021/07/23/MvH7kQdb2KFwert.png" alt=""></p><p>给定一个经过预处理的扩展Levi图，根据其异构性将扩展Levi图分成多个子图。</p><p>在每个Graph Encoder中，基于其在当前子图中的相邻节点来更新不同子图中的节点表示。然后，将该节点在不同子图中的所有表示组合在一起，以获得其最终表示。</p><h3 id="Graph-Encoder"><a href="#Graph-Encoder" class="headerlink" title="Graph Encoder"></a>Graph Encoder</h3><p>与其他Graph Transformer不同的是仅使用相对位置编码来隐藏结构信息。</p><p>在更新每个节点的表示时，直接屏蔽了非相邻节点的注意力。mask attention $\alpha_{ij}\notin N_i$  ，此外这个作者还尝试用了加性注意力这就和GAT几乎很像了。</p><p>因此，给定输入序列 $x=(x_1,…,x_n)$，每个关注头中表示为 $z_i$ 的节点i的输出表示如下计算：</p><script type="math/tex; mode=display">z_i = \sum_{j\in N_i} \alpha_{ij}(x_j W^V)</script><h3 id="Heterogeneous-Mechanism"><a href="#Heterogeneous-Mechanism" class="headerlink" title="Heterogeneous Mechanism"></a>Heterogeneous Mechanism</h3><p>在多头机制成功的激励下，提出了异质机制。考虑到一个句子，多头注意允许模型隐含地注意到来自不同位置的不同表示子空间的信息。相应地，异构机制使得模型显式地关注不同子图中的信息，对应于图的不同表示子空间，从而增强了模型的编码能力。</p><p>首先将所有的边类型组合成一个单一的边类型，从而得到一个同质连通子图。该连通子图实际上是一个包含原始图中完全连通信息的无向图。除了学习直连关系，还引入了一个完全连通子图来学习间接连接节点之间的隐含关系。</p><p>每个编码层中的输出z计算如下：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  z &= FFN(concat(z^{G^{sub}_1},...,z^{G_M^{sub}})W^O)\\ z_i^{G_m^sub} &= \sum_{j\in N_i^{G^{sub}_m} }\alpha_{ij}(x_jW^V), m\in[1,M]     \end{split}\end{equation}</script><p>$W^O\in R^{Md_z\times d_z}$参数矩阵 </p><p>作者还采用了子层之间的残差连接、FFN以及层归一化。</p><h3 id="Layer-Aggregation"><a href="#Layer-Aggregation" class="headerlink" title="Layer Aggregation"></a>Layer Aggregation</h3><p>编码层之间更好的信息传播可能带来更好的性能。</p><p>因此，我们研究了三种不同的Layer Aggregation方法，如图3所示。</p><p><img src="https://i.loli.net/2021/07/23/gfL3GKQDxzCamqX.png" alt=""></p><p>当更新第 $l$ 层节点的表示时，最近的方法是先聚合邻居，然后将聚合结果与来自 $(l−1)$ 层的节点表示相结合。此策略可视为不同图层之间跳过连接的一种形式。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  z_{N_i}^{(l)} &= AGGREGATE(\{z_j^{(l-1)}, \forall j \in N_i\})\\ z_i^{(l)} &= COMBINE(z_{N_i}^{(l)}, z_i^{(l-1)})    \end{split}\end{equation}</script><p>残差连接是另一种著名的跳跃连接，它使用identity mapping作为组合函数来帮助信号传播，但这些跳跃连接不能独立自适应地调整最后一层表示的邻域大小。</p><p>如果我们为$z_i^{(l)}$ skip一个层，则所有后续的单元例（如使用此表示的$z_i^{(l+j)}$) 都将隐式的使用此skip</p><p>因此，为了有选择地聚合前几层的输出，我们在模型中引入了跳跃体系。</p><p>在编码器的最后一层L，通过concat的方式组合前几个编码层的所有输出，以帮助模型有选择地聚合所有这些中间表示。</p><script type="math/tex; mode=display">z_i^{final} = Concat(z_i^{(L)},...,z_i^{(1)},x_i) W_{jump}</script><p>$W_{jump}\in R^{(Ld_z+d_x)\times d_z}$</p><p>此外，为了更好地改善信息传播，还可以引入稠密连通性。通过密集连接，l层中的节点不仅从第(l−1)层获取输入，而且还从所有前面的层提取信息： </p><script type="math/tex; mode=display">z_i^{(l)} = Concat(z_i^{(l-1)},..,z_i^{(1)},x_i) W^{(l)}_{dense}</script><p>$W^{(l)}_{dense} \in R^{d^{(l)}\times d_z}, d^{(l)}=d_x+d_z\times(l-1)$</p><p><img src="https://i.loli.net/2021/07/23/kqOvxdBz6YrnXj8.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Heterogeneous-Graph-Transformer-for-Graph-to-Sequence-Learning&quot;&gt;&lt;a href=&quot;#Heterogeneous-Graph-Transformer-for-Graph-to-Sequence-Lear</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>Graph Transformer for Graph-to-Sequence Learning</title>
    <link href="http://example.com/2021/07/23/Graph-Transformer-for-Graph-to-Sequence-Learning/"/>
    <id>http://example.com/2021/07/23/Graph-Transformer-for-Graph-to-Sequence-Learning/</id>
    <published>2021-07-23T02:11:03.000Z</published>
    <updated>2021-07-23T05:41:14.752Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Graph-Transformer-for-Graph-to-Sequence-Learning"><a href="#Graph-Transformer-for-Graph-to-Sequence-Learning" class="headerlink" title="Graph Transformer for Graph-to-Sequence Learning"></a>Graph Transformer for Graph-to-Sequence Learning</h1><p>这篇论文应用于在基于抽象语义表示(AMR)的文本生成和基于句法的神经机器翻译，句法机器翻译并入源端语法信息可以提高翻译质量。如图给出了AMR到文本生成的示例。</p><p><img src="https://i.loli.net/2021/07/23/QDi75OPI64YbHgA.png" alt=""></p><p>论文应用Graph Transformer，其与限制近邻之间信息交换的图神经网络不同，Graph Transformer使用显式关系编码，允许两个远距离节点之间的直接通信。它为全局图结构建模提供了一种更有效的方法。</p><p>这篇论文想解决的是打破传统GNN的局部邻接特性，使用高效的全局信息。</p><h2 id="Methed"><a href="#Methed" class="headerlink" title="Methed"></a>Methed</h2><p>对于n个节点的图，以前的图神经网络将节点表示$v_i$计算为输入节点 $i$ 及其所有一阶邻域 $N(i)$的函数。图结构由每个节点表示的感受野隐式反映。然而，这种本地通信设计对于远程信息交换可能是低效的。</p><p>所以引入Graph Transformer，它提供了一种截然不同的范例，可以实现关系感知的全球通信。</p><p><img src="https://i.loli.net/2021/07/23/FtimYg4NB5MTkoS.png" alt=""></p><p>作者提出的是关系增强的全局注意力机制，和Graphromer一样任何节点对之间的关系被描述为它们之间的最短关系路径。</p><h3 id="Graph-Encoder"><a href="#Graph-Encoder" class="headerlink" title="Graph Encoder"></a>Graph Encoder</h3><p>责将输入图形转换为一组相应的节点嵌入。核心问题是如何在允许全连通通信的同时保持图的拓扑结构。</p><p> 作者的想法是将两个节点之间的显式关系表示融入到它们的表示学习中。在标准的多头注意中，元素 $x_i$ 和元素 $x_j$之间的注意分数简单地分别是它们的查询向量和键向量的点积：</p><script type="math/tex; mode=display">s_{ij} = f(x_i,x_j) =x_iW^T_qW_kx_j</script><p>假设我们已经学习了节点i和节点j之间的关系 $r_{ij}$ 的矢量表示，我们将其称为关系编码。</p><script type="math/tex; mode=display">[r_{i\to j};r_{j\to i}] = W_r r_{ij}</script><p>$r<em>{i\to j};r</em>{j\to i}$ 为正向和反向关系编码。</p><p>如果把正反两个关系编码加到节点embedding中，注意力分数计算可以为：</p><script type="math/tex; mode=display">\begin{equation}\begin{split} s_{ij} &= g(x_i, x_j, r_{ij})\\            & = (x_i + r_{i\to j})W_q^TW_k(x_j + r_{j\to i})\\            &= \underbrace{x_iW_q^TW_kx_j}_{(a)} +  \underbrace{x_iW_q^TW_kr_{j\to i}}_{(b)} \\            &+ \underbrace{r_{i\to j}W_q^TW_kx_j}_{(c)} + \underbrace{r_{i\to j}W_q^TW_kr_{j\to i}}_{(d)}    \end{split}\end{equation}</script><p>直观上，等式项意义:</p><ul><li>(a) 捕获纯粹基于内容的content-based addressing,，这是普通注意力机制中的原始term。</li><li>(b) 依赖于源节点的关系偏置。</li><li>(c) 依赖于目标节点的关系偏置。</li><li>(d) 对通用的关系偏差进行编码。</li></ul><p>在这里作者使用的是节点间的最短路径来表示关系。</p><h4 id="Relation-Encoder"><a href="#Relation-Encoder" class="headerlink" title="Relation Encoder"></a>Relation Encoder</h4><p>从概念上讲，关系编码为模型提供了关于应该如何收集和分发信息的全局指导，即在哪里关注。</p><p>对于NLP中的大多数图形结构，边标签传达了相邻节点之间的直接关系(例如，概念到概念所扮演的语义角色，以及两个单词之间的依存关系)。</p><p>作者将这种单跳关系定义扩展到多跳关系推理中，以刻画任意两个节点之间的关系。</p><p>例如第一个图中 want-01 到 girl的最短路径概念为，$\text{want-01} \to^{ARG1} \text{believe-01}\to^{ARG0} girl$ 传达girl是wanted的目标。</p><p>直观地说，两个节点之间的最短路径给出了它们之间最密切且可以说是最重要的关系</p><p>作者使用GRU将关系序列转换为分布表示。$i$ 到 $j$ 的最短路径关系为: $sp_{i\to j}=  [e(i,k_1), e(k1,k2),…,e(k_n,j)]$</p><p>其中$e(,)$是边标签，$k_{1:n}$ 是中继节点。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  \overrightarrow s_t &= GRU_f(\overrightarrow s_{t-1}, sp_t)\\ \overleftarrow s_t &= GRU_f(\overleftarrow s_{t+1},sp_t)    \end{split}\end{equation}</script><p>concat最终关系表达为$r_{ij} = [\overrightarrow s_n; \overleftarrow s_0]$</p><h4 id="Bidirectionality"><a href="#Bidirectionality" class="headerlink" title="Bidirectionality"></a>Bidirectionality</h4><p>因为应用任务常是DAG，作者给做成理论双向交互的。反转边连接与原始边相同的两个节点，但方向不同，并使用反转标签。</p><p>此外作者还在每个图中引入一个额外的全局节点和自环边，该节点具有特殊标签GLOBAL与其他所有节点都有一条直接边。全局节点的最终表示$x_{global}$用作整个图表示。</p><h4 id="Absolute-Position"><a href="#Absolute-Position" class="headerlink" title="Absolute Position"></a>Absolute Position</h4><p>除了成对关系之外，一些绝对位置信息也是有益的。例如，AMR图的根作为整体焦点的粗略表示，使得到根节点的最小距离部分地反映了相应概念在整句语义中的重要性。 </p><p>位置嵌入添加到编码器堆栈底部的输入embedding中。例如，第一个中的Want-01是AMR图的根节点，因此其索引应该为0。也将全局节点的索引表示为0。</p><h3 id="Sequence-Decoder"><a href="#Sequence-Decoder" class="headerlink" title="Sequence Decoder"></a>Sequence Decoder</h3><p>和普通Transformer Decoder没什么大的区别</p><p>特殊的一点，使用全局图形表示$x_{global}$来初始化每个时间步的隐藏状态。</p><p>然后，通过在编码器的输出上交错多轮关注来更新每个时间步骤t处的隐藏状态 $h_t$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Graph-Transformer-for-Graph-to-Sequence-Learning&quot;&gt;&lt;a href=&quot;#Graph-Transformer-for-Graph-to-Sequence-Learning&quot; class=&quot;headerlink&quot; tit</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>Rethinking Graph Transformers with Spectral Attention</title>
    <link href="http://example.com/2021/07/21/Rethinking-Graph-Transformers-with-Spectral-Attention/"/>
    <id>http://example.com/2021/07/21/Rethinking-Graph-Transformers-with-Spectral-Attention/</id>
    <published>2021-07-21T03:56:04.000Z</published>
    <updated>2021-07-27T02:13:11.344Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Rethinking-Graph-Transformers-with-Spectral-Attention"><a href="#Rethinking-Graph-Transformers-with-Spectral-Attention" class="headerlink" title="Rethinking Graph Transformers with Spectral Attention"></a>Rethinking Graph Transformers with Spectral Attention</h1><p>提出了<em>Spectral Attention Network</em>(SAN)，它使用学习的位置编码(LPE)，可以利用全拉普拉斯频谱来学习给定图中每个节点的位置。通过利用拉普拉斯的全谱，模型在理论上具有强大的区分图形的能力，并且可以更好地从它们的共振中检测出相似的子结构。</p><p>在这项工作中，作者还是研究如何将Transformer体系结构应用于图形表示学习。开发了强大的可学习的位置编码方法，这些方法植根于谱图理论。 谱注意力网络(SAN)架构解决了先前图形转换器工作中的关键理论限制，并且明显超过了标准消息传递GNN的表达能力。</p><p>SAN方法的优势对比：</p><p><img src="https://i.loli.net/2021/07/26/x7U849s2RSCdQau.png" alt=""></p><ul><li>保持注意中的局部结构</li><li>使用边特征</li><li>连接非相邻节点</li><li>使用基于特征向量的PE进行注意</li><li>使用具有结构信息的PE</li><li>考虑特征值的排序</li><li>特征向量的范数不变量</li><li>考虑特征值的谱               (SAN独有)</li><li>考虑特征向量的变量#       (SAN独有)</li><li>意识到特征值的多重性    (SAN独有)</li><li>对特征向量的符号不变</li></ul><p>也就是说SAN结合了稀疏和稠密GT的特性，并且还考虑了特征值的谱、征向量的变量#、意识到特征值的多重性。</p><h2 id="基于特征函数的绝对和相对位置编码"><a href="#基于特征函数的绝对和相对位置编码" class="headerlink" title="基于特征函数的绝对和相对位置编码"></a>基于特征函数的绝对和相对位置编码</h2><p>因为不存在对节点进行排序或定义轴的规范方法。在本节中，作者将研究如何使用拉普拉斯的特征函数来定义图形中的绝对和相对PE，测量节点之间的物理相互作用，并使特定的子结构能够“听到”-类似于鼓的声音，揭示其结构。</p><h3 id="特征向量等价于图上的正弦函数"><a href="#特征向量等价于图上的正弦函数" class="headerlink" title="特征向量等价于图上的正弦函数"></a>特征向量等价于图上的正弦函数</h3><p>在Transformer架构中，一个基本方面是使用正弦和余弦函数作为序列的PE。然而，对于任意图形，sinusoids正弦不能被清楚地定义，因为沿轴的位置没有清晰的概念。取而代之的是，它们的等价性由图Laplacian L的特征向量 $\Phi$ 给出。</p><p>事实上，在欧几里得空间中，拉普拉斯算子对应于梯度的散度，其特征函数是正弦/余弦函数，平方频率对应于特征值(我们有时从这里起将这两个概念互换)。因此，在图域中，图的Laplacian的特征向量与正弦函数自然等价，并且这一直觉被用于最近的多项工作中，这些工作将特征向量用作GNN(Benchmarking graph neural networks)、定向流(Directional graph networks. ICML2021)和GT的PE。</p><p>在与正弦函数等价的情况下，我们很自然地发现，$\mathcal{F}[f]$的傅里叶变换函数应用于图$\mathcal{F}<a href="\lambda_i">f</a> = \langle f, \phi_i \rangle$，其中特征值被认为是该图的傅立叶域中的一个位置。因此，最好将特征向量视为位于特征值轴上的向量，而不是矩阵的组成部分，如图所示。</p><p><img src="https://i.loli.net/2021/07/26/Bg3QbcITZuMRLjs.png" alt=""></p><h3 id="关于相对位置，特征函数告诉我们什么？-物理应用"><a href="#关于相对位置，特征函数告诉我们什么？-物理应用" class="headerlink" title="关于相对位置，特征函数告诉我们什么？(物理应用)"></a>关于相对位置，特征函数告诉我们什么？(物理应用)</h3><p>除了模拟正弦函数外，拉普拉斯函数的特征向量还包含有关系统物理的重要信息，可以揭示距离度量。因为拉普拉斯运算符是物理学中的一个基本运算符，在麦克斯韦方程和热扩散中都有显著的应用。</p><p>在电磁理论中，拉普拉斯的(伪)逆，在数学上称为拉普拉斯的格林函数，表示电荷的静电势。</p><p>在图中，相同的概念使用拉普拉斯G的伪逆，并且可以通过其特征函数来计算。</p><p>如下公式，$G(j_1,j_2)$ 是节点$j_1,j_2$ 之间的电势。 $\hat \phi_i,\hat \lambda_i$ 为对称Laplacian$D^{\frac{-1}{2}}LD^{\frac{-1}{2}}$第 $i$个特征值和特征向量。</p><script type="math/tex; mode=display">G(j_1,j_2) = d_{j_1}^{\frac{1}{2}}d_{j_2}^{\frac{-1}{2}}\sum_{i>0}\frac{(\hat \phi_{i,j_1},\hat \phi_{i,j_2})^2}{\hat \lambda_i}</script><p>此外，傅立叶给出的热方程的原始解依赖于被称为傅立叶级数的正弦/余弦的和。由于拉普拉斯函数的特征向量是这些函数在图中的近似，我们找到了近似的解。热核与随机游走相关，我们利用两个热核之间的相互作用在下面方程中定义节点$j_1,j_2$之间的扩散距离$d_D$。类似的二次谐波距离$d_B$是一种不同的距离测量方法。这里我们使用正则拉普拉斯L的特征函数：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  d^2_D(j_1,j_2) &= \sum_{k>0} e^{-2t\lambda_i}(\phi_{i,j_1} - \phi_{i,j_2})^2\\ d_B^2(j_1,j_2)&=\sum_{i>0}\frac{(\phi_{i,j_1} - \phi_{i,j_2})^2}{\lambda_i^2}    \end{split}\end{equation}</script><p>这个方程，首先强调了在提供有关图中相对位置的信息时将特征向量与其对应的特征值配对的重要性。其次，我们注意到特征向量的乘积与静电相互作用成正比，而减法与扩散距离和重谐距离成正比。最后，所有3个方程都有一个一致的模式：在确定节点之间的距离时，频率/特征值越小，权重越大。</p><h3 id="听图的形状及其子结构"><a href="#听图的形状及其子结构" class="headerlink" title="听图的形状及其子结构"></a>听图的形状及其子结构</h3><p>特征值的另一个众所周知的性质是它们如何用于区分不同的图结构和子结构，因为它们可以解释为图的共振频率。</p><p>这就引出了一个著名的问题，即我们是否能从鼓的特征值中听到鼓的形状，同样的问题也适用于几何物体和3D分子。</p><p>通过将特征函数用于部分功能对应、算法理解几何和样式对应。分子图的特征向量的例子如图所示。</p><p><img src="https://i.loli.net/2021/07/26/o4M9SwJnLWTKCsj.png" alt=""></p><h2 id="Laplace-Eigenfunctions的规范"><a href="#Laplace-Eigenfunctions的规范" class="headerlink" title="Laplace Eigenfunctions的规范"></a>Laplace Eigenfunctions的规范</h2><p>在欧几里德空间和序列中，使用正弦波作为PE是很简单的：我们可以简单地选择一组频率，计算正弦波，并将它们添加或拼接到输入嵌入，就像在原始变压器中所做的那样。然而，在任意图中，复制这些步骤并不那么简单，因为每个图都有一组唯一的特征函数。</p><p>在接下来的部分中，将介绍谱图理论中的关键原则，在为图构造PE时要考虑这些原则，这些原则大部分被以前的方法忽略了。包括正则化，特征值及其多样性的重要性，特征向量的数量是可变的，以及符号模糊性。作者的LPE架构旨在解决这些问题。</p><p><strong>Normalization</strong> 给定拉普拉斯的特征值，就有一个维数大于1的相关特征空间。为了在模型中利用这些信息，必须选择一个单一的特征向量。在我们的工作中，我们使用L2正则化，因为它与格林公式也就是上面的第一个公式的定义是兼容的。因此，我们将始终选择特征向量$\phi$，使$⟨\phi，\phi⟩=1$。</p><p><strong>Eigenvalues</strong> 另一个基本方面是与每个特征向量相关联的特征值提供了有价值的信息。基于特征向量的特征值的排序在序列中起作用，因为频率是预先确定的。然而，这一假设在图中不起作用，因为它们的谱中的特征值可以改变。例如，在上图中，我们观察到排序如何忽略两个分子在 $λ = 1$ 以不同方式共振的事实。</p><p><strong>Multiplicities</strong> 选择特征函数的另一个重要问题是特征值高度多样的可能性，即当一个特征值多次作为特征多项式的根出现时。在这种情况下，相关联的特征空间可以具有2维或更多维，因为我们可以从具有相同特征值的任何特征向量的线性组合中生成有效的特征向量。这进一步复杂化了选择用于算法计算的特征向量的问题，并突出了拥有能够处理这种歧义的模型的重要性。</p><p><strong>Variable number of eigenvectors</strong> 图 $G_i$ 至多可以有 $N_i$ 个线性独立的特征向量，其中 $N_i$ 是它的节点数。最重要的是，$N_i$ 可以在数据集中的所有的 $G_i$ 都有所不同。GT选择了固定数目的k个特征向量给每个图，其中 $k≤N_i$，$∀i$。当数据集中最小的图的节点比最大的图少得多时，这就产生了一个主要的瓶颈，因为很小比例的特征向量将用于大型图。这不可避免地造成信息丢失，并激发了对构建k维固定PE的模型的需求，其中k不依赖于图中的特征向量的数目。</p><p><strong>Sign invariance</strong> 如前所述，特征向量存在符号歧义。由于φ的符号与它的正则化无关，在选择图的k个特征向量时，我们只剩下2k个可能的符号组合。以前的工作已经提出通过随机反转特征向量的符号来进行数据增强，虽然当k较小时可以工作，但是对于较大的k会变得困难。</p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>我们提出了一种体系结构，它可以使用特征函数作为PE，同时解决上述规范中提出的问题。我们的 <em>Spectral Attention Network</em> (SAN)模型输入图的特征函数，并将其投影到固定大小的学习位置编码(LPE)中。LPE允许网络使用每个图的整个拉普拉斯频谱，学习频率如何交互，并决定哪些频率对给定任务最重要。</p><p><img src="https://i.loli.net/2021/07/26/8uAUjDZ5EdNQpFs.png" alt=""></p><p>如图分为两步学习过程。</p><p>图中的(c-d-e)描述了第一步，在每个节点的特征函数上应用一个Transformer，为每个图生成一个LPE矩阵。</p><p>然后将LPE连接到节点嵌入图中(g-h)，然后将其传递给Graph Trabsformer (i)。如果任务涉及图分类或回归，则最终节点嵌入随后将传递到最终池化层。</p><h3 id="LPE-Transformer-Over-Nodes"><a href="#LPE-Transformer-Over-Nodes" class="headerlink" title="LPE Transformer Over Nodes"></a>LPE Transformer Over Nodes</h3><p>使用拉普拉斯编码作为节点特征在有关该主题的文献中是普遍存在的。LPE的想法受到上面第二个图的启发，其中特征向量 $\phi$ 被表示为一个非均匀序列，特征值λ是频率轴上的位置。使用此表示法，Transformers是处理它们并生成固定大小PE的自然选择。</p><p>LPE结构如图所示：</p><p><img src="https://i.loli.net/2021/07/26/yxhrKLAwXcvBTgD.png" alt=""></p><p>学习位置编码(LPE)结构，模型通过考虑m个特征值和特征向量来学习图的拉普拉斯谱，其中允许 $m≤N$，其中N表示节点数。</p><p>首先，我们通过将m个最低特征值与其关联的特征向量连接起来，为每个节点$j$ 创建一个大小为 $2×m$ 的嵌入矩阵。这里，m是要计算的特征向量的最大数目的超参数，并且类似于标准变压器的可变长度序列。对于 $m&gt;N$ 的图，只需添加掩码填充。注意，要捕获所有图的整个谱，只需选择m，使其等于图在数据集中具有的最大节点数。然后在大小为2的维度上应用线性层以生成大小为k的新嵌入。然后，Transformer编码器对长度为m且隐藏维数为k的序列计算self-attention。最后，sum pooling将该序列简化为固定的k维节点嵌入。</p><p>通过将特征值与归一化特征向量连接起来，该模型直接处理前三个规范。即将特征向量归一化，将特征向量与其特征值配对，并将特征向量的个数作为变量。此外，该模型意识到了多重性，并且有可能线性组合或忽略一些重复的特征值。</p><p>然而，这种方法仍然没有解决预先计算的特征向量的符号是任意的限制。为了解决这个问题，我们像以前的工作[13，12]所采用的那样，在训练过程中随机反转预先计算的特征向量的符号，以促进符号歧义的不变性。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Rethinking-Graph-Transformers-with-Spectral-Attention&quot;&gt;&lt;a href=&quot;#Rethinking-Graph-Transformers-with-Spectral-Attention&quot; class=&quot;heade</summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Heterogeneous Graph Transformer</title>
    <link href="http://example.com/2021/07/09/Heterogeneous-Graph-Transformer/"/>
    <id>http://example.com/2021/07/09/Heterogeneous-Graph-Transformer/</id>
    <published>2021-07-09T09:18:10.000Z</published>
    <updated>2021-07-09T13:42:02.369Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Heterogeneous-Graph-Transformer"><a href="#Heterogeneous-Graph-Transformer" class="headerlink" title="Heterogeneous Graph Transformer"></a>Heterogeneous Graph Transformer</h1><p>提出了一种用于Web规模异构图建模的异构图Transformer(HGT)体系结构。</p><p>其一是设计了节点和边类型相关的参数来表征对每条边的异构attention，使得HGT能够维护不同类型的节点和边的专用表示。</p><p>其二为了处理Web规模的图形数据，我们设计了异构小批量图形采样算法HG Samples，以实现高效和可扩展的训练</p><p>作者使用的是OAG学术图，其存在的异构关系如下图：</p><p><img src="https://z3.ax1x.com/2021/07/09/RxMqUA.png" alt=""></p><h2 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h2><p>GNN以前可以处理异质图是基于元路径的方法有PathSim, methpath2vec等。GNN火起来以后也出现了好多处理异质图的工作。</p><p>作者认为面临着几个问题：首先，它们大多涉及为每种类型的异构图设计元路径，需要特定的领域知识；其次，它们要么简单地假设不同类型的节点/边共享相同的特征和表示空间，要么只针对节点类型或边类型保持不同的非共享权重，使得它们不足以捕捉异构图的属性；最后，其固有的设计和实现使得它们无法对Web规模的异构图进行建模。</p><p>作者的目标是：保持节点和边类型的依赖表示，避免定制的元路径，并且能够扩展到Web规模的异构图。</p><h3 id="做法："><a href="#做法：" class="headerlink" title="做法："></a>做法：</h3><h4 id="异质处理"><a href="#异质处理" class="headerlink" title="异质处理"></a>异质处理</h4><p>为了处理图的异构性，引入了节点和边型依赖的注意机制。HGT中的异构相互关注度不是参数化的，而是通过基于其元关系三元组分解每条边e=(s，t)来定义的，即 <s的节点类型、s&t之间的e的边类型、t的节点类型>。上图说明了异质学术图的元关系。使用这些元关系来参数化权重矩阵，以计算每条边上的关注度。因此，允许不同类型的节点和边保持其特定的表示空间。</p><p>同时，不同类型的连接节点仍然可以交互、传递和汇聚消息，而不受其分布差距的限制。由于其体系结构的本质，HGT可以通过跨层的消息传递来融合来自不同类型的高阶邻居的信息，这可以被认为是“软”元路径。也就是说，即使HGT只将其一跳边作为输入，而不需要人工设计元路径，所提出的注意机制也可以自动和隐式地学习和提取对不同下游任务重要的“元路径”。</p><h4 id="异质子图采样法"><a href="#异质子图采样法" class="headerlink" title="异质子图采样法"></a>异质子图采样法</h4><p>为了对Web规模的异构图进行建模，设计了第一个用于小批量GNN训练的异构子图采样算法HG Samples。它的主要思想是对不同类型节点比例相近的异构子图进行采样。此外，它还被设计成保持采样子图的稠密性，以最大限度地减少信息损失。有了HG-sample，所有的GNN模型都可以在任意大小的异构图上进行训练和推断。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>思想：利用异构图的元关系来参数化异构相互关注、消息传递和传播步骤的权重矩阵。</p><p>有向图 $G = (V,E,A,R)$ ,  节点 $v \in V$，每个边$e \in E$ 。他们的类型映射函数为 $\tau(v):V \to A$ 、$\phi(e):E\to R$ </p><h3 id="元关系"><a href="#元关系" class="headerlink" title="元关系"></a>元关系</h3><p>对于一个边 $e = (s,t)$ ，元关系定义为 $&lt;\tau(s),\phi(e),\tau(t)&gt;$ 。$\phi(e)^{-1}$ 是关系的反向表达。</p><h3 id="HGT架构"><a href="#HGT架构" class="headerlink" title="HGT架构"></a>HGT架构</h3><p><img src="https://i.loli.net/2021/07/09/8CepfwW4dgEzsjc.png" alt=""></p><p>主要的三个组件：Heterogeneous Mutual Attention、Heterogeneous Message Passin和特定于Target-Specific Aggregation。</p><p>定义第$l$ 层的输出为 $H^l$, 也是第$l+1$层的输入。</p><h4 id="Heterogeneous-Mutual-Attention"><a href="#Heterogeneous-Mutual-Attention" class="headerlink" title="Heterogeneous Mutual Attention"></a>Heterogeneous Mutual Attention</h4><p>首先计算源节点 s 到目标节点 t 之间的 Mutual Attention。</p><p>针对问题是：通过使用一个权重矩阵W来假设s和t具有相同的特征分布。这种假设对于异构图通常是不正确的，因为在异构图中，每种类型的节点都可以有自己的特征分布。</p><p>给出目标节点 t ，以及它的邻居节点 $s \in N(t)$ 它们可能属于不同的分布。通过元关系三元组 $&lt;\tau(s),\phi(e),\tau(t)&gt;$, 计算mutual attention。</p><p>将目标节点t映射为query向量，将源节点s映射为key向量，并计算它们的点积作为关注度。</p><p>与Vanilla Transformer相比关键区别在于，Vanilla Transformer对所有单词使用一组投影映射，HGT的每个元关系都应该有一组不同的投影权重。</p><p>为了在保持不同关系特性的同时最大限度地实现参数共享，提出将交互算子的权重矩阵参数化为源节点投影、边投影和目标节点投影。</p><p>对每个边$e=(s,t)$进行h heads attention :</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  Attention_{HGT}(s,e,t) &= Softmax_{\forall s\in N(t)}(  \|_{i\in[1,h]} \text{ATT-head}^i(s,e,t) )\\\text {ATT-head}^i(s,e,t) &=(K^i(s) W^{ATT}_{\phi(e)}Q^i(t)^T) \cdot \frac{\mu<\tau(s),\phi(e),\tau(t)>}{\sqrt d} \\K^i(s) &= \text{K-Linear}_{\tau(s)}^i(H^{(l-1)}[s])\\Q^i(t) &= Q-Linear_{\tau(t)}^i (H^{(l-1)}[t])    \end{split}\end{equation}</script><p>$\text{ATT-head^i(s,e,t)}$ 是第 $i$ 个注意力头。$\text{K-Linear}^i_{\tau{(s)}}:R^d \to R^{\frac{d}{h}}$ 编码了源接地那s类型$\tau(s)$ 意味着每每个类型节点有独一无二的线性映射最大限度地对分布差异进行建模。</p><p>然后计算Query和Key的相似度， 异构图的一个独特特征是在节点类型对之间可能存在不同的边类型(关系)，例如 $τ(S)$和$τ(T)$ 。因此，与直接计算查询和键向量之间的点积的Vanilla Transformer不同，我们为每个边类型$\phi(e)$保留了一个不同的基于边的矩阵 $W^{ATT}_{\phi(e)}\in R^{\frac{d}{h}\times \frac{d}{h}}$ 。这样，即使在相同的节点类型对之间，该模型也可以捕获不同的语义关系。</p><p>此外，由于不是所有的关系对目标节点的贡献相等，我们增加了一个先验张量 $\mu\in R^{|A|\times |R|\times |A|}$ 表示每个元关系三元组的一般意义，作为对注意力的自适应缩放。</p><p>最后，我们将注意力集中在一起，以获得每个节点对的attention向量。</p><h4 id="Heterogeneous-Message-Passing"><a href="#Heterogeneous-Message-Passing" class="headerlink" title="Heterogeneous Message Passing"></a>Heterogeneous Message Passing</h4><p>在计算Mutual Attention的同时，将信息从源节点传递到目标节点。</p><p>与attention过程类似，希望在消息传递过程中加入边的元关系，以缓解不同类型节点和边的分布差异。</p><p>对于一对节点 $e=(s,t)$，我们通过以下公式计算其多头 Message:</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  Message_{HGT(s,e,t)} &= \|_{i\in [1,h]} \text{MSG-head}^i(s,e,t)\\ \text{MSG-head}^i(s,e,t) &= \text{M-Linear}_{\tau(s)}^i(H^{(l-1)}[s])W^{MSG}_{\phi(e)}    \end{split}\end{equation}</script><h4 id="Target-Specific-Aggregation"><a href="#Target-Specific-Aggregation" class="headerlink" title="Target-Specific Aggregation"></a>Target-Specific Aggregation</h4><script type="math/tex; mode=display">\hat H^{(l)}[t] =  \oplus_{\forall s\in N(t)} (Attention_{HGT}(s,e,t) \cdot Message_{HGT}(s,e,t))</script><p>这将来自不同特征分布的所有近邻(源节点)的信息聚集到目标节点 t。</p><p>最后一步是将目标节点t的向量映射回按其节点类型τ(T)索引的特定于类型的分布。为此，我们将线性投影A-线性τ(T)应用于更新后的向量H􏰅(L)[t]，随后是非线性激活和剩余连接[5]，如下所示：</p><h3 id="HGSampling"><a href="#HGSampling" class="headerlink" title="HGSampling"></a>HGSampling</h3><p><img src="https://i.loli.net/2021/07/09/5yz2s4vhCZPINX9.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Heterogeneous-Graph-Transformer&quot;&gt;&lt;a href=&quot;#Heterogeneous-Graph-Transformer&quot; class=&quot;headerlink&quot; title=&quot;Heterogeneous Graph Transforme</summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Transformer的辅助</title>
    <link href="http://example.com/2021/07/04/Transformer%E7%9A%84%E8%BE%85%E5%8A%A9/"/>
    <id>http://example.com/2021/07/04/Transformer%E7%9A%84%E8%BE%85%E5%8A%A9/</id>
    <published>2021-07-04T15:14:01.000Z</published>
    <updated>2021-07-05T02:07:18.843Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transformer的辅助"><a href="#Transformer的辅助" class="headerlink" title="Transformer的辅助"></a>Transformer的辅助</h1><p>转载：<a href="https://zhuanlan.zhihu.com/p/149634836">https://zhuanlan.zhihu.com/p/149634836</a></p><h2 id="为什么Transformer需要进行Multi-head-Attention"><a href="#为什么Transformer需要进行Multi-head-Attention" class="headerlink" title="为什么Transformer需要进行Multi-head Attention"></a><a href="https://www.zhihu.com/question/341222779/answer/814111138">为什么Transformer需要进行Multi-head Attention</a></h2><p>Attention is all you need论文中讲模型分为多个头，形成多个子空间，每个头关注不同方面的信息。</p><p>如果Multi-Head作用是关注句子的不同方面，那么不同的head就应该关注不同的Token；当然也有可能是关注的pattern相同，但是关注的内容不同，即V不同。</p><p>但是大量的paper表明，transformer或Bert的特定层有独特的功能，底层更偏向于关注语法；顶层更偏向于关注语义。</p><p>所以对Multi-head而言，同一层Transformer_block关注的方面应该整体是一致的。不同的head关注点也是一样。但是可视化同一层的head后，发现总有那么一两个头独一无二的，和其他头的关注不一样。</p><p>众多研究表明Multi-Head其实不是必须的，去掉一些头效果依然有不错的效果（而且效果下降可能是因为参数量下降），这是因为在头足够的情况下，这些头已经能够有关注位置信息、关注语法信息、关注罕见词的能力了，再多一些头，无非是一种enhance或noise而已。</p><h3 id="相关paper"><a href="#相关paper" class="headerlink" title="相关paper"></a>相关paper</h3><ul><li>A Multiscale Visualization of Attention in the Transformer Model <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1906.05714.pdf">https://arxiv.org/pdf/1906.05714.pdf</a></li><li>What Does BERT Look At? An Analysis of BERT’s Attention <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1906.04341v1.pdf">https://arxiv.org/pdf/1906.04341v1.pdf</a></li><li>Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1908.11365.pdf">https://arxiv.org/pdf/1908.11365.pdf</a></li><li>Adaptively Sparse Transformers<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1909.00015.pdf">https://arxiv.org/pdf/1909.00015.pdf</a></li><li>Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1905.09418.pdf">https://arxiv.org/pdf/1905.0941</a></li></ul><h2 id="Transformer为什么Q和K使用不同的权重矩阵生成，为什么不能使用同一个值进行自身的点乘？（注意和第一个问题的区别）"><a href="#Transformer为什么Q和K使用不同的权重矩阵生成，为什么不能使用同一个值进行自身的点乘？（注意和第一个问题的区别）" class="headerlink" title="Transformer为什么Q和K使用不同的权重矩阵生成，为什么不能使用同一个值进行自身的点乘？（注意和第一个问题的区别）"></a><a href="https://www.zhihu.com/question/319339652">Transformer为什么Q和K使用不同的权重矩阵生成，为什么不能使用同一个值进行自身的点乘？（注意和第一个问题的区别）</a></h2><p>既然K和Q差不多（唯一区别是W_k和W_Q权值不同），直接拿K自己点乘就行了，何必再创建一个Q？创建了还要花内存去保存，浪费资源，还得更新参数。</p><p><strong>为什么要计算Q和K的点乘？</strong></p><p>我们知道K和Q的点乘是为了得到一个attention score 矩阵，用来对V进行提纯。K和Q使用了不同的W_k, W_Q来计算，可以理解为是在不同空间上的投影。正因为有了这种不同空间的投影，增加了表达能力，这样计算得到的attention score矩阵的泛化能力更高。这里解释下我理解的泛化能力，因为K和Q使用了不同的W_k, W_Q来计算，得到的也是两个完全不同的矩阵，所以表达能力更强。</p><p> 但是如果不用Q，直接拿K和K点乘的话，你会发现attention score 矩阵是一个对称矩阵。因为是同样一个矩阵，都投影到了同样一个空间，所以泛化能力很差。这样的矩阵导致对V进行提纯的时候，效果也不会好。</p><h2 id="为什么在进行softmax之前需要对注意进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解"><a href="#为什么在进行softmax之前需要对注意进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解" class="headerlink" title="为什么在进行softmax之前需要对注意进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解"></a><a href="https://www.zhihu.com/question/339723385/">为什么在进行softmax之前需要对注意进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解</a></h2><p><strong>（</strong>论文中解释是：向量的点积结果会很大，将softmax函数push到梯度很小的区域，scaled会缓解这种现象。怎么理解将sotfmax函数push到梯度很小区域？还有为什么scaled是维度的根号，不是其他的数？<strong>）</strong></p><p>以数组为例，2个长度是len，均值是0，方差是1的数组点积会生成长度是len，均值是0，方差是len的数组。而方差变大会导致softmax的输入推向正无穷或负无穷，这时的梯度会无限趋近于0，不利于训练的收敛。因此除以len的开方，可以是数组的方差重新回归到1，有利于训练的收敛。</p><p>@LinT成功人士（） 以下感谢分享</p><p><strong>1. 为什么比较大的输入会使得softmax的梯度变得很小？</strong></p><p>对于一个输入向量$x\in R^d$, softmax函数将其映射/归一化到一个分布$\hat y\in R^d$。在这个过程中softmax先用一个自然底数$e$ 将输入中的元素检举先“拉大”，然后归一化为一个分布。假设某个输入x 中最大的元素下班是k，如果输入的数量级变大(每个元素都很大)，那么$\hat y_k$会非常接近1。</p><p>举个例子$x$ 的数量级对输入最大元素对应的预测概率$\hat y_k$的影响。假定输入 $x = [a,a,2a]^T$, 我们看看不同量级的$a$ 产生的$\hat y_3$有什么区别。</p><ul><li><img src="https://www.zhihu.com/equation?tex=a%3D1+" alt="[公式]"> 时， <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_3%3D0.5761168847658291" alt="[公式]"> ;</li><li><img src="https://www.zhihu.com/equation?tex=a%3D10++" alt="[公式]"> 时， <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_3%3D0.999909208384341" alt="[公式]">;</li><li><img src="https://www.zhihu.com/equation?tex=a%3D100++" alt="[公式]"> 时， <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_3%5Capprox+1.0" alt="[公式]"> (计算机精度限制)。</li></ul><p>可以看出，第三个元素里的值随着数量级的增加而接近于1。而我们知道softmax层后的每个元素之后为1。也就是说，向量里最大值索引的元素基本上占据所有的概率了。为了理解方便，可视化如下。可以看出，随着向量里最大元素的数量级的增大，它就越近于1，相当于整个输出变成了one-hot编码了<a href="#">y = [0,0,1]</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x  = np.linspace(<span class="number">0</span>,<span class="number">100</span>,<span class="number">100</span>)</span><br><span class="line">f = <span class="keyword">lambda</span> x: np.exp(x * <span class="number">2</span>) / (np.exp(x) + np.exp(x) + np.exp(<span class="number">2</span> * x))</span><br><span class="line">y = [f(temp) <span class="keyword">for</span> temp <span class="keyword">in</span> x]</span><br><span class="line">plt.plot(x,y)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/07/05/25qZGxyD17w9UgM.png" alt=""></p><p><img src="https://i.loli.net/2021/07/05/NZpiYSf8y7vVWEL.png" alt=""></p><p><strong>2. 维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩？</strong></p><p><img src="https://pic1.zhimg.com/80/v2-493286fbea075e160bf3bac214d2ac60_720w.jpg" alt="v2-493286fbea075e160bf3bac214d2ac60_720w"></p><p><img src="https://i.loli.net/2021/07/05/5BnlfzVdOjh14EA.png" alt=""></p><hr><h2 id="在计算注意力分数的时候如何对padding做mask操作？"><a href="#在计算注意力分数的时候如何对padding做mask操作？" class="headerlink" title="在计算注意力分数的时候如何对padding做mask操作？"></a><strong>在计算注意力分数的时候如何对padding做mask操作？</strong></h2><p>mask是将一些不要用的值掩盖掉，使其不产生作用。有两种mask，第一种是<strong>padding mask</strong>，在所有scaled dot-product attention都用到；第二种是<strong>sequence mask，</strong>在decoder的self-attention里面用到。</p><p><strong>padding mask：</strong>因为一个批量输入中，所有序列的长度使不同的。为了符合模型的输入方式，会用padding的方式来填充（比如填0），使所有序列的长度一致。但填充部分是没有意义的，所以在计算注意力的时候，不需要也不应该有注意力分配到这些填充的值上面。所以解决方式就是在填充的位置赋予一个<strong>很小的负值/负无穷（-np.inf）</strong>的值，<strong>经过softmax后的得分为0</strong>，即没有注意力分配到这个上面。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">padding_mask</span>(<span class="params">seq_k, seq_q</span>):</span></span><br><span class="line">  <span class="comment"># shape(seq_k)=(B,L_k) , shape(seq_q) = (B, L_q)</span></span><br><span class="line">  <span class="comment"># 因为要计算seq_k和seq_q的相似程度，来表示注意力的得分</span></span><br><span class="line">  <span class="comment"># padding mask要作用在 QK^T上，所以padding mask是跟seq_k和seq_q序列长度相关的矩阵</span></span><br><span class="line">  <span class="comment"># shape(padding mask) = (B, L_q, L_k)</span></span><br><span class="line">  len_q = seq_q.size(<span class="number">1</span>)</span><br><span class="line">  <span class="comment"># PAD is 0 这里要计算seq_k序列中，padding为0的地方，并将相应位置变为True，方便后续处理</span></span><br><span class="line">  pad_mask = seq_k.eq(<span class="number">0</span>)</span><br><span class="line">  <span class="comment"># 将每个seq_k序列扩展len_q次，shape[B, L_q, L_k]</span></span><br><span class="line">  pad_mask = pad_mask.unsqueeze(<span class="number">1</span>).expand(-<span class="number">1</span>, len_q, -<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">return</span> pad_mask</span><br><span class="line">  </span><br></pre></td></tr></table></figure><p>以上方法为大部分padding mask的计算形式，但实际上，这里做了seq_q全部有效的假设（没有padding），并不够精确 。自己的看法：上述代码expand操作，只是将seq_k中padding的部分重复了L_q次，并没有注意到，seq_q也有padding的部分。即在一个(L_q,L_k)矩阵中，只有最后几列需要掩码，实际矩阵的最后几行也需要掩码。（以后上图更形象）</p><p><strong>sequence mask：</strong>在decoder部分，因为不能见到下文信息（防止泄漏），所以用mask的方式掩盖掉当前时刻t及之后的下文信息。具体，可产生一个对角线为0的上三角矩阵，将其作用到每个decoder的输入列上。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sequence_mask</span>(<span class="params">seq</span>):</span></span><br><span class="line">    batch_size, seq_len = seq.size()</span><br><span class="line">    mask = torch.triu(torch.ones((seq_len, seq_len), dtype=torch.uint8),</span><br><span class="line">                    diagonal=<span class="number">1</span>)</span><br><span class="line">    mask = mask.unsqueeze(<span class="number">0</span>).expand(batch_size, -<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># [B, L, L]</span></span><br><span class="line">    <span class="comment"># 三角矩阵中，为1的部分是需要被掩码掉的</span></span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure><p>decoder-block有两个multi-head attention，下面的multi-head attention是目标输入的self-attention，需要用到1.padding mask：去除padding位置的影响；2.sequence mask：去掉下文穿越的影响。上面的multi-head attention只需要padding mask，因为下面的多头注意力已经磨平了下文信息。当<strong>encoder和decoder的输入序列长度一样时</strong>，可以通过padding mask+sequence mask作为scaled dot-product attention的attn_mask来实现。</p><p>其他情况的attn_mask（代码中的表达）等于padding mask</p><h2 id="为什么在进行多头关注的时候需要对每个head进行切割？"><a href="#为什么在进行多头关注的时候需要对每个head进行切割？" class="headerlink" title="为什么在进行多头关注的时候需要对每个head进行切割？"></a><a href="https://www.zhihu.com/question/350369171">为什么在进行多头关注的时候需要对每个head进行切割？</a></h2><p>@何妨吟啸且徐行 感谢回答</p><p>Transformer的多头注意力看上去是借鉴了CNN中同一卷积层内使用多个卷积核的思想，原文中使用了 8 个“scaled dot-product attention”，在同一“multi-head attention”层中，输入均为“KQV”，<strong>同时</strong>进行注意力的计算，彼此之前<strong>参数不共享</strong>，最终将结果<strong>拼接</strong>起来，这样可以允许模型在<strong>不同的表示子空间里学习到相关的信息</strong>，在此之前的 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1703.03130">A Structured Self-attentive Sentence Embedding</a> 也有着类似的思想。简而言之，就是希望每个注意力头，只关注最终输出序列中一个子空间，互相<strong>独立</strong>。其核心思想在于，抽取到更加丰富的<strong>特征信息</strong>。<br>回到题主的问题上来，如果只使用 one head 并且维度为 <img src="https://www.zhihu.com/equation?tex=d_%7Bmodel%7D" alt="[公式]"> ，相较于 8 head 并且维度为 <img src="https://www.zhihu.com/equation?tex=d_%7Bmodel%7D+%2F+8" alt="[公式]">。首先存在计算量极大的问题，并且高维空间下的学习难度也会相应提升，这就难免文中实验出现的参数量大且效果不佳的情况，于是将原有的高维空间转化为多个低维空间并再最后进行拼接，形成同样维度的输出，借此丰富特性信息，降低了计算量，而且取得了更好的效果，十分巧妙。</p><h2 id="为何在获取输入词向量之后需要对矩阵乘以embeddding-size的开方？意义是什么？"><a href="#为何在获取输入词向量之后需要对矩阵乘以embeddding-size的开方？意义是什么？" class="headerlink" title="为何在获取输入词向量之后需要对矩阵乘以embeddding size的开方？意义是什么？"></a><strong>为何在获取输入词向量之后需要对矩阵乘以embeddding size的开方？意义是什么？</strong></h2><p>embedding matrix的初始化方式是xavier init，这种方式的方差是1/embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。</p><h2 id="你还了解些关于位置编码的技术，各自的优缺点是什么？"><a href="#你还了解些关于位置编码的技术，各自的优缺点是什么？" class="headerlink" title="你还了解些关于位置编码的技术，各自的优缺点是什么？"></a><strong>你还了解些关于位置编码的技术，各自的优缺点是什么？</strong></h2><p><a href="https://zhuanlan.zhihu.com/p/105001610">https://zhuanlan.zhihu.com/p/105001610</a></p><h2 id="Transformer-为什么使用-layer-normalization，而不是其他的归一化方法？"><a href="#Transformer-为什么使用-layer-normalization，而不是其他的归一化方法？" class="headerlink" title="Transformer 为什么使用 layer normalization，而不是其他的归一化方法？"></a><a href="https://www.zhihu.com/question/395811291">Transformer 为什么使用 layer normalization，而不是其他的归一化方法？</a></h2><p><a href="https://www.zhihu.com/question/395811291/answer/1251829041">https://www.zhihu.com/question/395811291/answer/1251829041</a></p><h2 id="Transformer如何并行化的？解码器端可以做并行化吗？"><a href="#Transformer如何并行化的？解码器端可以做并行化吗？" class="headerlink" title="Transformer如何并行化的？解码器端可以做并行化吗？"></a><strong>Transformer如何并行化的？</strong>解码器端可以做并行化吗？</h2><p>Transformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，在self-attention模块，对于某个序列<img src="https://www.zhihu.com/equation?tex=x_%7B1%7D%2C+x_%7B2%7D%2C+%5Cdots%2C+x_%7Bn%7D" alt="[公式]">，self-attention模块可以直接计算<img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D%2C+x_%7Bj%7D" alt="[公式]">的点乘结果，而RNN系列的模型就必须按照顺序从<img src="https://www.zhihu.com/equation?tex=x_%7B1%7D" alt="[公式]">计算到<img src="https://www.zhihu.com/equation?tex=x_%7Bn%7D" alt="[公式]">。</p><h2 id="简单描述一下wordpiece-model和字节对编码？"><a href="#简单描述一下wordpiece-model和字节对编码？" class="headerlink" title="简单描述一下wordpiece model和字节对编码？"></a><strong>简单描述一下wordpiece model和字节对编码？</strong></h2><p><a href="https://zhuanlan.zhihu.com/p/86965595">https://zhuanlan.zhihu.com/p/86965595</a></p><h2 id="Transformer训练的时候学习率是如何设定的？"><a href="#Transformer训练的时候学习率是如何设定的？" class="headerlink" title="Transformer训练的时候学习率是如何设定的？"></a><strong>Transformer训练的时候学习率是如何设定的？</strong></h2><p><img src="https://i.loli.net/2021/07/05/FDESyq27uWr1lvJ.png" alt=""></p><h2 id="Transformer中multi-head机制是如何实现每个head提取的信息空间互斥的？"><a href="#Transformer中multi-head机制是如何实现每个head提取的信息空间互斥的？" class="headerlink" title="Transformer中multi-head机制是如何实现每个head提取的信息空间互斥的？"></a><a href="https://www.zhihu.com/question/357565475">Transformer中multi-head机制是如何实现每个head提取的信息空间互斥的？</a></h2><p>Transformer的多头注意力看上去是借鉴了CNN中同一卷积层内使用多个卷积核的思想，原文中使用了 8 个“scaled dot-product attention”，在同一“multi-head attention”层中，输入均为“KQV”，<strong>同时</strong>进行注意力的计算，彼此之前<strong>参数不共享</strong>，最终将结果<strong>拼接</strong>起来，这样可以允许模型在<strong>不同的表示子空间里学习到相关的信息</strong>。简而言之，就是希望每个注意力头，只关注最终输出序列中一个子空间，互相<strong>独立</strong>。其核心思想在于，抽取到更加丰富的<strong>特征信息</strong>。</p><h2 id="Transformer的细节到底是怎么样的？"><a href="#Transformer的细节到底是怎么样的？" class="headerlink" title="Transformer的细节到底是怎么样的？"></a><strong>Transformer的细节到底是怎么样的？</strong></h2><p><a href="https://www.zhihu.com/question/362131975/answer/945357471">https://www.zhihu.com/question/362131975/answer/945357471</a></p><h2 id="为什么Bert的三个Embedding可以进行相加？"><a href="#为什么Bert的三个Embedding可以进行相加？" class="headerlink" title="为什么Bert的三个Embedding可以进行相加？"></a><strong><a href="https://www.zhihu.com/question/374835153/answer/1040767499">为什么Bert的三个Embedding可以进行相加？</a></strong></h2><p>(Token Embedding、Segment Embedding、Position Embedding三个向量为什么可以相加呢？相加后向量的大小和方向就变了，语义不就变了吗？) 深度神经网络里变得非常复杂，本质上神经网络中每个神经元收到的信号也是“权重”相加得来。这三个向量为什么可以相加呢？因为三个embedding相加等价于三个原始one-hot的拼接再经过一个全连接网络。 相加后向量的大小和方向就变了，语义不就变了吗？这里不是语义变了，而是在训练的时候就是这几个向量相加进行训练的，训练完之后，将lookup后的向量进行相加，就能得到比较好的表示了。 从梯度的角度解释：</p><script type="math/tex; mode=display">(f+g+h)' = f'+g'+h'</script><h2 id="为什么BERT输入的最大长度要限制为512？"><a href="#为什么BERT输入的最大长度要限制为512？" class="headerlink" title="为什么BERT输入的最大长度要限制为512？"></a><strong><a href="https://www.zhihu.com/question/395903256">为什么BERT输入的最大长度要限制为512？</a></strong></h2><p>个人推断是考虑了计算与运行效率综合做出的限制。</p><p>BERT输入的最大长度限制为512, 其中还需要包括[CLS]和[SEP]. 那么实际可用的长度仅为510.<strong>但是别忘了,</strong>每个单词tokenizer之后也有可能被分成好几部分. 所以实际可输入的句子长度远不足510.</p><p>BERT由于position-embedding的限制只能处理最长512个词的句子。如果文本长度超过512，有以下几种方式进行处理：</p><p><strong>a）直接截断：</strong>从长文本中截取一部分，具体截取哪些片段需要观察数据，如新闻数据一般第一段比较重要就可以截取前边部分；</p><p><strong>b）抽取重要片段：</strong>抽取长文本的关键句子作为摘要，然后进入BERT；</p><p><strong>c）分段：</strong>把长文本分成几段，每段经过BERT之后再进行拼接或求平均或者接入其他网络如lstm。</p><p>另外transformer-xl 、<a href="https://zhuanlan.zhihu.com/p/133491514">LongFormer：用稀疏自注意力拓展模型文本容纳量</a>等优秀设计也可以解决长文本。</p><h2 id="为什么BERT选择mask掉15-这个比例的词，可以是其他的比例吗？"><a href="#为什么BERT选择mask掉15-这个比例的词，可以是其他的比例吗？" class="headerlink" title="为什么BERT选择mask掉15%这个比例的词，可以是其他的比例吗？"></a><strong>为什么BERT选择mask掉15%这个比例的词，可以是其他的比例吗？</strong></h2><p>来自@海晨威的算法屋</p><p>BERT采用的Masked LM，会选取语料中所有词的15%进行随机mask，论文中表示是受到完形填空任务的启发，但其实<strong>与CBOW也有异曲同工之妙</strong>。</p><p>从CBOW的角度，这里 <img src="https://www.zhihu.com/equation?tex=p%3D15%5C%25" alt="[公式]"> 有一个比较好的解释是：在一个大小为 <img src="https://www.zhihu.com/equation?tex=1%2Fp%3D100%2F15%5Capprox7" alt="[公式]"> 的窗口中随机选一个词，类似CBOW中滑动窗口的中心词，区别是这里的滑动窗口是非重叠的。</p><p>那从CBOW的滑动窗口角度，10%~20%都是还ok的比例。</p><p>上述非官方解释，是来自我的一位朋友提供的一个理解切入的角度，供参考。</p><p>来自@Serendipity</p><p>15%的概率是通过实验得到的最好的概率，xlnet也是在这个概率附近，说明在这个概率下，既能有充分的mask样本可以学习，又不至于让segment的信息损失太多，以至于影响mask样本上下文信息的表达。然而因为在下游任务中不会出现token“<mask>”，所以预训练和fine-tune出现了不一致，为了减弱不一致性给模型带来的影响，被mask的token有80%的概率用“<mask>”表示，有10%的概率随机替换成某一个token，有10%的概率保留原来的token，这3个百分比也是多次实验得到的最佳组合，在这3个百分比的情况下，下游任务的fine-tune可以达到最佳的实验结果。</p><h2 id="为什么BERT在第一句前会加一个-CLS-标志"><a href="#为什么BERT在第一句前会加一个-CLS-标志" class="headerlink" title="为什么BERT在第一句前会加一个[CLS]标志?"></a><strong>为什么BERT在第一句前会加一个[CLS]标志?</strong></h2><p>bert在token序列之前加了一个特定的token“[cls]”，这个token对应的向量后续会用在分类任务上；如果是句子对的任务，那么两个句子间使用特定的token“[seq]”来分割。</p><p>为什么选它呢，因为与文本中已有的其它词相比，这个无明显语义信息的符号会<strong>更“公平”地融合文本中各个词的语义信息</strong>，从而更好的表示整句话的语义。</p><p>这里补充一下bert的输出，有两种：</p><p>一种是get_pooled_out()，就是上述[CLS]的表示，输出shape是[batch size,hidden size]。</p><p>一种是get_sequence_out()，获取的是整个句子每一个token的向量表示，输出shape是[batch_size, seq_length, hidden_size]，这里也包括[CLS]，因此在做token级别的任务时要注意它。</p><h2 id="Bert和Transformer在loss上的差异"><a href="#Bert和Transformer在loss上的差异" class="headerlink" title="Bert和Transformer在loss上的差异"></a><strong>Bert和Transformer在loss上的差异</strong></h2><p>transformer的loss是在decoder阶段计算的。bert预训练的loss由2部分构成，一部分是NSP的loss，就是token“[cls]”经过1层Dense，然后接一个二分类的loss，其中0表示segment B是segment A的下一句，1表示segment A和segment B来自2篇不同的文本；另一部分是MLM的loss，segment中每个token都有15%的概率被mask，而被mask的token有80%的概率用“<mask>”表示，有10%的概率随机替换成某一个token，有10%的概率保留原来的token，被mask的token经过encoder后乘以embedding matrix的转置会生成在vocab上的分布，然后计算分布和真实的token的one-hot形式的cross entropy，最后sum起来当作loss。这两部分loss相加起来当作total loss，利用adam进行训练。bert fine-tune的loss会根据任务性质来设计，例如分类任务中就是token“[cls]”经过1层Dense，然后接了一个二分类的loss；例如问题回答任务中会在paragraph上的token中预测一个起始位置，一个终止位置，然后以起始位置和终止位置的预测分布和真实分布为基础设计loss；例如序列标注，预测每一个token的词性，然后以每一个token在词性的预测分布和真实分布为基础设计loss。</p><p>bert在encoder之后，在计算NSP和MLM的loss之前，分别对NSP和MLM的输入加了一个Dense操作，这部分参数只对预训练有用，对fine-tune没用。而transformer在decoder之后就直接计算loss了，中间没有Dense操作。</p><h2 id="为什么bert需要额外的segment-embedding"><a href="#为什么bert需要额外的segment-embedding" class="headerlink" title="为什么bert需要额外的segment embedding?"></a><strong>为什么bert需要额外的segment embedding?</strong></h2><p>因为bert预训练的其中一个任务是判断segment A和segment B之间的关系，这就需要embedding中能包含当前token属于哪个segment的信息，然而无论是token embedding，还是position embedding都无法表示出这种信息，因此额外创建一个segment embedding matrix用来表示当前token属于哪个segment的信息，segment vocab size就是2，其中index=0表示token属于segment A，index=1表示token属于segment B。</p><h2 id="为什么transformer的embedding后面接了一个dropout，而bert是先接了一个layer-normalization，再接dropout"><a href="#为什么transformer的embedding后面接了一个dropout，而bert是先接了一个layer-normalization，再接dropout" class="headerlink" title="为什么transformer的embedding后面接了一个dropout，而bert是先接了一个layer normalization，再接dropout?"></a><strong>为什么transformer的embedding后面接了一个dropout，而bert是先接了一个layer normalization，再接dropout?</strong></h2><p>LN是为了解决梯度消失的问题，dropout是为了解决过拟合的问题。在embedding后面加LN有利于embedding matrix的收敛。</p><h2 id="BERT模型有什么调参技巧"><a href="#BERT模型有什么调参技巧" class="headerlink" title="BERT模型有什么调参技巧?"></a><strong>BERT模型有什么调参技巧?</strong></h2><p><a href="https://www.zhihu.com/question/373856698/answer/1034691809">https://www.zhihu.com/question/373856698/answer/1034691809</a></p><h2 id="Transformer中warm-up和LayerNorm的重要性？"><a href="#Transformer中warm-up和LayerNorm的重要性？" class="headerlink" title="Transformer中warm-up和LayerNorm的重要性？"></a><strong>Transformer中warm-up和LayerNorm的重要性？</strong></h2><p><a href="https://zhuanlan.zhihu.com/p/84614490">https://zhuanlan.zhihu.com/p/84614490</a></p><h2 id="Bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？"><a href="#Bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？" class="headerlink" title="Bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？"></a><a href="https://www.zhihu.com/question/318355038">Bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</a></h2><p><a href="https://www.zhihu.com/question/318355038">https://www.zhihu.com/question/318355038</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Transformer的辅助&quot;&gt;&lt;a href=&quot;#Transformer的辅助&quot; class=&quot;headerlink&quot; title=&quot;Transformer的辅助&quot;&gt;&lt;/a&gt;Transformer的辅助&lt;/h1&gt;&lt;p&gt;转载：&lt;a href=&quot;https://zh</summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
</feed>
