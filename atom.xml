<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Coding-Zuo</title>
  
  <subtitle>Coding And Studying</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-09-18T11:54:42.557Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Coding-Zuo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>La-MAML: Look-ahead Meta Learning for Continual Learning</title>
    <link href="http://example.com/2021/09/17/La-MAML-Look-ahead-Meta-Learning-for-Continual-Learning/"/>
    <id>http://example.com/2021/09/17/La-MAML-Look-ahead-Meta-Learning-for-Continual-Learning/</id>
    <published>2021-09-17T06:37:09.000Z</published>
    <updated>2021-09-18T11:54:42.557Z</updated>
    
    <content type="html"><![CDATA[<h1 id="La-MAML-Look-ahead-Meta-Learning-for-Continual-Learning"><a href="#La-MAML-Look-ahead-Meta-Learning-for-Continual-Learning" class="headerlink" title="La-MAML: Look-ahead Meta Learning for Continual Learning"></a>La-MAML: Look-ahead Meta Learning for Continual Learning</h1><p>持续学习问题涉及能力有限的训练模型，这些模型在一组未知数量的顺序到达的任务上表现良好。</p><p>虽然元学习在减少新旧任务之间的干扰方面显展示出潜力，但目前的训练过程往往要么很慢，要么离线，而且对许多超参数很敏感。</p><p>作者提出<em>Look-ahead MAML (La-MAML)</em> 一种optimisation-based的快速元学习算法，用于在线持续学习，并辅之以小情节记忆。</p><p>作者在元学习更新中提出的对每个参数学习率的调制，并将其与先前关于超梯度 <em>hypergradients</em> 和元下降 <em>meta-descent</em> 的工作联系起来。</p><p>与传统的基于先验的方法相比，这提供了一种更灵活和更有效的方式来减轻灾难性遗忘。</p><p>作者开发了一种基于梯度的元学习算法，以实现高效的在线持续学习。先提出了一种连续元学习的基本算法，称为连续MAML(C-MAML)，它利用replay-buffer并优化了一个减轻遗忘的元目标。随后提出了一种对C-MAML的改进，称为La-MAML，它包括对每参数学习率(LRs)的调制，以跨任务和时间调整模型的学习速度。</p><h2 id="连续学习与元学习"><a href="#连续学习与元学习" class="headerlink" title="连续学习与元学习"></a>连续学习与元学习</h2><h3 id="涉及论文"><a href="#涉及论文" class="headerlink" title="涉及论文"></a>涉及论文</h3><ul><li>GEM：Gradient episodic memory for continual learning.</li><li>MER：Learning to learn without forgetting by maximizing transfer and minimizing interference (2019)</li><li>Reptile：On first-order meta-learning algorithms  (2017)</li><li>12：Meta-learning representations for continual learning.  (2019)</li><li>Generative-replay：Continual learning with deep generative replay. (2019)</li><li>A-GEM：Efficient lifelong learning with a-GEM  (2019)</li><li>Online-aware Meta Learning (OML) : Meta-learning representations for continual learning (2019)</li><li>2 : Continuous adaptation via meta-learning in nonstationary and competitive environments (2018)</li><li>10 : Online meta-learning (2019)</li><li>19:Continual adaptation for model-based RL (2019)</li><li>BGD：Task Agnostic Continual Learning Using Online Variational Bayes</li><li>UCB：Uncertainty-guided continual learning with bayesian neural networks</li><li>AlphaMAML : Adaptive Model-Agnostic Meta-Learning.</li></ul><p>灾难性的遗忘是Continual Learning的最大的挑战之一，当随机梯度下降（SGD）所需的i.i.d.采样条件被违反时，可能会发生这种情况，因为属于要学习的不同任务的数据按顺序到达。</p><p>连续学习(CL)算法还必须有效地利用其有限的模型容量，因为未来任务的数量是未知的。因此，确保各任务之间的梯度对齐至关重要，以便在实现其目标方面取得共同进展。</p><p>梯度情节记忆(GEM)研究了CL中权重分担和遗忘之间的关系，并开发了一种显式尝试最小化梯度干扰的算法。</p><p>Meta Experience Replay(MER)形式化了迁移-干扰权衡，并表明GEM的梯度排列目标与一阶元学习算法Reptile优化的目标一致。</p><p>除了对齐梯度外，元学习算法对 CL 也很有前景，因为它们可以直接使用元目标来影响模型优化并改进泛化或迁移等辅助目标。这避免了为了更好的CL而定义诸如稀疏性这样的启发式激励。缺点是它们通常很慢，很难调整，使它们更适合离线继续学习[12]。</p><h3 id="持续学习"><a href="#持续学习" class="headerlink" title="持续学习"></a>持续学习</h3><p>方法大致三种类型 replay-based, regularisation (or prior-based) 和  meta-learning-based .</p><ul><li>replay-based：为了避免灾难性遗忘的问题，replay-based的方法在内存中维护以前任务的样本集合。利用情节缓冲器（<em>episodic-buffer</em>）统一采样旧数据以模拟独立同分布的方法。<em>Generative-replay</em>训练生成模型能够重放过去的样本，但由于复杂的非平稳分布建模的困难而引起的可扩展性问题。GEM和A-GEM将存储器样本考虑在内，来确定改变的低干扰梯度以更新参数。</li><li>Regularisation-based：是一种启发式的方法，确保保留先前任务的性能的来约束网络权重，从而完全避免使用重放。这包括惩罚被认为对旧任务很重要的权重的改变，或者强制实施权重或表征稀疏性，以确保在任何时间点只有一部分神经元保持活跃。</li><li>Meta-Learning-based：这些方法是最近才出现的。MER 受 GEM 的启发，利用重放来激励新旧任务之间的梯度对齐。 OML引入了用于预训练算法的元目标，以离线学习最优表示，该最优表示随后被冻结并用于CL。[2，10，19]研究正交设置，其中学习代理使用所有先前看到的数据来快速适应传入的数据流，从而忽略灾难性遗忘的问题。</li></ul><h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><p>首先通过顺序地观察 $T$ 个任务的训练数据$[D_1,D_2,…,D_T]$ 来学习 $T$ 个任务的序列 $[\tau_1,\tau_2 ,…,\tau_T]$  </p><p>定义$X^i,Y^i = {(x<em>n^i,y_n^i)}</em>{n=0}^{N_i}$ 为$N_i$个输入标签集合从数据 $D_i$ 中随机抽取。</p><p>在在线学习过程中的任意时间步长 $j$，我们的目标是最小化模型在迄今看到的所有 $t$ 个任务上的经验风险 $(τ_{1:t})$，给定对来自先前任务 $τ_i(i&lt;t)$ 的数据 $(X_i，Y_i)$ 限制访问。我们将这一目标称为累积风险，具体如下：</p><script type="math/tex; mode=display">\sum_{i=1}^t \mathbf{E}_{(X^i,Y^i)}[l_i(f_i(X^i;\theta) ,Y^i)] = \mathbf{E}_{(X^{1:t},Y^{1:t})}[L_t(f(X^{1:t};\theta), Y^{1:t})]</script><p>其中 $l_i$ 是在任务$\tau_i$上的loss， $f_i$ 是学习器，参数 $θ_0^j$ 是从输入到输出的特定任务映射参数。</p><p>$L<em>t = \sum</em>{i=1}^t l<em>i$  是任务 $τ</em>{1:t}$ 的所有任务损失之和，其中 $t$ 从 1到 $T$ 。设 $l$ 表示要最小化的某些损失目标。</p><p>作用于参数$θ_0^j$ (由 $U(θ_0^j)$ ) 表示的SGD运算定义为：</p><script type="math/tex; mode=display">U(\theta^j_0)=\theta_1^j = \theta_0^j-\alpha \nabla_{\theta_0^j} l(\theta_0^j) = \theta_0^j - \alpha g_0^j</script><p>$U$ 可以为 $U_k(\theta_0^j) = U…\circ U\circ U(\theta_0^j) =\theta_k^j$</p><h3 id="Model-Agnostic-Meta-Learning-MAML"><a href="#Model-Agnostic-Meta-Learning-MAML" class="headerlink" title="Model-Agnostic Meta-Learning (MAML):"></a>Model-Agnostic Meta-Learning (MAML):</h3><p>meta learning 成为一种流行的训练模型的方法，能够在有限的数据上进行快速调整。MAML建议优化模型参数，以学习一组任务，同时改进辅助目标，如任务分布中的few-shot 少样本泛化。</p><p>基于梯度的元学习中使用的一些常用术语：</p><ul><li><p>初始化：在训练期间的给定时间步长 $j$ 处，模型参数 $θ_0^j$ (或为简单起见，$θ_0$)通常被称为初始化，因为其目的是找到对不可见数据进行 few-shot 基于梯度的适配的理想起点。</p></li><li><p>inner-updates快速或内部更新：是对 $θ_0$ 的副本进行基于梯度的更新，以优化某些内部目标(在本例中，对于某些$τ_i$，为$l_i$)。</p></li><li><p>meta-update：元更新涉及从 $θ<em>0$ 到 $θ_k$ 的快速更新的轨迹，然后进行到 $θ_0$的永久梯度更新(或缓慢更新)。该缓慢更新是通过评估 $θ_k$ 上的辅助目标(或元损失meta loss $L</em>{meta}$)并通过轨迹微分以获得$\nabla<em>{\theta_k}L</em>{meta}(\theta<em>k)$来计算的。因此，MAML在时间步 $j$ 优化 $θ_0^j$，以便在对它们的样本进行几次梯度更新之后，对 ${\tau</em>{1:t}}$中的任务执行最佳性能。它在每一次元更新中都进行了优化，目标是：</p><script type="math/tex; mode=display">min_{\theta_0^j}\mathbf{E}_{\tau_{1:t}}[L_{meta}(U_k(\theta_0^j))] = min_{\theta_0^j}\mathbf{E}_{\tau_{1:t}}[L_{meta}(\theta_k^j)]</script></li></ul><h3 id="元学习与合作学习目标的等价性"><a href="#元学习与合作学习目标的等价性" class="headerlink" title="元学习与合作学习目标的等价性:"></a>元学习与合作学习目标的等价性:</h3><p>Reptile证明了Reptile算法和MAML算法等一阶和二阶元学习算法的近似等价性</p><p>MER随后表明，他们的CL目标是在一组任务$\tau_{1:t}$之间最小化损失并调整梯度，直到任何时间 $j$（在左边），可以通过Reptile目标（在右边）进行优化，即：</p><script type="math/tex; mode=display">min_{\theta_0^j} (\sum_{i=1}^t(l_i(\theta_0^j)) - \alpha\sum_{p,q\le t}(\frac{\partial l_p(\theta_0^j)} {\partial\theta_0^j} \cdot \frac{\partial l_q(\theta_0^j)}{\partial\theta_0^j})) =min_{\theta_{0}^j }\mathbf{E_{\tau_{1:t}}}[L_t(U_k(\theta_0^j))]</script><p>其中 meta-loss $L<em>t = \sum</em>{i=1}^t l<em>i$  根据Tasks $\tau</em>{1:t}$中的样本进行评估。这意味着元学习初始化的过程与学习CL的最优参数一致。</p><h3 id="在线感知元学习-OML-："><a href="#在线感知元学习-OML-：" class="headerlink" title="在线感知元学习(OML)："></a>在线感知元学习(OML)：</h3><p>[12]提出了元学习 <em>Representation-Learning Network</em>  网络(RLN)的概念，为<em>Task-Learning Network</em> (TLN)提供适合协作学习的表示。</p><p>RLN的表示是在离线阶段学习的，在该阶段使用灾难性遗忘作为学习信号进行训练。当TLN经历时间相关更新时，来自固定任务集（$\tau_{val}$）的数据被反复用于评估RLN和TLN。</p><p>在每个元更新的内循环中，TLN使用冻结的RLN对流式任务数据进行快速更新。然后，通过根据来自$\tau_{val}$的数据以及当前任务计算的 meta loss来评估RLN和更新的TLN。</p><p>这将测试在尝试学习流任务的过程中，模型在 $\tau_{val}$ 上的性能发生了怎样的变化。然后对meta loss进行微分，以获得针对TLN和RLN的缓慢更新的梯度。</p><p>这两个loss的组合被称为OML目标，以模拟内环中的CL和测试外环中的遗忘。RLN学习最终为CL的TLN提供更好的表示，该表示被证明具有紧急稀疏性。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>在上一节中，我们看到OML目标可以直接规范CL行为，并且MER利用了元学习和CL目标的近似等价性。我们注意到，OML离线训练静态表示和MER算法慢得令人望而却步。</p><p>作者表明，通过多步MAML过程在线优化OML目标等同于更有效的样本效率CL目标。</p><h3 id="Continual-MAML-C-MAML"><a href="#Continual-MAML-C-MAML" class="headerlink" title="Continual-MAML (C-MAML)"></a>Continual-MAML (C-MAML)</h3><p>C-MAML旨在在线优化OML目标，这样学习当前任务就不会导致忘记以前见过的任务。我们定义了这个目标，适用于优化模型的参数 θ 而不是时间步 j 的表示，如下所示：</p><script type="math/tex; mode=display">min_{\theta_0^j} OML(\theta_0^j,t) = min_{\theta_0^j} \sum_{S_k^j\sim D_t}[L_t(U_k(\theta_0^j,S_k^j))]</script><p>其中 $S<em>k^j$ 是来自前任务$\tau</em>{t}$ 中的k个数据元组的流 $(X<em>{j+l}^t,Y</em>{j+l}^t)^k$ ，这是模型在时间 $j$ 处看到的。</p><p>Meta loss $L<em>t = \sum</em>{i=1}^l l<em>i$ 在 $\theta</em>{k}^j = U<em>k(\theta</em>{0}^j ,S_k^j)$ 上评估。它评估 $\theta_k^j$ 对于上面第一个公式中定义的持续学习预测任务的适合性，直到$\tau_t$.</p><p>省略了隐含的数据参数 $(x^i,y^i) \sim (X^i,Y^i)$ 这是任何任务$\tau_i$的 $L_t$ 中每个损失 $l_i$ 的输入。附录B</p><script type="math/tex; mode=display">min_{\theta_0^j}\mathbf{E}_{\tau_{1:t}} [L_t(U_k(\theta_0^j))] = min_{\theta_0^j} \sum_{i=1}^t(l_i(\theta_0^j) - \alpha \frac{\partial l_i(\theta_0^j)} {\partial\theta_0^j} \cdot \frac{\partial l_t(\theta_0^j)}{\partial\theta_0^j})</script><p>与上文元学习与合作学习目标的等价性那个公式不同它是不对称的，它集中于对其$\tau<em>t$的梯度和$\tau</em>{1:t}$的平均梯度，而不是在任务$\tau_{1:t}$之间对齐所有成对梯度。附录D</p><p>作者的经验表明，旧任务之间的梯度对齐不会退化，而学习了新任务，避免了重复优化它们之间的任务间对齐的需要。</p><p>这导致MER目标的显著加速，该目标试图将所有$\tau_{1:t}$ 上相乘均匀分布的批次。由于每个s 在梯度更新中有 $1/t-th$ 次贡献，因此MER有必要对包括s在内的许多此类均匀批次进行多次传递。</p><p>在训练期间，如MER中所示，通过对输入数据流的存储采样来填充重放缓冲器R。</p><p>在每次元更新开始时，从当前任务中采batch b。b还与从R采样的批次组合以形成元批次meta-batch $b<em>m$，其表示来自旧任务和新任务的样本。$\theta_0^j$通过k个基于sgd的内部更新进行更新，每次从b查看一个当前任务的样本。外部损失或元损失$L_t(\theta_k^j)$是在$b_m$上评估的。它指示参数 $\theta_k^j$ 在时间 $j$ 之前看到的所有任务 $\tau</em>{1:t}$上的性能。 附录C</p><h3 id="Lookahead-MAML-La-MAML"><a href="#Lookahead-MAML-La-MAML" class="headerlink" title="Lookahead-MAML (La-MAML)"></a>Lookahead-MAML (La-MAML)</h3><p>尽管元学习激励了任务内和任务间组的梯度对齐，但在新旧任务的梯度之间仍然可能有一些干扰，$\tau<em>{1:t−1}$和 $\tau</em>{t}$。</p><p>这将导致忘记 $\tau<em>{1:t−1}$，因为它的数据不再对我们完全可用。在训练新任务的开始阶段尤其如此，因为新任务的梯度不一定与旧任务一致。因此，需要一种机制来确保元更新相对于$\tau</em>{1:t-1}$是保守的、避免负迁移。元更新的幅度和方向需要根据更新对 $\tau_{1:t-1}$ 损失的影响程度进行调整。La-MAML包括一组可学习的pre-parameter学习率(LR)，用于内部更新，如图1所示。</p><p><img src="https://i.loli.net/2021/09/18/LhcnFOblKsCEPAZ.png" alt=""></p><p>对于每批数据，初始权重经历一系列 $k$ 次快速更新以获得 $\theta_k^j$ (这里 $j=0$)，其针对元损失进行评估以相对于权重 $\theta_0$ 和LRs $\alpha_0$ 反向传播梯度。首先，$\alpha^0$更新为$\alpha^1$，然后用于将 $\theta_0^0$更新为 $\theta_0^1$，蓝色框表示快速权重，绿色框表示慢速更新的梯度。LRs和权重以异步方式更新。</p><p>这是因为我们观察到上面OML的方程的梯度相对于内循环的 LR 的表达式直接反映了旧任务和新任务之间的对齐情况。扩充的学习目标被定义为:</p><script type="math/tex; mode=display">min_{\theta_0^j,\alpha^j} \sum_{S_k^j\sim D_t}[L_t(U_k(\alpha^j,\theta_0^j,S_k^j))]</script><p>以及该目标在时间 $j$ 的梯度，相对于 LR 向量 $\alpha^j$ （定义为$g_{MAML}(\alpha^j)$）:</p><script type="math/tex; mode=display">g_{MAML}(\alpha^j) = \frac{\partial}{\partial\alpha^j} L_t(\theta^j_k) = \frac{\partial}{\partial\theta^j_k}L_t(\theta^j_k) \cdot (-\sum_{k'=0}^{k-1}\frac{\partial}{\partial\theta_k^j} l_t(\theta_{k'}^j))</script><p>附录A推导</p><p>$g<em>{MAML}(\alpha)$ 中的第一项对应于元损失的梯度在batch上 $b_m:g</em>{meta}$。第二项表示来自内部更新的累积梯度：$g_{traj}$。</p><p>该表达式表明，当$g<em>{meta}$和 $g</em>{traj}$之间的内积较高时，LRs的梯度将为负，即两者对齐；当两者正交（不干扰）时为零，当两者之间存在干扰时为正。</p><p>负的(正的)LR梯度会拉高(降低)LR的大小。如下图：</p><p><img src="https://i.loli.net/2021/09/18/jQSTzt4dbLsVyqa.png" alt=""></p><p>$g<em>{traj}$ (蓝色虚线)和 $g</em>{meta}$对齐的不同场景，从扰动(左)到对齐(右)。黄色箭头表示内部更新。当梯度对齐(扰动)时，LR $\alpha$ </p><p>增加(减少)。</p><p>我们建议在元更新中异步更新网络权重和LRs。设$\alpha^{j+1}$ 为更新的LR 向量，该向量通过在时间 $j$ 处采用上一个等式中的LR梯度进行SGD而获得。然后，我们将权重更新为：</p><script type="math/tex; mode=display">\theta_0^{j+1} \leftarrow \theta_0^j - max(0,\alpha^{j+1}) \cdot \nabla_{\theta_0^j} L_t(\theta_k^j)</script><p>其中k是在内循环中采取的步数。 将 LRs $\alpha^{j+1}$ 修剪为正值，以避免上升梯度，并且也避免进行扰动的参数更新，从而减轻灾难性遗忘。因此，元目标保守地调节学习的速度和方向，以便在新任务上取得更快的学习进度，同时促进旧任务的迁移。</p><p><img src="https://i.loli.net/2021/09/18/oB2XVDnMRjteq3b.png" alt=""></p><p>Line(a)，(b)是C-MAML和La-MAML之间的唯一区别，C-MAML使用固定标量LR $\alpha$ 进行元更新到 $\theta_0^j$，而不是 $\alpha^{j+1}$。</p><p>作者的基于元学习的算法结合了基于先验和基于回放的方法的概念。LR在重放样本上的梯度和流任务之间的相互作用的指导下，以数据驱动的方式调制参数更新。然而，由于LR随着每次元更新而演变，它们的衰变是暂时的。这与许多基于先验的方法不同，在这些方法中，对参数更改的惩罚逐渐变得非常高，以至于网络容量饱和。</p><p>随着任务的到来，可学习的LR可以调整为高值和低值，因此是一种更简单、灵活和优雅的约束权重的方法。这种异步更新类似于信任区域优化或前瞻搜索，因为每个参数的步长是根据对它们应用假设更新后产生的损失进行调整的。</p><h3 id="与其他工作的联系"><a href="#与其他工作的联系" class="headerlink" title="与其他工作的联系"></a>与其他工作的联系</h3><h4 id="Stochastic-Meta-Descent-SMD"><a href="#Stochastic-Meta-Descent-SMD" class="headerlink" title="Stochastic Meta-Descent (SMD)"></a>Stochastic Meta-Descent (SMD)</h4><p>当学习非平稳数据分布时，使用衰减的LR策略并不常见。严格递减 LR 策略旨在更接近收敛于固定分布的固定 mimima，这与在线学习的目标不一致。由于数据分布的范围未知，因此也不可能手动调整这些计划。</p><p>然而，LRS的适应性仍然是非常需要的，以适应优化的场景，加速学习，调节适应的程度，以减少灾难性遗忘。我们的自适应LRS可以连接到离线监督学习(OSL)中的元下降。虽然存在几种不同的变种，但它们背后的核心思想和我们的方法是获得适应。当我们根据新旧任务梯度之间的相关性调整增益以在所有任务上共享进展时，[4，25]利用两个连续随机梯度之间的相关性来更快地收敛。我们利用元目标关于LRS的可微性，自动获得LR超梯度。</p><h4 id="Learning-LRs-in-meta-learning"><a href="#Learning-LRs-in-meta-learning" class="headerlink" title="Learning LRs in meta-learning"></a>Learning LRs in meta-learning</h4><p>Meta-SGD 建议学习MAML中的LRS以进行few-shot学习。他们的更新和我们的更新有一些显著的不同。它们同步更新权重和LR，而我们对LRS的异步更新用于执行更保守的权重更新。</p><p>我们更新的直觉来自于需要减轻梯度干扰及其与持续学习中普遍存在的转移-干扰权衡的联系。α-MAML解析地更新了MAML更新中的两个标量LR，以实现更自适应的few-shot学习。我们的每个参数的LR通过反向传播被隐式地调制，以基于它们在任务之间的排列来调节参数的变化，为我们的模型在CL领域提供了更强大的适应性。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>指标</p><p>使用保留精度retained accuracy（RA）度量来比较各种方法。RA 是模型在训练结束时跨任务的平均准确率。</p><p>反向传输和干扰 <em>backward-transfer and interference</em>(BTI)值，它衡量每个任务从学习到最后一个任务结束的准确性的平均变化。 较小的 BTI 意味着训练期间遗忘较少。</p><p><em>Efficient Lifelong Learning (ELL)</em>:  高效终身学习 (LLL)：在A-GAM中形式化，高效终身学习的设置假设每个任务的传入数据必须仅通过一次处理：一旦处理，数据样本将不再可访问，除非它们被添加到 回放记忆。</p><p><img src="https://i.loli.net/2021/09/18/DrAbfMtzHEpoaBP.png" alt=""></p><p>随着训练的进行，模型会演变成对遗忘的抵抗力。这意味着超过一个点，它可以在传入样本的一个小窗口上持续进行梯度更新，而不需要进行元更新。</p><p><img src="https://i.loli.net/2021/09/18/mdJEbSojDI7aBvf.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;La-MAML-Look-ahead-Meta-Learning-for-Continual-Learning&quot;&gt;&lt;a href=&quot;#La-MAML-Look-ahead-Meta-Learning-for-Continual-Learning&quot; class=&quot;h</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>4-三种方法彻底解决中位数问题</title>
    <link href="http://example.com/2021/09/15/4-%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95%E5%BD%BB%E5%BA%95%E8%A7%A3%E5%86%B3%E4%B8%AD%E4%BD%8D%E6%95%B0%E9%97%AE%E9%A2%98/"/>
    <id>http://example.com/2021/09/15/4-%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95%E5%BD%BB%E5%BA%95%E8%A7%A3%E5%86%B3%E4%B8%AD%E4%BD%8D%E6%95%B0%E9%97%AE%E9%A2%98/</id>
    <published>2021-09-15T01:31:49.000Z</published>
    <updated>2021-09-16T03:23:36.548Z</updated>
    
    <content type="html"><![CDATA[<h1 id="4-三种方法彻底解决中位数问题"><a href="#4-三种方法彻底解决中位数问题" class="headerlink" title="4-三种方法彻底解决中位数问题"></a>4-三种方法彻底解决中位数问题</h1><h4 id="4-寻找两个正序数组的中位数"><a href="#4-寻找两个正序数组的中位数" class="headerlink" title="4. 寻找两个正序数组的中位数"></a>4. 寻找两个正序数组的中位数</h4><h2 id="两种思想"><a href="#两种思想" class="headerlink" title="两种思想"></a>两种思想</h2><ul><li>真合并：使用归并的方式，合并两个有序数组，得到一个大的有序数组，大的有序数组中的中间位置的元素即为中位数。$O(n+m),O(n+m)$</li><li>假合并：不需要合并两个有序数组，只要找到中位数的位置即可。由于两个数组的长度已知，因此中位数对应的两个数组下标之和也是已知的。维护两个指针，初始时分别指向两个数组的下标为0的位置，每次将指向较小的指针后移一位（如果一个指针已经到达数组末尾，则只需要移动另一个数组的指针），直到达到中位数的位置。$O(n+m),O(1)$</li></ul><h2 id="常见的思想改进：假合并、奇偶合并"><a href="#常见的思想改进：假合并、奇偶合并" class="headerlink" title="常见的思想改进：假合并、奇偶合并"></a>常见的思想改进：假合并、奇偶合并</h2><p>通过假合并的思想可以将空间复杂度优化到$O(1)$但对于时间复杂度并没有什么优化，此方法代码复杂，不仅要考虑奇偶问题，更需要高了一个数组遍历后的各种边界问题。</p><p>假合并的一个优化点是 将奇偶两种情况合并到了一起：</p><ul><li>如果是奇数，我们需要知道第(len+1)/2 个数就可以了，如果遍历的话需要遍历int(len/2)+1次</li><li>如果是偶数，需要知道第(len/2) 和 len/2 + 1个数，也是需要遍历len/2 + 1次</li><li>返回中位数，奇数需要最后一次遍历结果就可以，偶数需要最后一次和上一次的结果。所以用两个变量left和right。right保存当前循环的结果，在每次循环前将right赋值给left。这样在最后一次循环的时候，left将得到right的值，也就是上一次的循环结果，加下来right更新为最后一次的结果len/2+1次</li></ul><p>另一种合并的思想是: 我们可以在奇数的时候, 在<strong>末尾等处添加一个占位符</strong>#等, 这样也是可以将奇数合并成偶数的情况的.此方法的另一个优化点就是 通过在if条件中<strong>加入大量的限制条件</strong>, 从而实现了对于各种边界问题的处理, 这也是一种很重要的思想.</p><p><img src="https://i.loli.net/2021/09/15/rwI2miAu4SPgOX7.jpg" alt=""></p><p><img src="https://i.loli.net/2021/09/15/G1OiltFhg24JmY8.jpg" alt=""></p><p>此方法的时间复杂度相对于下面两种思想还是太高了, 大家不用特意掌握此方法, 但是这两个优化的思想还是很重要的, 要好好的理解一下.</p><p>接下来我们就来详细讲解两个时间复杂度超低的算法代码思想.</p><h2 id="寻找第k小数-记住这个"><a href="#寻找第k小数-记住这个" class="headerlink" title="寻找第k小数(记住这个)"></a>寻找第k小数(记住这个)</h2><p> 主要就是根据两个数的三种比较结果, 不断地去除不满足的元素的过程.</p><p><img src="https://i.loli.net/2021/09/15/hKBdPugr5fRxXeC.jpg" alt=""></p><p>这个思想最难的点在于 <strong>三种特殊情况的处理</strong>, 我们能否想到这三种情况, 并将他们<strong>完美的融入到代码之中</strong>, 我感觉这才是真正的难点所在.</p><p><img src="https://i.loli.net/2021/09/15/AdgYMm3xTtQSup8.jpg" alt=""></p><p>最开始对于奇数和偶数的两种情况进行了判断, 其实是可以将两种情况合并的, 只需要在奇数时求两次同样的k就可以了.</p><p><img src="https://i.loli.net/2021/09/15/rmU7B5WJRGELvjd.jpg" alt=""></p><p>接下来处理了三种特殊情况中的两种特殊情况: 一个数组为空 和 k=1.</p><p><img src="https://i.loli.net/2021/09/15/XGbolki98Dv6BCK.jpg" alt=""></p><p>下面的<strong>几个定义</strong>就非常重要了, 一定要弄清这些定义的含义, 才能更轻松的理解代码.</p><p><img src="https://i.loli.net/2021/09/15/vkMmQjqNbPZGIwn.jpg" alt=""></p><p>index1, index2作为数组的起始点的下标, 初值都是0, 但是随着两个数组不断被删除元素, 这两个起始点也是在不断的进行变化, 具体变化方式就是 index1 = newIndex1 + 1, 因为在删除元素的时候 连同比较位置也一同删去了, 所以新的开始是 比较位置 的后一位.</p><p>newindex1, newindex2作为比较点就是图中被框中的两个数的下标, 它的赋值过程就涉及到了 最后一个边界情况. 因为当一个数组较短时, 其中一个比较点可能已经到达了数组的最后, 所以它的值是 两种情况下较小的那个数.</p><p><img src="https://i.loli.net/2021/09/15/mAW7YsNX5GBcKgH.jpg" alt=""></p><p>接下来就是根据两个比较点的大小来进行不同的操作过程了, 这里最难理解的点就是 k -= (newIndex1 - index1 + 1), 也就是减去元素的个数问题了. 我们根据上面的图来举例, 图中index1的值为0, newindex1的值经过计算为1, 通过比较后, 可以看到 红色的数 就是被删除的数, 也就是两个, 所以我们需要在最后+1才是真实被删去的个数. 对于此类问题在确定最终个数的时候, 我们都可以通过这样的特例来决定代码的书写, 至此代码就全部讲解完成了.</p><p><img src="https://i.loli.net/2021/09/15/1hmHAU4Ets7iFJ9.jpg" alt=""></p><h2 id="理解中位数作用进行划分数组"><a href="#理解中位数作用进行划分数组" class="headerlink" title="理解中位数作用进行划分数组"></a>理解中位数作用进行划分数组</h2><p>最后这种思想时间复杂度比上面的还低，上面的思想每一轮循环可以将查找范围减少一半，因此时间复杂度是$O(log(m+n))$ ，但这种思想可以对确定的较短的数组进行二分查找，所以它的时间复杂度是$O(logmin(m,n))$</p><p>划分数组正好和上面算法完全相反，它的思想特别复杂，但思想理解了，代码写起来倒是没太大难度。</p><p>首先要明白中位数的作用：将一个集合划分为两个长度相等的子集，其中一个子集中的元素总是大于另一个子集中的元素。这种思想无论是在机构数组中都是适用的，这就衍生出了下面的思想。</p><p>首先讨论奇偶两种不同情况的不同划分方式，</p><p><img src="https://i.loli.net/2021/09/16/8n9sSjVk7C3GKZM.jpg" alt=""></p><p>然后在写代码时，由于计算机的取整操作，我们是可以将这两种情况合并成一种代码书写方式，其中的$i$和$j$分别是两个数组的划分位置。</p><p><img src="https://i.loli.net/2021/09/16/LzOv1Zxn8pBUVQo.jpg" alt=""></p><p>同样我们也会遇到复杂的边界问题, 但下面这种处理方式是真的非常优秀.</p><p><img src="https://i.loli.net/2021/09/16/bBhSzkpAOunIXNj.jpg" alt=""></p><p>上面问题都考虑完了, 其实就可以写代码了, 但是我们需要进行两个条件的判断: B[j−1]≤A[i] 以及A[i−1]≤B[j], 为了优化代码, 经过分析后, 我们发现这两种情况是可以等价转换的. 也就是只需要进行一个条件的判断即可.</p><p><img src="https://i.loli.net/2021/09/16/YZ2lJiL9IomnaKH.jpg" alt=""></p><p>代码中有个注意点就是java中的<strong>三目运算符? :</strong> 在Python中是没有引入这个符号的, 但是Python利用了已有的关键字if…else实现了这个功能.</p><p><img src="https://i.loli.net/2021/09/16/gRSNcoWaidyOnhz.jpg" alt=""></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//常规思想 假合并</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">findMedianSortedArrays</span><span class="params">(<span class="keyword">int</span>[] nums1, <span class="keyword">int</span>[] nums2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> m = nums1.length;</span><br><span class="line">        <span class="keyword">int</span> n = nums2.length;</span><br><span class="line">        <span class="keyword">int</span> len = m+n;</span><br><span class="line">        <span class="keyword">int</span> left = -<span class="number">1</span>, right= -<span class="number">1</span>; <span class="comment">// 记录前后两个数</span></span><br><span class="line">        <span class="keyword">int</span> aStart=<span class="number">0</span>, bStart=<span class="number">0</span>;  <span class="comment">// 记录两个数组的移动</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;=len/<span class="number">2</span>;i++)&#123;</span><br><span class="line">            left = right; <span class="comment">// 每次循环前将right的值赋给left</span></span><br><span class="line">            <span class="comment">// A移动的条件：B遍历到最后 或当前 A&lt;B 满足一个即可</span></span><br><span class="line">            <span class="keyword">if</span>(aStart &lt; m &amp;&amp; (bStart&gt;=n || nums1[aStart] &lt; nums2[bStart]))&#123;</span><br><span class="line">                right = nums1[aStart++];</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                right= nums2[bStart++];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>((len &amp; <span class="number">1</span>) == <span class="number">0</span>) <span class="comment">// 与1交，判断奇偶数，更快速</span></span><br><span class="line">            <span class="keyword">return</span> (left+right)/<span class="number">2.0</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> right;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 第k小数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">findMedianSortedArrays</span><span class="params">(<span class="keyword">int</span>[] nums1, <span class="keyword">int</span>[] nums2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> length1 = nums1.length;</span><br><span class="line">        <span class="keyword">int</span> length2 = nums2.length;</span><br><span class="line">        <span class="keyword">int</span> totallength = length1+ length2;</span><br><span class="line">        <span class="keyword">if</span>(totallength % <span class="number">2</span> == <span class="number">1</span>)&#123; <span class="comment">// 可以将两种情况合并，奇数会求两次同样的k</span></span><br><span class="line">            <span class="keyword">int</span> midIndex = totallength/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">double</span> median = getKthElement(nums1, nums2, midIndex+<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">return</span> median;</span><br><span class="line">        &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">int</span> midIndex1 = totallength/<span class="number">2</span> -<span class="number">1</span> , midIndex2 = totallength/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">double</span> median = (getKthElement(nums1, nums2, midIndex1 + <span class="number">1</span>) + getKthElement(nums1, nums2, midIndex2 + <span class="number">1</span>)) / <span class="number">2.0</span>;</span><br><span class="line">            <span class="keyword">return</span> median;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getKthElement</span><span class="params">(<span class="keyword">int</span>[] nums1, <span class="keyword">int</span>[] nums2, <span class="keyword">int</span> k)</span></span>&#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">        主要思路：要找到第k(k&gt;1)小的元素，那么就取pivot1 = nums1[k/2-1] 和 pivot2 = nums2[k/2-1] 进行比较</span></span><br><span class="line"><span class="comment">        这里的“/” 表示整除</span></span><br><span class="line"><span class="comment">        nums1 中小于等于pivot1的元素有 nums1[0..k/2-2] 共计k/2-1个</span></span><br><span class="line"><span class="comment">        nums2 中小于等于pivot2的元素有 nums2[0..k/2-2] 共计k/2-1个</span></span><br><span class="line"><span class="comment">        取 pivot = min(pivot1 , pivot2) 两个数组中小于等于 pivot的元素共计不会超过(k/2-1)+(k/2-1) &lt;=k-2个</span></span><br><span class="line"><span class="comment">        这样pivot本身最大也只能是第k-1小的元素</span></span><br><span class="line"><span class="comment">        * 如果 pivot = pivot1,那么 nums1[0 .. k/2-1] 都不可能是第 k 小的元素。把这些元素全部 &quot;删除&quot;,剩下的作为新的 nums1 数组</span></span><br><span class="line"><span class="comment">         * 如果 pivot = pivot2,那么 nums2[0 .. k/2-1] 都不可能是第 k 小的元素。把这些元素全部 &quot;删除&quot;,剩下的作为新的 nums2 数组</span></span><br><span class="line"><span class="comment">        由于我们 &quot;删除&quot; 了一些元素（这些元素都比第 k 小的元素要小）,因此需要修改 k 的值,减去删除的数的个数</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">int</span> length1 = nums1.length, length2=nums2.length;</span><br><span class="line">        <span class="keyword">int</span> index1 = <span class="number">0</span>, index2=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> kthElemnt=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">            <span class="comment">//特殊情况</span></span><br><span class="line">            <span class="keyword">if</span>(index1==length1)&#123;<span class="comment">//第二种特殊情况，一个数组为空</span></span><br><span class="line">                <span class="keyword">return</span> nums2[index2+k-<span class="number">1</span>];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(index2==length2)&#123;<span class="comment">//一个数组为空</span></span><br><span class="line">                <span class="keyword">return</span> nums1[index1+k-<span class="number">1</span>];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(k==<span class="number">1</span>)&#123; <span class="comment">// 第三种情况，k=1</span></span><br><span class="line">                <span class="keyword">return</span> Math.min(nums1[index1], nums2[index2]);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 正常情况， index1，index2作为起始点，newindex1,newindex2作为比较点 在不停的更新</span></span><br><span class="line">            <span class="keyword">int</span> half = k/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">int</span> newIndex1 = Math.min(index1 + half, length1) -<span class="number">1</span>;<span class="comment">// 第一种情况，发生越界，需要记录比较的位置</span></span><br><span class="line">            <span class="keyword">int</span> newIndex2 = Math.min(index2 + half, length2) - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">int</span> pivot1 = nums1[newIndex1], pivot2=nums2[newIndex2]; <span class="comment">//获取两个需要比较的数</span></span><br><span class="line">            <span class="keyword">if</span>(pivot1&lt;=pivot2)&#123;</span><br><span class="line">                <span class="comment">// 将两种情况合并</span></span><br><span class="line">                k -= (newIndex1 -index1 +<span class="number">1</span>); <span class="comment">//两者相减后+1，这才是真正减去的长度</span></span><br><span class="line">                index1 = newIndex1+<span class="number">1</span>; <span class="comment">//连同比较位置也一同删去了，所以新的开始是 比较位置的后一位</span></span><br><span class="line">            &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">                k -= (newIndex2-index2+<span class="number">1</span>);</span><br><span class="line">                index2 = newIndex2+<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">findMedianSortedArrays</span><span class="params">(<span class="keyword">int</span>[] nums1, <span class="keyword">int</span>[] nums2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (nums1.length &gt; nums2.length) &#123;</span><br><span class="line">            <span class="keyword">return</span> findMedianSortedArrays(nums2, nums1);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> m = nums1.length;</span><br><span class="line">        <span class="keyword">int</span> n = nums2.length;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = m;</span><br><span class="line">        <span class="comment">// median1：前一部分的最大值</span></span><br><span class="line">        <span class="comment">// median2：后一部分的最小值</span></span><br><span class="line">        <span class="keyword">int</span> median1 = <span class="number">0</span>, median2 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (left &lt;= right) &#123; <span class="comment">// 一直循环找到一个最大的i满足A[i-1]≤B[j]</span></span><br><span class="line">            <span class="comment">// 前一部分包含 nums1[0 .. i-1] 和 nums2[0 .. j-1]</span></span><br><span class="line">            <span class="comment">// 后一部分包含 nums1[i .. m-1] 和 nums2[j .. n-1]</span></span><br><span class="line">            <span class="keyword">int</span> i = (left + right) / <span class="number">2</span>; <span class="comment">//二分法,i从区间中间开始</span></span><br><span class="line">            <span class="keyword">int</span> j = (m + n + <span class="number">1</span>) / <span class="number">2</span> - i;<span class="comment">//+1的操作将总数为奇数和偶数合并为一种情况</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">//nums_im1, nums_i, nums_jm1, nums_j 分别表示 nums1[i-1], nums1[i], nums2[j-1], nums2[j]</span></span><br><span class="line">            <span class="comment">//当一个数组不出现在前一部分时,对应的值为负无穷,就不会对前一部分的最大值产生影响</span></span><br><span class="line">            <span class="keyword">int</span> nums_im1 = (i == <span class="number">0</span> ? Integer.MIN_VALUE : nums1[i - <span class="number">1</span>]);</span><br><span class="line">            <span class="comment">//当一个数组不出现在后一部分时,对应的值为正无穷,就不会对后一部分的最小值产生影响</span></span><br><span class="line">            <span class="keyword">int</span> nums_i = (i == m ? Integer.MAX_VALUE : nums1[i]);</span><br><span class="line">            <span class="keyword">int</span> nums_jm1 = (j == <span class="number">0</span> ? Integer.MIN_VALUE : nums2[j - <span class="number">1</span>]);</span><br><span class="line">            <span class="keyword">int</span> nums_j = (j == n ? Integer.MAX_VALUE : nums2[j]);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (nums_im1 &lt;= nums_j) &#123;</span><br><span class="line">                median1 = Math.max(nums_im1, nums_jm1);</span><br><span class="line">                median2 = Math.min(nums_i, nums_j);</span><br><span class="line">                left = i + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                right = i - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> (m + n) % <span class="number">2</span> == <span class="number">0</span> ? (median1 + median2) / <span class="number">2.0</span> : median1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="comment"># 常规思想</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findMedianSortedArrays1</span>(<span class="params">self, nums1, nums2</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type nums1: List[int]</span></span><br><span class="line"><span class="string">        :type nums2: List[int]</span></span><br><span class="line"><span class="string">        :rtype: float</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        m = <span class="built_in">len</span>(nums1)</span><br><span class="line">        n = <span class="built_in">len</span>(nums2)</span><br><span class="line">        lens = m+n</span><br><span class="line">        left, right = -<span class="number">1</span>, -<span class="number">1</span></span><br><span class="line">        aStart,bStart = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(lens//<span class="number">2</span>+<span class="number">1</span>):</span><br><span class="line">            left = right  <span class="comment"># 每次循环将right的值赋值给left</span></span><br><span class="line">            <span class="comment"># A移动的条件，B遍历到最后 或A&lt;B满足一个</span></span><br><span class="line">            <span class="keyword">if</span> aStart&lt;m <span class="keyword">and</span> (bStart &gt;= n <span class="keyword">or</span> nums1[aStart] &lt; nums2[bStart]):</span><br><span class="line">                right = nums1[aStart]</span><br><span class="line">                aStart+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                right = nums2[bStart]</span><br><span class="line">                bStart+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> (lens &amp; <span class="number">1</span>)  == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> (left+right)/<span class="number">2.0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> right</span><br><span class="line">          </span><br><span class="line">          </span><br><span class="line">    <span class="comment"># 第k小数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findMedianSortedArrays</span>(<span class="params">self, nums1, nums2</span>):</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">getKthElemnt</span>(<span class="params">k</span>):</span></span><br><span class="line">            index1, index2 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                <span class="comment"># 特殊情况</span></span><br><span class="line">                <span class="keyword">if</span> index1 == m:</span><br><span class="line">                    <span class="keyword">return</span> nums2[index2+k-<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">if</span> index2 == n:</span><br><span class="line">                    <span class="keyword">return</span> nums1[index1+k-<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">if</span> k==<span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">return</span> <span class="built_in">min</span>(nums1[index1], nums2[index2])</span><br><span class="line"></span><br><span class="line">                newIndex1 = <span class="built_in">min</span>(index1 + k//<span class="number">2</span> -<span class="number">1</span>, m-<span class="number">1</span>)</span><br><span class="line">                newIndex2 = <span class="built_in">min</span>(index2 + k//<span class="number">2</span> -<span class="number">1</span>, n-<span class="number">1</span>)</span><br><span class="line">                privot1,privot2 =nums1[newIndex1], nums2[newIndex2]</span><br><span class="line">                <span class="keyword">if</span> privot1&lt;=privot2:</span><br><span class="line">                    k-=newIndex2-index1 +<span class="number">1</span></span><br><span class="line">                    index1 = newIndex2 + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    k -= newIndex2 - index2 +<span class="number">1</span></span><br><span class="line">                    index2 = newIndex2 +<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        m,n = <span class="built_in">len</span>(nums1), <span class="built_in">len</span>(nums2)</span><br><span class="line">        totalLength = m+n</span><br><span class="line">        <span class="keyword">if</span> totalLength % <span class="number">2</span>==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> getKthElemnt((totalLength+<span class="number">1</span>)//<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (getKthElemnt(totalLength//<span class="number">2</span>)+ getKthElemnt(totalLength//<span class="number">2</span>+<span class="number">1</span>)) /<span class="number">2</span> </span><br><span class="line">          </span><br><span class="line">    <span class="comment"># 划分数组</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findMedianSortedArrays</span>(<span class="params">self, nums1: List[<span class="built_in">int</span>], nums2: List[<span class="built_in">int</span>]</span>) -&gt; float:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums1) &gt; <span class="built_in">len</span>(nums2):</span><br><span class="line">            <span class="keyword">return</span> self.findMedianSortedArrays(nums2, nums1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        infinty = <span class="number">2</span>**<span class="number">40</span>  <span class="comment"># 代表正无穷</span></span><br><span class="line">        m, n = <span class="built_in">len</span>(nums1), <span class="built_in">len</span>(nums2)</span><br><span class="line">        left, right = <span class="number">0</span>, m</span><br><span class="line">        <span class="comment"># median1：前一部分的最大值</span></span><br><span class="line">        <span class="comment"># median2：后一部分的最小值</span></span><br><span class="line">        median1, median2 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> left &lt;= right: <span class="comment"># 一直循环找到一个最大的i满足A[i−1]≤B[j]</span></span><br><span class="line">            <span class="comment"># 前一部分包含 nums1[0 .. i-1] 和 nums2[0 .. j-1]</span></span><br><span class="line">            <span class="comment"># // 后一部分包含 nums1[i .. m-1] 和 nums2[j .. n-1]</span></span><br><span class="line">            i = (left + right) // <span class="number">2</span></span><br><span class="line">            j = (m + n + <span class="number">1</span>) // <span class="number">2</span> - i</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># nums_im1, nums_i, nums_jm1, nums_j 分别表示 nums1[i-1], nums1[i], nums2[j-1], nums2[j]</span></span><br><span class="line">            <span class="comment"># 当一个数组不出现在前一部分时,对应的值为负无穷,就不会对前一部分的最大值产生影响</span></span><br><span class="line">            nums_im1 = (-infinty <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> nums1[i - <span class="number">1</span>]) <span class="comment"># 注意写法与java不同</span></span><br><span class="line">            <span class="comment"># 当一个数组不出现在后一部分时,对应的值为正无穷,就不会对后一部分的最小值产生影响</span></span><br><span class="line">            nums_i = (infinty <span class="keyword">if</span> i == m <span class="keyword">else</span> nums1[i])</span><br><span class="line">            nums_jm1 = (-infinty <span class="keyword">if</span> j == <span class="number">0</span> <span class="keyword">else</span> nums2[j - <span class="number">1</span>])</span><br><span class="line">            nums_j = (infinty <span class="keyword">if</span> j == n <span class="keyword">else</span> nums2[j])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> nums_im1 &lt;= nums_j:</span><br><span class="line">                median1, median2 = <span class="built_in">max</span>(nums_im1, nums_jm1), <span class="built_in">min</span>(nums_i, nums_j)</span><br><span class="line">                left = i + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                right = i - <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (median1 + median2) / <span class="number">2</span> <span class="keyword">if</span> (m + n) % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">else</span> median1</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id=""><a href="#" class="headerlink" title=""></a><a href="https://leetcode-cn.com/problems/median-of-two-sorted-arrays/"></a></h4><p><img src="https://i.loli.net/2021/09/15/roqxyuNvdGnAlX7.jpg" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;4-三种方法彻底解决中位数问题&quot;&gt;&lt;a href=&quot;#4-三种方法彻底解决中位数问题&quot; class=&quot;headerlink&quot; title=&quot;4-三种方法彻底解决中位数问题&quot;&gt;&lt;/a&gt;4-三种方法彻底解决中位数问题&lt;/h1&gt;&lt;h4 id=&quot;4-寻找两个正序数组的中位</summary>
      
    
    
    
    
    <category term="LeetCode" scheme="http://example.com/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>Multimodal Emergent Fake News Detection via Meta Neural Process Networks</title>
    <link href="http://example.com/2021/09/11/Multimodal-Emergent-Fake-News-Detection-via-Meta-Neural-Process-Networks/"/>
    <id>http://example.com/2021/09/11/Multimodal-Emergent-Fake-News-Detection-via-Meta-Neural-Process-Networks/</id>
    <published>2021-09-11T12:20:00.000Z</published>
    <updated>2021-09-17T06:38:04.895Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Multimodal-Emergent-Fake-News-Detection-via-Meta-Neural-Process-Networks"><a href="#Multimodal-Emergent-Fake-News-Detection-via-Meta-Neural-Process-Networks" class="headerlink" title="Multimodal Emergent Fake News Detection via Meta Neural Process Networks"></a>Multimodal Emergent Fake News Detection via Meta Neural Process Networks</h1><p>基于深度学习的模型在对感兴趣事件的大量标注数据进行训练时表现出较好的性能，而在其他事件上由于领域漂移的影响，模型的性能往往会下降。因此，对于难以获得大规模标签数据集的突发事件假新闻的检测，现有的检测方法面临着巨大的挑战。此外，添加来自新出现的事件的知识需要从头开始构建新的模型或继续微调模型，这对于现实世界的设置来说是不切实际的。</p><p>MetaFEND 将元学习(meta learning)和神经过程(np)方法集成在一起, 特别地，提出了标签嵌入模块和硬注意机制，通过处理分类信息和修剪无关帖子来提高效率。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Multimodal-Emergent-Fake-News-Detection-via-Meta-Neural-Process-Networks&quot;&gt;&lt;a href=&quot;#Multimodal-Emergent-Fake-News-Detection-via-Meta</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>1-两数之和到四数之和</title>
    <link href="http://example.com/2021/09/11/1-%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C%E5%88%B0%E5%9B%9B%E6%95%B0%E4%B9%8B%E5%92%8C/"/>
    <id>http://example.com/2021/09/11/1-%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C%E5%88%B0%E5%9B%9B%E6%95%B0%E4%B9%8B%E5%92%8C/</id>
    <published>2021-09-11T01:32:00.000Z</published>
    <updated>2021-09-14T02:00:38.444Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-两数之和到四数之和"><a href="#1-两数之和到四数之和" class="headerlink" title="1-两数之和到四数之和"></a>1-两数之和到四数之和</h1><p>给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target  的那 两个 整数，并返回它们的数组下标。</p><p>你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。</p><p>你可以按任意顺序返回答案。</p><p>示例 1：</p><p>输入：nums = [2,7,11,15], target = 9<br>输出：[0,1]<br>解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。<br>示例 2：</p><p>输入：nums = [3,2,4], target = 6<br>输出：[1,2]<br>示例 3：</p><p>输入：nums = [3,3], target = 6<br>输出：[0,1]</p><h2 id="两数之和"><a href="#两数之和" class="headerlink" title="两数之和"></a>两数之和</h2><h3 id="哈希表法"><a href="#哈希表法" class="headerlink" title="哈希表法"></a>哈希表法</h3><p>哈希表的方法首先用在了两数之和(无序数组)上, 哈希表的使用最主要的目的就是为了降低时间复杂度, 缩减寻找第二个元素使用的时间(将时间复杂度由O(n)降为O(1)), 其中无序数组是哈希表使用的重要条件, 因为当数组有序后, 我们完全可以直接使用 双指针 的方法来降低时间复杂度, 它的使用比 哈希表 更加方便快捷, 空间复杂度也更低, 所以数组有序之后, 我们应该首选 双指针 的方法.</p><p>在使用哈希表的时候, 也有一个很重要的优化点, 就是 遍历两遍哈希表 和 遍历一遍哈希表 的区别. 简单来说就是, 如果我们先将第一个元素放入哈希表中, 然后再寻找第二个元素, 那么我们就需要 遍历两遍哈希表, 如果我们先寻找第二个元素, 之后再将元素放入到哈希表中, 那么就只需要 遍历一遍哈希表.</p><p><img src="https://i.loli.net/2021/09/11/vUaHFiVBCbOrKlx.jpg" alt=""></p><p>上面是我们第一次使用哈希表的情况, 第二次使用哈希表就到了 《四数之和II四数组相加》, 首先由于它具有四个独立的数组, 相当于四维空间, 所以我们很难在这么高的空间维度上直接使用 双指针 的方法, 其次它并没有要求 不重复元组 的情况, 这就给了我们使用 哈希表 的可能性, 因为不用担心复杂的去重操作, 但是使用哈希表一般也是两维的空间, 所以我们必须先进行降维操作, 也就是将四个数组进行分组, 由三种结果的时间复杂度来判断, 我们很容易就选择了 两两分组 的情况.</p><p><img src="https://i.loli.net/2021/09/11/Gi9Cge7pl1Uuhm5.jpg" alt=""></p><p>之后对于哈希表的使用, 就是两种不同情况的使用了。如果需要直接返回相应数组的下标值, 那是很简单的, 我们只需要将 下标值 当做 哈希表的值 即可。(两数之和中的使用)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">int</span>[] twoSum(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target)&#123;</span><br><span class="line">  Map&lt;Integer,Integer&gt; map = HashMap&lt;Integer,Integer&gt;();</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i &lt; nums.length;i++)&#123;</span><br><span class="line">      <span class="keyword">if</span>(map.containsKey(target - nums[i]))&#123;</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;map.get(target-nums[i]), i&#125;;</span><br><span class="line">        &#125;</span><br><span class="line">      map.put(nums[i], i);</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span>(<span class="params">self, nums, target</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type target: int</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        hashtable = <span class="built_in">dict</span>()</span><br><span class="line">        <span class="keyword">for</span> i,num <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">          <span class="keyword">if</span> target - num <span class="keyword">in</span> hashtable:</span><br><span class="line">              <span class="keyword">return</span> [hashtable[target-nums], i]</span><br><span class="line">            hashtable[num] = i</span><br><span class="line">        <span class="keyword">return</span> []</span><br></pre></td></tr></table></figure><h3 id="双指针法"><a href="#双指针法" class="headerlink" title="双指针法"></a>双指针法</h3><p>对于n数之和, 除了哈希表的方法, 最常用的就是 双指针 的方法了, 上文也提到了, 使用双指针最重要的条件就是数组是有序的, 当然这只是针对n数之和的题型, 对于其他题型, 并不需要要求数组是有序</p><p>在n数之和中使用双指针必要条件就是数组是有序的, 这就需要我们根据实际情况来判断 数组是否需要进行排序. 比如在 两数之和 中, 就算使用暴力法也才$O(n^2)$, 但进行排序最快也需要$O(nlogn)$的时间复杂度, 所以对于两数之和来说, 是真的没必要.</p><p>但是对于 三数之和 和 四数之和 就很有必要了, 因为它们时间复杂度实在太高了, 最关键的是它们元组的重复情况也比较多, 想利用哈希表进行去重是非常困难的, 最终只能选择将数组排序后使用 双指针 的方法.</p><h4 id="167-两数之和-II-输入有序数组"><a href="#167-两数之和-II-输入有序数组" class="headerlink" title="167. 两数之和 II - 输入有序数组"></a><a href="https://leetcode-cn.com/problems/two-sum-ii-input-array-is-sorted/">167. 两数之和 II - 输入有序数组</a></h4><p><img src="https://z3.ax1x.com/2021/09/11/hxu1ER.png" alt=""></p><h4 id="两数之和中有序和无序的区别"><a href="#两数之和中有序和无序的区别" class="headerlink" title="两数之和中有序和无序的区别"></a>两数之和中有序和无序的区别</h4><p>在无序数组中寻找第二个数就没有多少捷径, 毕竟数组无序, 很多经典的方法都用不上, 最后只能牺牲空间来换取时间, 利用哈希表将空间复杂度增加到了$O(n)$, 从而降低了寻找第二个数的时间复杂度.</p><p>但是当数组有序之后, 就能使用一些经典的算法同时仍然保证空间复杂度为O(1), 不需要牺牲空间来换取时间, 比如下面马上介绍的 二分法 和 双指针 方法.</p><p>这里给我们提供了一种思维, 那我们是不是也可以将无序数组先进行排序后, 再使用这些经典算法呢? 当然可以这么做, 但对于两数之和来说, 必要性不是太大. 因为最快的排序算法也需要O(nlogn)的时间复杂度, 对于两数之和确实提升也不是太大, 但是对于 三数之和/四数之和 还是挺实用的, </p><h4 id="二分法和寻找插入位置的区别"><a href="#二分法和寻找插入位置的区别" class="headerlink" title="二分法和寻找插入位置的区别"></a>二分法和寻找插入位置的区别</h4><p>数组有序了，使用二分法寻找第二个数可以将时间复杂度降到$O(nlogn)$ 。</p><p>寻找插入位置无论，最终无论是否找到目标值，返回的位置结果都是相同的，而且题中说明数组中无重元素，保证了返回位置的唯一性，所以最终 left == right == mid，返回哪个都无所谓，也并不是需要特殊的将等于目标值这种情况单独写出来。所以代码只讨论了两种情况，最终返回一个值，非常简洁。</p><p>本题使用的二分法，首先并没有要求数组无重复元素，其次我们要的是具体的等于目标值的位置，并不是寻找插入位置，所以在找不到的情况下，只能返回[-1,-1]。首先的返回结果就有了两种情况.</p><p>其次由于有重复元素的存在, 若直接使用之前的只讨论两种情况的二分法是会出错的, 这里必须要讨论三种情况, 且在相等的情况下直接返回正确的结果, 在找不到的情况下返回 [-1, -1].</p><p>本题另外的一个小的注意点是: 返回的下标从1开始, 只要在原本的返回结果上+1就可以了.</p><p>还有一个注意点是, 为了避免重复寻找,在寻找第二个数时,只在第一个数的右侧寻找, 也就是left = i+1.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="comment"># 二分法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span>(<span class="params">self, numbers, target</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type numbers: List[int]</span></span><br><span class="line"><span class="string">        :type target: int</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        n = <span class="built_in">len</span>(numbers)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            left, right = i+<span class="number">1</span>, n  <span class="comment"># 采用左闭右开区间[left, right), left+1避免重复</span></span><br><span class="line">            <span class="keyword">while</span> left&lt;right:</span><br><span class="line">                mid = (right - left) //<span class="number">2</span> + left; <span class="comment"># 防止溢出</span></span><br><span class="line">                <span class="keyword">if</span> numbers[mid] == target - numbers[i]: <span class="comment"># 数组中存在重复元素，必须判断相等</span></span><br><span class="line">                    <span class="keyword">return</span> [i+<span class="number">1</span>, mid+<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">elif</span> numbers[mid] &gt; target - numbers[i]:</span><br><span class="line">                    right = mid <span class="comment"># 右开，真正右端点为mid-1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    left = mid + <span class="number">1</span> <span class="comment"># 左闭，所以小+1</span></span><br><span class="line">        <span class="keyword">return</span> [-<span class="number">1</span>, -<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 双指针</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum1</span>(<span class="params">self, numbers, target</span>):</span></span><br><span class="line">        low, high =<span class="number">0</span>, <span class="built_in">len</span>(numbers-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">while</span> low&lt;high:</span><br><span class="line">            total = numbers[low] + numbers[high]</span><br><span class="line">            <span class="keyword">if</span> total == target:</span><br><span class="line">                <span class="keyword">return</span> [low+<span class="number">1</span>, high+<span class="number">1</span>] <span class="comment"># 返回下标从1开始</span></span><br><span class="line">            <span class="keyword">elif</span> total&lt;target:</span><br><span class="line">                low+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                high-=<span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 二分法</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] twoSum(<span class="keyword">int</span>[] numbers, <span class="keyword">int</span> target) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;numbers.length;i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> left = i+<span class="number">1</span>, rigth = numbers.length;  <span class="comment">// 采用左闭右开区间[left,right),left+1避免重复</span></span><br><span class="line">            <span class="keyword">while</span>(left&lt;right)&#123;</span><br><span class="line">                mid = left + right /<span class="number">2</span> + left; <span class="comment">// 防止溢出</span></span><br><span class="line">                <span class="keyword">if</span>(numbers[mid] == target - numbers[i])&#123; <span class="comment">// 数组中存在重复元素,必须判断相等</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;i+<span class="number">1</span>, mid+<span class="number">1</span>&#125; ; <span class="comment">// 返回的下标从1开始,都+1</span></span><br><span class="line">                &#125;<span class="keyword">else</span> <span class="keyword">if</span>(numbers[mid] &gt; target-numbers[i])&#123; <span class="comment">//中点大于目标值,在左侧</span></span><br><span class="line">                    right = mid;<span class="comment">//右开，真正右端点为mid-1</span></span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    left = mid+<span class="number">1</span>; <span class="comment">// 左闭 +1</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;-<span class="number">1</span>,-<span class="number">1</span>&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 双指针法</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] twoSum2(<span class="keyword">int</span>[] numbers, <span class="keyword">int</span> target)&#123;</span><br><span class="line">        <span class="keyword">int</span> low = <span class="number">0</span>, high = numbers.length-<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (low &lt; high) &#123; <span class="comment">// 指针移动条件</span></span><br><span class="line">            <span class="keyword">int</span> sum = numbers[low] + numbers[high];</span><br><span class="line">            <span class="keyword">if</span> (sum == target) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;low + <span class="number">1</span>, high + <span class="number">1</span>&#125;; <span class="comment">// 返回下标从1开始</span></span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (sum &lt; target) &#123;</span><br><span class="line">                ++low;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                --high;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;-<span class="number">1</span>, -<span class="number">1</span>&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="三数之和"><a href="#三数之和" class="headerlink" title="三数之和"></a>三数之和</h2><h4 id="15-三数之和"><a href="#15-三数之和" class="headerlink" title="15. 三数之和"></a><a href="https://leetcode-cn.com/problems/3sum/">15. 三数之和</a></h4><p><img src="https://i.loli.net/2021/09/12/6x3jq9QeCmGfa2B.jpg" alt=""></p><p>难点在于题目要求的不重复的三元组，它的可重复情况是非常多的，无法像两数之和那样，只要将第一个元素放入哈希表中，就可以轻松解决元素重复的问题了。</p><p>对于三数之和，即使使用哈希表去重，它的操作也是比较困难的，所以不能简单的使用三重循环枚举所有的三元组，然后用哈希表去重，这样工作量比较大。</p><p>因此必须换一种做法，从源头上解决元素重复问题，如果给定数组是有序的，那么其中可重复的情况就是可控的了，处理起来也简单，所以先把数组从小到大排序，随后用普通的三重循环就可。</p><p>然后就会发现，重复的元素都是相邻元素，只要保证每一重循环时，相邻两次枚举的元素不是相同的元素，这样就可以避免元组重复的情况了。</p><h3 id="双指针"><a href="#双指针" class="headerlink" title="双指针"></a>双指针</h3><p>使用普通的三层循环确实也能解决问题, 但是 $O(n^3)$ 的时间复杂度也确实太高了, 这时我们想到了在 有序数组的两数之和 中使用的双指针的方式, 虽然现在是三数之和, 但当我们正常遍历了第一层循环之后, 剩下的两个数不就形成了 有序数组的两数之和 了吗? 所以我们只要 保持第二重循环不变, 而将第三重循环变成一个从数组最右端开始向左移动的指针, 同时加上上述讨论的避免重复的条件, 最终代码就完成了.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> List&lt;List&lt;Integer&gt;&gt; threeSum(<span class="keyword">int</span>[] nums) &#123;</span><br><span class="line">        List&lt;List&lt;Integer&gt;&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(nums.length==<span class="number">0</span> ||  (nums[<span class="number">0</span>]==<span class="number">0</span> &amp;&amp; nums.length==<span class="number">1</span>))  <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">        Arrays.sort(nums);</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line">        <span class="comment">// 枚举a</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> first=<span class="number">0</span>;first&lt;nums.length;first++)&#123;</span><br><span class="line">            <span class="comment">//需要和上一次枚举的数不同</span></span><br><span class="line">            <span class="keyword">if</span>(first&gt;<span class="number">0</span> &amp;&amp; nums[first] == nums[first-<span class="number">1</span>])&#123;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// c对应的指针初始指向数组的最右排</span></span><br><span class="line">            <span class="keyword">int</span> third = nums.length-<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">int</span> target = -nums[first];</span><br><span class="line">            <span class="comment">//枚举b</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> second= first+<span class="number">1</span> ;second&lt;nums.length;second++)&#123;</span><br><span class="line">                <span class="comment">// 需要和上一次枚举不同</span></span><br><span class="line">                <span class="keyword">if</span>(second &gt; first+<span class="number">1</span> &amp;&amp; nums[second] == nums[second-<span class="number">1</span>]) &#123;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 需要保证b指针在c指针的左侧</span></span><br><span class="line">                <span class="keyword">while</span>(second &lt; third &amp;&amp; nums[second]+nums[third] &gt; target)&#123;</span><br><span class="line">                    --third;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//如果指针重合，后续也不会满足条件，可以退出循环</span></span><br><span class="line">                <span class="keyword">if</span>(second==third) <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">if</span> (nums[second] + nums[third] == target)&#123;</span><br><span class="line">                    res.add(Arrays.asList(nums[first],nums[second],nums[third]));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">threeSum</span>(<span class="params">self, nums</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :rtype: List[List[int]]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        res = <span class="built_in">list</span>()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums) == <span class="number">0</span>: <span class="keyword">return</span> res</span><br><span class="line">        nums.sort()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 枚举a</span></span><br><span class="line">        <span class="keyword">for</span> first <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> first&gt;<span class="number">0</span> <span class="keyword">and</span> nums[first] == nums[first-<span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            thrid = <span class="built_in">len</span>(nums) -<span class="number">1</span></span><br><span class="line">            target = -nums[first]</span><br><span class="line">            <span class="keyword">for</span> second <span class="keyword">in</span> <span class="built_in">range</span>(first+<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">                <span class="keyword">if</span> second&gt; first+<span class="number">1</span> <span class="keyword">and</span> nums[second] == nums[second-<span class="number">1</span>]:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">while</span> second&lt;thrid <span class="keyword">and</span> nums[second] + nums[thrid] &gt; target:</span><br><span class="line">                    thrid-=<span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> second==thrid:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> nums[second]+nums[thrid] == target:</span><br><span class="line">                    res.append([nums[first], nums[second], nums[thrid]])</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">        </span><br></pre></td></tr></table></figure><h2 id="四数之和"><a href="#四数之和" class="headerlink" title="四数之和"></a>四数之和</h2><h4 id="18-四数之和"><a href="#18-四数之和" class="headerlink" title="18. 四数之和"></a><a href="https://leetcode-cn.com/problems/4sum/">18. 四数之和</a></h4><p><img src="https://i.loli.net/2021/09/13/mxBF4fzwX9Crky5.jpg" alt=""></p><h3 id="思想同三数之和：排序-双指针"><a href="#思想同三数之和：排序-双指针" class="headerlink" title="思想同三数之和：排序+双指针"></a>思想同三数之和：排序+双指针</h3><p>四数之和 本质上 和三数之和一样的，由于都有大量的重复元素在，都不能使用哈希表进行简单的去重，都需要先进行排序后才方便遍历处理，同时为了优化时间复杂度，再加上双指针方法的使用，如果只是想简单实现的话，那么在三数之和上直接多加一重循环。但在代码上不同点在于：并非只是简单的家一重循环而已，而是进行了优化处理。</p><p>因为四数之和相比较于三数之和来说，情况更加复杂，时间复杂度也比较高，而且这个时间复杂度通过算法降下来很难。只能通过对代码的优化，直接减少大量不必要的遍历情况，从而来缩短代码的运行时间。</p><p>对于代码的优化主要分为两大块：一部分是为了避免出现重复的四元组，在遍历上的优化，这部分和三数之和相似，不过更复杂。</p><p>首先是对前两重循环进行的去重操作, 当 i 或者 j 的值与前面的值相等时忽略, 之后又对 双指针 进行了去重操作, 这里有个重要的注意点: 一定注意代码中是 先进行了指针的移动还是先进行了去重的比较, 对于不同的顺序, 比较的元素是完全不同的. 如果先进行了指针的移动, 对于左指针来说, 需要比较的元素就是 当前元素和前面的一个元素, 如果是先进行去重的比较, 那比较的元素就是 当前元素和后面的一个元素, 再进行指针的移动. 对于右指针的情况正好是完全相反的.</p><p>第二部分就是在循环遍历中先通过计算特定的四个数之和, 以此来判断接下来的循环操作情况.</p><p>比如 在确定第一个数 nums[i] 之后, 如果nums[i]+nums[i+1]+nums[i+2]+nums[i+3]&gt;target, 也就是此时的最小的4个数之和都大于target, 说明此时剩下的三个数无论取什么值, 四数之和一定大于 target, 因此直接退出第一重循环就可以了</p><p>在确定第一个数 nums[i] 之后,如果nums[i]+nums[n−3]+nums[n−2]+nums[n−1]&lt;target, 也就是此时的最大的4个数之和都小于target, 说明此时剩下的三个数无论取什么值, 四数之和一定小于 target,因此第一重循环直接进入下一轮, 枚举nums[i+1], 使用 continue 关键字.</p><p>对于第二层循环也是同样的判断方法, 通过这两层循环的判断优化, 能直接删去大量的不满足情况, 减少代码运行的时间. 这也能给我们带来启发, 在算法层面不能进行优化的时候, 可以选择对代码的细节进行优化, 同样可以起到节省时间的效果.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fourSum</span>(<span class="params">self, nums, target</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type target: int</span></span><br><span class="line"><span class="string">        :rtype: List[List[int]]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        res = <span class="built_in">list</span>()</span><br><span class="line">        length = <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> nums <span class="keyword">or</span> length&lt;<span class="number">4</span>:</span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">        nums.sort()</span><br><span class="line">        <span class="comment"># 定义4个指针i,j,left,right  i从0开始遍历,j从i+1开始遍历,留下left和right作为双指针</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length - <span class="number">3</span>):</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> nums[i] == nums[i - <span class="number">1</span>]: <span class="comment"># 当i的值与前面的值相等时忽略</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 获取当前最小值,如果最小值比目标值大,说明后面越来越大的值根本没戏</span></span><br><span class="line">            <span class="keyword">if</span> nums[i] + nums[i + <span class="number">1</span>] + nums[i + <span class="number">2</span>] + nums[i + <span class="number">3</span>] &gt; target:</span><br><span class="line">                <span class="keyword">break</span> <span class="comment"># 这里使用的break,直接退出此次循环,因为后面的数只会更大</span></span><br><span class="line">            <span class="comment"># 获取当前最大值,如果最大值比目标值小,说明后面越来越小的值根本没戏,忽略</span></span><br><span class="line">            <span class="keyword">if</span> nums[i] + nums[length - <span class="number">3</span>] + nums[length - <span class="number">2</span>] + nums[length - <span class="number">1</span>] &lt; target:</span><br><span class="line">                <span class="keyword">continue</span> <span class="comment"># 这里使用continue,继续下一次循环,因为下一次循环有更大的数</span></span><br><span class="line">            <span class="comment"># 第二层循环j,初始值指向i+1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, length - <span class="number">2</span>):</span><br><span class="line">                <span class="keyword">if</span> j &gt; i + <span class="number">1</span> <span class="keyword">and</span> nums[j] == nums[j - <span class="number">1</span>]: <span class="comment"># 当j的值与前面的值相等时忽略</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> nums[i] + nums[j] + nums[j + <span class="number">1</span>] + nums[j + <span class="number">2</span>] &gt; target:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> nums[i] + nums[j] + nums[length - <span class="number">2</span>] + nums[length - <span class="number">1</span>] &lt; target:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                left, right = j + <span class="number">1</span>, length - <span class="number">1</span></span><br><span class="line">                <span class="comment"># 双指针遍历,如果等于目标值,left++并去重,right--并去重,当当前和大于目标值时right--,当当前和小于目标值时left++</span></span><br><span class="line">                <span class="keyword">while</span> left &lt; right:</span><br><span class="line">                    total = nums[i] + nums[j] + nums[left] + nums[right]</span><br><span class="line">                    <span class="keyword">if</span> total == target:</span><br><span class="line">                        res.append([nums[i], nums[j], nums[left], nums[right]])</span><br><span class="line">                        left += <span class="number">1</span> <span class="comment"># left先+1之后,和它前面的left-1进行比较,若后+1,则和它后面的left+1进行比较</span></span><br><span class="line">                        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> nums[left] == nums[left - <span class="number">1</span>]:</span><br><span class="line">                            left += <span class="number">1</span></span><br><span class="line">                        right -= <span class="number">1</span></span><br><span class="line">                        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> nums[right] == nums[right + <span class="number">1</span>]:</span><br><span class="line">                            right -= <span class="number">1</span>   </span><br><span class="line">                    <span class="keyword">elif</span> total &lt; target:</span><br><span class="line">                        left += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        right -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> List&lt;List&lt;Integer&gt;&gt; fourSum(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target) &#123;</span><br><span class="line">        List&lt;List&lt;Integer&gt;&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(nums==<span class="keyword">null</span> || nums.length==<span class="number">0</span>) <span class="keyword">return</span> res;</span><br><span class="line">        <span class="keyword">int</span> length=nums.length;</span><br><span class="line"></span><br><span class="line">        Arrays.sort(nums);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义4个指针， i,j,left,right  i从0开始遍历，j从i+1开始遍历，留下left和right作为双指针</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;length - <span class="number">3</span>;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(i &gt; <span class="number">0</span> &amp;&amp; nums[i] == nums[i-<span class="number">1</span>]) <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取当前最小值，如果最小值比目标值大，说明后面越来越大的值根本没戏</span></span><br><span class="line">            <span class="keyword">if</span>(nums[i] + nums[i+<span class="number">1</span>] + nums[i+<span class="number">2</span>] +nums[i+<span class="number">3</span>] &gt; target)&#123;</span><br><span class="line">                <span class="keyword">break</span>; <span class="comment">// 这里使用的break，直接退出此次循环，因为后面的数只会更大</span></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取当前最大值，如果最大值比目标值小，说明后面的值越来越小</span></span><br><span class="line">            <span class="keyword">if</span>(nums[i] + nums[length-<span class="number">3</span>] + nums[length-<span class="number">2</span>] + nums[length-<span class="number">1</span>] &lt; target)&#123;</span><br><span class="line">                <span class="keyword">continue</span>; <span class="comment">// 继续下一次循环，因为下一次循环有更大的数</span></span><br><span class="line">            &#125; </span><br><span class="line"></span><br><span class="line">            <span class="comment">// 第二层循环j，初始值指向i+1</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=i+<span class="number">1</span>; j&lt;length-<span class="number">2</span>;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(j&gt;i+<span class="number">1</span> &amp;&amp; nums[j] == nums[j-<span class="number">1</span>])&#123;  <span class="comment">// 当j的值与前面的值相等时忽略</span></span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(nums[i] + nums[j] + nums[j+<span class="number">1</span>] + nums[j+<span class="number">1</span>] &gt; target) &#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(nums[i] + nums[j] + nums[length-<span class="number">2</span>] + nums[length-<span class="number">1</span>] &lt; target)&#123;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">int</span> left = j+<span class="number">1</span>, right=length-<span class="number">1</span>;</span><br><span class="line">                <span class="comment">//双指针遍历，如果等于目标值，left++并去重，right--并去重，当当前和大于目标值时right-- 否则left++</span></span><br><span class="line">                <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">                    <span class="keyword">int</span> sum = nums[i] + nums[j] + nums[left] + nums[right];</span><br><span class="line">                    <span class="keyword">if</span>(sum==target)&#123;</span><br><span class="line">                        res.add(Arrays.asList(nums[i], nums[j], nums[left], nums[right]));</span><br><span class="line">                        left++; <span class="comment">// left先+1之后，和它前面的left-1进行比较，若后+1，则和它后面的left+1进行比较</span></span><br><span class="line">                        <span class="keyword">while</span>(left &lt; right &amp;&amp; nums[left]==nums[left-<span class="number">1</span>])&#123;</span><br><span class="line">                            left++;</span><br><span class="line">                        &#125;</span><br><span class="line">                        right--;</span><br><span class="line">                        <span class="keyword">while</span>(left&lt;right &amp;&amp; nums[right]== nums[right+<span class="number">1</span>])&#123;</span><br><span class="line">                            right--;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;<span class="keyword">else</span> <span class="keyword">if</span>(sum&lt;target)&#123;</span><br><span class="line">                        left++;</span><br><span class="line">                    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                        right--;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="四数之和二"><a href="#四数之和二" class="headerlink" title="四数之和二"></a>四数之和二</h2><h4 id="454-四数相加-II"><a href="#454-四数相加-II" class="headerlink" title="454. 四数相加 II"></a><a href="https://leetcode-cn.com/problems/4sum-ii/">454. 四数相加 II</a></h4><p><img src="https://i.loli.net/2021/09/14/met83ga7UN9wGL2.jpg" alt=""></p><h3 id="维数太高，分治处理"><a href="#维数太高，分治处理" class="headerlink" title="维数太高，分治处理"></a>维数太高，分治处理</h3><p>此题一看似乎和四数之和差不多，但本质上却有很大的区别，首先无论是三数之和还是四数之和，它们都是在一个数组上的操作，本质上都是一维的，同时它们都要求找到不重复的元组，这就限制了我们不能简单的使用哈希表进行去重操作。最终只能将数组排序后使用双指针。</p><p>但本题是四个独立的数组，相当于是四个维度，想在四个维度上使用双指针的方法显然不现实。同时此题只要求我们找到所有4个元素的和为0的元组个数即可，并没有要求是不重复的元组，这样就简单了很多，也是可以使用哈希表法。</p><p>此题是在使用哈希表的时候，会遇到如下三种情况：</p><ul><li>hashmap存一个数组，如A。然后计算三个数组之和，如BCD。时间复杂度为：$O(n)+O(n^3)=O(n^3)$</li><li>hashmap存三个数组之和，如ABC。然后计算一个数组，如D。时间复杂度为：$O(n^3)+O(n) = O(n^3)$</li><li>hashmap存两个数组之和，如AB，然后计算两个数组之和，如CD，时间复杂度为：$O(n^2)+O(n^2) = O(n^2)$</li></ul><p>根据事件复杂度来看，选第三种情况</p><p>确定了使用的方法(哈希表)以及分类方法(两两分组)。此题和两数之和中使用的哈希表有很大的区别，在两数之和中，我们需要的是满足条件的下标值，所以在哈希表中存的是元组的下标值。</p><p>但在本题中，我们需要的是元组个数，所以哈希表中的值应存取出现的次数，这就有点难度。使用java map中的merge或getOrDefault</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">fourSumCount</span><span class="params">(<span class="keyword">int</span>[] A, <span class="keyword">int</span>[] B, <span class="keyword">int</span>[] C, <span class="keyword">int</span>[] D)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(A.length==<span class="number">0</span>) <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">        Map&lt;Integer,Integer&gt; countAB = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> u: A)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> v:B)&#123;</span><br><span class="line">                <span class="comment">// 存储u+v的结果，不存在赋值为1，存在在原来基础上+1</span></span><br><span class="line">                <span class="comment">// 另一种表达countAB.merge(u+v, 1, (old,new_)-&gt; old+1);</span></span><br><span class="line">                countAB.put(u+v, countAB.getOrDefault(u+v, <span class="number">0</span>) + <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> u: C)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> v:D)&#123;</span><br><span class="line">                <span class="keyword">if</span>(countAB.containsKey(-u-v))&#123;</span><br><span class="line">                    res+=countAB.get(-u-v);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fourSumCount</span>(<span class="params">self, A, B, C, D</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type nums1: List[int]</span></span><br><span class="line"><span class="string">        :type nums2: List[int]</span></span><br><span class="line"><span class="string">        :type nums3: List[int]</span></span><br><span class="line"><span class="string">        :type nums4: List[int]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Counter类是dict()子类，用于计数可哈希对象</span></span><br><span class="line">        <span class="comment"># 它是一个集合，元素字段键(key) 一样存储，他们的技术存储为值</span></span><br><span class="line">        countAB = collections.Counter(u+v <span class="keyword">for</span> u <span class="keyword">in</span> A <span class="keyword">for</span> v <span class="keyword">in</span> B)</span><br><span class="line">        ans = <span class="number">0</span> </span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> C:</span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> D:</span><br><span class="line">                <span class="keyword">if</span> -u-v <span class="keyword">in</span> countAB:</span><br><span class="line">                    ans+=countAB[-u-v]</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><h2 id="n数之和方法总结"><a href="#n数之和方法总结" class="headerlink" title="n数之和方法总结"></a>n数之和方法总结</h2><p>对于两数之和，看它是否有序的，如果是无序的就用哈希表法，如果是有序的可以使用双指针。</p><p>对于一个数组上的三数之和、四数之和等，无论数组是否有序，都排序后使用双指针法</p><p>对于多个数组之和的情况，首先对它们进行分组来实现降维操作，一般来说分为两个相等的小组，之后再使用哈希表法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-两数之和到四数之和&quot;&gt;&lt;a href=&quot;#1-两数之和到四数之和&quot; class=&quot;headerlink&quot; title=&quot;1-两数之和到四数之和&quot;&gt;&lt;/a&gt;1-两数之和到四数之和&lt;/h1&gt;&lt;p&gt;给定一个整数数组 nums 和一个整数目标值 target，请你在该</summary>
      
    
    
    
    
    <category term="LeetCode" scheme="http://example.com/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
    <link href="http://example.com/2021/09/10/Model-Agnostic-Meta-Learning-for-Fast-Adaptation-of-Deep-Networks/"/>
    <id>http://example.com/2021/09/10/Model-Agnostic-Meta-Learning-for-Fast-Adaptation-of-Deep-Networks/</id>
    <published>2021-09-10T11:15:31.000Z</published>
    <updated>2021-09-17T08:23:26.670Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Model-Agnostic-Meta-Learning-for-Fast-Adaptation-of-Deep-Networks"><a href="#Model-Agnostic-Meta-Learning-for-Fast-Adaptation-of-Deep-Networks" class="headerlink" title="Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"></a>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</h1><p>MAML一种模型无关的元学习算法，即它可以与任何经过梯度下降训练的模型兼容，并适用于各种不同的学习问题，包括分类、回归和强化学习。</p><p>元学习的目标是在各种学习任务上训练一个模型，它只需要少量的训练样本就可以解决新的学习任务。</p><p>MAML模型的参数被显式地训练，使得少量的梯度步骤和来自新任务的少量训练数据将在该任务上产生良好的泛化性能，易于微调。</p><p>核心思想是训练模型的初始参数，以便在参数通过一个或多个梯度步骤更新后，模型在新任务上具有最大性能，该梯度步骤使用来自该新任务的少量数据计算。</p><p>这种快速灵活的学习是一种挑战，因为模型必须将其先前的经验与少量新信息相结合，同时避免过拟合新数据。此外，先验经验和新数据的形式将取决于任务。因此，为了获得最大的适用性，learn to learn (或元学习)的机制应该适用于任务和完成任务所需的计算形式。</p><h2 id="Model-Agnostic-Meta-Learning"><a href="#Model-Agnostic-Meta-Learning" class="headerlink" title="Model-Agnostic Meta-Learning"></a>Model-Agnostic Meta-Learning</h2><p>目标是训练能够实现快速适应的模型，这是一种通常被形式化为few-shot learning的问题设置。</p><h3 id="Meta-Learning-Problem-Set-Up"><a href="#Meta-Learning-Problem-Set-Up" class="headerlink" title="Meta-Learning Problem Set-Up"></a>Meta-Learning Problem Set-Up</h3><p>few-shot learning元学习的目标是训练一个只使用几个数据点和训练迭代就能快速适应新任务的模型。</p><p>实际上，元学习问题将整个任务视为训练示例。</p><p>定义一个模型为f，它将观测值 $x$ 映射到输出 $a$ 。在元学习期间，模型被训练成能够适应大量或无限数量的任务。</p><p>定义每个任务 $T =  {L(x<em>1,a_1,…,x_H,a_H) , q(x_1), q(x</em>{t+1}|x_t,a_t ), H}$</p><p>$L$ 是loss function，初始观测 $q(x<em>1)$ 的分布，通过$q(x</em>{t+1}|x_t,a_t)$ 转换分布，事件长度$H$</p><p>在独立同分布的有监督学习问题中，长度H=1。该模型可以通过在每个时间步 $t$ 选择一个输出来产生长度为 $H$ 的样本。Loss函数提供任务特殊的反馈。</p><p>在元学习场景中，我们考虑希望模型能够适应的任务$p(T)$上的分布。在 K-shot 学习设置中，该模型被训练成仅从$q<em>i$ 中提取的K个样本和由任务 $T_i$ 生成的反馈 $L</em>{T_i}$ 中学习 从$p(T)$中提取的新任务$T_i$。</p><p>在元训练过程中，从 $p(T)$ 中抽取一个任务 $T<em>i$，用K个样本训练模型，并从$T_i$中相应的$L</em>{T_i}$ 损失反馈，然后在$T_i$的新样品上进行测试。然后，通过考虑来自 $q_i$ 的新数据上的测试误差如何相对于参数变化来改进模型 $f$。</p><p>实际上，采样任务 $T_i$ 上的测试误差充当元学习过程的训练误差。在元训练 (meta-training)结束时，从$p(T)$采样新任务，从K个样本中学习后，通过模型的性能来衡量元性能(meta-performance)。一般来说，元测试 (meta-testing) 任务是在元训练 (meta-training) 期间执行的。</p><h3 id="A-Model-Agnostic-Meta-Learning-Algorithm"><a href="#A-Model-Agnostic-Meta-Learning-Algorithm" class="headerlink" title="A Model-Agnostic Meta-Learning Algorithm"></a>A Model-Agnostic Meta-Learning Algorithm</h3><p>我们怎样才能鼓励这种通用型代表的出现呢？例如，神经网络可能学习广泛适用于 $p(T)$ 中所有任务的内部特征，而不是单个任务。</p><p>MAML对这个问题采取了明确的方法：由于模型将在新任务上使用基于梯度的学习规则进行微调，因此我们的目标是学习一个模型，使此基于梯度的学习规则能够在从$p(T)$提取的新任务上取得快速进展，而不会过拟合。</p><p>实际上，我们的目标是找到对任务变化敏感的模型参数，以便参数的微小变化将在沿损失梯度方向改变时，对从 $p(T)$ 得出的任何任务的损失函数产生较大的改进。如下图：</p><p><img src="https://i.loli.net/2021/09/11/UE2pXjfwd81z4hb.png" alt=""></p><p>我们对模型的形式不做任何假设，只是假设它由一些参数向量 $θ$ 参数化，并且损失函数在 $θ$ 中足够平滑，因此我们可以使用基于梯度的学习技术。</p><p>定义这个参数化的模型为$f_{\theta}$ , 参数为$\theta$。 当适应一个新的任务$T_i$时，模型的参数 $θ$ 变为 $θ_i’$。</p><p>使用任务 $T_i$ 上的一个或多个梯度下降更新来计算更新后的参数向量 $θ_i’$。例如，当使用一个梯度更新时:</p><script type="math/tex; mode=display">\theta_i' = \theta - \alpha \nabla_{\theta}L_{T_i}(f_{\theta})</script><p>步长 $α$ 可以固定为超参数。</p><p>通过优化 $f_{θ_i’}$相对于从 $p(T)$ 采样的任务的 $θ$ 的性能，来训练模型参数。更具体地说，元目标如下：</p><script type="math/tex; mode=display">min_{\theta} \sum_{T_i\sim p(T)} L_{T_i}(f_{\theta_i'}) = \sum_{T_i\sim p(T)} L_{T_i}(f_{\theta - \alpha \nabla_{\theta}L_{T_i}(f_{\theta})})</script><p>注意，元优化 （meta-optimization）是在模型参数θ上执行的，而目标是使用更新后的模型参数 $θ’$来计算的。实际上，MAML提出的方法旨在优化模型参数，使得新任务上的一个或少量梯度步骤将在该任务上产生最有效的行为。</p><p>跨任务的元优化通过随机梯度下降（SGD），模型参数θ更新如下：</p><script type="math/tex; mode=display">\theta \leftarrow \theta - \beta \nabla_{\theta} \sum_{T_i \sim p(T)} L_{T_i}(f_{\theta_i’})</script><p>$β$ 是元步长。下图概述了一般情况下完整的算法：</p><p><img src="https://i.loli.net/2021/09/11/WxXC1fqEFamYi6n.png" alt=""></p><p>输入任务不同分布的任务$p(T)$ 和两个学习步长超参数 $\alpha, \beta$ ,并且随机初始化模型参数。</p><p>第一层循环，遍历每个任务 $T_i \sim p(T)$ 中采样的batch</p><p>第二层循环，在每个任务 $T<em>i$  中 评估关于 K 个样本的 $∇</em>θ L<em>{T_i}(f</em>θ)$  使用梯度下降计算自适应参数：$θ<em>i’ = θ − α\nabla</em>θL<em>{T_i}(f</em>θ)$</p><p>然后根据所有任务上的损失梯度更新模型的参数  $\theta \leftarrow \theta - \beta \nabla<em>{\theta} \sum</em>{T<em>i \sim p(T)} L</em>{T<em>i}(f</em>{\theta_i’})$</p><p>MAML元梯度更新涉及通过梯度的梯度, 在计算上，这需要额外的反向传播 $f$ 来计算Hessian-vector乘积。</p><h3 id="Species-of-MAML"><a href="#Species-of-MAML" class="headerlink" title="Species of MAML"></a>Species of MAML</h3><p>监督学习和强化学习的元学习算法的具体实例。他们在损失函数的形式以及数据由任务生成并呈现给模型的方式上不同。</p><h4 id="Supervised-Regression-and-Classification"><a href="#Supervised-Regression-and-Classification" class="headerlink" title="Supervised Regression and Classification"></a>Supervised Regression and Classification</h4><p><img src="https://i.loli.net/2021/09/11/w39XVGQOYhamNUk.png" alt=""></p><p>Few-shot 学习在监督任务领域得到了很好的研究，其目标是仅从该任务的几个输入/输出对中学习新函数，使用类似任务的先前数据进行元学习。</p><p>我们可以定义horizon H=1并将timestep下标放到 $x_t$上，因为模型接受单个输入并生成单个输出，而不是一系列输入和输出。</p><p>任务 $T_i$ 从 $q_i$生成K个观测值 $x$ ，并且任务损失由模型的输出 $x$ 与该观测值和任务对应的目标值 $y$ 之间的误差表示。</p><p>用于监督分类和回归的两种常见损失函数是交叉熵和均方误差(MSE)，对于使用均方误差的回归任务，损失形式为：</p><script type="math/tex; mode=display">L_{T_i}f(\phi) = \sum_{x^{(j)},y^{(y)} \sim T_i} ||f_{\phi}(x^{(j)}) - y^{(j)}||_2^2</script><p>其中 $x^{(j)}, y^{(j)}$ 是任务$T_i$的一对输入输出，类似地，对于具有交叉熵损失的离散分类任务，损失采取以下形式：</p><script type="math/tex; mode=display">L_{T_i}f(\phi)= \sum_{x^{(j)},y^{(y)} \sim T_i} y^{(j)} log f_{\phi}(x^{(j)}) + (1-y^{(j)})log(1-f_{\phi}(x^{(j)}))</script><h4 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h4><p><img src="https://i.loli.net/2021/09/11/Jx6hAWtNTHg9B2O.png" alt=""></p><p>在强化学习（RL）中，Few-shot 元学习的目标是使模型能够仅使用测试设置中的少量经验快速获取新测试任务的策略。</p><p>例如，一个模型可能会学习如何快速找到如何导航迷宫，以便在面对新的迷宫时，只需几个样本就可以确定如何可靠地到达出口。</p><p>每个RL 任务$T<em>i$ 包含一个初始化状态分布$q_i(x_1)$ 和一个转换分布 $q_i(x</em>{t+1}| x<em>i,a_t)$ , 损失函数为 $L</em>{T_i}$</p><p>因此，整个任务是一个具有horizon H的马尔可夫决策过程（MDP），在该过程中，学习者可以查询有限数量的样本轨迹以进行少量镜头学习。</p><p>MDP的任何方面都可以在 $p(T)$ 中的任务之间改变。正在学习的模型 $f_θ$ 是一个策略，它从状态 $x_t$映射到每个操作上的分布时间步长 $t∈{1，.，H}$。任务$T_i$ 的模型的损失:</p><script type="math/tex; mode=display">L_{T_i}(f_\phi) = -\mathbf{E} _{x_t,a_t\sim f_{\phi},q_{T_i}} [\sum_{t=1}^H R_i(x_t, a_t)]</script><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>MAML能实现新任务的快速学习吗？</p><p>MAML是否可以用于多个不同领域的元学习，包括有监督的回归、分类和强化学习？</p><p>用MAML学习的模型可以通过额外的渐变更新和/或示例继续改进吗？</p><p><img src="https://i.loli.net/2021/09/11/NRBL1JGZyIUeY9C.png" alt=""></p><p><img src="https://i.loli.net/2021/09/11/myPCEawdMIN1Yt2.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Model-Agnostic-Meta-Learning-for-Fast-Adaptation-of-Deep-Networks&quot;&gt;&lt;a href=&quot;#Model-Agnostic-Meta-Learning-for-Fast-Adaptation-of-Dee</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>Dynamically Addressing Unseen Rumor via Continual Learning</title>
    <link href="http://example.com/2021/09/07/Dynamically-Addressing-Unseen-Rumor-via-Continual-Learning/"/>
    <id>http://example.com/2021/09/07/Dynamically-Addressing-Unseen-Rumor-via-Continual-Learning/</id>
    <published>2021-09-07T11:06:19.000Z</published>
    <updated>2021-09-07T13:28:32.525Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Dynamically-Addressing-Unseen-Rumor-via-Continual-Learning"><a href="#Dynamically-Addressing-Unseen-Rumor-via-Continual-Learning" class="headerlink" title="Dynamically Addressing Unseen Rumor via Continual Learning"></a>Dynamically Addressing Unseen Rumor via Continual Learning</h1><p>谣言往往与新出现的事件联系在一起，因此，处理没见过的谣言的能力对于谣言真实性分类模型至关重要。</p><p>以前的工作通过改进模型的泛化能力来解决这个问题，假设即使在新的事件爆发之后，模型也会保持不变。</p><p>在这项工作中，提出了一种解决方案，以根据谣言域创建的动态不断更新模型。</p><p>与这种新方法相关的最大技术挑战是由于新的学习而灾难性地忘记了以前的学习。</p><p>作者采用持续学习策略来控制新的学习，以避免灾难性的遗忘，并提出了一种可以联合使用的额外策略来加强遗忘的缓解。</p><p>谣言检测任务两个重要难点：</p><ul><li><p>处理没见过的谣言的能力 (处理训练阶段未见的新谣言)</p></li><li><p>谣言早发现</p></li></ul><p>此前的方法试图通过关注静态设置中的模型泛化通用性来解决这一挑战，如下图a。目标是提高模型$M<em>{\theta</em>{t=1}}$ 在不更新模型的情况下，在看不见的话题领域(“古尔利特”和“普京失踪”)上表现好。然而，增强模型的泛化能力是一个困难的问题，特别是对于总是引入新主题和词汇的任务。</p><p><img src="https://i.loli.net/2021/09/07/yUqli7RJVG489HX.png" alt=""></p><p>因此，作为另一种解决方案，通过训练一个能够不断适应新出现的谣言的分类器，在动态设置中对未见过谣言进行分类。如上图b。</p><p>通过这种方式，可以及时发现虚假谣言，而不必考虑可见谣言和不可见谣言之间的巨大分布差距。</p><p>持续学习想解决的主要挑战是在学习新的Domain时灾难性地忘记以前学习的Domain。</p><p>这篇文章作者采用了基于排练的(rehearsal- based)持续学习(CL)策略，利用以前遇到的领域的情景记忆来约束未来的学习，并提出了一种简单的技术TTOKENS，可以联合使用来进一步减少灾难性遗忘。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h3><p>谣言真实性分类是识别给定谣言文本 $X$ 是真、假还是无法核实的任务。</p><p>谣言数据集及其对应的标签为集合 $D = {(X_i,y_i,Rm)}_i^N$ 其中 $y\in{True, False,Unverifiabel}$</p><p>RM是谣言域标签，N是数据集的大小。</p><p>主要目标是训练一个谣言真实性分类模型M，该模型可以从传言领域流中学习，通过时间t而不会发生灾难性的遗忘。 </p><p>将谣言域流定义为 $S={D_1,···,D_T}$，其中 $D_t$ 表示流中第 $t$ 个谣言域的数据集，$T$ 是流的长度。</p><p>T也等于时间戳的长度和谣言域的数量。</p><p>在每个时间戳，使用一个新的谣言域数据集 $D<em>t$ 来顺序训练模型 $M$，并用时间 $k$ 处的谣言域表示训练后的模型参数，$θ</em>{t=k}$。</p><h3 id="Base-Model"><a href="#Base-Model" class="headerlink" title="Base Model"></a>Base Model</h3><p>BERT-BASE 编码器和一个分类层。给定输入谣言 $X=x_1,···,x_m$，该模型计算：</p><script type="math/tex; mode=display">H= BERT([CLS] +X)</script><script type="math/tex; mode=display">P(y|X) = Softmax(WH_{[CLS]}+b)</script><p>$H_{[cls]}$是 $[cls]$ 标记的嵌入,可训练参数为 $θ=[W，b]$ </p><p>在训练期间，冻结编码层，并且仅使用交叉熵损失来训练分类器参数θ：</p><script type="math/tex; mode=display">L_{\theta_t}(D_t) = -\sum_j^{D_t}log P(y|X)</script><h3 id="Rehearsal-based-CL-Strategies"><a href="#Rehearsal-based-CL-Strategies" class="headerlink" title="Rehearsal-based CL Strategies"></a>Rehearsal-based CL Strategies</h3><p>基于排练的CL策略依赖于“episodic memory” $M$ 来存储先前遇到的样本。$M$ 被定期重播，以避免灾难性遗忘，并加强过去知识和新知识之间的联系。</p><h4 id="REPLAY-Robins-1995"><a href="#REPLAY-Robins-1995" class="headerlink" title="REPLAY (Robins, 1995)"></a>REPLAY (Robins, 1995)</h4><p>CL的Memory M的一个简单利用是扩展当前任务数据 $D<em>t$，并使用 $L</em>{θ_t}(D_t+M)$来优化模型的参数。基本上，它可以被视为一个数据效率高的多任务框架，它只利用受Memory M大小限制的数据集的一小部分。</p><h4 id="Gradient-Episodic-Memory-GEM-Lopez-Paz-and-Ranzato-2017"><a href="#Gradient-Episodic-Memory-GEM-Lopez-Paz-and-Ranzato-2017" class="headerlink" title="Gradient Episodic Memory (GEM) (Lopez- Paz and Ranzato, 2017)"></a>Gradient Episodic Memory (GEM) (Lopez- Paz and Ranzato, 2017)</h4><p>另一种利用方法是使用当前域样本来约束梯度更新，使得Memory M中的样本的损失永远不会增加：</p><script type="math/tex; mode=display">L_{\theta_t} s.t. L_{\theta_t}(M) \le L_{\theta_{t-1}} (M)</script><p>GEM通过随模型参数数量变化的二次规划求解器计算梯度约束。</p><h3 id="Task-Specific-Tokens-TTOKENS"><a href="#Task-Specific-Tokens-TTOKENS" class="headerlink" title="Task-Specific Tokens (TTOKENS)"></a>Task-Specific Tokens (TTOKENS)</h3><p>各种工作表明，对大型预训练语言模型的输入上下文对模型的结果有巨大影响。换句话说，可以利用LM/MLM的这种上下文相关特性来有意地控制/区分不同域的表示。为了应用该策略，对输入文本 $X$ 进行预处理以从其对应的谣言域标签 Rm开始。形式上，给定第 $t$ 传言域Rm：</p><script type="math/tex; mode=display">H = BERT([CLS] + Rm + X)</script><p>该策略可以很容易地与其他CL策略一起使用，因为它是在数据处理步骤中完成的。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>数据集PHEME的一个显著特点是根据谣言事件进行分类。总共有9个不同的事件/域，更多详细信息如表1所示。</p><p><img src="https://i.loli.net/2021/09/07/eyQsigEztvZm8Bf.png" alt=""></p><p>以前的工作在静态设置中利用了这个数据集，其中8个域被组合成一个训练集，剩下的1个域用作单个不可见的测试域。在这项工作中，本文的任务是在动态设置中进行的。为了与动态设置结合，将PHEME的每个域视为单独的特定于域的数据集$D_T$，并以0.4/0.1/0.5的比率将它们拆分为Train/dev/test。</p><h3 id="Evaluation-Method"><a href="#Evaluation-Method" class="headerlink" title="Evaluation Method"></a>Evaluation Method</h3><p>在完成对第k个领域的学习后，对其在所有T领域测试集上的测试性能进行了评估。</p><p>这一步的结果是矩阵 $R \in \mathbf{R}^{T×T}$，其中 $R_{i,j}$ 是在观察到来自第 $i$ 域的最后一个样本之后，模型在第 $j$ 域上的测试分类精度。</p><p>基于此矩阵，计算了两个特定于CL的度量：</p><ul><li>Avg. Accuracy (ACC) 对于了解模型在学习新领域时性能如何变化很有用。此指标的计算方法如下：</li></ul><script type="math/tex; mode=display">ACC = \frac {1}{T}\sum_{i=1}^T R_{T,i}</script><p>请注意，在流的末尾，t=9，即Avg. Accuracy (ACC) 正好是所有任务的平均精确度。</p><ul><li>Backward Transfer (BWT) 是一种用于测量新学习任务对先前学习任务影响的CL方法。此指标的计算公式为：</li></ul><script type="math/tex; mode=display">BWT = \frac{1}{T-1} \sum_{i=1}^{T-1} R_{T,i} - R_{i,i}</script><p>负的BWT表明模型灾难性地忘记了以前的任务。</p><h3 id="model"><a href="#model" class="headerlink" title="model"></a>model</h3><p>两个没有CL策略的基线模型，并在我们的动态设置中进行了评估。BERT-BL是指在PHEME数据集上微调的BERT基础模型。M2-BL是指对统一错误信息表示进行微调的另一个基线，该基线被证明能有效提高未发现领域的泛化性。</p><p><img src="https://i.loli.net/2021/09/07/Xcvu63ni5SpVJoQ.png" alt=""></p><p>对于作者提出的模型，作者用上文提到的CL策略的各种组合来训练基于BERT的分类器，以评估所采用的策略在对不可见领域的鲁棒性方面的有效性。</p><p><img src="https://i.loli.net/2021/09/07/PUig5Oe9qcIFbjR.png" alt=""></p><p>模型性能随时间变化的可视化分析。用于Replay的ACC热力图(A)没有TTOKENS和(B)有TTOKENS。比较(A)和(B)之间的每一列，使用TTO-KENS的(B)列通常显示较深的阴影，这代表更好的性能。</p><p>对于TTOKENS成功背后的推理，作者有两个假设：1）TTOKENS隐含地充当了领域差异的信号，并鼓励模型为每个领域学习单独的知识。或者，2）TTOKENS作为一个良好的开端“上下文”，帮助基于LM的编码器在必要时将输入编码为更可分离的——这意味着，来自相同域的输入在向量空间中比来自不同域的输入更接近。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Dynamically-Addressing-Unseen-Rumor-via-Continual-Learning&quot;&gt;&lt;a href=&quot;#Dynamically-Addressing-Unseen-Rumor-via-Continual-Learning&quot; cl</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>Rumor Detection on Social Media with Bi-Directional Graph Convolutional Networks</title>
    <link href="http://example.com/2021/09/04/Rumor-Detection-on-Social-Media-with-Bi-Directional-Graph-Convolutional-Networks/"/>
    <id>http://example.com/2021/09/04/Rumor-Detection-on-Social-Media-with-Bi-Directional-Graph-Convolutional-Networks/</id>
    <published>2021-09-04T10:54:37.000Z</published>
    <updated>2021-09-06T07:12:00.292Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Rumor-Detection-on-Social-Media-with-Bi-Directional-Graph-Convolutional-Networks"><a href="#Rumor-Detection-on-Social-Media-with-Bi-Directional-Graph-Convolutional-Networks" class="headerlink" title="Rumor Detection on Social Media with Bi-Directional Graph Convolutional Networks"></a>Rumor Detection on Social Media with Bi-Directional Graph Convolutional Networks</h1><p><a href="https://github.com/TianBian95/BiGCN">https://github.com/TianBian95/BiGCN</a></p><p>从社交媒体上如此海量的信息中识别谣言正成为一项艰巨的挑战。</p><p>一些深度学习方法被应用于通过谣言传播的方式来发现谣言，如递归神经网络(RvNN)等。然而，这些深度学习方法只考虑了深度传播的特点，而忽略了谣言检测中广泛分散的结构。</p><p>实际上，propagation(传播)和dispersion(扩散)是谣言的两个关键特征。</p><p>作者提出一种新的双向图模型，称为双向图卷积网络(Bi-GCN)，通过对谣言的自上而下和自下而上的传播进行操作，来探索这两个特性。</p><ul><li>利用具有自上而下谣言传播有向图的GCN来学习谣言传播模式</li><li>具有相反方向的谣言扩散图，以捕获谣言扩散的结构</li></ul><p>此外，来自消息来源的信息涉及到GCN的每一层，以增强谣言根源的影响力。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h3><p>传统的检测方法主要采用用户特征、文本内容和传播模式等手工制作的特征来训练监督分类器，例如：Decision Tree、 Random Forest、Support Vector Machine (SVM)。</p><p>一些研究应用了有效的特征，如用户评论、时间结构特征，以及帖子的情感态度。</p><p>然而，这些方法主要依赖于特征工程，非常耗时费力。此外，这些手工制作的特征通常缺乏从谣言的传播和扩散中提取的高层次表示。</p><h3 id="深度学习方法"><a href="#深度学习方法" class="headerlink" title="深度学习方法"></a>深度学习方法</h3><p>最近的研究已经利用从传播路径/树或网络中挖掘高层表示的深度学习方法来识别谣言。也就是深度学习方法：</p><h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><p>长短期记忆(LSTM)、门控递归单元(GRU)和递归神经网络(RvNN)，因为他们能够从随着时间的谣言传播中学习序列特征。</p><p>然而，这些方法在效率上有很大的局限性，因为时间结构特征只关注谣言的顺序传播，而忽略了谣言散布的影响。</p><h4 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h4><p>谣言传播的结构也表明了谣言的某些传播行为。因此，一些研究试图通过引用基于卷积神经网络(CNN)的方法来涉及谣言传播结构中的信息。基于CNN的方法可以获得局部邻域内的相关特征，但不能处理图或树中全局结构关系。因此，这些方法忽略了谣言传播的全局结构特征。实际上，CNN不是为从结构化数据中学习高级表示而设计的，但图形卷积网络(GCN)是。</p><h3 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h3><p>GCN，或称为无向GCN(UD-GCN)，只聚合依赖于相关帖子之间的关系的信息，而丢失了以下内容的顺序。</p><p>虽然UD-GCN具有处理谣言传播的全局性结构特征的能力(其实传统基于消息传递的GNN只是局部特征)，但它没有考虑谣言传播的方向，但这已被证明是谣言检测的重要线索。</p><p>具体而言，沿着关系链的深度传播和在社区的广泛扩散是谣言的两个主要特征。</p><p>为了同时处理谣言的传播和扩散，本文提出了一种新的双向GCN(Bi-GCN)，它同时适用于谣言的自上而下和自下而上的传播。</p><p><img src="https://i.loli.net/2021/09/05/cs9LYDfpmCzixIV.png" alt=""></p><p>Top-Down graph convolutional Networks (TD-GCN)         /     Bottom-Up graph convolutional Networks (BU-GCN)</p><p>TD-GCN从谣言树中节点的父节点获取信息来制定谣言传播，而BU-GCN从树中节点的子节点收集信息来表示谣言的散布。</p><p>Detect rumors on twitter by promoting information campaigns with generative adversarial learning.采用对抗性学习方法来提高谣言分类器的性能，其中鉴别器用作分类器，相应的生成器通过产生冲突噪声来改进鉴别器。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="符号"><a href="#符号" class="headerlink" title="符号"></a>符号</h3><p>$C = {c_1,c_2,…,c_m}$ 为数据集，$c_i$ 是第 $i$ 个事件，$m$是事件的数量。</p><p>$c<em>i = {r_i,w_1^i,…,w</em>{n_i-1}^i,G_i}$ , $n_i$ 是事件$c_i$中帖子的数量，$r_i$是源帖子，每个$w_j^i$代表第$j$个相关回应帖子, $G_i$ 指的是传播结构。$ G_i$定义为 $⟨V_i，E_i⟩$其中 $r_i$是根节点</p><p>类别标签${N,F,T,U}$ (Non-rumor, False Rumor, True Rumor, and Unverified Rumor)</p><h3 id="DropEdge"><a href="#DropEdge" class="headerlink" title="DropEdge"></a>DropEdge</h3><p>DropEdge是一种减少基于GCN模型的过拟合的新方法。在每个训练周期内，随机地从输入图中剔除边，以一定的速率生成不同的变形副本。</p><p>因此，这种方法增加了输入数据的随机性和多样性，就像随机旋转或摆动图像一样。形式上，假设图 $A$ 中的边总数为$N_e$，dropping率为$p$，则DropEdge之后的邻接矩阵$A‘$计算如下：</p><script type="math/tex; mode=display">A' = A - A_{drop}</script><p>其中$A_{drop}$ 是$N_e\times p$ 随机采样原始边集合得到</p><h3 id="Bi-GCN-Rumor-Detection-Model"><a href="#Bi-GCN-Rumor-Detection-Model" class="headerlink" title="Bi-GCN Rumor Detection Model"></a>Bi-GCN Rumor Detection Model</h3><p><img src="https://i.loli.net/2021/09/06/YR7cEdVmlXPBwtb.png" alt=""></p><p>Bi-GCN的核心思想是从谣言传播和谣言扩散两个方面学习合适的高层表示。</p><p>在Bi-GCN模型中，采用两层1stChebNet作为基本的GCN组件。如图2所示，分4个步骤详细阐述了使用Bi-GCN进行谣言检测的过程。</p><p>首先讨论如何将Bi-GCN模型应用于一个事件，即第i个事件的$c_i→y_i$。其他事件的计算方式相同。为了更好地展示我们的方法，我们在下面的内容中省略了下标i。</p><h4 id="1-Construct-Propagation-and-Dispersion-Graphs"><a href="#1-Construct-Propagation-and-Dispersion-Graphs" class="headerlink" title="1 Construct Propagation and Dispersion Graphs"></a>1 Construct Propagation and Dispersion Graphs</h4><p>基于转发和回复关系，我们构造了传播结构 $⟨V, E⟩$ 对于谣言事件。</p><p>然后，设 $A\in R^{n_i×n_i}$和 $X$ 分别为其对应的基于谣言传播树$c_i$的邻接矩阵和特征矩阵。</p><p>$A$ 经过drop变成$A’$ ,基于$A’$ 和 $X$ ，我们可以建立我们的Bi-GCN模型。我们的Bi-GCN由两个组件组成：</p><p> Top-Down Graph Convolutional Network (TD- GCN)</p><p>Bottom-Up Graph Convolutional Network (BU- GCN)</p><p>两个分量的邻接矩阵是不同的</p><p>TD-GCN中，$A^{TD}=A’$  , 对于BU-GCN，邻接矩阵为 $A^{BU}=A’^⊤$。TD-GCN和BU-GCN采用相同的特征矩阵X。</p><h4 id="2-Calculate-the-High-level-Node-Representations"><a href="#2-Calculate-the-High-level-Node-Representations" class="headerlink" title="2 Calculate the High-level Node Representations"></a>2 Calculate the High-level Node Representations</h4><p>TD-GCN经过两层图卷积:</p><script type="math/tex; mode=display">H_1^{TD} = \sigma(\hat A^{TD}XW_0^{TD})</script><script type="math/tex; mode=display">H_2^{TD} = \sigma(\hat A^{TD}H_1^{TD}W_1^{TD})</script><p>BU-GCN相同</p><h4 id="3-Root-Feature-Enhancement"><a href="#3-Root-Feature-Enhancement" class="headerlink" title="3 Root Feature Enhancement"></a>3 Root Feature Enhancement</h4><p>众所周知，谣言事件的来源帖子总是信息丰富，影响广泛。要更好地利用源帖信息，从节点与源帖的关系中学习更准确的节点表示。</p><p>对于第k个GCL处的TD-GCN，我们将每个节点的隐藏特征向量与来自第(k−1)个GCL的根节点的隐藏特征向量连接起来，以构造新的特征矩阵</p><script type="math/tex; mode=display">\hat H_k^{TD} = concat(H_k^{TD},(H_{k-1}^{TD})^{root})</script><p>其中$H_0^{TD}=X$</p><h4 id="4-Representations-of-Propagation-and-Dispersion-for-Rumor-Classification"><a href="#4-Representations-of-Propagation-and-Dispersion-for-Rumor-Classification" class="headerlink" title="4 Representations of Propagation and Dispersion for Rumor Classification"></a>4 Representations of Propagation and Dispersion for Rumor Classification</h4><p>传播表示和扩散表示分别由TD-GCN和BU-GCN的节点表示聚合而成。</p><p>在这里，我们使用均值合并算子来聚集来自这两组节点表示的信息。它的公式是</p><script type="math/tex; mode=display">S^{TD} = MEAN(\hat H_2^{TD})</script><script type="math/tex; mode=display">S^{BU} = MEAN(\hat H_2^{BU})</script><p>然后，我们将传播的表示和扩散的表示连接起来，将信息合并为</p><script type="math/tex; mode=display">S=concat(S^{TD},S^{BU})</script><p>最后，事件ˆy的标签通过几个全连接层和SoftMax层：</p><script type="math/tex; mode=display">\hat y = Softmax(FC(S))</script><p>$\hat y \in R^{1\times C}$</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://z3.ax1x.com/2021/09/06/hhosuq.png" alt=""></p><p><img src="https://z3.ax1x.com/2021/09/06/hhTFIS.png" alt=""></p><p><img src="https://z3.ax1x.com/2021/09/06/hhTQaT.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Rumor-Detection-on-Social-Media-with-Bi-Directional-Graph-Convolutional-Networks&quot;&gt;&lt;a href=&quot;#Rumor-Detection-on-Social-Media-with-Bi-</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>Interpretable Rumor Detection in Microblogs by Attending to User Interactions</title>
    <link href="http://example.com/2021/09/04/Interpretable-Rumor-Detection-in-Microblogs-by-Attending-to-User-Interactions/"/>
    <id>http://example.com/2021/09/04/Interpretable-Rumor-Detection-in-Microblogs-by-Attending-to-User-Interactions/</id>
    <published>2021-09-04T10:52:02.000Z</published>
    <updated>2021-09-05T03:10:18.157Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Interpretable-Rumor-Detection-in-Microblogs-by-Attending-to-User-Interactions"><a href="#Interpretable-Rumor-Detection-in-Microblogs-by-Attending-to-User-Interactions" class="headerlink" title="Interpretable Rumor Detection in Microblogs by Attending to User Interactions"></a>Interpretable Rumor Detection in Microblogs by Attending to User Interactions</h1><p><a href="https://github.com/serenaklm/rumor_detection">https://github.com/serenaklm/rumor_detection</a></p><p>通过学习区分社区对微博中真假claim的响应来解决谣言检测问题。</p><p>现有最先进的模型是基于对会话树建模的树模型。然而，在社交媒体中，发布回复的用户可能是对整个thread的回复，而不是对特定用户的回复。</p><p>提出Multi-head post-level attention模型(PLAN)来构建推文之间的远距离交互。并提出几个变体：</p><ul><li>结构感知自注意模型(StA-PLAN)，将树形结构信息合并到Transformer中</li><li>分层token和post-leve attention(StA-HiTPLAN), 通过token-level 自注意力学习句子表征</li></ul><p>这篇工作重点是利用社区对虚假claim的响应来检测虚假索赔。这一研究领域通过将自然语言处理应用于针对claim的评论来利用社区的集体智慧。这些工作背后的关键原则是，社交媒体上的用户会分享对不准确信息的看法、猜测和证据。</p><h2 id="样本"><a href="#样本" class="headerlink" title="样本"></a>样本</h2><p><img src="https://i.loli.net/2021/09/04/FwWU2TKkf1Ji5B3.png" alt=""></p><p>源贴：“沃尔玛捐赠1万美元支持达伦·威尔逊和正在进行的种族主义警察谋杀案#弗格森#抵制沃尔玛URL”。</p><p>推特R_1及其回复推文R_1_1对消息来源的真实性表示怀疑。</p><p>推特R_2_1和R_3_1提供了确凿的证据，揭穿了消息来源的说法是假的。</p><p>虽然R_2_1和R_3_1分别是R_2和R_3的子节点，但它们可以为树上的所有其他节点(如R_1_1和R_1)提供重要信息。</p><p>因此，应该考虑所有tweet之间的互动，而不仅仅是父节点和他们的孩子节点之间的互动。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>两篇使用树形结构进行建模的sota对社交媒体中谣言检测有限制。</p><p>Rumor detection on twitter with tree-structured recursive neural networks(2018) ，将来源claim及其回复推文组织成树形结构，使用递归神经网络对传播树中的信息传播进行建模。来自不同节点的信号以自下而上或自上而下的方式进行粗略地重新聚合。在自下而上的模型中，信息从子节点传播到父节点，在自上而下的模型中，信息从父节点传播到子节点，反之亦然。</p><p>Tree lstms with convolution units to predict stance and rumor veracity in social media conver- sations.(2019) ，组织了树状结构的对话线程，并探索了用于谣言检测的branch和tree LSTM的几种变体。</p><p>这两篇论文都使用了树模型，目的是对会话线索中存在的结构信息进行建模。在树模型中，信息从父级传播到子级，反之亦然。然而，社交媒体对话中的线索结构有所不同，每个用户通常能够观察到对话的不同分支中的所有回复。揭穿假新闻的用户不能只针对他回复的人创建的内容也可能适用于该帖子中的其他推文。树模型不会对来自其他分支的节点之间的交互进行显式建模，这是对社交媒体会话建模时的一个关键限制。</p><p>自动区分真假Claims的现有方法利用了各种特征：</p><ul><li>Claims的内容</li><li>Claims来源的重点考虑和社交网络</li><li>使用可信来源(例如，维基百科)进行事实核查</li><li>社区对Claims的反应。</li></ul><p>这篇重点在社区响应，接下来展开介绍介绍。</p><h3 id="Content-Information"><a href="#Content-Information" class="headerlink" title="Content Information"></a>Content Information</h3><p>早期关于欺骗性内容检测的工作研究了语言线索的使用，例如代词的百分比、词长、动词数量和词类。也有工作对关于虚假的评论，目击者的陈述，和讽刺。利用语言特征对假新闻的检测也进行了研究。这种对概念内容的分析依赖于可能是领域或主题所独有的语言特征。</p><h3 id="Source-and-Social-Network"><a href="#Source-and-Social-Network" class="headerlink" title="Source and Social Network"></a>Source and Social Network</h3><p>研究假新闻的来源及其社交网络，在内容中加入来源信息提高了假新闻分类准确率。为传播假新闻而创建的账户往往具有不同的社交网络特征。</p><h3 id="Fact-Checking"><a href="#Fact-Checking" class="headerlink" title="Fact Checking"></a>Fact Checking</h3><p>事实核查网站，如PolitiFact。com和snopes.com依靠人工验证来揭穿假新闻，但无法匹配假新闻的生成速度(Funke 2019)。自动事实核查旨在对照诸如维基百科(Ciampaglia et al.2015年)。</p><p>最近，索恩等人(2018)提出了FEVER共享任务，针对包含500万维基百科文档的数据库验证输入Claim，并将每个Claim分为三类：支持、驳斥或信息不足。</p><p>事实核查是一种更有原则的假新闻检测方法。然而，它也需要建立一个经过核实的事实语料库，而且可能不适用于证据很少的新Claim。</p><h3 id="Community-Response"><a href="#Community-Response" class="headerlink" title="Community Response"></a>Community Response</h3><p>研究人员致力于通过构建分类器来自动预测claim的有效性，分类器利用对社交媒体帖子的评论和回复，以及传播模式。</p><p>Ma(2018a)采用多任务学习方法构建了一个学习立场感知特征的分类器用于谣言检测。</p><p>Li (2019)对他的模型采用了多任务学习方法，并在他的模型中包括了用户信息。</p><p>Chen(2017)提出汇集截然不同的特征，以捕捉帖子随时间的上下文变化。</p><p>除了语言特征，其他研究人员也关注了用户的人口统计或交互预测来确定用户的可信度。</p><p>Yang(2012)收集了从事传播假新闻的用户特征，通过对传播路径进行分类，仅利用用户特征构建了假新闻检测器</p><p>Li(2019)使用用户信息和内容特征相结合的方式训练具有多任务学习目标的LSTM。</p><p>在本文中，仅从帖子和评论两个方面来检测谣言和假新闻。提出了一种用于谣言检测的Transformer，而不是递归树模型。</p><h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>问题陈述，将每个线程thread定义为：</p><script type="math/tex; mode=display">X = \{x_1,x_2,...,x_n\}</script><p>其中$x_1$是源tweet，$x_i$是按时间顺序排列的第 $i$ 条tweet，$n$是线程中的tweet数量。</p><p>会话树中除了文本信息外，还有可以利用的结构信息。在树形结构模型中，如果$x_i$对$x_j$进行应答，反之亦然，则只对Twitter $x_i$和$x_j$进行关联。</p><p>本文模型允许任何帖子关注同一主题中的任何其他帖子。</p><p>在提出的结构感知模型中，用关系标签来标记任何一对推文 $x_i$ 和 $x_j$之间的关系 $R(i,j)\in {\text{parent, child, before, after, self}}$ 。$R(i，j)$ 的值是通过依次应用以下规则集来获得的：</p><ul><li>parent：如果 $x_i$ 直接回复 $x_j$</li><li>child：如果 $x_j$ 直接回复 $x_i$</li><li>before：如果 $x_i$ 在 $x_j$之前到来</li><li>after：如果 $x_i$ 在 $x_j$之后</li><li>self：如果i=j</li></ul><p>谣言检测任务简化为学习预测每个$(X，R)$到其谣言类别 $y$。</p><p>在两个谣言检测数据集上进行了实验，即Twitter15和Twitter16数据，以及PHEME 5数据。</p><p>对于我们正在处理的数据集，分类标签是不同的：</p><ul><li>Twitter15 and Twitter16：$y\in {\text{non-rumor, false-rumor, true-rumor, unverified}}$</li><li>PHEME: $y\in {\text{false-rumor, true-rumor, unverified}}$</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>然而，正如我们将在下表中的数据统计中看到的那样，数据集中的树非常浅，大部分评论直接回复源tweet，而不是回复其他tweet。</p><p><img src="https://i.loli.net/2021/09/05/2RAGI9fpbJ5D387.png" alt=""></p><p>我们发现，在社交媒体中，由于整个帖子通常都是可见的，回复根帖子的用户可能会继续与更活跃的用户进行对话，而不是专门为根帖子撰写回复。因此，在对社交媒体对话进行建模时，没有对推文之间的每一种可能的成对交互进行显式建模的树模型是次优的，所以用Transformer-based模型。</p><h3 id="Post-Level-Attention-Network-PLAN"><a href="#Post-Level-Attention-Network-PLAN" class="headerlink" title="Post-Level Attention Network (PLAN)"></a>Post-Level Attention Network (PLAN)</h3><p><img src="https://i.loli.net/2021/09/05/aq5MoQL1iACXujd.png" alt=""></p><p>首先将对话树的结构展平，并将推文按时间顺序排列成直线结构，源推文作为第一条推文。对于我们的计划模型，我们在线性结构中对每个推文 $x_i$ 应用最大池化来获得它的句子表示 $x_i$。</p><p>然后传递一个句子嵌入序列 $X’ = (x_1’,x_2’,…,x_n’)$  通过s个数的多头注意力(MHA)层来模拟推文之间的交互。</p><p>我们将这些MHA层称为post-level attention层。因此，这将改变 $X’ =(x_1’,x_2’,…,x_n’)$ 为 $U=(u_1,u_2,…,u_n)$</p><p>最后，使用注意力机制对推文进行插值，然后通过一个全连接层进行预测。</p><h3 id="Structure-Aware-Post-Level-Attention-Network-StA-PLAN"><a href="#Structure-Aware-Post-Level-Attention-Network-StA-PLAN" class="headerlink" title="Structure Aware Post-Level Attention Network (StA-PLAN)"></a>Structure Aware Post-Level Attention Network (StA-PLAN)</h3><p>模型的一个可能的局限性是，我们通过以线性结构组织推文来丢失结构信息。转换树中固有存在的结构化信息对于假新闻检测可能仍然有用。</p><p>树模型在这方面更优越，因为结构信息是显式建模的。为了将树模型的优点和自我注意机制结合起来，对计划模型进行了扩展，使其显式地包含了结构信息。</p><script type="math/tex; mode=display">\alpha_{ij} = softmax(\frac{q_ik_j^T+a_{ij}^K}{\sqrt{d_k}})</script><script type="math/tex; mode=display">z_i = \sum_{j=1}^n\alpha_{ij}(v_j+a_{ij}^V)</script><p>$a^V<em>{ij}$ 和 $a^K</em>{ij}$ 都是代表tweet对之间五种可能的结构关系(即parent, child, before, after, self)之一的向量</p><h3 id="Structure-Aware-Hierarchical-Token-and-Post-Level-Attention-Network-StA-HiTPLAN"><a href="#Structure-Aware-Hierarchical-Token-and-Post-Level-Attention-Network-StA-HiTPLAN" class="headerlink" title="Structure Aware Hierarchical Token and Post-Level Attention Network (StA-HiTPLAN)"></a>Structure Aware Hierarchical Token and Post-Level Attention Network (StA-HiTPLAN)</h3><p><img src="https://i.loli.net/2021/09/05/Cx2RNBdOwAUcMI1.png" alt=""></p><p>PLAN模型执行最大池化，以获得每条推文的句子表示。</p><p>然而，让模型学习单词向量的重要性可能会更理想。因此，提出了一种分层的注意模型—token-level的注意和post-level的注意力。分层模型的概述如图所示。</p><p>在使用注意机制插入输出之前，我们执行token-level自注意力，而不是使用最大池化来获得句子表示。</p><p>每条推文可以表示为一系列单词记号$x<em>i=(x</em>{i,1}，x<em>{i,2}，…，x</em>{i,|xi|})$。我们在一条推文中通过MHA层传递了单词token的序列。这允许tweet中的token之间进行交互，将这些层称为token-level关注层。</p><h3 id="Time-Delay-Embedding"><a href="#Time-Delay-Embedding" class="headerlink" title="Time Delay Embedding"></a>Time Delay Embedding</h3><p>在不同的时间间隔创建的推文可以有不同的解释。首次创建源claim时表示不相信的推文可能很常见，因为claim可能尚未经过验证。然而，在传播的后期阶段，可疑的推文可能表明消息来源的说法是假的倾向很高。</p><p>因此，提出的三个模型PLAN、STA-PLAN和STA-HiTPLAN研究了带有时延信息的Tweet编码的实用性。</p><p>为了包括每个tweet的时间延迟信息，根据从源tweet创建时起的延迟将tweet绑定。</p><p>将时间箱的总数设置为100，每个箱代表10分钟的间隔。延迟超过1000分钟的推文将落入最后一个时间段。</p><script type="math/tex; mode=display">TDE_{pos,2i} = sin\frac{pos}{10000^{2i/d_model}}</script><script type="math/tex; mode=display">TDE_{pos,2i+1} = cos\frac{pos}{10000^{2i/d_model}}</script><p>其中pos表达为时间bin，$pos\in[0,100)$</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p><img src="https://i.loli.net/2021/09/05/2RAGI9fpbJ5D387.png" alt=""></p><p><img src="https://i.loli.net/2021/09/05/MUWvDm5j36NYksw.png" alt=""></p><ul><li>Twitter15 and Twitter16 ：对于Twitter15和Twitter16的数据集，每个声明中都有很大比例的转发：Twit-15和Twitter16分别为89%和90%。因为作者假设转发不会给模型带来新信息，所以删除了Twitter15和Twitter16的所有转发。在删除转发后，观察到少数索赔将只剩下来源Claim。既然作者的方法背后的原则是，我们可以利用人群的信号来侦测谣言，那么没有任何回复的说法就应该是“未经核实的(unverified)”。因此，在训练数据中修改了这类说法的标签为“未经证实”</li></ul><p><img src="https://i.loli.net/2021/09/05/vaNkVL2UIAhD4n1.png" alt=""></p><p><img src="https://i.loli.net/2021/09/05/7riuNwZBWUvxjE6.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Interpretable-Rumor-Detection-in-Microblogs-by-Attending-to-User-Interactions&quot;&gt;&lt;a href=&quot;#Interpretable-Rumor-Detection-in-Microblogs</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>Dice Loss for Data-imbalanced NLP Tasks</title>
    <link href="http://example.com/2021/09/01/Dice-Loss-for-Data-imbalanced-NLP-Tasks/"/>
    <id>http://example.com/2021/09/01/Dice-Loss-for-Data-imbalanced-NLP-Tasks/</id>
    <published>2021-09-01T07:37:09.000Z</published>
    <updated>2021-09-01T12:54:41.077Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Dice-Loss-for-Data-imbalanced-NLP-Tasks"><a href="#Dice-Loss-for-Data-imbalanced-NLP-Tasks" class="headerlink" title="Dice Loss for Data-imbalanced NLP Tasks"></a>Dice Loss for Data-imbalanced NLP Tasks</h1><p>许多自然语言处理任务，如序列标注和机器阅读理解(MRC)，都面临着严重的数据失衡问题：</p><ul><li>负样本明显多于正样本，占据绝大多数的负例会支配模型的训练过程，导致模型倾向于负例，而测试时使用的F1指标需要每个类都能准确预测；</li><li>大量简单负例（easy-negative）使训练不堪重负。负例占绝大多数也意味着其中包含了很多简单样本，这些简单样本对于模型学习困难样本几乎没有帮助，反而会在交叉熵的作用下推动模型遗忘对困难样本的知识。</li></ul><p>loss中最常用的交叉熵实际上是以精度为导向的，这造成了训练和测试之间的差异。在训练时，每个训练样本对目标函数的贡献相等，而在测试时，F1 score更关注正例。</p><p>本文认为这种问题是交叉熵本身的特点带来的：交叉熵“平等”地看待每一个样本，无论正负，都尽力把它们推向1（正例）或0（负例）。但实际上，对分类而言，将一个样本分类为负只需要它的概率＜0.5即可，完全没有必要将它推向0。</p><p>基于这个观察，作者使用现有的Dice Loss，并提出一个基于Dice Loss的自适应损失——DSC，在训练时推动模型更加关注困难的样本，降低简单负例的学习度，从而在整体上提高基于F1值的效果。</p><h2 id="从Cross-Entropy-到-Dice-Losses"><a href="#从Cross-Entropy-到-Dice-Losses" class="headerlink" title="从Cross Entropy 到 Dice Losses"></a>从Cross Entropy 到 Dice Losses</h2><h3 id="交叉熵损失-CE"><a href="#交叉熵损失-CE" class="headerlink" title="交叉熵损失(CE)"></a>交叉熵损失(CE)</h3><p>以二分类作为说明，记输入为 $x$, 输出为一个二值概率 $p = [p_0,p_1]$, 并且有一个二元真值 $y = [y_0,y_1]$</p><p>首先交叉熵损失是：</p><script type="math/tex; mode=display">CE =  -(y_0log\ p_0 + y_1log \ p_1)</script><p>显然，对每个样本，CE都对它们一视同仁，不管当前样本是简单还是复杂。当简单样本有很多时，模型训练就会被这些简单的样本占据，使得模型难以从复杂样本中学习。于是，一种简单的改进方法是，降低模型在简单样本上的学习速率，从而得到下述加权交叉损失：</p><script type="math/tex; mode=display">Weighted \ CE = -\alpha(y_0log \ p_0+y_1log \ p_1)</script><p>对不同样本，我们可以设置不同的权重，从而控制模型在该样本上学习的程度。但是此时，权重的选择又变得比较困难。因为我们的目标是缓解数据集的不平衡问题，从而提高基于F1评测标准的效果，我们希望有一种损失函数能够直接作用于F1。</p><h3 id="Sorensen–Dice系数（DSC）"><a href="#Sorensen–Dice系数（DSC）" class="headerlink" title="Sørensen–Dice系数（DSC）"></a>Sørensen–Dice系数（DSC）</h3><p>一种现有的方法——Sørensen–Dice系数（简称DSC）——去衡量F1。</p><p>DSC是一种用于衡量两个集合之间相似度的指标：</p><script type="math/tex; mode=display">DSC(A,B) = \frac{2|A\cap B|}{|A|+|B|}</script><script type="math/tex; mode=display">F1 = \frac{2(precision*recall)}{precision+recall}</script><script type="math/tex; mode=display">A = precision = \frac{TP}{TP+FP} ,  B = recall =\frac{TP}{TP+FN}</script><p>如果我们令A是所有模型预测为正的样本的集合，令B是所有实际上为正的样本集合，那么DSC就可以重写为：</p><script type="math/tex; mode=display">DSC(D,f) = \frac{2TP}{2TP+FN+FP}=F1</script><p>其中D数据集，f是一个分类模型。于是在这个意义上DSC与F1是等价的。</p><p>既然如此，就直接优化DSC，然而上述表达式是离散的，为此，需要把上述DSC表达式转化为连续的版本，从而可以视作一种soft F1。</p><p>对于单个样本x，直接定义它的DSC：</p><script type="math/tex; mode=display">DSC(x,f) = \frac{2p_1y_1}{p_1+y_1}</script><p>可以看到如果x是父类，那么它的DSC就为0，从而不会对训练有贡献。为了让父类也能有所贡献，所以增加一个平滑项：</p><script type="math/tex; mode=display">DSC_s(x,f) = \frac{2p_1y_1 + \epsilon}{p_1+y_1+\epsilon}</script><p>但这样一来，又需要我们根据不同的数据集手动地调整平滑项。而且当easy-negative样本很多的时候，即便使用上述平滑项，整个模型训练过程仍然会被它们主导。基于此，我们使用一种“自调节”的DSC（这里就和focal loss很像）：</p><script type="math/tex; mode=display">DSC(x,f) = \frac{2(1-p_1)p_1\cdot y_1 + \epsilon}{(1-p_1)p_1 + y_1 + \epsilon}</script><p>比较上面两个DSC，可以发现，$1-p_1$ 实际上充当了缩放系数，对于简单样本($p_1$ 趋向于1或0)，$(1-p_1)p_1$ 使得模型更少地去关注他们。</p><p>从导数上看，一旦模型正确分类当前样本（刚刚经过0.5），DSC就会使模型更少关注它，而不是像交叉熵那样，鼓励模型迫近0或1这两个点。这就能有效避免因简单样本过多导致模型训练受到简单样本的支配。</p><p>事实上，这比较类似Focal Loss(FL)，降低已分好类的样本的学习权重：</p><script type="math/tex; mode=display">FL = -(y_0(1-p_0)^\gamma log p_0 + y_1(1-p_1)^\gamma log p_1)</script><p>不过，FL即使能对简单样本降低学习权重，但它本质上仍然是在鼓励简单样本趋向于0或1，这就和DSC有了本质上的区别。因此，说DSC通过“平衡”简单样本和困难样本的学习过程，从而提高了最终的F1值（因为F1要求各类都有比较好的结果）</p><h3 id="Dice-Loss-DL-与Tversky-Loss-TL"><a href="#Dice-Loss-DL-与Tversky-Loss-TL" class="headerlink" title="Dice Loss(DL)与Tversky Loss(TL)"></a>Dice Loss(DL)与Tversky Loss(TL)</h3><p>除了上述DSC外，还比较了两种$DSC_s(x,f)$的变体，分别是Dice Loss（DL）和Tversky Loss（TL）：</p><script type="math/tex; mode=display">DL = 1 - \frac{2p_1y_1+\epsilon}{p_1^2+y_1^2+\epsilon}</script><script type="math/tex; mode=display">TL = 1-\frac{p_1y_1 + \epsilon}{p_1y_1+\alpha p_1y_0+\beta p_0y_1 + \epsilon}</script><p>在$\alpha=\beta=0.5$时，TL就退化为DSC。           </p><h3 id="损失总结"><a href="#损失总结" class="headerlink" title="损失总结"></a>损失总结</h3><p><img src="https://i.loli.net/2021/09/01/YkHOMIlVSPjG5aw.png" alt=""></p><p>后三个统称为Dice loss</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Dice-Loss-for-Data-imbalanced-NLP-Tasks&quot;&gt;&lt;a href=&quot;#Dice-Loss-for-Data-imbalanced-NLP-Tasks&quot; class=&quot;headerlink&quot; title=&quot;Dice Loss for </summary>
      
    
    
    
    
    <category term="nlp" scheme="http://example.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs</title>
    <link href="http://example.com/2021/08/29/Edge-augmented-Graph-Transformers-Global-Self-attention-is-Enough-for-Graphs/"/>
    <id>http://example.com/2021/08/29/Edge-augmented-Graph-Transformers-Global-Self-attention-is-Enough-for-Graphs/</id>
    <published>2021-08-29T08:32:53.000Z</published>
    <updated>2021-09-01T13:41:21.763Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Edge-augmented-Graph-Transformers-Global-Self-attention-is-Enough-for-Graphs"><a href="#Edge-augmented-Graph-Transformers-Global-Self-attention-is-Enough-for-Graphs" class="headerlink" title="Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs"></a>Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs</h1><p>—do_train —do_eval —train_batch_size 64 —num_train_epochs 50 —embeddings_learning_rate 0.7e-4 —encoder_learning_rate 0.7e-4 —classifier_learning_rate 7e-4 —warmup_steps 200 —max_seq_len 132 —dropout_rate 0.15 —metric_key_for_early_stop “macro avg<strong>f1-score</strong>level_2” —logging_steps 200 —patience 6 —label2freq_level_1_dir /data2/code/DaguanFengxian/bert_model/data/label2freq_level_1.json —label2freq_level_2_dir /data2/code/DaguanFengxian/bert_model/data/label2freq_level_2.json —processor_sep “\t” —loss_fct_name dice </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Edge-augmented-Graph-Transformers-Global-Self-attention-is-Enough-for-Graphs&quot;&gt;&lt;a href=&quot;#Edge-augmented-Graph-Transformers-Global-Sel</summary>
      
    
    
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Breadth First Reasoning Graph for Multi-hop Question Answering</title>
    <link href="http://example.com/2021/08/17/Breadth-First-Reasoning-Graph-for-Multi-hop-Question-Answering/"/>
    <id>http://example.com/2021/08/17/Breadth-First-Reasoning-Graph-for-Multi-hop-Question-Answering/</id>
    <published>2021-08-17T09:48:20.000Z</published>
    <updated>2021-08-19T12:23:50.481Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Breadth-First-Reasoning-Graph-for-Multi-hop-Question-Answering"><a href="#Breadth-First-Reasoning-Graph-for-Multi-hop-Question-Answering" class="headerlink" title="Breadth First Reasoning Graph for Multi-hop Question Answering"></a>Breadth First Reasoning Graph for Multi-hop Question Answering</h1><p>为了解决GNNs不必要的更新和简单的边结构阻碍直接提取准确的答案跨度，和更可解释性。</p><p>作者提出了一种新的广度优先推理图(BFR-Graph)模型，它提供了一种新的更符合推理过程的消息传递方式。</p><p>在BFR-Graph中，推理信息要求从问题结点开始，逐跳传递到下一个句子结点，直到所有的边都经过，可以有效地防止每个结点的过度平滑或不必要的多次更新。</p><p>为了引入更多的语义，我们还将推理图定义为考虑了共现关系数和句子间距离的加权图。</p><p>然后基于GNN提出了一种更直接、更易解释的方法来聚合不同粒度级别的分数。</p><h2 id="现有GNN方法的几个问题"><a href="#现有GNN方法的几个问题" class="headerlink" title="现有GNN方法的几个问题"></a>现有GNN方法的几个问题</h2><ul><li>首先，当前的方法将所有节点（包括一些不必要的节点）一起更新到每一层中，这可能导致节点收敛到相似的值，并失去对具有更多层的GNN的识别能力。</li><li>第二，虽然GNN设计了不同类型的边，但是在没有考虑句子之间的其他关系信息的情况下，相同类型的边之间没有更细粒度的区别。</li><li>第三，现有的方法只是潜在地融合了GNN和上下文编码器的隐藏表示，而没有以直接和可解释的方式进行答案广度提取。</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="https://i.loli.net/2021/08/19/NlBA3uLTydzhVwi.png" alt=""></p><p>与现有的基于GNN的方法不同，BFR-Graph对消息传递引入了新的限制：消息只从问题开始，然后逐跳传递到后面的句子节点。此外，考虑到共现实体和句子之间的距离，图被构造成一个加权图。此外，利用BFR图的推理结果，设计了多分数答案预测。</p><p>简言之，我们在加权图上提出了广度优先推理，然后在多任务联合训练的框架下，结合多级分数进行回答预测。</p><h3 id="Paragraph-Selection"><a href="#Paragraph-Selection" class="headerlink" title="Paragraph Selection"></a>Paragraph Selection</h3><p>训练一个二分类bert，对每个段落进行打分，选择得分在前N位的段落作为有用的段落，然后将这些段落连接在一起作为上下文C。</p><h3 id="Context-Encoding"><a href="#Context-Encoding" class="headerlink" title="Context Encoding"></a>Context Encoding</h3><p>bert输出+bi-attention后获得问题和上下文的编码表达</p><script type="math/tex; mode=display">H = \{h_0, ..., h_{L-1}\}</script><p>其中L是输入序列的长度(连接问题和上下文)，d是双关注层的输出维度(也是BERT的维度)。</p><p>为了实现句子级表示，首先获得每个句子的标记级表示：</p><script type="math/tex; mode=display">S_i^{seq} = H[s_i^{start}:s_i^{end}]\in \mathbb{R}^{L_{s_i}\times d}</script><p>获得每个句子的表示是用了Bi-LSTM的方法</p><script type="math/tex; mode=display">s_i = \sum_{k=0}^{L_s}\alpha_k^iS_i^{seq}[k, :]\in \mathbb{R}^d</script><p>$\alpha_k^i$ 是第i个句子中第k个token的权重，通过两层MLP output size=1获得</p><h3 id="Weighted-Graph-Construction"><a href="#Weighted-Graph-Construction" class="headerlink" title="Weighted Graph Construction"></a>Weighted Graph Construction</h3><p>为了更好地挖掘句子之间复杂的关系信息，定义了正相关和负相关两种类型的相关性：</p><ul><li>正相关：如果表示句子 $i$ 和 $j$ 的节点具有 n(n≥1) 个相同命名实体，则添加一条边，该边的权重为：</li></ul><script type="math/tex; mode=display">w_{ij} = \frac{1}{1+e^{-n+K_1}}</script><ul><li>负相关：否则，如果两个节点最初来自同一段落，则添加一条边，该边的权重为：</li></ul><script type="math/tex; mode=display">w_{ij} = \frac{1}{1+e^{d+K_2}}</script><p>其中d是两个句子的距离(例如，如果该句子紧跟在段落中的另一个句子之后，则d=1，如果它们之间有句子，则d=2，依此类推)。K1和K2是超参数。</p><p>是同质图，它包含单一类型的节点和边。</p><h3 id="Breadth-First-Reasoning"><a href="#Breadth-First-Reasoning" class="headerlink" title="Breadth First Reasoning"></a>Breadth First Reasoning</h3><p>下图直观地显示了BFR-Graph和典型GNN之间的区别。</p><p><img src="https://i.loli.net/2021/08/19/oCzcsTeIHXZGW3f.png" alt=""></p><p>当我们在段落上推理来回答一个问题时，我们从问题开始，一跳一跳地找到下一个句子。</p><p>对于节点表示句子的GNN，以下消息传递是不必要的，可能会抑制无用节点的干扰：</p><ul><li>从后一个节点到前一个节点</li><li>某个节点尚未收到来自问题的消息，但它会更新其他节点。</li></ul><p>具体地说，当同时满足以下条件时，节点i由节点j更新：</p><ul><li>节点 $i$ 和节点 $j$ 是邻居</li><li>节点 $j$ 是Active的</li><li>节点 $i$ 和节点 $j$ 之间的边以前没有经过</li></ul><p>BFR-Graph的整个消息传递过程:</p><p><img src="https://z3.ax1x.com/2021/08/19/fqlv7j.png" alt=""></p><p>消息更新传递的函数还是GAT</p><h3 id="Multi-score-Answer-Prediction"><a href="#Multi-score-Answer-Prediction" class="headerlink" title="Multi-score Answer Prediction"></a>Multi-score Answer Prediction</h3><p>HotpotQA数据集中的答案是上下文的span。现有工作仅计算编码器输出（如BERT）上的跨度概率，或额外连接GNN的隐藏输出。不同的是，我们通过计算从GNN获得的句子分数和段落分数来使用更易于解释的方法。如下图：</p><p><img src="https://z3.ax1x.com/2021/08/19/fq3Cad.png" alt=""></p><p>通常，作为答案跨度的开始/结束的上下文中的第y个单词的分数通过以下方式计算：</p><script type="math/tex; mode=display">\phi_{start}(y) = MLP_1(H[y,:])</script><script type="math/tex; mode=display">\phi_{end}(y) = MLP_2(H[y,:])</script><p>然后，计算GNN中每个节点对应的句子得分：</p><script type="math/tex; mode=display">\phi_{sent}(s_i) =MLP_3(s_i)</script><p>计算段落分数, 通过全局最大池：</p><script type="math/tex; mode=display">\phi_{para}(p_j) = MLP_4(Max(\{s_0^{p_j},...,s_{L_{p_j}-1}^{P_j}\}))</script><p>$s_i^{P_j}$是第$i$句话在第Pj段中的表达。这也可以通过在所有语句节点上取每个维度上的最大隐藏值来实现。</p><p>最后，上下文中第y个单词作为答案范围开始的概率由以下公式确定：</p><script type="math/tex; mode=display">p_{start}(y) = softmax(\phi'_{start}(y))</script><script type="math/tex; mode=display">\phi'_{start}(y) = \phi_{start}(y) + \phi_{sent}(s_i) + \phi_{para}(p_j)</script><p>并且可以类似地计算上下文中的第y个单词作为答案跨度结束的概率。</p><p>如果一个句子或段落的得分较高，则位于其中的单词更有可能是答案。</p><p>最后是一个多任务预测</p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p><img src="https://z3.ax1x.com/2021/08/19/fqGFjf.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Breadth-First-Reasoning-Graph-for-Multi-hop-Question-Answering&quot;&gt;&lt;a href=&quot;#Breadth-First-Reasoning-Graph-for-Multi-hop-Question-Answe</summary>
      
    
    
    
    
    <category term="GNN&amp;nlp" scheme="http://example.com/tags/GNN-nlp/"/>
    
  </entry>
  
  <entry>
    <title>RealFormer: Transformer Likes Residual Attention</title>
    <link href="http://example.com/2021/08/15/RealFormer-Transformer-Likes-Residual-Attention/"/>
    <id>http://example.com/2021/08/15/RealFormer-Transformer-Likes-Residual-Attention/</id>
    <published>2021-08-15T03:16:37.000Z</published>
    <updated>2021-08-15T15:43:15.910Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RealFormer-Transformer-Likes-Residual-Attention"><a href="#RealFormer-Transformer-Likes-Residual-Attention" class="headerlink" title="RealFormer: Transformer Likes Residual Attention"></a>RealFormer: Transformer Likes Residual Attention</h1><p>提出了一个简单的基于Transformer的体系结构，创建一条“直接”路径在整个网络中传播原始注意力分数</p><p>如下图(c), 每个RealFormer层都获取前一层中所有注意力头部的原始注意力分数，并在顶部添加“残差分数”(计算方式与常规Transformers中的注意力分数相同)。</p><p>换句话说，RealFormer可以被视为向Post-LN Transformer添加一个简单的跳跃连接。不会向计算图中添加任何乘法运算，因此预期性能与之相当。</p><p>RealFormer中的AT往往更稀疏，跨层相关性更强，我们认为这可能具有一些正则化效应，可以稳定训练，有利于微调。</p><p><img src="https://i.loli.net/2021/08/15/jZrDYmkL5EzFqi6.png" alt="">t</p><ul><li>(a) 传统transformer的PostLN</li><li>(b) PreLN 论文：ON LAYER NORMALIZATION IN THE TRANSFORMER ARCHITECTURE，这种设计为每个子层增加了LN作为“预处理”步骤。</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>标准Transformer Encoder</p><script type="math/tex; mode=display">\text{MultiHead}(Q,K,V) = Concat(head_1,...,head_h)W^O</script><script type="math/tex; mode=display">head_i = Attention(QW_i^Q, KW^K_i,VW^V_i)</script><script type="math/tex; mode=display">Attention(Q',K',V') = Softmax(\frac{Q'K'^T}{\sqrt{d_k}})V'</script><script type="math/tex; mode=display">FFN(x) = \sigma(xW_1+b_1)W_2+b_2</script><p>Post-LN是Vaswani等人提出的原创体系结构。对每个子层末尾的输出进行标准化。</p><p>相反，Pre-LN规格化子层输入，并创建直接路径(没有LN)来传播序列中的令牌嵌入。</p><h3 id="Residual-Attention-Layer-Transformer"><a href="#Residual-Attention-Layer-Transformer" class="headerlink" title="Residual Attention Layer Transformer"></a>Residual Attention Layer Transformer</h3><p>RealFormer紧跟Post-LN设计，简单地增加了一个skip edge来连接相邻层中的多头注意力，如上图c所示。</p><p>形式上添加一个$Prev$，是上一个softmax的注意力分数也就是pre-softmax，形状为$(heads,\text{from_seq_len},\text{to_seq_len})^2$</p><script type="math/tex; mode=display">\text{ResidualMultiHead}(Q,K,V,Prev) = Concat(head1,...,head_h)W^O</script><script type="math/tex; mode=display">head_i = \text{ResidualAttention}(QW_i^Q,KW_i^K,VW_i^V,Prev_i)</script><p>$Prev_i$ 的形状为$(\text{from_seq_len,to_seq_len})$ 对应于每个$head_i$</p><script type="math/tex; mode=display">\text{ResidualAttention}(Q',K',V',Prev') = \text{Softmax}(\frac{Q'K'^T}{\sqrt{d_k}}+Prev')V'</script><p>新的注意力分数$\frac{Q’K’^T}{\sqrt{d_k}}+Prev’$</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://i.loli.net/2021/08/15/K86nvAXLs954SMf.png" alt=""></p><p>值得特别指出的是第一张图和第四张图。从第一张图我们可以看到，对于RealFormer结构，加大模型规模（large到xlarge）可以带来性能的明显提升，而ALBERT论文曾经提到加大BERT的模型规模并不能带来明显受益，结合两者说明这可能是PostLN的毛病而不是BERT的固有毛病，换成RealFormer可以改善这一点。从第四张图我们可以看到，RealFormer结构训练50万步，效果就相当于PostLN训练100万步，这表明RealFormer有着很高的训练效率。</p><p>除了上述实验外，论文还对比了不同学习率、不同Dropout比例的效果，表明RealFormer确实对这些参数是比较鲁棒的。原论文还分析了RealFormer的Attention值分布，表明RealFormer的Attention结果更加合理。</p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>RealFormer对梯度下降更加友好，这不难理解，因为$A<em>n = \frac{Q_nK_n^T}{\sqrt{d_k}} + A</em>{n-1}$的设计确实提供了一条直通路，使得第一层的Attention能够直通最后一层，自然就没有什么梯度消失的风险了。相比之下，PostLN是 $LayerNorm(x+f(x))$ 的结构，看上去$x+f(x)$防止了梯度消失，但是LayerNorm这一步会重新增加了梯度消失的风险，造成的后果是初始阶段前面的层梯度很小，后面的层梯度很大，如果用大学习率，后面的层容易崩，如果用小学习率，前面的层学不好，因此PostLN更难训练，需要小的学习率加warmup慢慢训。</p><p>还有一个就是叠加的问题PreLN每一步都是$x+f(x)$的形式，到了最后一层变成了$x+f_1(x)+f_2(x)+…++f_n(x)$的形式，一层层累加，可能导致数值和方差都很大，最后迫不得已强制加一层Layer Norm让输出稳定下来。这样，尽管PreLN改善了梯度状况，但它本身设计上就存在一些不稳定因素。</p><p>Realformer的$A<em>n = \frac{Q_nK_n^T}{\sqrt{d_k}} + A</em>{n-1}$存在叠加问题吗？如果只看A，那么确实有这样的问题，但A后面还要做个softmax归一化后才参与运行，也就是说，模型对矩阵A是自带归一化功能的，所以它不会有数值发散的风险。而且刚刚相反，随着层数的增加，A的叠加会使得A的元素绝对值可能越来越大，Attention趋近于onehot形式，造成后面的层梯度消失，但是别忘了，我们刚才说PostLN前面的层梯度小后面的大，而现在也进一步缩小了后面层的梯度，反而使得两者更同步，从而更好优化了；</p><p>另一方面Attention的概率值可能会有趋同的趋势，也就是说Attention的模式可能越来越稳定了。带来类似ALBERT参数共享的正则化效应，这对模型效果来说可能是有利的。同时，直觉上来想，用RealFormer结构去做FastBert之类的自适应层数的改进，效果会更好，因为RealFormer的Attention本身会有趋同趋势，更加符合FastBert设计的出发点。</p><p>此外，我们也可以将RealFormer理解为还是使用了常规的残差结构，但是残差结构只用在<strong>Q</strong>,<strong>K</strong>而没有用在<strong>V</strong>上。</p><p>为啥<strong>V</strong>“不值得”一个残差呢？从近来的一些相对位置编码的改进中，笔者发现似乎有一个共同的趋势，那就是去掉了<strong>V</strong>的偏置，比如像NEZHA的相对位置编码，是同时在Attention矩阵（即<strong>Q</strong>,<strong>K</strong>）和<strong>V</strong>上施加的，而较新的XLNET和T5的相对位置编码则只施加在Attention矩阵上，所以，似乎去掉<strong>V</strong>的不必要的偏置是一个比较好的选择，而RealFormer再次体现了这一点。</p><h3 id="RealFormer与Baseline-Transformers在本质上有什么不同？"><a href="#RealFormer与Baseline-Transformers在本质上有什么不同？" class="headerlink" title="RealFormer与Baseline Transformers在本质上有什么不同？"></a>RealFormer与Baseline Transformers在本质上有什么不同？</h3><p>dev set中随机抽样了8,192个示例，并可视化了这些示例中每个token(不包括padding)在表2中的三个预先训练的BERT-Base模型中的所有层和所有头部的注意概率分布。</p><p>特别地，对于每个(token、layer、head)三元组，我们计算关注权重(概率)的熵作为关注度的“稀疏度量”。直观地说，熵越低，注意力权重分布就越偏斜，因此注意力就越稀疏。</p><p><img src="https://i.loli.net/2021/08/15/av6cdqPg5HxWShY.png" alt=""></p><p>用RealFormer训练好的BERT-BASE对8192个突出例子的标记注意概率的熵分布</p><p>为了更好地辨认，每一层中的注意力都是按照熵的中位数排序的。根据熵的中位数对分布重新进行颜色编码：红色(中位数&gt;4.5)、黄色(1.5≤中位数≤4.5)、蓝色(中位数&lt;1.5)。也就是说，颜色越冷意味着注意力越稀疏。有一个明显的趋势是，较高的层往往具有较稀疏的注意力。</p><p>下面的是post-LN和pre-LN的熵分布</p><p><img src="https://i.loli.net/2021/08/15/MIfAtTSK6QV1a4h.png" alt=""></p><p><img src="https://i.loli.net/2021/08/15/yIdzx7tDBTnNuUM.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;RealFormer-Transformer-Likes-Residual-Attention&quot;&gt;&lt;a href=&quot;#RealFormer-Transformer-Likes-Residual-Attention&quot; class=&quot;headerlink&quot; title</summary>
      
    
    
    
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Centos 6无法使用yum解决办法</title>
    <link href="http://example.com/2021/08/12/Centos-6%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8yum%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <id>http://example.com/2021/08/12/Centos-6%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8yum%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</id>
    <published>2021-08-12T08:54:16.000Z</published>
    <updated>2021-08-12T08:58:01.013Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Centos-6无法使用yum解决办法"><a href="#Centos-6无法使用yum解决办法" class="headerlink" title="Centos 6无法使用yum解决办法"></a>Centos 6无法使用yum解决办法</h1><p>12月后Centos 6 系统无法使用yum出现错误(文章底部看)</p><p>相信已经有一部分朋友今天连接到CentOS 6的服务器后执行yum后发现报错，那么发生了什么？</p><p>CentOS 6已经随着2020年11月的结束进入了EOL（Reaches End of Life），不过有一些老设备依然需要支持，CentOS官方也给这些还不想把CentOS 6扔进垃圾堆的用户保留了最后一个版本的镜像，只是这个镜像不会再有更新了</p><p>官方便在12月2日正式将CentOS 6相关的软件源移出了官方源，随之而来逐级镜像也会陆续将其删除。</p><p>不过有一些老设备依然需要维持在当前系统，CentOS官方也给这些还不想把CentOS 6扔进垃圾堆的用户保留了各个版本软件源的镜像，只是这个软件源不会再有更新了。</p><h2 id="错误详情"><a href="#错误详情" class="headerlink" title="错误详情"></a>错误详情</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@c8-20 ~]<span class="comment"># yum makecache</span></span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line">YumRepo Error: All mirror URLs are not using ftp, http[s] or file.</span><br><span class="line"> Eg. Invalid release/repo/arch combination/</span><br><span class="line">removing mirrorlist with no valid mirrors: /var/cache/yum/x86_64/6/base/mirrorlist.txt</span><br><span class="line">Error: Cannot find a valid baseurl <span class="keyword">for</span> repo: base</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@li496-237 ~]<span class="comment"># yum -y install unzip zip</span></span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">Setting up Install Process</span><br><span class="line">Determining fastest mirrors</span><br><span class="line">YumRepo Error: All mirror URLs are not using ftp, http[s] or file.</span><br><span class="line">Eg. Invalid release/repo/arch combination/</span><br><span class="line">YumRepo Error: All mirror URLs are not using ftp, http[s] or file.</span><br><span class="line">Eg. Invalid release/repo/arch combination/</span><br><span class="line">YumRepo Error: All mirror URLs are not using ftp, http[s] or file.</span><br><span class="line">Eg. Invalid release/repo/arch combination/</span><br><span class="line">http://mirrors.linode.com/centos/6/os/x86_64/repodata/repomd.xml: [Errno 14] PYCURL ERROR 22 - <span class="string">&quot;The requested URL returned error: 404 Not Found&quot;</span></span><br><span class="line">Trying other mirror.</span><br><span class="line">To address this issue please refer to the below knowledge base article </span><br><span class="line"></span><br><span class="line">https://access.redhat.com/articles/1320623</span><br><span class="line"></span><br><span class="line">If above article doesn<span class="string">&#x27;t help to resolve this issue please open a ticket with Red Hat Support.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Error: Cannot retrieve repository metadata (repomd.xml) for repository: base. Please verify its path and try again</span></span><br></pre></td></tr></table></figure><p>一键修复</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;s|enabled&#x3D;1|enabled&#x3D;0|g&quot; &#x2F;etc&#x2F;yum&#x2F;pluginconf.d&#x2F;fastestmirror.conf</span><br><span class="line">mv &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo.backup</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo https:&#x2F;&#x2F;www.xmpan.com&#x2F;Centos-6-Vault-Aliyun.repo </span><br><span class="line">yum clean all</span><br><span class="line">yum makecache</span><br></pre></td></tr></table></figure><p>手动修复教程:</p><p>首先把fastestmirrors关了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#编辑</span><br><span class="line">vi &#x2F;etc&#x2F;yum&#x2F;pluginconf.d&#x2F;fastestmirror.conf</span><br><span class="line">#修改</span><br><span class="line">enable&#x3D;0</span><br><span class="line">#或者执行以下命令</span><br><span class="line">sed -i &quot;s|enabled&#x3D;1|enabled&#x3D;0|g&quot; &#x2F;etc&#x2F;yum&#x2F;pluginconf.d&#x2F;fastestmirror.conf</span><br></pre></td></tr></table></figure><p>先把之前的repo挪到备份，然后下面两个二选一</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo.bak</span><br></pre></td></tr></table></figure><p>替换为官方Vault源(海外服务器用)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo https:&#x2F;&#x2F;www.xmpan.com&#x2F;Centos-6-Vault-Official.repo</span><br></pre></td></tr></table></figure><p>或者替换为阿里云Vault镜像(国内服务器用)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo https:&#x2F;&#x2F;www.xmpan.com&#x2F;Centos-6-Vault-Aliyun.repo</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Centos-6无法使用yum解决办法&quot;&gt;&lt;a href=&quot;#Centos-6无法使用yum解决办法&quot; class=&quot;headerlink&quot; title=&quot;Centos 6无法使用yum解决办法&quot;&gt;&lt;/a&gt;Centos 6无法使用yum解决办法&lt;/h1&gt;&lt;p&gt;12</summary>
      
    
    
    
    
    <category term="配置记录" scheme="http://example.com/tags/%E9%85%8D%E7%BD%AE%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>从 SGD 到 AdamW 原理和代码解读</title>
    <link href="http://example.com/2021/08/12/%E4%BB%8E-SGD-%E5%88%B0-AdamW-%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/"/>
    <id>http://example.com/2021/08/12/%E4%BB%8E-SGD-%E5%88%B0-AdamW-%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/</id>
    <published>2021-08-12T05:00:18.000Z</published>
    <updated>2021-08-13T08:34:00.830Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从-SGD-到-AdamW-原理和代码解读"><a href="#从-SGD-到-AdamW-原理和代码解读" class="headerlink" title="从 SGD 到 AdamW 原理和代码解读"></a>从 SGD 到 AdamW 原理和代码解读</h1><p>深度学习优化算法经历了 SGD -&gt; SGDM -&gt; NAG -&gt;AdaGrad -&gt; AdaDelta -&gt; Adam -&gt; Nadam -&gt; AdamW 这样的发展历程。</p><p>接下来用一个框架来梳理所有的优化算法。</p><p>首先定义：待优化参数：$w$ , 目标函数：$f(x)$ , 初始学习率 $\alpha$</p><p>然后，开始进行迭代优化。在每个epoch $t$:</p><ol><li>计算目标函数关于当前参数的梯度: $g_t = ∇f(w_t)$</li><li>根据历史梯度计算一阶动量和二阶动量: $m_t = \phi(g_1,g_2,…,g_t); V_t =\psi(g_1,g_2,…,g_t)$</li><li>计算当前时刻的下降梯度: $\eta = \alpha \cdot m_t/\sqrt V_t$</li><li>根据下降梯度进行更新: $w_{t+1} = w_t -\eta_t$</li></ol><p>步骤3、4对于各个算法几乎都是一致的，主要的差别就体现在1和2上。</p><p>也就是计算一阶动量$m_t$ 和 二阶动量$V_t$时采用不同的套路。</p><p>此外在所有优化器代码里有一些函数作用是相通的：</p><blockquote><p>共性的方法有：</p></blockquote><ul><li>add_param_group(param_group) : 把参数放进优化器中，这在Fine-tune预训练时可以使冻结层可训练，并随着训练的进行添加到优化器中。</li><li>load_state_dict(state_dict): 把优化器的状态加载进去。</li><li>state_dict():返回优化器状态，以dict形式</li><li>step(closure=None):优化一步参数</li><li>zero_grad(set_to_none=False):把所有的梯度值设为0</li></ul><blockquote><p>使用方法：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> dataset:</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">closure</span>():</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br><span class="line">    loss = loss_fn(output, target)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line">  optimizer.step(closure)</span><br></pre></td></tr></table></figure><h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p>SGD没有动量的概念，也就是说：</p><script type="math/tex; mode=display">m_t = g_t ; V_t=I^2</script><p>代入步骤3，可以看到下降梯度就是最简单的</p><script type="math/tex; mode=display">\eta_t = \alpha \cdot g_t</script><p>SGD最大的缺点就是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点。</p><h2 id="SGD-with-Momentum"><a href="#SGD-with-Momentum" class="headerlink" title="SGD with Momentum"></a>SGD with Momentum</h2><p>为了抑制震荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一点。</p><p>在SGD的基础上引入了一阶动量：</p><script type="math/tex; mode=display">m_t = \beta_1\cdot m_{t-1} +(1-\beta_1)\cdot g_t</script><p>一阶动量就是各个时刻梯度方向的指数移动平均，约等于最近$1/(1-\beta_1)$个时刻的梯度向量和的平均值。</p><p>也就是说，t 时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。</p><p>$\beta_1$的经验值为0.9，这意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。想象高速公路上汽车转弯，在高速向前的同时略微偏向，急转弯可是要出事的。</p><h2 id="SGD-with-Nesterov-Acceleration"><a href="#SGD-with-Nesterov-Acceleration" class="headerlink" title="SGD with Nesterov Acceleration"></a>SGD with Nesterov Acceleration</h2><p>SGD还有一个问题是困在局部最优的沟壑里震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能呆在这里了。可是你如果爬上高地，就会方向外卖的世界还很广阔。</p><p>因此外卖不能停留在当前位置去观察未来的方向，而是要向前一步，多看一步，看远一些。</p><p>NAG全称Nesterov Accelerated Gradient，是在SGD、SGDM的基础上的进一步改进，改进点在于步骤1。</p><p>我们知道在时刻 t 的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算。那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。</p><p>因此NAG在步骤1，不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向：</p><script type="math/tex; mode=display">g_t =∇ f(w_t-\beta_1\cdot m_{t-1}/\sqrt{V_{t-1}})</script><p>然后用下一个点的梯度方向，与历史累积动量结合，计算步骤2中当前时刻的累加动量。</p><blockquote><p>定义优化器：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.optim.SGD(params, lr&#x3D;&lt;required parameter&gt;, momentum&#x3D;0, dampening&#x3D;0, weight_decay&#x3D;0, nesterov&#x3D;False)</span><br></pre></td></tr></table></figure><blockquote><p>参数：</p></blockquote><ul><li><strong>params</strong> (iterable) – 优化器作用的模型参数。</li><li><strong>lr</strong> (float) – learning rate，相当于是统一框架中的 $\alpha$</li><li><strong>momentum</strong> (float, optional) – 动量参数。(默认值：0)</li><li><strong>weight_decay</strong> (float, optional) – 权重衰减系数 weight decay (L2 penalty) (默认值：0)</li><li><strong>dampening</strong> (float, optional) – dampening for momentum (默认值：0)</li><li><strong>nesterov</strong> (bool, optional) – 允许 Nesterov momentum (默认值：False)</li></ul><blockquote><p>源码解读：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer, required</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[docs]<span class="class"><span class="keyword">class</span> <span class="title">SGD</span>(<span class="params">Optimizer</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Implements stochastic gradient descent (optionally with momentum).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Nesterov momentum is based on the formula from</span></span><br><span class="line"><span class="string">    `On the importance of initialization and momentum in deep learning`__.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float): learning rate</span></span><br><span class="line"><span class="string">        momentum (float, optional): momentum factor (default: 0)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string">        dampening (float, optional): dampening for momentum (default: 0)</span></span><br><span class="line"><span class="string">        nesterov (bool, optional): enables Nesterov momentum (default: False)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer.zero_grad()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; loss_fn(model(input), target).backward()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer.step()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. note::</span></span><br><span class="line"><span class="string">        The implementation of SGD with Momentum/Nesterov subtly differs from</span></span><br><span class="line"><span class="string">        Sutskever et. al. and implementations in some other frameworks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Considering the specific case of Momentum, the update can be written as</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. math::</span></span><br><span class="line"><span class="string">            \begin&#123;aligned&#125;</span></span><br><span class="line"><span class="string">                v_&#123;t+1&#125; &amp; = \mu * v_&#123;t&#125; + g_&#123;t+1&#125;, \\</span></span><br><span class="line"><span class="string">                p_&#123;t+1&#125; &amp; = p_&#123;t&#125; - \text&#123;lr&#125; * v_&#123;t+1&#125;,</span></span><br><span class="line"><span class="string">            \end&#123;aligned&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        where :math:`p`, :math:`g`, :math:`v` and :math:`\mu` denote the </span></span><br><span class="line"><span class="string">        parameters, gradient, velocity, and momentum respectively.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        This is in contrast to Sutskever et. al. and</span></span><br><span class="line"><span class="string">        other frameworks which employ an update of the form</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. math::</span></span><br><span class="line"><span class="string">            \begin&#123;aligned&#125;</span></span><br><span class="line"><span class="string">                v_&#123;t+1&#125; &amp; = \mu * v_&#123;t&#125; + \text&#123;lr&#125; * g_&#123;t+1&#125;, \\</span></span><br><span class="line"><span class="string">                p_&#123;t+1&#125; &amp; = p_&#123;t&#125; - v_&#123;t+1&#125;.</span></span><br><span class="line"><span class="string">            \end&#123;aligned&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The Nesterov version is analogously modified.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, params, lr=required, momentum=<span class="number">0</span>, dampening=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">0</span>, nesterov=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> lr <span class="keyword">is</span> <span class="keyword">not</span> required <span class="keyword">and</span> lr &lt; <span class="number">0.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid learning rate: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr))</span><br><span class="line">        <span class="keyword">if</span> momentum &lt; <span class="number">0.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid momentum value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(momentum))</span><br><span class="line">        <span class="keyword">if</span> weight_decay &lt; <span class="number">0.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid weight_decay value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight_decay))</span><br><span class="line"></span><br><span class="line">        defaults = <span class="built_in">dict</span>(lr=lr, momentum=momentum, dampening=dampening,</span><br><span class="line">                        weight_decay=weight_decay, nesterov=nesterov)</span><br><span class="line">        <span class="keyword">if</span> nesterov <span class="keyword">and</span> (momentum &lt;= <span class="number">0</span> <span class="keyword">or</span> dampening != <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Nesterov momentum requires a momentum and zero dampening&quot;</span>)</span><br><span class="line">        <span class="built_in">super</span>(SGD, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SGD, self).__setstate__(state)</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            group.setdefault(<span class="string">&#x27;nesterov&#x27;</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">[docs]    @torch.no_grad()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, closure=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">                loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            weight_decay = group[<span class="string">&#x27;weight_decay&#x27;</span>]</span><br><span class="line">            momentum = group[<span class="string">&#x27;momentum&#x27;</span>]</span><br><span class="line">            dampening = group[<span class="string">&#x27;dampening&#x27;</span>]</span><br><span class="line">            nesterov = group[<span class="string">&#x27;nesterov&#x27;</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                d_p = p.grad  <span class="comment"># 得到每个参数的梯度，也就是g_t</span></span><br><span class="line">                <span class="keyword">if</span> weight_decay != <span class="number">0</span>: <span class="comment"># 如果使用weight_decay的话，相当于目标函数上加上 L2正则</span></span><br><span class="line">                    d_p = d_p.add(p, alpha=weight_decay)</span><br><span class="line">                <span class="keyword">if</span> momentum != <span class="number">0</span>:</span><br><span class="line">                    param_state = self.state[p]</span><br><span class="line">                    <span class="keyword">if</span> <span class="string">&#x27;momentum_buffer&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> param_state:</span><br><span class="line">                        buf = param_state[<span class="string">&#x27;momentum_buffer&#x27;</span>] = torch.clone(d_p).detach()</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        buf = param_state[<span class="string">&#x27;momentum_buffer&#x27;</span>]</span><br><span class="line">                        <span class="comment"># 计算动量，momentum参数beta_1一般取0.9，相当于之前的动量buf乘以0.9再加上此次梯度</span></span><br><span class="line">                        <span class="comment"># d_p乘以(1-beta_1)=0.1</span></span><br><span class="line">                        buf.mul_(momentum).add_(d_p, alpha=<span class="number">1</span> - dampening)</span><br><span class="line">                    <span class="keyword">if</span> nesterov:</span><br><span class="line">                      <span class="comment"># 如果通过nesterov方式更新参数，那么eta_t就相当于g_t+m_t*beta_1</span></span><br><span class="line">                        d_p = d_p.add(buf, alpha=momentum)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                      <span class="comment"># 如果不通过nesterov方式更新参数，那么\eta_t就是相当于是上一步计算出的动量m_t</span></span><br><span class="line">                        d_p = buf</span><br><span class="line"></span><br><span class="line">                p.add_(d_p, alpha=-group[<span class="string">&#x27;lr&#x27;</span>]) <span class="comment">#最后用学习率更新梯度</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>此前我们都没有用到二阶动量。二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。</p><p>SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到(想想大规模的embedding)。</p><p>对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学习一些，即学习率大一些。</p><p>怎么样去度量历史更新频率呢？那就是二阶动量——该维度上，迄今为止所有梯度值的平方和：</p><script type="math/tex; mode=display">V_t = \sum_{\tau=1}^t g_{\tau}^2</script><p>我们在回顾一些步骤3中的下降梯度：</p><script type="math/tex; mode=display">\eta_t = \alpha\cdot m_t/\sqrt{V_t}</script><p>可以看出，此时实质上的学习率由$\alpha$ 变成了，$\alpha/\sqrt{V_t}$。</p><p>一般为了避免分母为0，会在分母上加一个小的平滑项。因此$\sqrt{V_t}$是恒大于0的，而且参数更新越频繁，二阶动量越大，学习率就越小。</p><p>这一方法在稀疏数据场景下表现非常好，但也存在一些问题：因为$\sqrt{V_t}$ 是单调递增的，会使学习率单调递减至0，可能会使训练过程提前结束，即便后续还有数据也无法学到必要的知识。</p><blockquote><p>定义优化器：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.optim.Adagrad(params,lr&#x3D;0.01,lr_decay&#x3D;0,weight_decay&#x3D;0,initial_accumulator_value&#x3D;0,eps&#x3D;1e-10)</span><br></pre></td></tr></table></figure><blockquote><p>参数：</p></blockquote><ul><li><strong>params</strong> (iterable) – 优化器作用的模型参数。</li><li><strong>lr</strong> (float) – learning rate – 相当于是统一框架中的 。</li><li><strong>lr_decay</strong>(float,optional) – 学习率衰减 (默认值：0)</li><li><strong>weight_decay</strong> (float, optional) – 权重衰减系数 weight decay (L2 penalty) (默认值：0)</li><li><strong>eps</strong>(float,optional)：防止分母为0的一个小数 (默认值：1e-10)</li></ul><blockquote><p>源码解读：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">[docs]<span class="class"><span class="keyword">class</span> <span class="title">Adagrad</span>(<span class="params">Optimizer</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Implements Adagrad algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has been proposed in `Adaptive Subgradient Methods for Online Learning</span></span><br><span class="line"><span class="string">    and Stochastic Optimization`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-2)</span></span><br><span class="line"><span class="string">        lr_decay (float, optional): learning rate decay (default: 0)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-10)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Adaptive Subgradient Methods for Online Learning and Stochastic</span></span><br><span class="line"><span class="string">        Optimization: http://jmlr.org/papers/v12/duchi11a.html</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, params, lr=<span class="number">1e-2</span>, lr_decay=<span class="number">0</span>, weight_decay=<span class="number">0</span>, initial_accumulator_value=<span class="number">0</span>, eps=<span class="number">1e-10</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= lr:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid learning rate: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= lr_decay:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid lr_decay value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr_decay))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= weight_decay:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid weight_decay value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight_decay))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= initial_accumulator_value:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid initial_accumulator_value value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(initial_accumulator_value))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= eps:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid epsilon value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(eps))</span><br><span class="line"></span><br><span class="line">        defaults = <span class="built_in">dict</span>(lr=lr, lr_decay=lr_decay, eps=eps, weight_decay=weight_decay,</span><br><span class="line">                        initial_accumulator_value=initial_accumulator_value)</span><br><span class="line">        <span class="built_in">super</span>(Adagrad, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">                state = self.state[p]</span><br><span class="line">                state[<span class="string">&#x27;step&#x27;</span>] = <span class="number">0</span></span><br><span class="line">                state[<span class="string">&#x27;sum&#x27;</span>] = torch.full_like(p, initial_accumulator_value, memory_format=torch.preserve_format)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">share_memory</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">                state = self.state[p]</span><br><span class="line">                state[<span class="string">&#x27;sum&#x27;</span>].share_memory_()</span><br><span class="line"></span><br><span class="line">[docs]    @torch.no_grad()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, closure=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">                loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            params_with_grad = []</span><br><span class="line">            grads = []</span><br><span class="line">            state_sums = []</span><br><span class="line">            state_steps = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    params_with_grad.append(p)</span><br><span class="line">                    grads.append(p.grad)</span><br><span class="line">                    state = self.state[p]</span><br><span class="line">                    state_sums.append(state[<span class="string">&#x27;sum&#x27;</span>])</span><br><span class="line">                    <span class="comment"># update the steps for each param group update</span></span><br><span class="line">                    state[<span class="string">&#x27;step&#x27;</span>] += <span class="number">1</span></span><br><span class="line">                    <span class="comment"># record the step after step update</span></span><br><span class="line">                    state_steps.append(state[<span class="string">&#x27;step&#x27;</span>])</span><br><span class="line"></span><br><span class="line">            F.adagrad(params_with_grad,</span><br><span class="line">                      grads,</span><br><span class="line">                      state_sums,</span><br><span class="line">                      state_steps,</span><br><span class="line">                      group[<span class="string">&#x27;lr&#x27;</span>],</span><br><span class="line">                      group[<span class="string">&#x27;weight_decay&#x27;</span>],</span><br><span class="line">                      group[<span class="string">&#x27;lr_decay&#x27;</span>],</span><br><span class="line">                      group[<span class="string">&#x27;eps&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h2 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h2><p>由于AdaGrad 单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也是就是AdaDelta名称中Delta的来历。</p><p>修改思路很简单，前面讲到，指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量：</p><script type="math/tex; mode=display">V_t = \beta_2 * V_{t-1} + (1-\beta_2)g_t^2</script><p>接下来还是步骤3：</p><script type="math/tex; mode=display">\eta_t = \alpha \cdot g_t/\sqrt{V_t}</script><p>这就避免了二阶动量持续累积、导致训练过程提前结束的问题了。</p><h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><blockquote><p>定义优化器：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.optim.RMSprop(params, lr&#x3D;0.01, alpha&#x3D;0.99, eps&#x3D;1e-08, weight_decay&#x3D;0, momentum&#x3D;0, centered&#x3D;False)</span><br></pre></td></tr></table></figure><blockquote><p>参数：</p></blockquote><ul><li><strong>params</strong> (iterable) – 优化器作用的模型参数。</li><li><strong>lr</strong> (float) – learning rate – 相当于是统一框架中的 $\alpha$。</li><li><strong>momentum</strong> (float, optional) – 动量参数。(默认值：0)。</li><li><strong>alpha</strong>(<em>float,optional</em>) – 平滑常数 (默认值：0.99)。</li><li><strong>centered</strong>(bool,optional) – if<code>True</code>, compute the centered RMSProp, the gradient is normalized by an estimation of its variance，就是这一项是 True 的话就把方差使用梯度作归一化。</li><li><strong>weight_decay</strong> (float, optional) – 权重衰减系数 weight decay (L2 penalty) (默认值：0)</li><li><strong>eps</strong>(float,optional)：防止分母为0的一个小数 (默认值：1e-10)</li></ul><blockquote><p><strong>源码解读：</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[docs]<span class="class"><span class="keyword">class</span> <span class="title">RMSprop</span>(<span class="params">Optimizer</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Implements RMSprop algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Proposed by G. Hinton in his</span></span><br><span class="line"><span class="string">    `course &lt;https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The centered version first appears in `Generating Sequences</span></span><br><span class="line"><span class="string">    With Recurrent Neural Networks &lt;https://arxiv.org/pdf/1308.0850v5.pdf&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The implementation here takes the square root of the gradient average before</span></span><br><span class="line"><span class="string">    adding epsilon (note that TensorFlow interchanges these two operations). The effective</span></span><br><span class="line"><span class="string">    learning rate is thus :math:`\alpha/(\sqrt&#123;v&#125; + \epsilon)` where :math:`\alpha`</span></span><br><span class="line"><span class="string">    is the scheduled learning rate and :math:`v` is the weighted moving average</span></span><br><span class="line"><span class="string">    of the squared gradient.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-2)</span></span><br><span class="line"><span class="string">        momentum (float, optional): momentum factor (default: 0)</span></span><br><span class="line"><span class="string">        alpha (float, optional): smoothing constant (default: 0.99)</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        centered (bool, optional) : if ``True``, compute the centered RMSProp,</span></span><br><span class="line"><span class="string">            the gradient is normalized by an estimation of its variance</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, params, lr=<span class="number">1e-2</span>, alpha=<span class="number">0.99</span>, eps=<span class="number">1e-8</span>, weight_decay=<span class="number">0</span>, momentum=<span class="number">0</span>, centered=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= lr:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid learning rate: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= eps:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid epsilon value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(eps))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= momentum:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid momentum value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(momentum))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= weight_decay:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid weight_decay value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight_decay))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= alpha:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid alpha value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(alpha))</span><br><span class="line"></span><br><span class="line">        defaults = <span class="built_in">dict</span>(lr=lr, momentum=momentum, alpha=alpha, eps=eps, centered=centered, weight_decay=weight_decay)</span><br><span class="line">        <span class="built_in">super</span>(RMSprop, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(RMSprop, self).__setstate__(state)</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            group.setdefault(<span class="string">&#x27;momentum&#x27;</span>, <span class="number">0</span>)</span><br><span class="line">            group.setdefault(<span class="string">&#x27;centered&#x27;</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">[docs]    @torch.no_grad()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, closure=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">                loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                grad = p.grad</span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    <span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;RMSprop does not support sparse gradients&#x27;</span>)</span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># State initialization</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">&#x27;step&#x27;</span>] = <span class="number">0</span></span><br><span class="line">                    state[<span class="string">&#x27;square_avg&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line">                    <span class="keyword">if</span> group[<span class="string">&#x27;momentum&#x27;</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                        state[<span class="string">&#x27;momentum_buffer&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line">                    <span class="keyword">if</span> group[<span class="string">&#x27;centered&#x27;</span>]:</span><br><span class="line">                        state[<span class="string">&#x27;grad_avg&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line"></span><br><span class="line">                square_avg = state[<span class="string">&#x27;square_avg&#x27;</span>]</span><br><span class="line">                alpha = group[<span class="string">&#x27;alpha&#x27;</span>]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">&#x27;step&#x27;</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">&#x27;weight_decay&#x27;</span>] != <span class="number">0</span>:</span><br><span class="line">                    grad = grad.add(p, alpha=group[<span class="string">&#x27;weight_decay&#x27;</span>])</span><br><span class="line"></span><br><span class="line">                square_avg.mul_(alpha).addcmul_(grad, grad, value=<span class="number">1</span> - alpha)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">&#x27;centered&#x27;</span>]:</span><br><span class="line">                    grad_avg = state[<span class="string">&#x27;grad_avg&#x27;</span>]</span><br><span class="line">                    grad_avg.mul_(alpha).add_(grad, alpha=<span class="number">1</span> - alpha)</span><br><span class="line">                    avg = square_avg.addcmul(grad_avg, grad_avg, value=-<span class="number">1</span>).sqrt_().add_(group[<span class="string">&#x27;eps&#x27;</span>]) <span class="comment"># 计算当前步的动量</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    avg = square_avg.sqrt().add_(group[<span class="string">&#x27;eps&#x27;</span>])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">&#x27;momentum&#x27;</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                    buf = state[<span class="string">&#x27;momentum_buffer&#x27;</span>]</span><br><span class="line">                    buf.mul_(group[<span class="string">&#x27;momentum&#x27;</span>]).addcdiv_(grad, avg)</span><br><span class="line">                    p.add_(buf, alpha=-group[<span class="string">&#x27;lr&#x27;</span>])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    p.addcdiv_(grad, avg, value=-group[<span class="string">&#x27;lr&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间</p><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>谈到这里，Adam和Nadam的出现就很自然而然了——他们是前述方法的集大成者。</p><p>SGDM在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加的二阶动量。</p><p>把一阶动量和二阶动量都用起来就是Adam了——Adaptive + Momentum</p><p>SGD的一阶动量：</p><script type="math/tex; mode=display">m_t = \beta_1 \cdot m_{t-1} +(1-\beta_1)\cdot g_t</script><p>加上AdaDelta的二阶动量：</p><script type="math/tex; mode=display">\hat m_t =\frac{m_t}{1-\beta_1^t}</script><script type="math/tex; mode=display">\hat V_t = \frac{V_t}{1-\beta_2^t}</script><p>优化算法里最常见的两个超参数$\beta_1,\beta_2$就都在这里了，前者是控制一阶动量，后者控制二阶动量。</p><h2 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h2><p>都说Adam是集大成者，但它遗漏了Nesterov，安装NAG的步骤1：</p><script type="math/tex; mode=display">g_t = ∇f(w_t-\alpha\cdot m_{t-1}/\sqrt{V_t})</script><p>Nesterov+Adam = Nadam</p><blockquote><p>定义优化器：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.optim.Adam(params, lr&#x3D;0.001, betas&#x3D;(0.9, 0.999), eps&#x3D;1e-08, weight_decay&#x3D;0, amsgrad&#x3D;False)</span><br></pre></td></tr></table></figure><blockquote><p>参数：</p></blockquote><ul><li><strong>params</strong> (iterable) – 优化器作用的模型参数。</li><li><strong>lr</strong> (float) – learning rate – 相当于是统一框架中的 。</li><li><strong>betas</strong>(Tuple[float,float],optional) – coefficients used for computing running averages of gradient and its square ((默认值：(0.9, 0.999))</li><li><strong>weight_decay</strong> (float, optional) – 权重衰减系数 weight decay (L2 penalty) (默认值：0)</li><li><strong>eps</strong>(float,optional)：防止分母为0的一个小数 (默认值：1e-10)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[docs]<span class="class"><span class="keyword">class</span> <span class="title">Adam</span>(<span class="params">Optimizer</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Implements Adam algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has been proposed in `Adam: A Method for Stochastic Optimization`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-3)</span></span><br><span class="line"><span class="string">        betas (Tuple[float, float], optional): coefficients used for computing</span></span><br><span class="line"><span class="string">            running averages of gradient and its square (default: (0.9, 0.999))</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string">        amsgrad (boolean, optional): whether to use the AMSGrad variant of this</span></span><br><span class="line"><span class="string">            algorithm from the paper `On the Convergence of Adam and Beyond`_</span></span><br><span class="line"><span class="string">            (default: False)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Adam\: A Method for Stochastic Optimization:</span></span><br><span class="line"><span class="string">        https://arxiv.org/abs/1412.6980</span></span><br><span class="line"><span class="string">    .. _On the Convergence of Adam and Beyond:</span></span><br><span class="line"><span class="string">        https://openreview.net/forum?id=ryQu7f-RZ</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, params, lr=<span class="number">1e-3</span>, betas=(<span class="params"><span class="number">0.9</span>, <span class="number">0.999</span></span>), eps=<span class="number">1e-8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">0</span>, amsgrad=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= lr:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid learning rate: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= eps:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid epsilon value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(eps))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">0</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid beta parameter at index 0: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(betas[<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">1</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid beta parameter at index 1: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(betas[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= weight_decay:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid weight_decay value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight_decay))</span><br><span class="line">        defaults = <span class="built_in">dict</span>(lr=lr, betas=betas, eps=eps,</span><br><span class="line">                        weight_decay=weight_decay, amsgrad=amsgrad)</span><br><span class="line">        <span class="built_in">super</span>(Adam, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Adam, self).__setstate__(state)</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            group.setdefault(<span class="string">&#x27;amsgrad&#x27;</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">[docs]    @torch.no_grad()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, closure=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">                loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                grad = p.grad</span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    <span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;Adam does not support sparse gradients, please consider SparseAdam instead&#x27;</span>)</span><br><span class="line">                amsgrad = group[<span class="string">&#x27;amsgrad&#x27;</span>]</span><br><span class="line"></span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># State initialization</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">&#x27;step&#x27;</span>] = <span class="number">0</span></span><br><span class="line">                    <span class="comment"># Exponential moving average of gradient values</span></span><br><span class="line">                    state[<span class="string">&#x27;exp_avg&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line">                    <span class="comment"># Exponential moving average of squared gradient values</span></span><br><span class="line">                    state[<span class="string">&#x27;exp_avg_sq&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line">                    <span class="keyword">if</span> amsgrad:</span><br><span class="line">                        <span class="comment"># Maintains max of all exp. moving avg. of sq. grad. values</span></span><br><span class="line">                        state[<span class="string">&#x27;max_exp_avg_sq&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line"></span><br><span class="line">                exp_avg, exp_avg_sq = state[<span class="string">&#x27;exp_avg&#x27;</span>], state[<span class="string">&#x27;exp_avg_sq&#x27;</span>]</span><br><span class="line">                <span class="keyword">if</span> amsgrad:</span><br><span class="line">                    max_exp_avg_sq = state[<span class="string">&#x27;max_exp_avg_sq&#x27;</span>]</span><br><span class="line">                beta1, beta2 = group[<span class="string">&#x27;betas&#x27;</span>]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">&#x27;step&#x27;</span>] += <span class="number">1</span></span><br><span class="line">                bias_correction1 = <span class="number">1</span> - beta1 ** state[<span class="string">&#x27;step&#x27;</span>]</span><br><span class="line">                bias_correction2 = <span class="number">1</span> - beta2 ** state[<span class="string">&#x27;step&#x27;</span>]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">&#x27;weight_decay&#x27;</span>] != <span class="number">0</span>:</span><br><span class="line">                    grad = grad.add(p, alpha=group[<span class="string">&#x27;weight_decay&#x27;</span>])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Decay the first and second moment running average coefficient</span></span><br><span class="line">                exp_avg.mul_(beta1).add_(grad, alpha=<span class="number">1</span> - beta1)</span><br><span class="line">                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=<span class="number">1</span> - beta2)</span><br><span class="line">                <span class="keyword">if</span> amsgrad:</span><br><span class="line">                    <span class="comment"># Maintains the maximum of all 2nd moment running avg. till now</span></span><br><span class="line">                    torch.<span class="built_in">max</span>(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)</span><br><span class="line">                    <span class="comment"># Use the max. for normalizing running avg. of gradient</span></span><br><span class="line">                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group[<span class="string">&#x27;eps&#x27;</span>])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group[<span class="string">&#x27;eps&#x27;</span>])</span><br><span class="line"></span><br><span class="line">                step_size = group[<span class="string">&#x27;lr&#x27;</span>] / bias_correction1</span><br><span class="line"></span><br><span class="line">                p.addcdiv_(exp_avg, denom, value=-step_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h2 id="AdamW"><a href="#AdamW" class="headerlink" title="AdamW"></a>AdamW</h2><p>Adam的另一个改进版AdamW</p><p>AdamW就是Adam优化器加上L2正则，来限制参数值不可太大。</p><p>以往的L2正则是直接加在损失函数上，比如这样：加入正则， 损失函数就会变成：</p><script type="math/tex; mode=display">L_{l_2}(\theta) = L(\theta) + 1/2\gamma||\theta||^2</script><p>所以在计算梯度$g_t$时要加上粉色的这一项。</p><p>但AdamW稍有不同，如下图所示，将正则加在了绿色位置。</p><p><img src="https://i.loli.net/2021/08/13/C37Bz8jYiwRI4fF.png" alt=""></p><p>至于为何这么做？直接摘录BERT里面的原话看看：</p><blockquote><p><strong>Just</strong> adding the square of the weights to the loss function is <em>not</em> the correct way of using L2 regularization/weight decay with Adam, since that will interact with the m and v parameters in strange ways. Instead we want to decay the weights in a manner that doesn’t interact with the m/v parameters. This is equivalent to adding the square of the weights to the loss with plain (non-momentum) SGD. Add weight decay at the end (fixed version).</p></blockquote><p>意思s 如果直接将L2正则加到loss上去，由于Adam优化器的后续操作，该正则项将会与$m_t$和$v_t$产生奇怪的作用。因而，AdamW选择将L2正则项加在了Adam的$m_t$和$v_t$等参数被计算完之后，在于学习率$\eta$相乘之前，所以这也表明了weight_decay和L2正则虽目的一致、公式一致，但用法还是不同，二者有明显的区别。</p><p>以 PyTorch1.7.0 中的AdamW代码为例：</p><blockquote><p>定义优化器：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.optim.AdamW(params, lr&#x3D;0.001, betas&#x3D;(0.9, 0.999), eps&#x3D;1e-08, weight_decay&#x3D;0.01, amsgrad&#x3D;False)</span><br></pre></td></tr></table></figure><ul><li><strong>params</strong> (iterable) – 优化器作用的模型参数。</li><li><strong>lr</strong> (float) – learning rate – 相当于是统一框架中的 。</li><li><strong>betas</strong>(Tuple[float,float],optional) – coefficients used for computing running averages of gradient and its square ((默认值：(0.9, 0.999))</li><li><strong>weight_decay</strong> (float, optional) – 权重衰减系数 weight decay (L2 penalty) (默认值：0)</li><li><strong>eps</strong>(float,optional)：防止分母为0的一个小数 (默认值：1e-10)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[docs]<span class="class"><span class="keyword">class</span> <span class="title">AdamW</span>(<span class="params">Optimizer</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Implements AdamW algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.</span></span><br><span class="line"><span class="string">    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-3)</span></span><br><span class="line"><span class="string">        betas (Tuple[float, float], optional): coefficients used for computing</span></span><br><span class="line"><span class="string">            running averages of gradient and its square (default: (0.9, 0.999))</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay coefficient (default: 1e-2)</span></span><br><span class="line"><span class="string">        amsgrad (boolean, optional): whether to use the AMSGrad variant of this</span></span><br><span class="line"><span class="string">            algorithm from the paper `On the Convergence of Adam and Beyond`_</span></span><br><span class="line"><span class="string">            (default: False)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Adam\: A Method for Stochastic Optimization:</span></span><br><span class="line"><span class="string">        https://arxiv.org/abs/1412.6980</span></span><br><span class="line"><span class="string">    .. _Decoupled Weight Decay Regularization:</span></span><br><span class="line"><span class="string">        https://arxiv.org/abs/1711.05101</span></span><br><span class="line"><span class="string">    .. _On the Convergence of Adam and Beyond:</span></span><br><span class="line"><span class="string">        https://openreview.net/forum?id=ryQu7f-RZ</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, params, lr=<span class="number">1e-3</span>, betas=(<span class="params"><span class="number">0.9</span>, <span class="number">0.999</span></span>), eps=<span class="number">1e-8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">1e-2</span>, amsgrad=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= lr:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid learning rate: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= eps:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid epsilon value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(eps))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">0</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid beta parameter at index 0: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(betas[<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">1</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid beta parameter at index 1: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(betas[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= weight_decay:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid weight_decay value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight_decay))</span><br><span class="line">        defaults = <span class="built_in">dict</span>(lr=lr, betas=betas, eps=eps,</span><br><span class="line">                        weight_decay=weight_decay, amsgrad=amsgrad)</span><br><span class="line">        <span class="built_in">super</span>(AdamW, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AdamW, self).__setstate__(state)</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            group.setdefault(<span class="string">&#x27;amsgrad&#x27;</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">[docs]    @torch.no_grad()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, closure=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">                loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Perform stepweight decay</span></span><br><span class="line">                p.mul_(<span class="number">1</span> - group[<span class="string">&#x27;lr&#x27;</span>] * group[<span class="string">&#x27;weight_decay&#x27;</span>])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Perform optimization step</span></span><br><span class="line">                grad = p.grad</span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    <span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;Adam does not support sparse gradients, please consider SparseAdam instead&#x27;</span>)</span><br><span class="line">                amsgrad = group[<span class="string">&#x27;amsgrad&#x27;</span>]</span><br><span class="line"></span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># State initialization</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">&#x27;step&#x27;</span>] = <span class="number">0</span></span><br><span class="line">                    <span class="comment"># Exponential moving average of gradient values</span></span><br><span class="line">                    state[<span class="string">&#x27;exp_avg&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line">                    <span class="comment"># Exponential moving average of squared gradient values</span></span><br><span class="line">                    state[<span class="string">&#x27;exp_avg_sq&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line">                    <span class="keyword">if</span> amsgrad:</span><br><span class="line">                        <span class="comment"># Maintains max of all exp. moving avg. of sq. grad. values</span></span><br><span class="line">                        state[<span class="string">&#x27;max_exp_avg_sq&#x27;</span>] = torch.zeros_like(p, memory_format=torch.preserve_format)</span><br><span class="line"></span><br><span class="line">                exp_avg, exp_avg_sq = state[<span class="string">&#x27;exp_avg&#x27;</span>], state[<span class="string">&#x27;exp_avg_sq&#x27;</span>]</span><br><span class="line">                <span class="keyword">if</span> amsgrad:</span><br><span class="line">                    max_exp_avg_sq = state[<span class="string">&#x27;max_exp_avg_sq&#x27;</span>]</span><br><span class="line">                beta1, beta2 = group[<span class="string">&#x27;betas&#x27;</span>]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">&#x27;step&#x27;</span>] += <span class="number">1</span></span><br><span class="line">                bias_correction1 = <span class="number">1</span> - beta1 ** state[<span class="string">&#x27;step&#x27;</span>]</span><br><span class="line">                bias_correction2 = <span class="number">1</span> - beta2 ** state[<span class="string">&#x27;step&#x27;</span>]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Decay the first and second moment running average coefficient</span></span><br><span class="line">                exp_avg.mul_(beta1).add_(grad, alpha=<span class="number">1</span> - beta1)</span><br><span class="line">                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=<span class="number">1</span> - beta2)</span><br><span class="line">                <span class="keyword">if</span> amsgrad:</span><br><span class="line">                    <span class="comment"># Maintains the maximum of all 2nd moment running avg. till now</span></span><br><span class="line">                    torch.<span class="built_in">max</span>(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)</span><br><span class="line">                    <span class="comment"># Use the max. for normalizing running avg. of gradient</span></span><br><span class="line">                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group[<span class="string">&#x27;eps&#x27;</span>])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group[<span class="string">&#x27;eps&#x27;</span>])</span><br><span class="line"></span><br><span class="line">                step_size = group[<span class="string">&#x27;lr&#x27;</span>] / bias_correction1</span><br><span class="line"></span><br><span class="line">                p.addcdiv_(exp_avg, denom, value=-step_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>与Adam不一样的地方是：</p><p>Adam如果使用weight_decay的话，那么相当于目标函数上加了$1/2\gamma||\theta||^2$，所以相当于是梯度加上$\gamma\theta$故Adam使用了</p><p>grad = grad.add(p, alpha=group[‘weight_decay’])</p><p>而 AdamW 是 p.mul_(1 - group[‘lr’] * group[‘weight_decay’]) 直接让参数：</p><p>$\theta<em>t =\theta</em>{t-1}-\alpha\cdot\lambda\cdot\theta_{t-1}-\alpha\cdot\eta_t$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;从-SGD-到-AdamW-原理和代码解读&quot;&gt;&lt;a href=&quot;#从-SGD-到-AdamW-原理和代码解读&quot; class=&quot;headerlink&quot; title=&quot;从 SGD 到 AdamW 原理和代码解读&quot;&gt;&lt;/a&gt;从 SGD 到 AdamW 原理和代码解读&lt;/</summary>
      
    
    
    
    
    <category term="ML&amp;DL" scheme="http://example.com/tags/ML-DL/"/>
    
  </entry>
  
  <entry>
    <title>TreeMap红黑树</title>
    <link href="http://example.com/2021/08/10/TreeMap%E7%BA%A2%E9%BB%91%E6%A0%91/"/>
    <id>http://example.com/2021/08/10/TreeMap%E7%BA%A2%E9%BB%91%E6%A0%91/</id>
    <published>2021-08-10T12:10:27.000Z</published>
    <updated>2021-08-30T06:58:27.655Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TreeMap红黑树"><a href="#TreeMap红黑树" class="headerlink" title="TreeMap红黑树"></a>TreeMap红黑树</h1><ul><li>前言</li><li>二叉查找树BST</li><li>BST存在的问题</li><li>2-3-4树</li><li>红黑树</li></ul><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>除了要学会着色、旋转规则，还要了解为什么变色，为什么旋转。</p><p>五大性质的缘由</p><p>jdk的TreeMap在红黑树上做了一些优化，原版红黑树删除操作它是找的前驱节点替代原删除节点，而TreeMap源码里是用的后继节点替代原删除节点，这两种方案实际效果一样，只不过树的结构不一样，但是对应的红黑树都是平衡的。</p><h2 id="二叉查找树-BST"><a href="#二叉查找树-BST" class="headerlink" title="二叉查找树(BST)"></a>二叉查找树(BST)</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>二叉查找树，就是一颗二叉树，他的左节点比父节点小，右节点比父节点大，他的高度决定查找效率。</p><p><img src="https://i.loli.net/2021/08/10/K6rNGpl3tUue714.png" alt=""></p><h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><p>查找(红黑树通用)：查找每个节点从根开始</p><ul><li>查找值比当前大，搜右子树</li><li>查找值等于当前值，停止查找，返回</li><li>查找值比当前值小，则搜右子树</li></ul><p>插入：要插入节点，必须先找到插入节点位置，按照搜索的流程，找到左子树或右子树为空的位置插入。</p><p>遍历(红黑树通用)：前中后序遍历。</p><p>查找最小值(红黑树通用)：沿着根节点的左子树一路查找，直到最后一个不为空的节点</p><p>查找最大值(红黑树通用)：沿着根节点的右子树一路查找，直到最后一个不为空的节点</p><p>查找前驱节点(红黑树通用)：小于当前节点的最大值</p><p>查找后继节点(红黑树通用)：大于当前节点的最小值</p><p><img src="https://z3.ax1x.com/2021/08/20/fX2DBT.png" alt=""></p><p>删除 : 本质上是找前驱或后继节点来替代</p><ul><li>叶子节点直接删除(没有前驱或或后继)</li><li>只有一个子节点的用子节点替代(本质上就是找的前驱节点或者后继节点，左节点就是前驱节点，右节点就是后继节点)</li><li>有两个子节点的，需要找到替代节点(替代节点也是前驱或者后继)</li></ul><p>删除操作和红黑树一样，只不过红黑树多了着色和旋转过程。</p><h3 id="BST存在的问题"><a href="#BST存在的问题" class="headerlink" title="BST存在的问题"></a>BST存在的问题</h3><p>BST存在的问题是，树在插入的时候会导倾斜，不同的插入顺序会导致高度不一样，而树的高度直接影响了树的查找效率。</p><p>基于这个问题平衡二叉树产生了，平衡树的插入和删除时，会通过旋转操作将高度保持在LogN。</p><p>其中两款有代表性的平衡树分别为</p><ul><li>AVL树（高度平衡树，具备二叉搜索树的全部特性，而且左右子树高度差不超过1）</li><li>红黑树</li></ul><p>面试题：有了AVL树为什么还要红黑树？</p><p>AVL树由于实现比较复杂，而且插入和删除性能差。AVL很多性能耗在旋转操作上</p><p>在实际环境下的应用不如红黑树。</p><p>红黑树的实际应用范围广，如java中的HashMap和TreeSet，java8中HashMap的实现因为用RBTree代替链表(链表长度大于8时)，性能有提升。</p><h2 id="2-3-4树"><a href="#2-3-4树" class="headerlink" title="2-3-4树"></a>2-3-4树</h2><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><p>2-3-4树是四阶B树(Balance Tree)，他属于一种多路查找树，他的结构有以下限制：</p><ul><li>所有叶子节点都拥有相同的深度。</li><li>节点只能是2-节点、3-节点、4-节点之一。</li><li><ul><li>2-节点：包含1个元素的节点，有2个子节点；</li><li>3-节点：包含2个元素的节点，有3个子节点；</li><li>4-节点：包含3个元素的节点，有4个子节点；</li><li>所有节点必须至少包含1个元素</li></ul></li><li>元素始终保持排序顺序，整体上保持二叉查找树的性质，即父结点大于左子节点，小于右子结点；而且结点有多个元素时，每个元素必须大于它左边的和它的右子树中元素。</li></ul><p><img src="https://i.loli.net/2021/08/14/dVxDmrkuKgUv6M9.png" alt=""></p><h3 id="结点插入"><a href="#结点插入" class="headerlink" title="结点插入"></a>结点插入</h3><p>2-3-4树中结点添加需要遵循以下规则：</p><ul><li>插入都是向最下面一层插入</li><li>升元：将插入结点由2-节点升级成3-节点，或由3-结点升级成4-结点</li><li>向4-结点插入元素后，需要将中间元素提到父节点升元，原节点变成两个2-节点，再把元素插入2-结点中，如果父节点也是4-结点，则递归向上层升元，直到根节点后将树高加1。</li></ul><p>而将这些规则对应到红黑树里，就是：</p><ul><li>新插入的结点颜色为<strong>红色</strong>，这样才可能不会对红黑树的高度产生影响。</li><li>2-结点对应红黑树中的单个黑色结点，插入时直接成功(对应2-结点升元)</li><li>3-结点对应红黑树中的<strong>黑+红</strong>子树，插入后将其修复成<strong>红+黑+红</strong>子树</li><li>4-结点对应红黑树中的<strong>红+黑+红</strong>子树，插入后将其修复成<strong>红色祖父+黑色父叔+红色孩子</strong>子树，然后再把祖父结点当成新插入的红色结点递归向上层修复，直至修复成功或遇到root结点。</li></ul><p>公式：<strong>红黑树+新增一个节点(红色) = 对等的2-3-4树+新增一个节点</strong></p><h3 id="删除结点"><a href="#删除结点" class="headerlink" title="删除结点"></a>删除结点</h3><p>2-3-4树的删除可以全部转换为叶子节点的删除</p><p>删除原则是先看能不能和下面的叶子节点合并，能合并的直接合并完后删除，不能合并的就要找个元素替换上去，最终都是要保持平衡。</p><p>合并—&gt;删除</p><p>合并—&gt;替换—&gt;删除</p><p>合并—&gt;无法替换—&gt;再合并—&gt;删除</p><p><strong>红色结点一定全部都在多元素节点中</strong></p><p>红黑树的删除要比插入复杂，还是类比2-3-4树：</p><ul><li>查找最近的叶子结点的元素替代被删除元素，删除替代元素后，从替代元素所处叶子结点开始处理</li><li>降元：4-结点变3-结点，3-结点变2-结点。</li><li>2-结点中只有一个元素，所以借兄弟结点中的元素来补充删除后的造成的空结点。</li><li>当兄弟结点中也没有多个元素可以补充时，尝试将父节点降元，失败时向上递归，直到子树降元成功或root结点树高减一</li></ul><p>将这些规则对应到红黑树中即：</p><ul><li>查找离当前结点最近的叶子结点作为替代节点，(左子树的最右结点或右子树的最左结点都能保证替换后二叉树的节点排序性质，叶子节点的替代结点是自身) 替换掉被删除结点，从替代的叶子结点向上递归修复。</li><li>替代结点颜色为红色(对应2-3-4树中 4-节点或3-结点) 时删除子结点直接成功</li><li>替代节点为黑色(对应2-3-4树中 2-节点)时， 意味着替代结点所在的子树会降一层，需要依次检验以下三项，以恢复子树高度：</li><li><ul><li>兄弟结点的子结点中有红色节点(兄弟结点对应3-结点或4-结点) 能够“借用”，旋转过来后修正颜色。</li><li>父结点是红色结点（父结点对应3-结点或4-结点，可以降元）时，将父结点变为黑色，自身和兄弟结点变红色后删除。</li><li>父结点和兄弟结点都是黑色时，将子树降一层后把 <strong>父结点当做替代结点</strong> 递归向上处理。</li></ul></li></ul><h3 id="红黑树对应一颗2-3-4数，一颗2-3-4树对应多颗红黑树"><a href="#红黑树对应一颗2-3-4数，一颗2-3-4树对应多颗红黑树" class="headerlink" title="红黑树对应一颗2-3-4数，一颗2-3-4树对应多颗红黑树"></a>红黑树对应一颗2-3-4数，一颗2-3-4树对应多颗红黑树</h3><p>红黑树和2-3-4树的结点添加和删除都有一共基本规则：避免子树高度变化，因为无论是2-3-4树还是红黑树，一旦子树高度有变动，势必会影响其他子树进行调整。所以我们在插入和删除节点时尽量通过子树内部调整来达到平衡。</p><p>2-3-4树实现平衡是通过结点的旋转和结点元素变化，红黑树是通过结点旋转和变色。</p><p><img src="https://i.loli.net/2021/08/14/kgC4yNQ5frcoXBm.png" alt=""></p><p>2节点全是黑色，3节点有左倾右倾两种情况，4节点上黑下红</p><p>2-3-4树的裂变状态: 红黑树新增都是以红色节点进来的，11裂变上去变成红色，下面两个变成黑色</p><p><img src="https://i.loli.net/2021/08/14/ykrG7CEatD3hULF.png" alt=""></p><p>整体对比2-3-4树和红黑树</p><p><img src="https://i.loli.net/2021/08/14/2OrLtl73KcwIunk.png" alt=""></p><p><img src="https://i.loli.net/2021/08/14/5DzbNJyHqQRv8dx.png" alt=""></p><h2 id="红黑树"><a href="#红黑树" class="headerlink" title="红黑树"></a>红黑树</h2><p><img src="https://i.loli.net/2021/08/14/CmA461TBsgPtdjG.png" style="zoom:50%;" /></p><h3 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h3><p>红黑树是一种结点带有颜色属性的二叉查找树，但它在二叉查找树之外还有以下五大性质：</p><ul><li>结点是红色或黑色</li><li>根是黑色</li><li>所有叶子都是黑色(叶子是NIL节点，这类节点不可忽视，否则代码会看不懂)</li><li>每个红色节点必须有两个黑色子节点（从每个叶子到根的所有路径上不能有两个连续的红色结点）</li><li>从任意一节点到其每个叶子的所有简单路径都包含相同数目的黑色结点（黑色平衡）</li></ul><h3 id="常见操作"><a href="#常见操作" class="headerlink" title="常见操作"></a>常见操作</h3><p><strong>变色、左旋、右旋</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> RBNode root;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 围绕p左旋</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> p</span></span><br><span class="line"><span class="comment"> *              pf                  pf</span></span><br><span class="line"><span class="comment"> *            /                   /</span></span><br><span class="line"><span class="comment"> *           p                   pr(r)</span></span><br><span class="line"><span class="comment"> *          / \                 /  \</span></span><br><span class="line"><span class="comment"> *         pl  pr(r)    -&gt;     p   rr</span></span><br><span class="line"><span class="comment"> *             / \            / \</span></span><br><span class="line"><span class="comment"> *            rl rr          pl  rl</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">leftRotate</span><span class="params">(RBNode p)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (p != <span class="keyword">null</span>) &#123;</span><br><span class="line">        RBNode r = p.right;</span><br><span class="line">        p.right = r.left;</span><br><span class="line">        <span class="keyword">if</span> (r.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">            r.left.parent = p;</span><br><span class="line">        &#125;</span><br><span class="line">        r.parent = p.parent;</span><br><span class="line">        <span class="keyword">if</span> (p.parent == <span class="keyword">null</span>) &#123;</span><br><span class="line">            root = r;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (p.parent.left == p) &#123;</span><br><span class="line">            p.parent.left = r;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            p.parent.right = r;</span><br><span class="line">        &#125;</span><br><span class="line">        r.left = p;</span><br><span class="line">        p.parent = r;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 围绕p右旋</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> p</span></span><br><span class="line"><span class="comment"> *          pf                        pf</span></span><br><span class="line"><span class="comment"> *           \                         \</span></span><br><span class="line"><span class="comment"> *            p                         pl(l)</span></span><br><span class="line"><span class="comment"> *           / \           -&gt;           /  \</span></span><br><span class="line"><span class="comment"> *       pl(l)  pr                    ll    p</span></span><br><span class="line"><span class="comment"> *       / \                               / \</span></span><br><span class="line"><span class="comment"> *     ll   lr                            lr  pr</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">rightRotate</span><span class="params">(RBNode p)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (p != <span class="keyword">null</span>) &#123;</span><br><span class="line">        RBNode l = p.left;</span><br><span class="line">        p.left = l.right;</span><br><span class="line">        <span class="keyword">if</span> (l.right != <span class="keyword">null</span>) &#123;</span><br><span class="line">            l.right.parent = p;</span><br><span class="line">        &#125;</span><br><span class="line">        l.parent = p.parent;</span><br><span class="line">        <span class="keyword">if</span> (p.parent != <span class="keyword">null</span>) &#123;</span><br><span class="line">            root = l;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (p.parent.right == p) &#123;</span><br><span class="line">            p.parent.right = l;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            p.parent.left = l;</span><br><span class="line">        &#125;</span><br><span class="line">        l.right = p;</span><br><span class="line">        p.parent = l;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>新增</strong> （七种情况，五种情况需要考虑自平衡）</p><p><img src="https://i.loli.net/2021/08/30/1zoZ7HYrsJDvkUy.png" alt=""></p><p>分情况讨论，主要是找到插入位置，然后自平衡(左旋或者右旋) 且插入结点是红色节点(如果是黑色的话那么当前分支上就会多出一个黑色结点出来，从而破坏了黑色平衡)，以下分析全部以左子树为例，右子树的情况则相反。</p><ul><li>情况1、如果插入的是第一个节点(根节点)，红色变黑色</li><li>情况2、如果父节点为黑色，则直接插入，不需要变色</li><li>如果父节点为红色，叔叔节点也是红色（此种情况爷爷节点一定是黑色），则父节点和叔叔节点变黑色，爷爷节点变红色（如果爷爷节点是根节点，则再变成黑色），爷爷节点此时需要递归（把爷爷节点当做新插入的节点再次进行比较）</li><li>如果父节点是红色，没有叔叔节点或者叔叔节点是黑色（此时只能是NIL节点），则以爷爷节点为支点右旋，旋转之后原来的爷爷节点变红色，原来的父节点变黑色。</li></ul><p>还是与2-3-4树对比，新增一定在叶子节点上</p><p>情况1</p><p><img src="https://i.loli.net/2021/08/14/cNnouZS7wDbf48r.png" alt=""></p><p>情况2——右边相当于左右旋了</p><p><img src="https://i.loli.net/2021/08/14/rWCDmgQLfAUb7s3.png" alt=""></p><p>与3节点合并情况</p><p><img src="https://i.loli.net/2021/08/14/rWPIiuGDKaRx8UQ.png" alt=""></p><p><img src="https://i.loli.net/2021/08/14/yDrf4qQhVX2t65a.png" alt=""></p><p><img src="https://i.loli.net/2021/08/14/8D9eqhpGnMKFwTZ.png" alt=""></p><p>裂变情况</p><p><img src="https://i.loli.net/2021/08/14/ahr8mFQO14gBfCY.png" alt=""></p><p><img src="https://z3.ax1x.com/2021/08/20/fXRvW9.png" alt=""></p><p><img src="https://z3.ax1x.com/2021/08/20/fXfK39.png" alt=""></p><p><strong>删除</strong> (重点) 五种情况、两种需要考虑自平衡，又细分八种情况，其中四种为镜像情况</p><p>先看二叉搜索树的删除：</p><ul><li>删除叶子节点，直接删除</li><li>删除的节点有一个子节点，那么用子节点来替代</li><li>如果删除的节点有2个子节点，此时需要找到前驱节点或者后继节点来替代</li></ul><p>写代码时，删除方案：</p><ul><li>找到前驱节点，复制前驱节点值覆盖准备删除的节点值，然后删除前驱节点</li><li>找到后继节点，复制后继节点的值覆盖准备删除的节点值，然后删除后继节点</li><li>被删除的前驱节点或者后继节点只有两种情况：1、被删除节点是叶子节点。2、被删除节点只有一个孩子。</li></ul><p>找前驱后继代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 找到指定节点的前驱节点，即找小于node节点的最大值</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> node</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> RBNode <span class="title">preedecessor</span><span class="params">(RBNode node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (node.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">            RBNode p = node.left;</span><br><span class="line">            <span class="keyword">while</span> (p.right != <span class="keyword">null</span>) &#123;</span><br><span class="line">                p = p.right;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> p;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 删除时不一定用到，但找前驱时，左子树就得向上找了</span></span><br><span class="line">            <span class="comment">// 找到第一个左拐的地方</span></span><br><span class="line">            RBNode p = node.parent;</span><br><span class="line">            RBNode ch = node;</span><br><span class="line">            <span class="keyword">while</span> (p != <span class="keyword">null</span> &amp;&amp; p.left == ch) &#123;</span><br><span class="line">                ch = p;</span><br><span class="line">                p = p.parent;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> p;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 找到指定节点的后继节点，大于节点的最小值</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> node</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> RBNode <span class="title">sucessor</span><span class="params">(RBNode node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (node.right != <span class="keyword">null</span>) &#123;</span><br><span class="line">            RBNode p = node.right;</span><br><span class="line">            <span class="keyword">while</span> (p.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">                p = p.left;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> p;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 删除时不一定用到，但找前驱时，左子树就得向上找了</span></span><br><span class="line">            <span class="comment">// 找到第一个左拐的地方</span></span><br><span class="line">            RBNode p = node.parent;</span><br><span class="line">            RBNode ch = node;</span><br><span class="line">            <span class="keyword">while</span> (p != <span class="keyword">null</span> &amp;&amp; p.right == ch) &#123;</span><br><span class="line">                ch = p;</span><br><span class="line">                p = p.parent;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> p;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>红黑树的删除：1先找到节点，2删除</p><p>红黑树上面的删除节点一定是2-3-4树上的叶子节点</p><p><img src="https://z3.ax1x.com/2021/08/20/fXVpDA.png" alt=""></p><p>三局话：</p><ul><li><strong>自己能搞定的自己搞定</strong>：</li><li><ul><li>如果删除的节点对应于2-3-4树的3节点或者4节点，则直接删除，不用和兄弟或父亲借。</li><li>如果删除的是红色节点，则直接删除；如果是黑色节点，则红色节点上来替代，变黑即可</li></ul></li><li><strong>搞不定的找兄弟和父亲帮忙</strong>:</li><li><ul><li>前提是找到真正的可用的兄弟结点 (真正的兄弟节点是对应于2-3-4树中的兄弟节点，如上图左，5的兄弟节点是8，图右中5的兄弟节点应该是7和7.5，可以通过旋转来找)</li><li>兄弟节点有的借(此时兄弟节点一定是黑色，如果是红色那说明这个节点不是真正的兄弟节点，需要回到上一步找真正的兄弟节点)</li><li>兄弟节点有两个子节点的情况(两个子节点肯定是红色，如果是黑色的话相当于此时兄弟节点对应2-3-4树是2节点，不可能有多余的元素可以借)，此时需要旋转变色</li><li>兄弟节点只有一个子节点的情况，此时需要旋转变色</li></ul></li><li><strong>父亲和兄弟帮不了那有福同享，有难同当(父亲和兄弟自损)</strong>：</li><li><ul><li>前提还是找到真正的兄弟节点</li><li>兄弟节点没有多余的元素可借（此时兄弟节点一定为黑色的2节点），此时兄弟节点所在分支也要自损一个黑色节点以达到黑色平衡，最快的方式就是兄弟节点直接变红(相当于减少一个黑色节点)，此时以父节点为root的子树又达到了平衡(两边都比之前少了一个黑色)。但是以祖父结点为root的树依然是不平衡的，此时需要递归处理。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;TreeMap红黑树&quot;&gt;&lt;a href=&quot;#TreeMap红黑树&quot; class=&quot;headerlink&quot; title=&quot;TreeMap红黑树&quot;&gt;&lt;/a&gt;TreeMap红黑树&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;前言&lt;/li&gt;
&lt;li&gt;二叉查找树BST&lt;/li&gt;
&lt;li&gt;BS</summary>
      
    
    
    
    
    <category term="数据结构" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>并查集</title>
    <link href="http://example.com/2021/08/10/%E5%B9%B6%E6%9F%A5%E9%9B%86/"/>
    <id>http://example.com/2021/08/10/%E5%B9%B6%E6%9F%A5%E9%9B%86/</id>
    <published>2021-08-10T07:16:29.000Z</published>
    <updated>2021-08-10T10:01:05.205Z</updated>
    
    <content type="html"><![CDATA[<h1 id="并查集"><a href="#并查集" class="headerlink" title="并查集"></a>并查集</h1><h2 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h2><p>解决元素分组问题，管理一系列不相交的集合</p><h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><ul><li>合并(Union)：把两个不相交的集合合并为一个集合</li><li>查询(Find)：查询两个元素是否在同一个集合</li></ul><h3 id="以一个应用场景为例应用场景"><a href="#以一个应用场景为例应用场景" class="headerlink" title="以一个应用场景为例应用场景"></a>以一个应用场景为例应用场景</h3><p>(洛谷P1551) 亲戚</p><blockquote><p><strong>题目背景</strong><br>若某个家族人员过于庞大，要判断两个是否是亲戚，确实还很不容易，现在给出某个亲戚关系图，求任意给出的两个人是否具有亲戚关系。<br><strong>题目描述</strong><br>规定：x和y是亲戚，y和z是亲戚，那么x和z也是亲戚。如果x,y是亲戚，那么x的亲戚都是y的亲戚，y的亲戚也都是x的亲戚。<br><strong>输入格式</strong><br>第一行：三个整数n,m,p，（n&lt;=5000,m&lt;=5000,p&lt;=5000），分别表示有n个人，m个亲戚关系，询问p对亲戚关系。<br>以下m行：每行两个数Mi，Mj，1&lt;=Mi，Mj&lt;=N，表示Mi和Mj具有亲戚关系。<br>接下来p行：每行两个数Pi，Pj，询问Pi和Pj是否具有亲戚关系。<br><strong>输出格式</strong><br>P行，每行一个’Yes’或’No’。表示第i个询问的答案为“具有”或“不具有”亲戚关系。</p></blockquote><p>把所有人划分到若干个不相交的集合中，每个集合里的人彼此是亲戚。为了判断两个人是否为亲戚，只需看它们是否属于同一个集合即可。因此，这里就可以考虑用并查集进行维护。</p><h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p><strong>用集合中的一个元素代表集合</strong>。我曾看过一个有趣的比喻，把集合比喻成<strong>帮派</strong>，而代表元素则是<strong>帮主</strong>。接下来我们利用这个比喻，看看并查集是如何运作的。</p><p><img src="https://i.loli.net/2021/08/10/2xK7tBoWcLDlghy.png" alt=""></p><p>最开始，每个元素的代表元素是自己。每个元素的最顶端是自己，是一个自环节点。</p><p>在比较两人是不是一个帮派的时候，就找自己的帮主，看看是不是一个帮主。每个元素向上找，找到不能再找了就是了。</p><p>然而帮派规模大了，肯定会造成等级(树的深度)变深。</p><p>有两个操作可以优化一个是路径压缩，一个是按秩压缩。</p><p>之前说找帮主来判断两个元素是否在同一集合内。找到帮主一样说明是一个集合里的，不一样把较小集合的帮主指向较大集合的帮主，这样做就是按秩压缩：</p><p><img src="https://i.loli.net/2021/08/10/QYl1ZJRFnekUhCB.png" alt=""></p><p>按秩压缩了之后其实还是可以发现树是深度是越来越深的，那么再采取路径压缩。</p><p>在每次向上查找的过程中，将路过的节点之间指向帮主，这是通过栈来实现的。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> 并查集 </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 样本进来会包一层，叫做元素</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Element</span>&lt;<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">public</span> V value;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">Element</span><span class="params">(V value)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.value = value;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UnionFindSet</span>&lt;<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> HashMap&lt;V, Element&lt;V&gt;&gt; elementMap;</span><br><span class="line">        <span class="comment">// key 某个元素value 该元素的父</span></span><br><span class="line">        <span class="keyword">public</span> HashMap&lt;Element&lt;V&gt;, Element&lt;V&gt;&gt; fatherMap;</span><br><span class="line">        <span class="comment">// key 某个集合的代表元素， value该集合的大小</span></span><br><span class="line">        <span class="keyword">public</span> HashMap&lt;Element&lt;V&gt;, Integer&gt; sizeMap;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">UnionFindSet</span><span class="params">(List&lt;V&gt; list)</span> </span>&#123;</span><br><span class="line">            elementMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            fatherMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            sizeMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (V value : list) &#123;</span><br><span class="line">                Element element = <span class="keyword">new</span> Element(value);</span><br><span class="line">                elementMap.put(value, element);</span><br><span class="line">                fatherMap.put(element, element);</span><br><span class="line">                sizeMap.put(element, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 给定一个element，网上找，把代表元素返回</span></span><br><span class="line">        <span class="function"><span class="keyword">private</span> Element&lt;V&gt; <span class="title">findHead</span><span class="params">(Element&lt;V&gt; element)</span> </span>&#123;</span><br><span class="line">            Stack&lt;Element&lt;V&gt;&gt; path = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">            <span class="keyword">while</span> (element != fatherMap.get(element)) &#123;</span><br><span class="line">                path.push(element);</span><br><span class="line">                element = fatherMap.get(element);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">while</span> (!path.isEmpty()) &#123;  <span class="comment">// 路径铺平</span></span><br><span class="line">                fatherMap.put(path.pop(), element);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> element;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isSameSet</span><span class="params">(V a, V b)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (elementMap.containsKey(a) &amp;&amp; elementMap.containsKey(b)) &#123;</span><br><span class="line">                <span class="keyword">return</span> findHead(elementMap.get(a)) == findHead(elementMap.get(b));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">union</span><span class="params">(V a, V b)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (elementMap.containsKey(a) &amp;&amp; elementMap.containsKey(b)) &#123;</span><br><span class="line">                Element&lt;V&gt; aF = findHead(elementMap.get(a));</span><br><span class="line">                Element&lt;V&gt; bF = findHead(elementMap.get(b));</span><br><span class="line">                <span class="keyword">if</span> (aF != bF) &#123;</span><br><span class="line">                    Element&lt;V&gt; big = sizeMap.get(aF) &gt;= sizeMap.get(bF) ? aF : bF;</span><br><span class="line">                    Element&lt;V&gt; small = big == aF ? bF : aF;</span><br><span class="line">                    fatherMap.put(small, big);</span><br><span class="line">                    sizeMap.put(big, sizeMap.get(aF) + sizeMap.get(bF));</span><br><span class="line">                    sizeMap.remove(small);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>洛谷P1551亲戚</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="keyword">int</span> n = scanner.nextInt(); <span class="comment">// 人数</span></span><br><span class="line">        <span class="keyword">int</span> m = scanner.nextInt(); <span class="comment">// 关系数</span></span><br><span class="line">        <span class="keyword">int</span> p = scanner.nextInt(); <span class="comment">// 询问多少个关系</span></span><br><span class="line"></span><br><span class="line">        List&lt;Integer&gt; personList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">int</span>[][] relation = <span class="keyword">new</span> <span class="keyword">int</span>[m][<span class="number">2</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> p1 = scanner.nextInt();</span><br><span class="line">            <span class="keyword">int</span> p2 = scanner.nextInt();</span><br><span class="line">            <span class="keyword">if</span> (!personList.contains(p1)) personList.add(p1);</span><br><span class="line">            <span class="keyword">if</span> (!personList.contains(p2)) personList.add(p2);</span><br><span class="line">            relation[i][<span class="number">0</span>] = p1;</span><br><span class="line">            relation[i][<span class="number">1</span>] = p2;</span><br><span class="line"><span class="comment">//            relationMap.put(p2, p1);</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        UnionFindSet&lt;Integer&gt; unionSet = <span class="keyword">new</span> UnionFindSet&lt;&gt;(personList);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//        for (Map.Entry&lt;Integer, Integer&gt; entry : relationMap.entrySet()) &#123;</span></span><br><span class="line"><span class="comment">//            unionSet.union(entry.getKey(), entry.getValue());</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">            unionSet.union(relation[i][<span class="number">0</span>], relation[i][<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; p; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> p1 = scanner.nextInt();</span><br><span class="line">            <span class="keyword">int</span> p2 = scanner.nextInt();</span><br><span class="line">            <span class="keyword">if</span> (unionSet.isSameSet(p1, p2)) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;Yes&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;No&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;并查集&quot;&gt;&lt;a href=&quot;#并查集&quot; class=&quot;headerlink&quot; title=&quot;并查集&quot;&gt;&lt;/a&gt;并查集&lt;/h1&gt;&lt;h2 id=&quot;用途&quot;&gt;&lt;a href=&quot;#用途&quot; class=&quot;headerlink&quot; title=&quot;用途&quot;&gt;&lt;/a&gt;用途&lt;/h2&gt;&lt;p</summary>
      
    
    
    
    
    <category term="数据结构" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Docker学习(四) 网络</title>
    <link href="http://example.com/2021/08/06/Docker%E5%AD%A6%E4%B9%A0-%E5%9B%9B-%E7%BD%91%E7%BB%9C/"/>
    <id>http://example.com/2021/08/06/Docker%E5%AD%A6%E4%B9%A0-%E5%9B%9B-%E7%BD%91%E7%BB%9C/</id>
    <published>2021-08-06T09:14:03.000Z</published>
    <updated>2021-08-06T13:24:05.990Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker学习-四-网络"><a href="#Docker学习-四-网络" class="headerlink" title="Docker学习(四) 网络"></a>Docker学习(四) 网络</h1><h2 id="理解docker0"><a href="#理解docker0" class="headerlink" title="理解docker0"></a>理解docker0</h2><p><img src="https://i.loli.net/2021/08/06/Aln7NGbIr8FKL2h.png" alt=""></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker 是如何处理容器网络访问的</span></span><br><span class="line"></span><br><span class="line">(base) root@linux:/home/cpss# docker run -d -P --name tomcat01 tomcat</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看容器的内部网络地址 ip addr</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 发现容器启动的时候会得到一个 eth0@if34 ip地址，docker分配的</span></span><br><span class="line">(base) root@linux:/home/cpss# docker exec -it tomcat01 ip addr</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">33: eth0@if34: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default </span><br><span class="line">    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 思考：linux能不能ping通容器内部</span></span><br><span class="line">(base) root@linux:/home/cpss# ping 172.17.0.2</span><br><span class="line">PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.107 ms</span><br><span class="line">64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.055 ms</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> linux 可以ping通docker容器内部</span></span><br></pre></td></tr></table></figure><blockquote><p>原理</p></blockquote><ul><li><p>我们每启动一个docker容器，docker就会给docker容器分配一个ip，只要安装了docker，就会有一个网卡docker0 桥接模式，使用的技术是 evth-pair技术</p><p>现在再在宿主机执行ip addr，发现多了一个</p><p><img src="https://i.loli.net/2021/08/06/nkoUMcDPLG6KRax.png" alt=""></p></li></ul><p>再启动一个容器测试</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss<span class="comment"># docker run -d -P --name tomcat02 tomcat</span></span><br><span class="line"><span class="comment"># 每启动一个就多一个网卡</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 发现这个容器带来的网卡，都是一对一对的</span></span><br><span class="line"><span class="comment"># evth-pair 就是一对的虚拟设备端口，他们都是成对出现的，一段连着协议，一段彼此相连</span></span><br><span class="line"><span class="comment"># 正因为有这个特性，evth-pair 充当一个桥梁,连接各种虚拟网络设备的</span></span><br><span class="line"><span class="comment"># Openstac，Docker容器之间的链接，ovs的链接，都是使用这个技术</span></span><br></pre></td></tr></table></figure><p>测试一下tomcat01和tomcat02是否能ping通</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:&#x2F;home&#x2F;cpss# docker exec -it tomcat02 ping 172.17.0.2</span><br><span class="line">PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from 172.17.0.2: icmp_seq&#x3D;1 ttl&#x3D;64 time&#x3D;0.135 ms</span><br><span class="line">64 bytes from 172.17.0.2: icmp_seq&#x3D;2 ttl&#x3D;64 time&#x3D;0.086 ms</span><br><span class="line">64 bytes from 172.17.0.2: icmp_seq&#x3D;3 ttl&#x3D;64 time&#x3D;0.067 ms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 结论 容器之间可以互相ping通的</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/06/8x9nU6PVTZepR7b.png" alt=""></p><p>Tomcat01 和tomcat02 是公用的一个路由器，docker0</p><p>所有的容器不指定网络的情况下，都是使用docker0路由的，docker会给我们的容器分配一个默认的可用ip</p><blockquote><p>小结</p></blockquote><p>docker 使用的是桥接，宿主机中是一个docker容器的网桥 docker0</p><p><img src="https://i.loli.net/2021/08/06/bTBl5Qsh7qVCXI4.png" alt=""></p><p>docker 中所有的网络端口都是虚拟的，虚拟的转发效率高(内网传递)</p><blockquote><p>思考一个场景，编写了一个微服务，database url=ip:, 项目不重启，数据库ip换掉了，我们希望可以处理这个问题，可以通过名字来进行访问容器</p></blockquote><h3 id="—link"><a href="#—link" class="headerlink" title="—link"></a>—link</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss# docker exec -it tomcat02 ping tomcat01</span><br><span class="line">ping: tomcat01: Name or service not known</span><br><span class="line"></span><br><span class="line">(base) root@linux:/home/cpss# docker run -d -P --name tomcat03 --link tomcat02 tomcat</span><br><span class="line">6f62526ba5484b2c542bc31b0891a1f06c1baedbdb8667322b9b051a1f443e06</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过--link 可以解决</span></span><br><span class="line">(base) root@linux:/home/cpss# docker exec -it tomcat03 ping tomcat02</span><br><span class="line">PING tomcat02 (172.17.0.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from tomcat02 (172.17.0.3): icmp_seq=1 ttl=64 time=0.108 ms</span><br><span class="line">64 bytes from tomcat02 (172.17.0.3): icmp_seq=2 ttl=64 time=0.066 ms</span><br><span class="line">64 bytes from tomcat02 (172.17.0.3): icmp_seq=3 ttl=64 time=0.069 ms</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 反向 ping不通</span></span><br><span class="line">(base) root@linux:/home/cpss# docker exec -it tomcat02 ping tomcat03</span><br><span class="line">ping: tomcat03: Name or service not known</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(base) root@linux:/home/cpss# docker network ls</span><br><span class="line">NETWORK ID     NAME      DRIVER    SCOPE</span><br><span class="line">27abb1e4b0d3   bridge    bridge    local</span><br><span class="line">e69e785a705e   host      host      local</span><br><span class="line">2412989a4eb3   none      null      local</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 其实这个tomcat03就是在本地配置了tomcat02的配置</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --link 就是我们在hosts配置中增加了一个 映射</span></span><br><span class="line">(base) root@linux:/home/cpss# docker exec -it tomcat03 cat /etc/hosts</span><br><span class="line">127.0.0.1localhost</span><br><span class="line">::1localhost ip6-localhost ip6-loopback</span><br><span class="line">fe00::0ip6-localnet</span><br><span class="line">ff00::0ip6-mcastprefix</span><br><span class="line">ff02::1ip6-allnodes</span><br><span class="line">ff02::2ip6-allrouters</span><br><span class="line">172.17.0.3tomcat02 9de7a985a568</span><br><span class="line">172.17.0.46f62526ba548</span><br></pre></td></tr></table></figure><p>现在玩docker已经不建议使用 —link了</p><p>自定义网络! 不适用docker0</p><p>docker0问题：他不支持容器名链接访问</p><h3 id="自定义网络-（容器互联）"><a href="#自定义网络-（容器互联）" class="headerlink" title="自定义网络 （容器互联）"></a>自定义网络 （容器互联）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss# docker network --help</span><br><span class="line">Commands:</span><br><span class="line">  connect     Connect a container to a network</span><br><span class="line">  create      Create a network</span><br><span class="line">  disconnect  Disconnect a container from a network</span><br><span class="line">  inspect     Display detailed information on one or more networks</span><br><span class="line">  ls          List networks</span><br><span class="line">  prune       Remove all unused networks</span><br><span class="line">  rm          Remove one or more networks</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:&#x2F;home&#x2F;cpss# docker network ls</span><br><span class="line">NETWORK ID     NAME      DRIVER    SCOPE</span><br><span class="line">27abb1e4b0d3   bridge    bridge    local</span><br><span class="line">e69e785a705e   host      host      local</span><br><span class="line">2412989a4eb3   none      null      local</span><br></pre></td></tr></table></figure><p>网络模式</p><ul><li>bridge：桥接docker (默认)</li><li>none: 不配置网络</li><li>host: 和宿主机共享网络</li><li>container：容器内网络连通 (用的少，局限性大)</li></ul><p>测试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 我们之间启动的命令 --net bridge 而这个就是我们的docker0</span></span><br><span class="line">docker run -d -P --name tomcat01 tomcat</span><br><span class="line">docker run -d -P --name tomcat01 --net bridge tomcat</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker0 特点。默认，域名不能访问， --link可以打通</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 我们可以自定义个网络</span></span><br><span class="line">(base) root@linux:/home/cpss# docker network create --help</span><br><span class="line">Options:</span><br><span class="line">      --attachable           Enable manual container attachment</span><br><span class="line">      --aux-address map      Auxiliary IPv4 or IPv6 addresses used by Network driver (default map[])</span><br><span class="line">      --config-from string   The network from which to copy the configuration</span><br><span class="line">      --config-only          Create a configuration only network</span><br><span class="line">  -d, --driver string        Driver to manage the Network (default &quot;bridge&quot;)</span><br><span class="line">      --gateway strings      IPv4 or IPv6 Gateway for the master subnet</span><br><span class="line">      --ingress              Create swarm routing-mesh network</span><br><span class="line">      --internal             Restrict external access to the network</span><br><span class="line">      --ip-range strings     Allocate container ip from a sub-range</span><br><span class="line">      --ipam-driver string   IP Address Management Driver (default &quot;default&quot;)</span><br><span class="line">      --ipam-opt map         Set IPAM driver specific options (default map[])</span><br><span class="line">      --ipv6                 Enable IPv6 networking</span><br><span class="line">      --label list           Set metadata on a network</span><br><span class="line">  -o, --opt map              Set driver specific options (default map[])</span><br><span class="line">      --scope string         Control the network&#x27;s scope</span><br><span class="line">      --subnet strings       Subnet in CIDR format that represents a network segment</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">docker network create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 mynet</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/06/vcuabfSy6KpP9rl.png" alt=""></p><p>不使用—link 也能连接了</p><p>我们自定义的网络docker都已经帮我们维护好了对应的关系</p><p>好处：</p><p>不同的集群使用不同的网络，保证集群是安全健康的。</p><h3 id="网络连通"><a href="#网络连通" class="headerlink" title="网络连通"></a>网络连通</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss<span class="comment"># docker network --help</span></span><br><span class="line">  connect     Connect a container to a network</span><br><span class="line">  </span><br><span class="line">(base) root@linux:/home/cpss<span class="comment"># docker network connect --help</span></span><br><span class="line">Options:</span><br><span class="line">      --<span class="built_in">alias</span> strings           Add network-scoped <span class="built_in">alias</span> <span class="keyword">for</span> the container</span><br><span class="line">      --driver-opt strings      driver options <span class="keyword">for</span> the network</span><br><span class="line">      --ip string               IPv4 address (e.g., 172.30.100.104)</span><br><span class="line">      --ip6 string              IPv6 address (e.g., 2001:db8::33)</span><br><span class="line">      --link list               Add link to another container</span><br><span class="line">      --link-local-ip strings   Add a link-local address <span class="keyword">for</span> the container</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试打通 tomcat01 - mynet</span></span><br><span class="line"><span class="comment"># 连通之后就是将tomcat01 放到了mynet网络下</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>结论：假设要跨网络操作别人，就需要使用docker network connect</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Docker学习-四-网络&quot;&gt;&lt;a href=&quot;#Docker学习-四-网络&quot; class=&quot;headerlink&quot; title=&quot;Docker学习(四) 网络&quot;&gt;&lt;/a&gt;Docker学习(四) 网络&lt;/h1&gt;&lt;h2 id=&quot;理解docker0&quot;&gt;&lt;a href=</summary>
      
    
    
    
    
    <category term="Docker" scheme="http://example.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker学习(三)</title>
    <link href="http://example.com/2021/08/05/Docker%E5%AD%A6%E4%B9%A0-%E4%B8%89/"/>
    <id>http://example.com/2021/08/05/Docker%E5%AD%A6%E4%B9%A0-%E4%B8%89/</id>
    <published>2021-08-05T11:33:59.000Z</published>
    <updated>2021-08-06T09:13:20.525Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker学习-三"><a href="#Docker学习-三" class="headerlink" title="Docker学习(三)"></a>Docker学习(三)</h1><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><h3 id="portainer-不常用"><a href="#portainer-不常用" class="headerlink" title="portainer (不常用)"></a>portainer (不常用)</h3><p>Docker的图形化界面管理工具</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 外部8088 内部9000 </span><br><span class="line"># -v 挂载</span><br><span class="line">docker run -d -p 8088:9000\</span><br><span class="line">--restart&#x3D;always -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock --privileged&#x3D;true portainer&#x2F;portainer</span><br></pre></td></tr></table></figure><h3 id="Rancher-CI-CD再用"><a href="#Rancher-CI-CD再用" class="headerlink" title="Rancher(CI/CD再用)"></a>Rancher(CI/CD再用)</h3><hr><h2 id="镜像是什么"><a href="#镜像是什么" class="headerlink" title="镜像是什么"></a>镜像是什么</h2><p>是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件。它包含某个软件所需的所有内容，包括代码、运行时、库、环境变量和配置文件。</p><p>如何得到：</p><ul><li>远程仓库下载</li><li>拷贝</li><li>自己制作一个镜像DockerFile</li></ul><h3 id="镜像加载原理"><a href="#镜像加载原理" class="headerlink" title="镜像加载原理"></a>镜像加载原理</h3><blockquote><p>UnionFS(联合文件系统)</p></blockquote><p>我们下载的时候看到的一层层就是这个</p><p>UnionFS是一种分层、轻量级且高性能的文件系统，它支持文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下.</p><p>特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。</p><p>docker的镜像实际上由一层一层的文件系统组成，这种层级文件系统就是UnionFS</p><p>bootfs(boot file system)，在Docker镜像的最底层是bootfs，这一层与典型的Linux、Unix系统是一样的，它主要包含bootloader和kernel，bootloader主要是引导加载kernel，Linux刚启动时会加载bootfs文件系统。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载掉bootfs</p><p>rootfs(root file system)，在bootfs之上，包含的是Linux系统中的/dev /proc /bin /etc 等标准目录和文件，rootfs就是各种不同的操作系统发行版，如ubantu，centos</p><p>对于一个精简的OS，rootfs可以很小，只包含最基本的命令，因为底层直接用host的kernel。</p><h2 id="分层理解"><a href="#分层理解" class="headerlink" title="分层理解"></a>分层理解</h2><blockquote><p>分层的镜像</p></blockquote><p>下载的日志输出，可以看到是一层一层的在下载</p><p><img src="https://i.loli.net/2021/08/05/SmgOoLaUYiwG6sr.png" alt=""></p><p>为什么采用这种分层结构 ？</p><p>最大的好处就是资源共享，比如有多个镜像都从相同的Base镜像构建而来，那么宿主机只需在磁盘上保留一份base镜像，同时内存中也只需要加载一份base镜像，这样就可以为所有的容器服务了，而且镜像的每一层都可以被共享。</p><p>查看镜像分层方式可以通过docker image inspect命令</p><blockquote><p>特点</p></blockquote><p>Docker镜像都是只读的，当容器启动时，一个新的可写层被加载到镜像的顶部。</p><p>这一层就是通常说的容器层，容器之下的都叫镜像层。</p><p><img src="https://i.loli.net/2021/08/05/Bo3v1qXJnadp5Kh.png" alt=""></p><h2 id="如何commit镜像"><a href="#如何commit镜像" class="headerlink" title="如何commit镜像"></a>如何commit镜像</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker commit 提交容器成为一个新的副本</span><br><span class="line"># 和git类似</span><br><span class="line">docker commit -m&#x3D;&quot;提交的描述信息&quot; -a&#x3D;&quot;作者&quot; 容器id 容器镜像名:[TAG]</span><br></pre></td></tr></table></figure><h2 id="容器数据卷"><a href="#容器数据卷" class="headerlink" title="容器数据卷"></a>容器数据卷</h2><p>数据？如果数据都在容器中，那么容器删除，数据就会丢失！需求：数据可以持久</p><p>容器之间可以有一个数据共享的技术，docker容器中产生的数据，同步到本地</p><p>目录挂载，将容器内的目录挂载到linux上。</p><p><img src="https://i.loli.net/2021/08/06/cCQPLRghqlwpUZr.png" alt=""></p><p>总结一句话，容器的持久化和同步操作，容器间也是可以数据共享的。</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><blockquote><p>方式1：直接使用命令挂载 -v</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-v 主机目录:容器内目录</span><br><span class="line">-p 主机端口:容器内端口</span><br><span class="line"></span><br><span class="line">docker run -it -v &#x2F;home&#x2F;ceshi:&#x2F;home centos &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/06/TpZcrXgRi3SV9hB.png" alt=""></p><p>是双向的同步，哪怕容器已经停止。</p><p>好处：以后修改只需在本地修改即可。</p><h3 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h3><p>思考：mysql的数据持久化问题 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss# docker pull mysql:5.7</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行，需要数据挂载</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装启动mysql时，需要配置密码的！</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -d 后台运行</span></span><br><span class="line">(base) root@linux:/home/cpss# docker run -d -p 3310:3306 -v /data2/mysql/conf:/etc/mysql/conf.d -v /data2/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root --name mysql01 mysql:5.7</span><br><span class="line">72180bd20207e871aebdc0a06fddfe10e30d39561620c78558328b9ac0a30b9c</span><br></pre></td></tr></table></figure><h3 id="具名和匿名挂载"><a href="#具名和匿名挂载" class="headerlink" title="具名和匿名挂载"></a>具名和匿名挂载</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 匿名挂载</span></span><br><span class="line">-v 容器内路径</span><br><span class="line">docker run -d -P --name nginx01 -v /etc/nginx nginx</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看所有的volume情况，匿名卷挂载</span></span><br><span class="line">(base) root@linux:/data2/mysql# docker volume ls</span><br><span class="line"><span class="meta">#</span><span class="bash"> 这种就是匿名挂载，在-v只写了容器内的路径，没有写容器外的路径！</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 具名挂载</span></span><br><span class="line">docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx nginx</span><br><span class="line">(base) root@linux:/data2/mysql# docker volume ls</span><br><span class="line">DRIVER    VOLUME NAME</span><br><span class="line">local     juming-nginx</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过-v 卷名：容器内路径</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看一下这个卷</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/06/HStUJkLaIY9sXm7.png" alt=""></p><p>所有的docker容器内的卷，没有指定目录的情况下都是在 /var/lib/docker/volumes/xxxxx/_data</p><p>通过具名挂载可以方便的找到一个卷，大多数情况在使用的是具名挂载。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如何确定是具名挂载还是匿名挂载， 还是指定路径挂载</span></span><br><span class="line">-v 容器内路径 <span class="comment"># 匿名挂载</span></span><br><span class="line">-v 卷名：容器内路径 <span class="comment"># 具名挂载</span></span><br><span class="line">-v /宿主机路径:容器内路径 <span class="comment"># 指定路径挂载</span></span><br></pre></td></tr></table></figure><p>扩展</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:ro nginx</span><br><span class="line">docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:rw nginx</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过 -v 容器内路径:ro rw改变读写权限</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  容器对我们挂载出来的内容就有限定了</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ro <span class="built_in">readonly</span> 只要看到ro就说明这个路径只能通过宿主机来操作，容器内部是无法操作的。</span></span><br></pre></td></tr></table></figure><blockquote><p>方式二、dockerfile 创建镜像时就挂载出来</p></blockquote><p>Dockerfile 就是用来构建docker镜像的构建文件</p><p>通过脚本可以生成镜像，镜像是一层一层的，脚本一个个的命令，每个命令都是一层</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/data2/docker-volume# pwd</span><br><span class="line">/data2/docker-volume</span><br><span class="line">(base) root@linux:/data2/docker-volume# vim dockerfile1</span><br><span class="line">(base) root@linux:/data2/docker-volume# cat dockerfile1 </span><br><span class="line">FROM centos</span><br><span class="line"></span><br><span class="line">VOLUME [&quot;volume01&quot;,&quot;volume02&quot;]</span><br><span class="line"></span><br><span class="line">CMD echo &quot;---end---&quot;</span><br><span class="line"></span><br><span class="line">CMD /bin/bash</span><br><span class="line">(base) root@linux:/data2/docker-volume# docker build -f dockerfile1 -t zuo/centos:1.0 .</span><br><span class="line">Sending build context to Docker daemon  2.048kB</span><br><span class="line">Step 1/4 : FROM centos</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 300e315adb2f</span></span><br><span class="line">Step 2/4 : VOLUME [&quot;volume01&quot;,&quot;volume02&quot;]</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> 26da05b75834</span></span><br><span class="line">Removing intermediate container 26da05b75834</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 5ae4812f35a4</span></span><br><span class="line">Step 3/4 : CMD echo &quot;---end---&quot;</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> 29c52fec2f47</span></span><br><span class="line">Removing intermediate container 29c52fec2f47</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> cb1793533f3d</span></span><br><span class="line">Step 4/4 : CMD /bin/bash</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> c4bc1543fe44</span></span><br><span class="line">Removing intermediate container c4bc1543fe44</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> c635584bb2a8</span></span><br><span class="line">Successfully built c635584bb2a8</span><br><span class="line">Successfully tagged zuo/centos:1.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(base) root@linux:/data2/docker-volume# docker images</span><br><span class="line">REPOSITORY      TAG       IMAGE ID       CREATED          SIZE</span><br><span class="line">zuo/centos      1.0       c635584bb2a8   59 seconds ago   209MB</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建dockerfile文件，名字可以随机</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 文件中的内容 指令(大写) 参数</span></span><br><span class="line">FROM centos</span><br><span class="line">每个命令就是镜像的一层</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/06/9LG7RVIYzOsK4Sc.png" alt=""></p><p>这个卷是匿名挂载，一定有外部的目录</p><p><img src="https://i.loli.net/2021/08/06/g9H4moazDAIJUTj.png" alt=""></p><h3 id="数据卷容器"><a href="#数据卷容器" class="headerlink" title="数据卷容器"></a>数据卷容器</h3><p>两个mysql同步数据 —volumes-from</p><p><img src="https://i.loli.net/2021/08/06/nZx8FVqjUbBS1I2.png" alt=""></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动3个容器，通过我们刚才自己的写镜像启动</span></span><br><span class="line">(base) root@linux:/data2/docker-volume# docker run -it --name docker01 zuo/centos:1.0</span><br><span class="line"></span><br><span class="line">(base) root@linux:/data2/docker-volume# docker run -it --name docker02 --volumes-from docker01 zuo/centos:1.0</span><br></pre></td></tr></table></figure><p>容器之间配置信息的传递，数据卷容器的生命周期一直持续到没有容器为止</p><p>一旦持久化到了本地，本地的数据是不会删除的。</p><h2 id="DockerFile"><a href="#DockerFile" class="headerlink" title="DockerFile"></a>DockerFile</h2><p>dockerfile 是用来构建docker镜像的文件，命令参数脚本</p><p>构建步骤：</p><ul><li>编写一个dockerfile文件</li><li>docker build构建成为一个镜像</li><li>docker run 运行镜像</li><li>docker push 发布镜像(docker hub，阿里云镜像仓库)</li></ul><h3 id="构建过程"><a href="#构建过程" class="headerlink" title="构建过程"></a>构建过程</h3><p>基础知识</p><ul><li>每个保留关键字指令都是大写字母</li><li>执行从上到下顺序执行</li><li>每个指令都会创建提交一个新的镜像层</li></ul><p><img src="https://i.loli.net/2021/08/06/3JdmShcx4r6znqw.png" alt=""></p><p>dockerfile是面向开发的，我们以后要发布项目做镜像，要写。</p><h3 id="指令"><a href="#指令" class="headerlink" title="指令"></a>指令</h3><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span>          <span class="comment"># 基础镜像，一切从这里开始构建</span></span><br><span class="line"><span class="keyword">MAINTAINER</span>    <span class="comment"># 镜像是谁写的，姓名+邮箱</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash">           <span class="comment"># 镜像构建时要运行的命令</span></span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash">           <span class="comment"># 步骤，添加内容</span></span></span><br><span class="line">WORKERDIR     <span class="comment"># 镜像的工作目录</span></span><br><span class="line"><span class="keyword">VOLUME</span><span class="bash">         <span class="comment"># 挂载的目录</span></span></span><br><span class="line"><span class="keyword">EXPOSE</span>         <span class="comment"># 暴露端口</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash">            <span class="comment"># 指定容器启动时需要运行的命令,只有最后一个会生效，可被替代</span></span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash">     <span class="comment"># 指定容器启动时需要运行的命令，可以追加命令</span></span></span><br><span class="line"><span class="keyword">ONBUILD</span>        <span class="comment"># 当构建一个被继承 Dockerfile </span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash">           <span class="comment">#  类似ADD 将文件拷贝到镜像中</span></span></span><br><span class="line"><span class="keyword">ENV</span>            <span class="comment"># 构建的时候设置环境变量</span></span><br></pre></td></tr></table></figure><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ubuntu:<span class="number">18.04</span>  <span class="comment"># 指定基础镜像 如果为scratch代表从下一行开始是镜像的第一层</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&#x27;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&#x27;</span> &gt; /usr/share/nginx/html/index.html <span class="comment"># RUN指令用来执行命令，每一行代表新建docker的一个layer</span></span></span><br><span class="line"><span class="comment">#能在一个layer内执行的指令就通过&amp;&amp; 进行联接，并可应用shell中的换行符\</span></span><br><span class="line"><span class="comment">#在dockerfile每层都要检查，下载，展开的多余文件，以及缓存等能删除的尽量都去掉</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> <span class="comment">#COPY 指令将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置。</span></span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> package.json /usr/src/app/ <span class="comment"># 将当前上下文路径的json文件复制到image的指定路径下</span></span></span><br><span class="line"></span><br><span class="line">AND <span class="comment">#丰富了COPY的功能，但是会降低构件image速度，如果不需要自动解压缩，则不推荐使用该指令</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> <span class="comment"># ？？？？？？？？？ 还没理解</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> <span class="comment"># 当存在 ENTRYPOINT 后，CMD 的内容将会作为参数传给ENTRYPOINT，从而达到了我们预期的效果。</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> <span class="comment">#用来设置环境变量  ENV &lt;key&gt; &lt;value&gt; 或 ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;...</span></span><br><span class="line"><span class="keyword">ENV</span> VERSION=<span class="number">1.0</span> DEBUG=on \</span><br><span class="line">    NAME=<span class="string">&quot;Happy ONE&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> LD_LIBRARY_PATH=\</span><br><span class="line">$LD_LIBRARY_PATH:\</span><br><span class="line">$NAME/alpha</span><br><span class="line"></span><br><span class="line"><span class="keyword">ARG</span> <span class="comment"># ARG &lt;参数名&gt;[=&lt;默认值&gt;] Dockerfile 中的 ARG 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令 docker build 中用 --build-arg &lt;参数名&gt;=&lt;值&gt; 来覆盖</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ARG</span> DOCKER_USERNAME=library <span class="comment"># 注意：在FROM之前定义的ARG参数，会消失，在FROM后需要重新定义</span></span><br><span class="line"><span class="comment"># ARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。但是不要因此就使用 ARG 保存密码之类的信息，因为 docker history 还是可以看到所有值的。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">VOLUME</span><span class="bash"> <span class="comment"># 用于指定image启动时挂载到容器中的默认卷，而不是写入容器存储层</span></span></span><br><span class="line"><span class="keyword">VOLUME</span><span class="bash"> /data <span class="comment"># VOLUME [&quot;&lt;路径1&gt;&quot;, &quot;&lt;路径2&gt;&quot;...] 或 VOLUME &lt;路径&gt;</span></span></span><br><span class="line">在image启动时可替换</span><br><span class="line">docker <span class="keyword">run</span><span class="bash"> -d -v mydata:/data xxxx <span class="comment">#其中的 -v mydata:/data 就是挂载宿主机的卷到容器内</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="comment"># EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...] EXPOSE 指令是声明容器运行时提供服务的端口，这只是一个声明，在容器运行时并不会因为这个声明应用就会开启这个端口的服务</span></span><br><span class="line"><span class="comment"># 在 Dockerfile 中写入这样的声明有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> <span class="comment"># WORKDIR &lt;工作目录路径&gt; 使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">USER</span>  <span class="comment"># USER &lt;用户名&gt;[:&lt;用户组&gt;] 指定当前用户</span></span><br><span class="line"><span class="keyword">HEALTHCHECK</span></span><br><span class="line"><span class="bash">ONBUILD</span></span><br><span class="line">LEBEL</span><br><span class="line"><span class="keyword">SHELL</span><span class="bash"> <span class="comment">#SHELL 指令可以指定 RUN ENTRYPOINT CMD 指令的 shell，Linux 中默认为 [&quot;/bin/sh&quot;, &quot;-c&quot;]   </span></span></span><br><span class="line">Dockerfile 多阶段构建</span><br></pre></td></tr></table></figure><blockquote><p>创建一个字节的centos</p></blockquote><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1 编写dockerfile文件</span></span><br><span class="line"><span class="keyword">FROM</span> centos</span><br><span class="line"><span class="keyword">MAINTAINER</span> zuo&lt;com&gt;</span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> MYPATH /usr/local</span><br><span class="line">WORKERDIR $MYPATH</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> yum -y install vim  <span class="comment"># 你想让他干啥</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">80</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> <span class="built_in">echo</span> <span class="variable">$MYPATH</span></span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&quot;----end----&quot;</span></span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> /bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2通过这个文件构建镜像</span></span><br><span class="line">docker build -f mydockerfile-centos -t mycentos:<span class="number">0.1</span> .</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3测试运行</span></span><br><span class="line">docker <span class="keyword">run</span><span class="bash"> -it mycentos</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看docker镜像构建历史</p><p><img src="https://i.loli.net/2021/08/06/VuB78orZcqWGS5v.png" alt=""></p><blockquote><p>CMD 和 ENTRYPOINT 的区别</p></blockquote><p><img src="https://i.loli.net/2021/08/06/rpFZ8xUYwROiauC.png" alt=""></p><p>ENTRYPOINT 是可以追加命令的</p><h3 id="做一个tomcat镜像"><a href="#做一个tomcat镜像" class="headerlink" title="做一个tomcat镜像"></a>做一个tomcat镜像</h3><ul><li>准备镜像文件，tomcat压缩包，jdk的压缩包</li><li>编写dockerfile文件，官方命名 Dockerfile，build会自动寻找这个文件，就不需要-f 指定了</li></ul><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> centos</span><br><span class="line"><span class="keyword">MAINTAINER</span> zuo&lt;com&gt;</span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> readme.txt /usr/<span class="built_in">local</span>/readme.txt</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> jdk-8ull-linux-x64.tar.gz /usr/<span class="built_in">local</span>/   <span class="comment"># 会自动解压</span></span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> apache-tomcat-9.0.22.tar.gz /usr/<span class="built_in">local</span>/ </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash">  yum -y install vim</span></span><br><span class="line"><span class="keyword">ENV</span> MYPATH /usr/local</span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> <span class="variable">$MYPATH</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> JAVA_HOME /<span class="keyword">user</span>/local/jdk1.<span class="number">8.0</span>_11</span><br><span class="line"><span class="keyword">ENV</span> CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line"><span class="keyword">ENV</span> CLASSPATH_HOME /usr/local/apache-tomcat-<span class="number">9.0</span>.<span class="number">22</span></span><br><span class="line"><span class="keyword">ENV</span> CLASSPATH_BASH /usr/local/apache-tomcat-<span class="number">9.0</span>.<span class="number">22</span></span><br><span class="line"><span class="keyword">ENV</span> PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin</span><br><span class="line"></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">8080</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> /usr/<span class="built_in">local</span>/apache-tomcat-9.0.22/bin/startup.sh &amp;&amp; tail -F /usr/<span class="built_in">local</span>/apache-tomcat-9.0.22/bin/logs</span></span><br></pre></td></tr></table></figure><p>构建镜像</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker build -t diytomcat .</span><br><span class="line"></span><br><span class="line">docker <span class="keyword">run</span><span class="bash"> -d -p 9090:8080 --name zuotomcat -v /home/zuo/build/tomcat/<span class="built_in">test</span>:/usr/<span class="built_in">local</span>/apache-tomcat-9.0.22/webapps/<span class="built_in">test</span> -v /home/zuo/build/tomcat/tomcatlogs/:/usr/<span class="built_in">local</span>/apache-tomcat-9.0.22/logs diytomcat</span></span><br></pre></td></tr></table></figure><h2 id="发布自己的镜像"><a href="#发布自己的镜像" class="headerlink" title="发布自己的镜像"></a>发布自己的镜像</h2><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss<span class="comment"># docker login --help</span></span><br><span class="line"></span><br><span class="line">Usage:  docker login [OPTIONS] [SERVER]</span><br><span class="line"></span><br><span class="line">Log in to a Docker registry.</span><br><span class="line">If no server is specified, the default is defined by the daemon.</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -p, --password string   Password</span><br><span class="line">      --password-stdin    Take the password <span class="keyword">from</span> stdin</span><br><span class="line">  -u, --username string   Username</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">(base) root@linux:/home/cpss<span class="comment"># docker login -u zzuuoo666</span></span><br><span class="line">Password: </span><br><span class="line">WARNING! Your password will be stored unencrypted in /root/.docker/config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/login/<span class="comment">#credentials-store</span></span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker push zzuuoo666/diytomcat:<span class="number">1.0</span> <span class="comment"># 就可以了 不加信息可能会被拒绝</span></span><br><span class="line"><span class="comment"># 如果镜像上传过被拒绝，可以添加一个tag</span></span><br><span class="line">docker tag ID zzuuoo666/tomcat:<span class="number">1.0</span></span><br><span class="line">docker push zzuuoo666/tomcat:<span class="number">1.0</span></span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><img src="https://i.loli.net/2021/08/06/VmDSgJkyvzeHXtZ.png" alt=""></p><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Docker学习-三&quot;&gt;&lt;a href=&quot;#Docker学习-三&quot; class=&quot;headerlink&quot; title=&quot;Docker学习(三)&quot;&gt;&lt;/a&gt;Docker学习(三)&lt;/h1&gt;&lt;h2 id=&quot;可视化&quot;&gt;&lt;a href=&quot;#可视化&quot; class=&quot;head</summary>
      
    
    
    
    
    <category term="Docker" scheme="http://example.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker学习(二)例子练习</title>
    <link href="http://example.com/2021/08/01/Docker%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E4%BE%8B%E5%AD%90%E7%BB%83%E4%B9%A0/"/>
    <id>http://example.com/2021/08/01/Docker%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E4%BE%8B%E5%AD%90%E7%BB%83%E4%B9%A0/</id>
    <published>2021-08-01T10:37:05.000Z</published>
    <updated>2021-08-05T11:34:55.748Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker学习-二-例子练习"><a href="#Docker学习-二-例子练习" class="headerlink" title="Docker学习(二)例子练习"></a>Docker学习(二)例子练习</h1><h2 id="部署Nginx"><a href="#部署Nginx" class="headerlink" title="部署Nginx"></a>部署Nginx</h2><ul><li>搜索镜像去docker hub上</li><li>下载镜像 docker pull nginx</li><li>docker run -d 后台运行 —name nginx01 -p 10024:80</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;home&#x2F;cpss# docker run -d --name nginx01 -p 10024:80 nginx</span><br><span class="line">84960293d8409dc9f7e70be88027c2149ece57d7cf02dc4d71eb81fe1651fc96</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;home&#x2F;cpss# docker ps</span><br><span class="line">CONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS                                     NAMES</span><br><span class="line">84960293d840   nginx     &quot;&#x2F;docker-entrypoint.…&quot;   21 seconds ago   Up 20 seconds   0.0.0.0:10024-&gt;80&#x2F;tcp, :::10024-&gt;80&#x2F;tcp   nginx01</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;home&#x2F;cpss# curl localhost:10024</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;&#x2F;title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;&#x2F;style&gt;</span><br><span class="line">&lt;&#x2F;head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;&#x2F;h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;&#x2F;p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;nginx.org&#x2F;&quot;&gt;nginx.org&lt;&#x2F;a&gt;.&lt;br&#x2F;&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;nginx.com&#x2F;&quot;&gt;nginx.com&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;</span><br><span class="line">&lt;&#x2F;body&gt;</span><br><span class="line">&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure><p>-p 暴露端口的概念</p><p><img src="https://i.loli.net/2021/08/04/4Bx8PzGlrD1T6vA.png" alt=""></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it nginx01 /bin/bash 进入容器</span><br><span class="line">root@84960293d840:/# whereis nginx</span><br><span class="line">nginx: /usr/sbin/nginx /usr/lib/nginx /etc/nginx /usr/share/nginx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(base) root@localhost:/home/cpss# docker ps</span><br><span class="line">CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS                                     NAMES</span><br><span class="line">84960293d840   nginx     &quot;/docker-entrypoint.…&quot;   8 minutes ago   Up 8 minutes   0.0.0.0:10024-&gt;80/tcp, :::10024-&gt;80/tcp   nginx01</span><br><span class="line">(base) root@localhost:/home/cpss# docker stop 84960293d840</span><br><span class="line">84960293d840</span><br></pre></td></tr></table></figure><p>思考：每次改动nginx配置文件，都需要进入容器内部，十分麻烦</p><p>可以在容器外部提供一个映射路径，达到在容器修改文件名，内部容器就可以自动修改。</p><p>这个技术是  -v 数据卷技术  </p><h2 id="部署-ES-Kibana"><a href="#部署-ES-Kibana" class="headerlink" title="部署 ES+Kibana"></a>部署 ES+Kibana</h2><p>ES暴露端口很多，也耗内存，数据一般需要放到安全目录，挂载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> --net somenetwork 网络配置</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --rm 用完就删掉</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 elasticsearch 比较耗内存</span></span><br><span class="line">docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; elasticsearch:7.14.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CONTAINER ID   NAME            CPU %     MEM USAGE / LIMIT     MEM %     NET I/O           BLOCK I/O       PIDS</span><br><span class="line">a9057d9c6e50   elasticsearch   2.54%     32.49GiB / 125.8GiB   25.83%    11.1kB / 1.94kB   100MB / 292MB   98</span><br><span class="line"></span><br><span class="line">(base) root@linux:/home/cpss# curl localhost:9200</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot; : &quot;a9057d9c6e50&quot;,</span><br><span class="line">  &quot;cluster_name&quot; : &quot;docker-cluster&quot;,</span><br><span class="line">  &quot;cluster_uuid&quot; : &quot;lkLPT_ssQ2CV30B55gn4bg&quot;,</span><br><span class="line">  &quot;version&quot; : &#123;</span><br><span class="line">    &quot;number&quot; : &quot;7.14.0&quot;,</span><br><span class="line">    &quot;build_flavor&quot; : &quot;default&quot;,</span><br><span class="line">    &quot;build_type&quot; : &quot;docker&quot;,</span><br><span class="line">    &quot;build_hash&quot; : &quot;dd5a0a2acaa2045ff9624f3729fc8a6f40835aa1&quot;,</span><br><span class="line">    &quot;build_date&quot; : &quot;2021-07-29T20:49:32.864135063Z&quot;,</span><br><span class="line">    &quot;build_snapshot&quot; : false,</span><br><span class="line">    &quot;lucene_version&quot; : &quot;8.9.0&quot;,</span><br><span class="line">    &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;,</span><br><span class="line">    &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;tagline&quot; : &quot;You Know, for Search&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 增加内存限制，修改卑职文件 -e 环境修改</span></span><br><span class="line">docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; -e ES_JAVA_OPTS=&quot;-Xms64m -Xmx512m&quot;   elasticsearch:7.14.0</span><br><span class="line"></span><br><span class="line">CONTAINER ID   NAME             CPU %     MEM USAGE / LIMIT     MEM %     NET I/O       BLOCK I/O         PIDS</span><br><span class="line">e40105c39e81   elasticsearch1   281.21%   676.5MiB / 125.8GiB   0.53%     2.84kB / 0B   26.9MB / 1.22MB   103</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/08/05/PiIaTA4C1DsMZz3.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Docker学习-二-例子练习&quot;&gt;&lt;a href=&quot;#Docker学习-二-例子练习&quot; class=&quot;headerlink&quot; title=&quot;Docker学习(二)例子练习&quot;&gt;&lt;/a&gt;Docker学习(二)例子练习&lt;/h1&gt;&lt;h2 id=&quot;部署Nginx&quot;&gt;&lt;a h</summary>
      
    
    
    
    
    <category term="Docker" scheme="http://example.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker学习(一)</title>
    <link href="http://example.com/2021/07/30/Docker%E5%AD%A6%E4%B9%A0-%E4%B8%80/"/>
    <id>http://example.com/2021/07/30/Docker%E5%AD%A6%E4%B9%A0-%E4%B8%80/</id>
    <published>2021-07-30T01:27:56.000Z</published>
    <updated>2021-08-01T10:36:24.367Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker学习-一"><a href="#Docker学习-一" class="headerlink" title="Docker学习(一)"></a>Docker学习(一)</h1><p>文档：<a href="https://docs.docker.com/">https://docs.docker.com/</a></p><p>Hub : <a href="https://hub.docker.com/">https://hub.docker.com/</a></p><h2 id="路线"><a href="#路线" class="headerlink" title="路线"></a>路线</h2><ul><li>Docker概述</li><li>Docker安装</li><li>Docker命令</li><li><ul><li>镜像命令</li><li>容器命令</li><li>操作命令</li><li>……</li></ul></li><li>Docker镜像</li><li>容器数据卷</li><li>DockerFile</li><li>Docker网络原理</li><li>Docker Compose</li><li>Docker Swarm</li><li>CI\CD Jenkins</li></ul><h2 id="Docker概述"><a href="#Docker概述" class="headerlink" title="Docker概述"></a>Docker概述</h2><p>Docker为什么会出现？</p><blockquote><p>环境配置十分麻烦，每个机器都要部署环境，很难跨平台，集群环境更浪费时间。项目能不能带上环境打包(镜像)。</p></blockquote><p>能干嘛？</p><blockquote><p>之前的虚拟机技术,浪费资源比较多</p></blockquote><p><img src="https://i.loli.net/2021/07/30/cG1EzWbCX5ArsTf.png" alt=""></p><p>缺点：</p><ul><li>资源占用多</li><li>冗余步骤多</li><li>启动很慢</li></ul><blockquote><p>容器化技术</p></blockquote><p>不是模拟一个完整的操作系统</p><p><img src="https://i.loli.net/2021/07/30/MojI6CL2lqRapYW.png" alt=""></p><p>不同之处：</p><ul><li>传统虚拟机，虚拟出一套硬件，运行一个完整的操作系统，然后在这个系统上运行安装软件</li><li>容器的应用直接运行在宿主机上的内核中，容器是没有自己的内核的，没有虚拟硬件，比较轻便</li><li>每个容器间是互相隔离的，每个容器内都有一共自己的文件系统，互不影响。</li></ul><blockquote><p>DevOps（开发 运维）</p></blockquote><p>更快速的交付和部署</p><p>传统：一堆帮助文档，安装程序</p><p>Docker: 打包镜像发布测试，一键运行</p><p>更便捷的升级和扩容缩容，更高效的计算资源利用，测试环境都高度一致。</p><p>Docker是内核级别的虚拟化，可以再一个物理机上运行很多的容器实例。</p><h2 id="Docker-基本组成"><a href="#Docker-基本组成" class="headerlink" title="Docker 基本组成"></a>Docker 基本组成</h2><p><img src="https://i.loli.net/2021/07/30/XMPGFCdjvi96tey.png" alt=""></p><p>从左到右，依次是客户端、服务器和仓库。</p><ul><li><p>镜像（Image）：docker镜像就好比是一个模板，可以通过这个模板来创建容器服务。如：tomcat镜像—-&gt;run—-&gt;tomcat01容器。通过这个镜像可以创建多个容器，最终服务运行或者项目运行就是在容器中的</p></li><li><p>容器（Containers）：Docker利用容器技术，可以独立运行一个或者一组应用，通过镜像来创建。启动，</p><p>停止，删除基本命令。目前可把这个容器简单理解为就是一个简易的linux系统。</p></li><li><p>仓库（Repository）：存放镜像的地方。分为公有仓库和私有仓库，和GitHub差不多。Docker hub默认是国外的，可以配阿里云镜像加速。</p></li></ul><h2 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h2><p><img src="https://i.loli.net/2021/07/30/oyDtNArhC1qmlXV.png" alt=""></p><p>如何查看hello world镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;home&#x2F;cpss# docker images</span><br><span class="line">REPOSITORY      TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">hello-world     latest    d1165f221234   4 months ago    13.3kB</span><br><span class="line">studyfang&#x2F;hgn   latest    37553493935b   10 months ago   8.88GB</span><br></pre></td></tr></table></figure><p>docker默认工作路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;home&#x2F;cpss# ls &#x2F;var&#x2F;lib&#x2F;docker&#x2F;</span><br><span class="line">buildkit  containers  image  network  overlay2  plugins  runtimes  swarm  tmp  trust  volumes</span><br></pre></td></tr></table></figure><h2 id="镜像加速"><a href="#镜像加速" class="headerlink" title="镜像加速"></a>镜像加速</h2><p>创建或修改 /etc/docker/daemon.json 文件，修改为如下形式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [</span><br><span class="line">    &quot;https:&#x2F;&#x2F;registry.docker-cn.com&quot;,</span><br><span class="line">    &quot;http:&#x2F;&#x2F;hub-mirror.c.163.com&quot;,</span><br><span class="line">    &quot;https:&#x2F;&#x2F;docker.mirrors.ustc.edu.cn&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:&#x2F;etc&#x2F;docker# vim daemon.json</span><br><span class="line">(base) root@localhost:&#x2F;etc&#x2F;docker# systemctl daemon-reload</span><br><span class="line">(base) root@localhost:&#x2F;etc&#x2F;docker# systemctl restart docker</span><br><span class="line">(base) root@localhost:&#x2F;etc&#x2F;docker# systemctl status docker</span><br></pre></td></tr></table></figure><p>使用docker info 查看镜像改变。</p><p><img src="https://i.loli.net/2021/07/30/AJiXGsvMfug9jc5.png" alt=""></p><h2 id="run流程原理"><a href="#run流程原理" class="headerlink" title="run流程原理"></a>run流程原理</h2><p> <img src="https://i.loli.net/2021/07/30/6jbSgOziPhAYNkd.png" alt=""></p><p>docker是怎么工作的？</p><blockquote><p>docker是一个client-server结构的系统，docker的守护进程运行在主机上，通过socket从客户端访问。</p><p>docker server 接收到docker client的指令就会执行这个命令</p></blockquote><p><img src="https://i.loli.net/2021/07/30/vFZk7yGAHVgfad1.png" alt=""></p><p>docker为什么比VM快？</p><blockquote><p>docker有比虚拟机更少的抽象层。</p><p>docker利用的是宿主机的内核，vm需要Guest OS</p></blockquote><p><img src="https://i.loli.net/2021/07/30/wormWUzExPMNBsh.png" style="zoom:150%;" /></p><p>所以新建一个容器的时候，docker不需要向虚拟机一样重新加载一个操作系统内核。避免引导操作，虚拟机是加载GuestOS，docker是利用宿主机的操作系统，省略了这个复杂的过程。</p><p><img src="https://i.loli.net/2021/07/30/ode6RsjDH1Y5yF3.png" style="zoom:150%;" /></p><h2 id="Docker的常用命令"><a href="#Docker的常用命令" class="headerlink" title="Docker的常用命令"></a>Docker的常用命令</h2><h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker version     <span class="comment"># docker的版本信息</span></span><br><span class="line">docker info <span class="comment"># 显示docker的系统信息，包括镜像和容器的数量</span></span><br><span class="line">docker 命令 --<span class="built_in">help</span>  <span class="comment"># 帮助命令</span></span><br></pre></td></tr></table></figure></h2><h3 id="镜像命令"><a href="#镜像命令" class="headerlink" title="镜像命令"></a>镜像命令</h3><p>docker images</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:/home/cpss<span class="comment"># docker images</span></span><br><span class="line">REPOSITORY      TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">hello-world     latest    d1165f221234   4 months ago    13.3kB</span><br><span class="line">studyfang/hgn   latest    37553493935b   10 months ago   8.88GB</span><br></pre></td></tr></table></figure><ul><li><p>REPOSITORY 镜像的仓库源</p></li><li><p>TAG 镜像的标签</p></li><li><p>IMAGE ID 镜像的id</p></li><li><p>CREATED 镜像的创建时间</p></li><li><p>SIZE 镜像大小</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Options:</span><br><span class="line">  -a, --all             Show all images (default hides intermediate images)</span><br><span class="line">  -q, --quiet           Only show image IDs</span><br></pre></td></tr></table></figure><p>docker search 搜索镜像</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(base) root@localhost:/home/cpss# docker search hotpotqa</span><br><span class="line">NAME                                  DESCRIPTION                     STARS     OFFICIAL   AUTOMATED</span><br><span class="line">qipeng/hotpotqa-eval                                                  0                    </span><br><span class="line">studyfang/hotpotqa                                                    0                    </span><br><span class="line">qipeng/hotpotqa-base                                                  0                    </span><br><span class="line">tuming1990/hotpotqa-docker                                            0                    </span><br><span class="line">hamishivi/hotpotqa-base               Hotpotqa with extra packages.   0                    </span><br><span class="line">qipeng/hotpotqa_submission_cuda10.2                                   0                    </span><br><span class="line">tswings2018/hotpotqa                  by deng                         0        </span><br></pre></td></tr></table></figure><p>docker pull</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># 下载镜像 docker pull 镜像名[:tag]</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/07/30/r9qWyoDSOTk7QKh.png" alt=""></p><p>docker rmi 删除镜像</p><p>可通过id 或者 名称来删</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi -f 镜像id</span><br></pre></td></tr></table></figure><h3 id="容器命令"><a href="#容器命令" class="headerlink" title="容器命令"></a>容器命令</h3><p>有了镜像才可以创建容器</p><p>这里下载一个centos镜像来测试学习</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull centos</span><br></pre></td></tr></table></figure><p>新建容器并启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">docker run [可选参数] image</span><br><span class="line"><span class="meta">#</span><span class="bash"> 参数说明</span></span><br><span class="line">--name=&quot;Name&quot; 容器名字  tomcat01 tomcat02 用来区分容器</span><br><span class="line">-d            后台方式运行 nohup</span><br><span class="line">-it           使用交互方式运行，进入容器查看内容</span><br><span class="line">-p            指定容器的端口  ip:主机端口:容器端口 主机端口:容器端口(常用)   容器端口</span><br><span class="line">-P随机指定端口</span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试 启动并进入容器</span></span><br><span class="line">(base) root@linux:/home/cpss# docker run -it centos /bin/bash</span><br><span class="line">[root@ef41db25d696 /]# 容器内就是自己的服务器环境</span><br><span class="line"></span><br><span class="line">docker ps # 查看正在运行的容器</span><br><span class="line">docker ps -a # 查看曾经运行过的容器</span><br><span class="line">docker ps -a -n=1 # 显示个数</span><br><span class="line">docker ps -aq # 只显示编号</span><br></pre></td></tr></table></figure><p>退出容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">exit # 直接退出容器并停止</span><br><span class="line">ctrl +p +q # 容器不停止退出</span><br></pre></td></tr></table></figure><p>删除容器</p><p>删除容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker rm 容器id                # 删除指定的容器 不能删除正在运行的容器 -f强制删除</span><br><span class="line">docker rm -f $(docker ps -aq)  # 删除所有的容器</span><br><span class="line"></span><br><span class="line">docker ps -a -q|xargs docker rm # 删除所有的容器</span><br></pre></td></tr></table></figure><p>启动和停止容器的操作</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker start 容器id</span><br><span class="line">docker restart 容器id</span><br><span class="line">docker stop 容器id</span><br><span class="line">docker <span class="built_in">kill</span> 容器</span><br></pre></td></tr></table></figure><h3 id="常用其他命令"><a href="#常用其他命令" class="headerlink" title="常用其他命令"></a>常用其他命令</h3><p>后台启动容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -d centos</span><br><span class="line"><span class="meta">#</span><span class="bash"> 问题 docker ps时发现centos停止了</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 常见的坑，docker 容器使用后台运行，就必须要有一个前台进程。docker发现没有应用就会自动停止。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 容器启动后，发现自己没有提供服务，就会立即停止</span></span><br></pre></td></tr></table></figure><p>查看日志</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">(base) root@linux:/home/cpss# docker logs --help</span><br><span class="line">Options:</span><br><span class="line">      --details        Show extra details provided to logs</span><br><span class="line">  -f, --follow         Follow log output</span><br><span class="line">      --since string   Show logs since timestamp (e.g. 2013-01-02T13:23:37Z) or relative (e.g. 42m for 42 minutes)</span><br><span class="line">  -n, --tail string    Number of lines to show from the end of the logs (default &quot;all&quot;)</span><br><span class="line">  -t, --timestamps     Show timestamps</span><br><span class="line">      --until string   Show logs before a timestamp (e.g. 2013-01-02T13:23:37Z) or relative (e.g. 42m for 42 minutes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> -tf 显示日志</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --tail number 要显示日志条数</span></span><br><span class="line">docker logs -tf --tail 10 f3c59b35b738</span><br><span class="line"><span class="meta">#</span><span class="bash"> 容器没有日志</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 自己写一段shell</span></span><br><span class="line">(base) root@linux:/home/cpss# docker run -d centos /bin/sh -c &quot;while true;do echo 111;sleep 1;done&quot;</span><br><span class="line">ba0ae87cb0949d44e179f03e2bb3e25a38b394bb98b7aa0f4a1a2b9ad68ca86d</span><br><span class="line">(base) root@linux:/home/cpss# docker ps</span><br><span class="line">CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS        PORTS     NAMES</span><br><span class="line">ba0ae87cb094   centos    &quot;/bin/sh -c &#x27;while t…&quot;   3 seconds ago   Up 1 second             determined_bouman</span><br><span class="line">(base) root@linux:/home/cpss# docker logs -tf --tail 10 ba0ae87cb094</span><br></pre></td></tr></table></figure><p>查看容器中的进程信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker top 容器id</span><br></pre></td></tr></table></figure><p>查看镜像的元数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">docker inspect 容器id</span><br><span class="line"></span><br><span class="line">(base) root@linux:/home/cpss# docker inspect ba0ae87cb094</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;Id&quot;: &quot;ba0ae87cb0949d44e179f03e2bb3e25a38b394bb98b7aa0f4a1a2b9ad68ca86d&quot;,</span><br><span class="line">        &quot;Created&quot;: &quot;2021-08-01T03:10:14.298411164Z&quot;,</span><br><span class="line">        &quot;Path&quot;: &quot;/bin/sh&quot;,</span><br><span class="line">        &quot;Args&quot;: [</span><br><span class="line">            &quot;-c&quot;,</span><br><span class="line">            &quot;while true;do echo 111;sleep 1;done&quot;</span><br><span class="line">        ],</span><br><span class="line">        &quot;State&quot;: &#123;</span><br><span class="line">            &quot;Status&quot;: &quot;exited&quot;,</span><br><span class="line">            &quot;Running&quot;: false,</span><br><span class="line">            &quot;Paused&quot;: false,</span><br><span class="line">            &quot;Restarting&quot;: false,</span><br><span class="line">            &quot;OOMKilled&quot;: false,</span><br><span class="line">            &quot;Dead&quot;: false,</span><br><span class="line">            &quot;Pid&quot;: 0,</span><br><span class="line">            &quot;ExitCode&quot;: 137,</span><br><span class="line">            &quot;Error&quot;: &quot;&quot;,</span><br><span class="line">            &quot;StartedAt&quot;: &quot;2021-08-01T03:10:15.270494437Z&quot;,</span><br><span class="line">            &quot;FinishedAt&quot;: &quot;2021-08-01T03:12:01.287526932Z&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;Image&quot;: &quot;sha256:300e315adb2f96afe5f0b2780b87f28ae95231fe3bdd1e16b9ba606307728f55&quot;,</span><br><span class="line">        .....</span><br><span class="line">]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>进入当前正在运行的容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 我们通常容器都是使用后台方式运行的，需要进入容器，修改一些配置</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 方式1</span></span><br><span class="line">docker exec -it 容器id bashShell</span><br><span class="line"><span class="meta">#</span><span class="bash"> 方式2</span></span><br><span class="line">docker attach 容器id </span><br><span class="line"><span class="meta">#</span><span class="bash"> 区别</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> attach 正在执行的代码 进入正在执行的终端，不会启动新的进程</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">exec</span> 进入容器后开启一个新的终端，可以在里面操作</span></span><br></pre></td></tr></table></figure><p>从容器内拷贝文件到主机上</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 容器停止也可以拷贝，容器在数据就在</span></span><br><span class="line">docker cp 容器id:容器内路径 目的的主机路径</span><br><span class="line"><span class="comment"># 拷贝是一个手动过程，以后可以使用 -v 卷的技术 可以实现自动同步</span></span><br></pre></td></tr></table></figure><h3 id="命令小结"><a href="#命令小结" class="headerlink" title="命令小结"></a>命令小结</h3><p><img src="https://i.loli.net/2021/08/01/ha7fdJZE2jnNIOU.png" alt=""></p><p>现在学的是Images 和 Cotainer里的命令，其他的还没学</p><p><img src="https://i.loli.net/2021/08/01/FH1ZyqXoS3wx2Dg.png" alt=""></p><p><img src="https://i.loli.net/2021/08/01/YTrjW8M1c3I5sQ9.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Docker学习-一&quot;&gt;&lt;a href=&quot;#Docker学习-一&quot; class=&quot;headerlink&quot; title=&quot;Docker学习(一)&quot;&gt;&lt;/a&gt;Docker学习(一)&lt;/h1&gt;&lt;p&gt;文档：&lt;a href=&quot;https://docs.docker.com/</summary>
      
    
    
    
    
    <category term="Docker" scheme="http://example.com/tags/Docker/"/>
    
  </entry>
  
</feed>
