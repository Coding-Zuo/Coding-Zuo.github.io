<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Coding-Zuo</title>
  
  <subtitle>Coding And Studying</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-11-30T15:46:44.472Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Coding-Zuo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Parameter-Efficient Tuning by Manipulating Hidden States of Pretrained Language Models For Classification Tasks</title>
    <link href="http://example.com/2021/11/30/Parameter-Efficient-Tuning-by-Manipulating-Hidden-States-of-Pretrained-Language-Models-For-Classification-Tasks/"/>
    <id>http://example.com/2021/11/30/Parameter-Efficient-Tuning-by-Manipulating-Hidden-States-of-Pretrained-Language-Models-For-Classification-Tasks/</id>
    <published>2021-11-30T11:46:34.000Z</published>
    <updated>2021-11-30T15:46:44.472Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Parameter-Efficient-Tuning-by-Manipulating-Hidden-States-of-Pretrained-Language-Models-For-Classification-Tasks"><a href="#Parameter-Efficient-Tuning-by-Manipulating-Hidden-States-of-Pretrained-Language-Models-For-Classification-Tasks" class="headerlink" title="Parameter-Efficient Tuning by Manipulating Hidden States of Pretrained Language Models For Classification Tasks"></a>Parameter-Efficient Tuning by Manipulating Hidden States of Pretrained Language Models For Classification Tasks</h1><p>Parameter-Efficient tuning旨在通过优化一些引入的参数的同时，冻结 PLMs 来提取下游任务的知识。</p><p>连续的 prompt tuning 在输入的嵌入中预先加入一些可训练向量。是其中的一种方法，由于其有效性和效率而受到广泛关注。这个系列的方法可以被理解为对PLM内部的隐藏状态进行了非线性转换。</p><p>然而，一个自然的问题被忽略了：隐藏状态能否直接用于分类而不改变它们？在本文中，我们旨在通过提出一种简单的 tuning 方法来回答这个问题，这种方法只引入了三个可训练的向量。</p><p>首先，我们使用引入的向量整合不同层的隐藏状态。然后，我们将整合后的隐藏状态输入到一个特定任务的线性分类器中，以预测类别。</p><p>这个方案类似于ELMo利用隐藏状态的方式，只是他们将隐藏状态反馈给基于LSTM的模型。 虽然我们提出的tuning 方案很简单，但它取得了与 P-tuning 和 P-tuning v2等 prompt tuning 方法相当的性能，验证了原始隐藏状态确实包含分类任务的有用信息。此外，我们的方法在时间和参数数量上比  prompt tuning  有优势。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>为了将 PLM 重用于不同的任务，已经提出了越来越多的 parameter-efficient tuning 方法。 他们旨在通过仅优化少量额外参数，从冻结的 PLM 中挖掘特定于任务的信息。  </p><ul><li><p>Adapter tuning 建议将两个可训练的 Adapter 插入到每个 transformer 中，这些适配器由一些简单的变换组成。 虽然适配器达到了近乎 state-of-the-art 的性能，但它引入的额外参数数量仍然非常多，并且原始 transformer 的架构也需要修改。</p></li><li><p>《Few- shotqa: A simple framework for few-shot learning of question answering tasks using pre-trained text- to-text models》 试图规避预训练和 tuning 过程之间的错位，因其在 few-shot 情况下的优越性而受到广泛关注。</p></li><li><p>《Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing》包括离散prompt tuning 和连续prompt tuning，是最流行的方法。离散prompt tuning通常在输入句子中插入一些标记（出现在模型的词汇中），将任务重新表述为填空问题。然而，离散prompt tuning的调整涉及到巨大的人工努力，在高资源数据集上的表现更差，限制了其使用。连续 prompt tuning 不是插入离散的标记，而是在输入句子的嵌入中增加特定任务的可训练向量。在训练期间，只有这些特定任务的向量被优化。他们的实验表明，通过这些额外的向量，有可能从冻结的PLMs中获取知识。</p></li><li><p>《The power of scale for parameter-efficient prompt tuning》试图利用提示向量的意义，发现它们的词邻往往是相似的词，说明连续 prompt tuning 与离散 prompt tuning 完全不同。另外，如图1所示，与P-tuning将可训练向量预置到词嵌入中不同，我们尝试将这些向量预置到特定的 transformer 中。图2描述了四个任务的准确性。我们可以看到，将向量预置到任何一层，甚至最后一层，都可以取得类似的结果。我们从这个现象中得到两个启发。(i) 当可训练的向量被插入到最后一个 transformer 时，P-tuning甚至也能发挥作用。这是否意味着这些原始的隐藏状态已经包含了大部分甚至所有用于分类的信息？(ii) 当向量被预置到第0层时，可能无法获得最好的准确性。这与ELMo类似，表现为不同层对下游任务有不同的权重</p><p><img src="https://i.loli.net/2021/11/30/ZBlXyOH7vKtF48A.png" alt=""></p></li></ul><p>在本文中，通过提出一种只引入三个可训练向量的简单方法，表明分类信息可以很容易地提炼出来。</p><p>这个方法首先获得了输入的所有隐藏状态，并学习了一个softmax归一化的权重向量来堆叠跨层的隐藏状态。其次，一个软掩码向量被用来选择隐藏状态中对下游任务有用的维度子集。最后，进行仅由一个可训练向量组成的自我注意操作，以输出用于分类的状态。</p><p>我们发现，分类信息在表面，这意味着没有必要使用复杂的分类头，如LSTM，CNN。我们的分类头只包含一个线性转换，然后是一个softmax函数。</p><p>我们在各种任务上进行了实验，结果表明，我们的方法可以达到与P-tuning和P-tuning v2所取得的性能相当。这证实了原始的隐藏状态确实包含了分类所需的信息，而且这种信息可以通过简单的线性转换来提取。我们希望这一发现能够帮助推进对连续 prompt tuning  方案的理解。</p><h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><p>在这一节中，我们首先介绍了一些 tuning 方案的背景，介绍了ELMo和 transformer 家族语言模型如何应用于下游分类任务。</p><h3 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h3><p>ELMo 的骨干是 L 层双向LSTM。经过预训练，当有一个下游任务时，ELMo学习了一个特定任务的跨层隐藏状态的线性组合，并将这些隐藏状态输入到基于 LSTM 的模型，以输出一个输入的目标：</p><script type="math/tex; mode=display">e_{k} = \gamma\sum_{i=0}^{L} s_jh_{k,j}</script><script type="math/tex; mode=display">\hat y = Model ([e_1,e_2,...,e_n])</script><p>其中 $h<em>{k,j}$  是第 $j$ 层中第 $k$ 个token的隐藏状态。$h</em>{k,0}$是第 $k$ 个token词嵌入层表示。$(s_0,s_1,…,s_L)$ 是任务特定的softmax归一化权重的表示，它们在训练下游任务时被优化。这些权重允许模型根据下游任务的属性，利用不同层次的表征（例如，语法、语义）。$\gamma$ 是可训练标量，其用于辅助优化过程。$e_k$ 是第 $k$个词的最终加权嵌入。最后，$[e_1, e_2, …, e_n]$被输入到量身定做的模型（通常是一个复杂的网络，如LSTM），以预测标签或生成特定任务的目标句。</p><h3 id="The-Transformer-Family"><a href="#The-Transformer-Family" class="headerlink" title="The Transformer Family"></a>The Transformer Family</h3><p>在根据下游任务调整PLM时，最常用的方法叫做微调，把PLM和特定任务的分类器 $C$ 一起优化:</p><script type="math/tex; mode=display">min_{\theta,\beta} \sum_{x\in D} L(C_{\beta}(E_{\theta}(x)), y)</script><p>其中 $C、E$分别代表特定任务的分类头和编码器（一个PLM）。$β，θ$ 分别是 $C$ 和 $E$ 的参数。Fine tuning LM通常可以获得令人满意的性能。然而，在新数据按顺序出现的情况下，这种调整方案不再起作用。 因为当一个新的数据集出现时，我们要么重新训练所有的数据，要么为新的数据训练一个单独的模型。由于耗费大量时间和存储，这两种方法都是不可接受的。因此，研究人员试图通过引入一些可训练的参数，从冻结的PLM中提炼出知识。</p><p>Adapter tuning 和连续  prompt tuning 是两种普遍的方法。这里，我们只介绍第二种。有许多关于连续 prompt tuning 的工作，我们对其中的两个进行了详细介绍</p><ul><li><p>P-tuning 对于一个输入句子 $x=(x_1,x_2,…,x_n)$，P-tuning在 $x$ 的词嵌入矩阵中加入K个向量，用 $P（x）=[w_0,p_1,p_2,…,p_K,w_1,…,w_n]$表示。$w_0$ 是[CLS] token 的嵌入。$P(x)$ 被输入到PLM的其余各层以预测 $x$ 的标签，优化目标是:</p><script type="math/tex; mode=display">min_{\beta, p_{1,...K}} \sum_{x\in D} L(C_{\beta}(E(P(x))) ,y)</script><p>我们可以看到，在训练期间，只有预置的向量和分类器被优化。虽然P-tuning的想法受到离散 prompt tuning 的启发，但这两者并没有显示出任何共同的特征。例如，在将 $p_i,…, p_K$ 映射到词汇中最接近的词后，这些词往往是类似的词（如technology，technologies），这并没有显示出连续 prompt tuning 背后的机制是像离散 prompt tuning 那样利用PLM的语言建模特性。最近的一项工作（He et al., 2021）揭示了P-tuning和适配器之间的联系，他们发现P-tuning相当于原始隐藏状态和由预置向量产生的新隐藏状态的加权平均值。应该注意的一点是，这些向量是由数据集中的所有样本共享的。同样的新隐藏状态可以应用于所有不同的样本，这有点不可思议。一个可能的解释是，原始的隐藏向量已经包含了用于分类的信息，而这正是我们论文的重点。</p></li><li><p>P-tuning v2  是一种变种的P-tuning，可以扩展到困难的任务，与P-tuning相比，取得了更好的性能。与P-tuning不同的是，P-tuning将向量插入到单词嵌入矩阵中，而这些向量在后面几层的隐藏状态是根据前一层的表示来计算的，P-tuning v2利用多层提示，将一组可训练的向量插入到PLM的每层。换句话说，第 $l$ 层的原始隐藏状态会受到第 $l - 1$ 层的提示向量的影响。由于引入了更多的可训练向量，这种修改使得每个任务的容量更大。</p></li></ul><h2 id="Proposed-Approach"><a href="#Proposed-Approach" class="headerlink" title="Proposed Approach"></a>Proposed Approach</h2><p><img src="https://i.loli.net/2021/11/30/ZCm7KItPuDd8bpQ.png" alt=""></p><p>该方法直接操纵不同层的隐藏状态。与通过改变隐藏状态来提炼有用信息的 adapters 和 prompt tuning 方法不同，我们对原始隐藏状态进行操作。由于有许多方法可以整合这些隐藏状态，我们只介绍最简单的方法，由三部分组成。</p><h3 id="Layer-Level-Weighted-Addition"><a href="#Layer-Level-Weighted-Addition" class="headerlink" title="Layer-Level Weighted Addition"></a>Layer-Level Weighted Addition</h3><p>这一步与公式1类似，只是我们不使用标量 $\gamma $。形式上，给定输入x，我们首先获得隐藏状态:</p><script type="math/tex; mode=display">\{H_i\}_{i=0}^L = \{h_{i,0},..., h_{i,N}\}_{i=0}^L = E(x)</script><p>其中 $L$ 为tansformer 层数， 第 $0$ 层是词嵌入层，$N$ 是 $x$ 的长度。每个 $h_{i,j}$ 是一个 $d$ 维向量，每个 $H_i$ 是一个 $d×（N+1）$维矩阵。然后，我们给每个层分配一个权重，因为以前的工作表明，低层、中层、顶层捕捉的信息粒度不同，不同的下游任务可能对这些层有不同的关注。</p><p>我们通过引入第一个可训练向量 $v_1\in R^{L+1}$来实现这一目标:</p><script type="math/tex; mode=display">H = \{h_0, h_1,..., h_N\} = \sum_{i=0}^L s_i H_i , \ \ \ s = softmax(v_1)</script><h3 id="Subspace-Mining"><a href="#Subspace-Mining" class="headerlink" title="Subspace Mining"></a>Subspace Mining</h3><p>我们认为，对于一个d 维的隐藏状态，存在一个维度为 $d_1$ 的子空间，它包含下游任务的区分信息。用二进制掩码向量选择这样的子空间是一种选择，但它在优化方面有困难《 Masking as an efficient alternative to finetuning for pretrained language models》。在此，我们通过引入第二个可训练向量 $v_2\in R^d$ 来采用软屏蔽策略:</p><script type="math/tex; mode=display">H = m \otimes H , \ \ \ m = sigmoid(v_2)</script><p>如果下游的任务是一个序列标记 任务，如命名实体识别，H可以直接用于分类。然而，对于情感分类、自然语言推理等任务，应该进行进一步的处理步骤。</p><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>使用 Self-Attention 来聚合矩阵 $H$ 。但是这种 Self-Attention  需要很多参数, 引入第三个可训练向量 $v_3\in R^d$ :</p><script type="math/tex; mode=display">h = H \cdot w , \ \ \ w=softmax(v_3\cdot H)</script><h3 id="Task-Specific-Classification"><a href="#Task-Specific-Classification" class="headerlink" title="Task-Specific Classification"></a>Task-Specific Classification</h3><p>有了 $h$ 和 $H$ ，我们可以把它们输入到特定任务的分类头。为了说明原始隐藏状态包含了大部分（如果不是全部）分类信息，而且这些信息都在表面，我们只进行了线性变换和softmax操作来输出标签，对于序列标签任务:</p><script type="math/tex; mode=display">y = argmax(softmax(WH))</script><p>对于情感分类 ：</p><script type="math/tex; mode=display">y = argmax(softmax(Wh))</script><p>在训练过程中，只有v1、v2、v3和w是被操作的。因此，与P-tuning和P-tuning v2相比，我们的方法在内存和时间方面更加有效，因为额外的参数不参与PLM内部的计算，而且输入长度也没有增加。在第4.2节，我们将提供一个详细的分析。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2021/11/30/DsPSl6HW8bNpLAE.png" alt=""></p><p><img src="https://i.loli.net/2021/11/30/EDne5dYPKgl8vN4.png" alt=""></p><p>为了考察这些调谐方法在额外参数数量上的表现，我们把P-tuning中的提示向量数量减少到3个，把P-tuning v2中的提示向量数量减少到1个，结果见图4。我们可以发现Ours在整体上优于P-tuning，与P-tuning v2相当。我们还尝试增加Ours的参数数量，即引入多个V3来形成多头自关注，但发现性能没有改善。这可能是我们方法的一个缺点。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Parameter-Efficient-Tuning-by-Manipulating-Hidden-States-of-Pretrained-Language-Models-For-Classification-Tasks&quot;&gt;&lt;a href=&quot;#Parameter</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>P-Tuning v2，Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</title>
    <link href="http://example.com/2021/11/25/P-Tuning-v2-Prompt-Tuning-Can-BeComparable-to-Fine-tuning-Universally-Across-Scales-and-Tasks/"/>
    <id>http://example.com/2021/11/25/P-Tuning-v2-Prompt-Tuning-Can-BeComparable-to-Fine-tuning-Universally-Across-Scales-and-Tasks/</id>
    <published>2021-11-25T09:29:41.000Z</published>
    <updated>2021-11-26T12:29:43.360Z</updated>
    
    <content type="html"><![CDATA[<h1 id="P-Tuning-v2-Prompt-Tuning-Can-Be-Comparable-to-Fine-tuning-Universally-Across-Scales-and-Tasks"><a href="#P-Tuning-v2-Prompt-Tuning-Can-Be-Comparable-to-Fine-tuning-Universally-Across-Scales-and-Tasks" class="headerlink" title="P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"></a>P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</h1><p>Prompt tuning 只用冻结的语言模型来调整连续的prompts，大大减少了训练时每个任务的存储和内存使用。但</p><ul><li><p>prompt tuning 对于正常大小的预训练模型来说表现并不理想。</p></li><li><p>现有的 prompt tuning 方法不能处理困难的序列标记任务，缺乏普适性。</p></li></ul><p>P-Tuning-v2 与微调的性能相匹配，并且只有0.1%-3%的调整参数。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>微调，为目标任务更新整个模型参数集。虽然微调能获得良好的性能，但在训练过程中却很耗费内存，因为必须存储所有参数的梯度和优化器状态。</p><p>此外，微调需要在推理过程中为每个任务保留一份模型参数，这很不方便，因为预训练的模型通常很大。</p><p><strong>Prompting</strong> 另一方面，冻结PLMs的所有参数并使用自然语言 prompts 来查询语言模型。例如，对于情感分析，我们可以将一个样本与一个 prompts 串联起来 “这部电影是[MASK]”，并要求预训练的语言模型预测被mask的 token。然后，我们可以使用 “好 “和 “坏 “是被 MASK 的标记的预测概率来预测样本的标签。Prompting 完全不需要训练，只需存储一份模型参数。然而，与微调相比，提示 在许多情况下会导致次优的性能（Liu等人，2021b；Lester等人，2021）。</p><p><strong>Prompt tuning</strong> 是一种只调谐连续 prompts 的想法。具体来说，Liu等人（2021b）；Lester等人（2021）提出在输入词嵌入的原始序列中加入可训练的连续嵌入。这些连续嵌入（也称为连续 prompts ）类似于 prompting 中离散的手动签名 prompts。在训练过程中，只有连续的 prompts 被更新。虽然 prompt tuning 在许多任务上比 prompting 有提高 （Liu等人，2021b；Lester等人，2021），但当模型规模较小，特别是小于100亿个参数时，它仍然不如 fine-tuning（Lester等人，2021）。</p><p>从技术上讲，P-tuning v2可以被看作是 prefix-tuning  的优化版本，适用于NLU。最显著的改进来自于使用深度 prompt tuning，即对预训练模型的每一层应用连续的 prompt。深度 prompt tuning 增加了连续 prompt 的能力，并缩小了在各种设置中进行微调的差距，特别是对于小模型和困难任务。此外，还提出了一些优化和实施的细节，以进一步提高结果。</p><h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><h3 id="NLU-Tasks"><a href="#NLU-Tasks" class="headerlink" title="NLU Tasks"></a>NLU Tasks</h3><p>在这项工作中，将NLU的挑战分为两个系列：</p><ul><li>Simple NLU tasks ：涉及单一标签的分类。GLUE（Wang等人，2018）和Super- GLUE（Wang等人，2019）的大多数数据集，包括文本分类（如SST-2）、自然语言推理（NLI，如MNLI-m，RTE）、多选题回答（如BoolQ）等，都属于这一类。</li><li>Hard sequence NLU tasks ：涉及对一连串标签的分类。它们大多是与信息提取有关的问题，如开放式信息提取、命名实体识别、提取式问题回答和语义角色标签。</li></ul><h3 id="Prompt-Tuning"><a href="#Prompt-Tuning" class="headerlink" title="Prompt Tuning"></a>Prompt Tuning</h3><p>Prompt tuning（Lester等人，2021），或P-tuning（Liu等人，2021b），引入了可训练的连续prompts，作为骨干语言模型的参数被冻结时对NLU的自然语言 prompts 的替代。例如，让 $V$ 指的是语言模型 $M$ 的词汇表，$e$ 作为 $M$ 的嵌入函数。</p><p>要把一篇电影评论 x=”Amazing movie!” 分类为正面或负面，很自然地就会想到在评论中加入一个 prompts  “It is [MASK]”，并生成MASK token被预测为 “好 “或 “坏 “的条件概率作为分类。在这种情况下，prompt tokens  {“It”、”is”、”[MASK]”} 都属于模型的词汇表 $V$，而输入嵌入序列将是：</p><script type="math/tex; mode=display">[e(x), e("It"), e("is"), e("[MASK]")]</script><p>然而，由于模型 $M$ 在本质上是连续的，从优化的角度来看，人们永远不可能用离散的自然 prompts 来达到最佳效果。相反，P-tuning建议用可训练的连续 embeddings $[h_0, …, h_i]$ 代替 prompt tokens ，并将输入序列变成:</p><script type="math/tex; mode=display">[e(x), h_0,...,h_i, e("[MASK]")]</script><p>并因此可以进行不同的优化（参考图2（a））。在骨干预训练模型的参数被冻结的严格约束下，在简单的NLU任务和知识探测中，prompt tuning 已被证明具有与100亿参数模型的 fine-tuning 相当的性能。</p><h2 id="P-Tuning-v2"><a href="#P-Tuning-v2" class="headerlink" title="P-Tuning v2"></a>P-Tuning v2</h2><p><img src="https://i.loli.net/2021/11/25/hAoQrbPSGTtIsnz.png" alt=""></p><h3 id="Lack-of-Universality"><a href="#Lack-of-Universality" class="headerlink" title="Lack of Universality"></a>Lack of Universality</h3><p>在许多NLP应用中，Prompt tuning 和  P-tuning 已经被证明是相当有效的。然而，考虑到缺乏普遍性， P-tuning 还不是微调的全面替代方法。</p><h3 id="Lack-of-universality-across-scales"><a href="#Lack-of-universality-across-scales" class="headerlink" title="Lack of universality across scales"></a>Lack of universality across scales</h3><p> Lester等人（2021）表明，当模型规模超过100亿个参数时，prompt tuning 可以与微调相媲美。但是对于那些较小的模型（从100M到1B），prompt tuning和微调的表现有很大差异，这大大限制了prompt tuning的适用性。</p><h3 id="Lack-of-universality-across-tasks"><a href="#Lack-of-universality-across-tasks" class="headerlink" title="Lack of universality across tasks."></a>Lack of universality across tasks.</h3><p>尽管Lester等人（2021年）和 P-tuning 在GLUE和SuperGLUE等NLU基准上显示出优越性，但它们在另一大类序列NLU任务（即序列标签）上的有效性却没有得到验证。</p><p>首先，序列标签需要预测一连串的标签，而不是单一的标签。其次，序列标签通常预测的是无实际意义的标签，这对变成有效的言语者来说可能是个挑战。</p><h3 id="Deep-Prompt-Tuning"><a href="#Deep-Prompt-Tuning" class="headerlink" title="Deep Prompt Tuning"></a>Deep Prompt Tuning</h3><p><img src="https://i.loli.net/2021/11/25/42eZyjHrnuWAmNv.png" alt=""></p><p>Prefix-tuning（Li and Liang, 2021）最初是为自然语言生成（NLG）任务提出的，但我们发现它对NLU也非常有效。我们描述了一个适合NLU的 Prefix-tuning 版本。</p><p>在(Lester et al., 2021)和P-tuning中，连续 prompts 只被插入到输入嵌入序列中（参考图2(a)），用于 transformer’s 第一层。在接下来的  transformer 中，插入连续 prompts 的位置的嵌入是由之前的transformer 层计算出来的，这可能导致两个可能的优化挑战。</p><ul><li>可调整的参数数量有限。大多数语言模型目前只能支持512的最大序列长度（由于注意力的二次计算复杂性的成本）。如果我们另外扣除我们的上下文的长度（例如，要分类的句子），那么我们用连续提示来填充的长度是有限的。</li><li>在用非常深的 transformers 进行调整时，稳定性有限。随着 transformers 的不断深入，由于许多中间层的计算（具有非线性激活函数），来自第一个transformers 层的提示的影响可能是意想不到的，这使得我们的优化不是一个非常平稳的优化。</li></ul><p>鉴于这些挑战，P-tuning v2利用多层 prompts（即深度prompt tuning）作为 prefix-tuning（Li and Liang, 2021）中的提示（参见图2（b）），作为对P-tuning和Lester等人（2021）的重大改进。不同层的 prompt 作为前缀 tokens 添加到输入序列中，并独立于其他层（而不是由之前的 transformers 层计算）。一方面，通过这种方式，P-tuning v2 有更多的可调整的特定任务参数（从0.01%到0.1%-3%），以允许更多的每个任务容量，而它仍然比完整的预训练语言模型小得多；另一方面，添加到更深层的 prompt（例如图2中的LayerN Prompts）可以对输出预测产生更直接和显著的影响，而中间的transformers层更少。</p><h3 id="Optimization-and-Implementation"><a href="#Optimization-and-Implementation" class="headerlink" title="Optimization and Implementation"></a>Optimization and Implementation</h3><p><strong>Optimization：Reparameterization</strong>。以前的方法利用重新参数化功能来提高训练速度、鲁棒性和性能（例如，MLP的 prefix-tuning 和 LSTM的P调整）。然而，对于NLU任务，我们发现这种技术的好处取决于任务和数据集。对于一些数据集（如RTE和CoNLL04），MLP的重新参数化带来了比嵌入更稳定的改进；对于其他的数据集，重新参数化可能没有任何效果（如BoolQ），有时甚至更糟（如CoNLL12）。</p><p><img src="https://i.loli.net/2021/11/25/T1axhfKtEoXPZOR.png" alt=""></p><p><strong>Optimization: Prompt length</strong> Prompt length在 prompt tuning 方法的超参数搜索中起着核心作用。在我们的实验中，我们发现不同的理解任务通常以不同的 Prompt length 达到最佳性能，这与 prefix- tuning 的发现一致，不同的文本生成任务可能有不同的最佳 Prompt length。</p><p>从图3中，我们观察到，对于简单的NLU任务，通常，较短的prompts 就能获得最佳性能；对于困难的序列任务，通常，比100长的 prompts 语会有帮助。<br>我们还发现，重新参数化与 Prompt length 长度有着密切的联系。例如，在RTE、CoNLL04和BoolQ中，MLP的重新参数化比嵌入更早达到最佳结果。这一结论可能有助于对P-tuning的优化特性进行一些思考。</p><p><strong>Optimization: Multi-task learning.</strong> 多任务学习对我们的方法来说是可选的，但可能是相当有帮助的。一方面，连续prompts 的随机惯性给优化带来了困难，这可以通过更多的训练数据或与任务相关的无监督预训练来缓解（Gu等人，2021）；另一方面，连续 prompts 是跨任务和数据集的特定任务知识的完美载体。 我们的实验表明，在一些困难的序列任务中，多任务学习可以作为P-tuning v2的有益补充，表示为MPT-2。</p><p><strong>Implementation: [CLS] and token classification, rather than verbalizers.</strong>  Verbalizer 一直是 Prompt tuning 的核心组成部分，它将一类的labels变成有意义的词，以利用预训练语言模型head 。尽管它在少数情况下有潜在的必要性，但在全数据监督的情况下，verbalizer 不是必须的。它阻碍了 Prompt tuning 在我们需要无实际意义的标签和语义嵌入的场景中的应用。因此，P-tuning v2回到了传统的[CLS]标签分类（参照图2）范式，采用随机初始化的线性头。见第4.4节中的比较。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在这项工作中，”prompt tuning”、”P-tuning”、”P-tuning v2 “和 “多任务P-tuning v2 “的所有结果都是通过冻结transformer的参数，只调整连续prompts。特定任务参数的比率（例如0.1%）是通过比较连续prompts 的参数和transformer的参数得出的。只有 “微调 “的结果是通过调整transformer的参数（不使用连续提示）得到的。</p><p>另外需要注意的是，我们的实验都是在全数据监督下的学习环境下进行的，而不是在少量的学习环境下进行的，这一点很重要，因为我们利用的一些特性（例如，使用具有线性head 的类标签而不是具有LM头的言语者）只可能在监督下进行。</p><p><img src="https://i.loli.net/2021/11/25/q12FUMtevPgoI3V.png" alt=""></p><h3 id="P-tuning-v2-Across-Scales"><a href="#P-tuning-v2-Across-Scales" class="headerlink" title="P-tuning v2: Across Scales"></a>P-tuning v2: Across Scales</h3><p>表1展示了P-tuning v2在不同模型规模下的表现。对于简单的NLU任务，如SST-2（单句分类），Lester等人（2021）和P-tuning在较小的规模下没有显示出明显的劣势。但是，当涉及到复杂的挑战，如自然语言推理（RTE）和多选题回答（BoolQ）时，它们的性能会非常差。相反，P-tuning v2在所有任务中以较小的规模与微调性能相匹配。令我们惊讶的是，P-tuning v2在RTE中的表现明显优于微调，特别是在BERT中。</p><p>就较大尺度（2B到10B）的GLM（Du等人，2021）而言，P-调谐&amp;Lester等人（2021）与精细调谐之间的差距逐渐缩小了。在10B尺度上，我们有一个类似于（Lester等人，2021）报告的观察结果，即及时调谐变得与精细调谐竞争。然而，P-tuning v2在所有尺度上都与微调相当，但与微调相比，只需要0.1%的任务特定参数。<br>此外，我们观察到，在一些数据集中，RoBERTa-large的性能比BERT-large差。部分原因是，我们根据经验发现及时调谐对超参数相当敏感，有时调谐会被困住。P-tuning v2在调谐过程中可以更加稳定和稳健。关于超参数的更多细节，请参考我们的代码库。</p><h3 id="P-tuning-v2-Across-Tasks"><a href="#P-tuning-v2-Across-Tasks" class="headerlink" title="P-tuning v2: Across Tasks"></a>P-tuning v2: Across Tasks</h3><p>在第4.2节中，我们讨论了P-tuning v2的一贯性，无论何种尺度，其性能都与微调相当。然而，GLUE和SuperGLUE的大多数任务都是相对简单的NLU问题。另一个重要的硬NLU挑战系列是序列标签，它与一些更高级的NLP应用有关，包括开放信息提取、阅读理解等。</p><p>为了评估P-tuning v2在这些硬NLU挑战上的能力，我们选择了三个典型的序列标签任务。名称实体识别、外部问题回答（QA）和语义角色标签（SRL），共八个数据集。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;P-Tuning-v2-Prompt-Tuning-Can-Be-Comparable-to-Fine-tuning-Universally-Across-Scales-and-Tasks&quot;&gt;&lt;a href=&quot;#P-Tuning-v2-Prompt-Tuning-</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>PADA: A Prompt-based Autoregressive Approach for Adaptation to Unseen Domains </title>
    <link href="http://example.com/2021/11/23/PADA-A-Prompt-based-Autoregressive-Approach-for-Adaptation-to-Unseen-Domains/"/>
    <id>http://example.com/2021/11/23/PADA-A-Prompt-based-Autoregressive-Approach-for-Adaptation-to-Unseen-Domains/</id>
    <published>2021-11-23T01:49:35.000Z</published>
    <updated>2021-11-23T06:48:57.620Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PADA-A-Prompt-based-Autoregressive-Approach-for-Adaptation-to-Unseen-Domains"><a href="#PADA-A-Prompt-based-Autoregressive-Approach-for-Adaptation-to-Unseen-Domains" class="headerlink" title="PADA: A Prompt-based Autoregressive Approach for Adaptation to Unseen Domains"></a>PADA: A Prompt-based Autoregressive Approach for Adaptation to Unseen Domains</h1><p>PADA: A Prompt-based Autoregressive Domain Adaptation algorithm, based on the T5 model</p><p>作者提出解决的是这个领域适应问题，即一个算法在几个源域上进行训练，然后应用于在训练时未知的未见过的领域的样本（更准确的说这其实是领域泛化问题）。</p><p>在训练时，没有任何样本，不管是有标签的还是无标签的，或者关于目标领域的任何其他知识。</p><p>给定一个测试样本，PADA首先生成一个独特的 prompt，然后以这个 prompt 为条件，在NLP任务方面给这个样本贴上标签。</p><p>prompt 是一个长度不受限制的序列，由预先定义的领域相关特征（DRFs）组成，这些特征是每个源域的特征。直观地说，prompt 是一个独特的签名，它将测试实例映射到源域所跨越的语义空间。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>NLP算法往往依赖于一个开创性的假设，即训练集和测试集来自相同的基础分布。不幸的是，这个假设往往不成立，因为文本可能来自许多不同的来源，每个来源都有独特的分布属性。由于超出训练分布的泛化仍然是一个基本的挑战，NLP算法在应用于非分布的应用时，会出现明显的退化。</p><p>领域适应（DA）明确地解决了上述挑战，努力改善NLP算法的 out-of- distribution 。DA算法在源领域的标注数据上进行训练，以有效地应用于各种目标领域。</p><p>多年来，人们为DA挑战付出了相当大的努力，专注于目标领域在训练时是已知的（例如，通过标记或未标记的数据），但仍未得到充分体现的各种情况（Roark和Bacchiani，2003；Daumé III和Marcu，2006；Reichart和Rappoport，2007；McClosky等人，2010；Rush等人，2012；Schnabel和Schütze，2014）。然而，对训练时未知的任何可能的目标领域的适应性挑战还没有得到充分的探索。</p><p>PADA具有独特的建模优势，因为目标感知算法通常需要为每个目标域训练一个单独的模型，导致整体解决方案效率低下。</p><p>直观地说，通过整合来自几个源域的知识，可以实现对未见过的事物更好的泛化。PADA：基于 Prompt 的自回归领域适应算法，它利用自回归语言模型（T5），并包括一个适应于多个源领域的新型 Prompt 机制。给定一个来自任何未知领域的新样本，该模型首先生成属于熟悉的（源）领域并与给定样本相关的属性。然后，在模型执行下游任务时，生成的属性被用作 prompts。</p><p>为了产生有效的 prompts ，我们从以前关于 pivot features 的工作中得到启发（Blitzer等人，2006；Ziser和Reichart，2018a；Ben-David等人，2020），定义了领域相关特征（DRFs）的集合。DRFs是与源域之一密切相关的特征，编码了特定领域的语义。我们利用各个源域的DRFs，以跨越它们的共享语义空间。这些DRFs共同反映了源域之间的相似性和差异性，以及特定领域的知识。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>在DA研究中，有两种突出的设置：有监督的和无监督的。有监督的算法利用来自目标领域的稀缺标记的例子（Daumé III, 2007），而无监督的方法只假设源标记的数据和未标记的源和目标数据在手（Blitzer等人，2006）。</p><p>我们首先描述无监督DA的研究，重点是基于pivot-based的方法。然后，我们继续研究多源的DA方法，重点是专家混合模型。最后，我们描述了自回归语言模型和我们采用T5进行DA的独特方式。</p><h3 id="Unsupervised-Domain-Adaptation-UDA"><a href="#Unsupervised-Domain-Adaptation-UDA" class="headerlink" title="Unsupervised Domain Adaptation (UDA)"></a>Unsupervised Domain Adaptation (UDA)</h3><p>随着深度神经网络（DNN）建模的突破，DA社区的注意力已经被引向了表征学习方法。其中一项工作是采用基于DNN的自编码器来学习潜在的表征。这些模型在无标签的源数据和目标数据上进行训练，并有输入重建损失（Glorot等人，2011；Chen等人，2012；Yang和Eisenstein，2014；Ganin等人，2016）。</p><p>另一个分支采用 pivot 特征来弥补源域和目标域之间的差距（Blitzer等人，2006，2007；Pan等人，2010）。支点特征对感兴趣的任务很突出，在源域和目标域都很丰富。最近，Ziser和Reichart（2017，2018b）将这两种方法结合起来。后来，Han和Eisenstein（2019）提出了一种预训练方法，随后Ben-David等人（2020）提出了一种基于 pivot-based 的变体，用于预训练语境词嵌入。</p><p>最重要的是，UDA模型假定在训练过程中可以获得来自目标领域的无标签数据。我们认为这是对超越训练分布的泛化目标的一个轻微的放松。此外，这个定义在工程上有缺点，因为每个目标域都需要一个新的模型。为此，我们追求任何领域的适应性设置，即在训练时无法获得未标记的目标数据。</p><p>我们从 pivot-based 的模型中得到启发。pivot 的定义依赖于标记的 源域数据 和 未标记的源域 和 目标域数据。特别是，好的 pivot 是与任务标签相关的。相反，我们定义了任务变量DRF，这些特征与领域的身份高度相关。由于领域与单词高度相关，我们的DRFs在本质上是词汇性的。</p><p>虽然我们的方法可以在单一的源域中运行，但我们利用多个源域来促进对未知目标域的推广。我们接下来讨论多源DA。</p><h3 id="Multi-Source-Domain-Adaptation"><a href="#Multi-Source-Domain-Adaptation" class="headerlink" title="Multi-Source Domain Adaptation"></a>Multi-Source Domain Adaptation</h3><p>大多数现有的多源DA方法遵循无监督DA的设置定义，同时考虑一个以上的源域。一个突出的方法是融合几个来源的模型。早期的工作是为每个领域训练一个分类器，并假设所有的源领域对测试样本都是同等重要的（Li和Zong，2008；Luo等人，2008）。最近，基于对抗的方法使用未标记的数据，将源域与目标域对齐（Zhao等人，2018；Chen和Cardie，2018）。同时，Kim等人（2017年）和Guo等人（2018年）根据目标实例和每个源域之间的关系，明确地对专家混合模型（MoE）模型进行加权。然而，Wright和Augenstein（2020）在这项工作之后，在基于Transfomers的MoE上测试了各种加权方法，发现加权方法非常有效。</p><p>我们认识到所提出的MoE解决方案的两个局限性。首先，它是不可扩展的，因为它要求每个源域都有一个专家，导致模型参数随着源域数量的增加而增加（通常是线性的）。第二，领域专家是针对特定领域的知识进行调整的。然而，测试实例可能来自未知的领域，并可能反映出来源的复杂组合。为了解决这个问题，MoE使用启发式方法将专家的预测集合起来，例如简单的平均数或基于领域分类器预测的加权平均数。我们的结果表明，这种方法是次优的。</p><p>在这项工作中，我们训练一个模型，该模型在所有领域中共享其参数。此外，我们对适应任何目标领域感兴趣，这样在训练时就不需要知道关于潜在目标领域的信息。上述一些工作（Wright和Augenstein，2020年）事实上避免了利用目标数据，因此它们适合任何领域的设置，并形成了我们的两个基线。然而，与这些作品相比，我们认为这个定义是本研究的核心部分。</p><h3 id="Autoregressive-Language-Modeling"><a href="#Autoregressive-Language-Modeling" class="headerlink" title="Autoregressive Language Modeling"></a>Autoregressive Language Modeling</h3><p>以前的工作在训练基于Transformer（Vaswani等人，2017）的语言模型时主要考虑两种方法。第一种实现了经典的马尔科夫语言建模方法，通过训练Transformer解码器模型，根据其之前的上下文自动生成下一个词（Radford和Narasimhan，2018；Radford等人，2019）。第二种是将自回归语言建模方法作为一个掩盖的标记预测任务，通过训练Transformer的编码器来得出上下文的单词嵌入（Devlin等人，2019；Liu等人，2019；Sanh等人，2019）。当为一个新的任务微调模型时，一个自定义的解码器被实现，并与模型的预训练编码器联合训练。</p><p>最近提出了第三种方法，试图将以前的方法中的优点结合起来。它提出训练一个完整的Transformer（编码器-解码器）语言模型，从输入序列中自动渐进地生成屏蔽的、缺失的或扰乱的标记跨度，作为一个序列到序列的任务（Raffel等人，2020；Lewis等人，2020b）。最近，Raffel等人（2020）提出了T5，一个基于转化器的模型，提出了文本到文本的迁移学习方法。T5基本上将所有的任务视为生成性的，同时利用一个提示短语来表示正在执行的具体任务。结合其文本到文本的方法，T5在许多鉴别性和生成性任务中显示出其优越性，同时消除了对特定任务网络结构的需求。</p><p>T5的一个特别有趣和有用的特点是其 prompting 机制。prompt 短语通常用于指示模型要执行的任务，被作为前缀添加到所有与任务相关的输入实例中。最近的工作也探索了这种 prompt 机制，以使语言模型适应不同的目的（Brown等人，2020年），激发情感或话题相关的信息（Jiang等人，2020年；Sun和Lai，2020年；Shin等人，2020年），或作为一种有效的微调方法（Li和Liang，2021年）。在这项工作中，我们利用T5的提示机制，作为激发模型编码与每个测试例子相关的特定领域特征的方式。</p><h2 id="Any-Domain-Adaptation"><a href="#Any-Domain-Adaptation" class="headerlink" title="Any-Domain Adaptation"></a>Any-Domain Adaptation</h2><h3 id="DA-and-Transfer-Learning"><a href="#DA-and-Transfer-Learning" class="headerlink" title="DA and Transfer Learning"></a>DA and Transfer Learning</h3><p>一个预测任务被定义为 $T = {Y}$，其中 $Y$ 是任务的标签空间。定义 $X$ 为特征空间，$P(X)$ 是 $X$ 上的边缘分布，$P(Y)$ 对 $Y$ 的先验分布。domain 被定义为 $D^T={X, P(X), P(Y),P(Y|X)}$</p><p>DA是迁移学习的一个特殊案例，即过渡性转移学习，其中 $T_S$ 和 $T_T$，即源任务和目标任务，是相同的。</p><p>$D<em>S^T, D^{T}</em>{T}$ 是源域和目标域，至少有一个基础概率分布不同，即$P(X),P(Y) \ or \ P(Y|X)$。</p><p>DA 的目标是从一组源域 ${ D<em>{S_i}}^K</em>{i=1}$ 中学习一个函数 $f$ ，这个函数可以很好地泛化到一组目标域$ { D<em>{T_i} }^M</em>{i=1}$ </p><h3 id="The-Any-Domain-Setting"><a href="#The-Any-Domain-Setting" class="headerlink" title="The Any-Domain Setting"></a>The Any-Domain Setting</h3><p>我们专注于为一个给定的任务建立一个能够适应任何领域的算法。为此，我们假设在训练时对目标领域 $D_T$ 的了解为零。因此，我们稍微修改了无监督多源领域适应的经典设置，假设我们不知道或无法获得目标领域的标记或未标记数据。</p><p>我们只假设可以从K个源域中获得标记的训练数据  $ {D<em>{s_i}}$，其中 $D</em>{S_i} = {(x_t^{S_i}, y_t^{S_i})}$。目标是学习一个仅使用源域数据的模型，该模型可以很好地概括到一个未知的目标域。</p><h3 id="Any-Domain-Adaptation-and-Zero-Shot-Learning"><a href="#Any-Domain-Adaptation-and-Zero-Shot-Learning" class="headerlink" title="Any-Domain Adaptation and Zero-Shot Learning"></a>Any-Domain Adaptation and Zero-Shot Learning</h3><p>我们避免将我们的设置命名为 “Zero-Shot DA”，因为我们认为Zero-Shot学习是一个过载的术语，而且它的用法在不同的作品中是不同的。一方面，GPT-3的作者（Brown等人，2020）用这个术语来表示向未知目标任务 $T^T$和未知领域 $D^T$的转变。</p><p>另一方面，Kodirov等人（2015）假设任务/标签空间漂移，而目标域在训练期间是已知的，Blitzer等人（2009）假设可以访问来自包括目标域在内的各种领域的无标签数据，而Peng等人（2018）使用来自目标域的不同任务数据。我们的问题设置与上述作品不同，但在某种程度上都可以被描述为 “Zero-shot”。在我们看来，这些差异应该被澄清，因此我们为我们的设置提出了一个指定的术语。</p><h2 id="Prompt-based-Autoregressive-DA"><a href="#Prompt-based-Autoregressive-DA" class="headerlink" title="Prompt-based Autoregressive DA"></a>Prompt-based Autoregressive DA</h2><p>正如前面第2节所讨论的，我们认识到基于MoE的方法所提出的解决方案有两个主要限制。(1) 它是不可扩展的。训练的参数总数，对于单个模型来说已经很大了，随着源域数量的增加而线性增长，因为需要为每个域分别训练一个专家模型。自然，这也增加了整体的训练时间；（2）在这种方法中，每个领域都要训练一个单独的模型。直观地说，针对特定领域的专家被调整为针对特定领域的知识，有时会牺牲跨领域的知识，因为跨领域的知识强调不同领域之间的关系。此外，由于领域的划分往往是任意的（例如考虑dvd和电影领域之间的差异），我们不希望将我们的模型严格限制在一个特定的分区，而是鼓励对领域边界采取更宽松的方法。</p><p>因此，我们提出一个单一的模型来编码来自多个领域的信息。我们的模型是这样设计的：来自新的未知领域的测试样本可以触发模型中最相关的参数。这样，我们允许我们的模型在各领域之间共享信息，并在测试时使用最相关的信息。我们的模型受到最近关于自回归语言模型 prompt 机制的研究启发。最近的工作显示了prompt 机制在激发这些模式方面的有效性（§2），尽管不是在DA的背景下。</p><p>我们首先（第4.1节）描述了我们模型的一般结构，然后（第4.2节）介绍了形成我们prompt的领域相关特征。</p><h3 id="The-Model"><a href="#The-Model" class="headerlink" title="The Model"></a>The Model</h3><p><img src="https://i.loli.net/2021/11/23/mtD8MgoxRXGOSJz.png" alt=""></p><p>我们提出了基于提示的领域适应自回归算法（PADA，图2a）。PADA采用了一个预先训练好的T5语言模型，并学习生成特定样本的领域相关特征（DRFs），以促进准确的任务预测。这是通过一个两步多任务机制实现的，首先生成一个DRF集以形成一个prompt，然后预测任务标签。</p><p>形式上，假设一个输入样本  $ (x_i, yi) \sim S_i $ ，这样 $x_i$ 是输入文本，$y_i$ 是任务label，$S_i$是这个例子的领域。</p><p> 对于输入的 $x_i$，PADA被训练成首先生成 $N_i$，即域名，然后是 $R_i$，即 $x_i$ 的DRF签名，并根据这个 prompt 来预测标签$y_i$。在测试时，当模型遇到一个来自未知领域的例子时，它会产生一个 prompt，该 prompt 可能由一个或多个域名以及一个或多个源域的DRF集的特征组成，并基于该 prompt 预测任务标签。</p><p><img src="https://i.loli.net/2021/11/23/ot5qEnNY1HCzx4K.png" alt=""></p><p>考虑到图1中的例子，它描述了一个情感分类模型，在餐馆、家庭家具、电子设备和电影等源域上进行训练。该模型观察到一个来自航空公司领域的测试例子，这是一个以前没有见过的领域，模型不知道其名称。该模型首先生成最适合这个例子的训练域的名称，在这个例子中是餐馆。然后，它继续生成 “食物 “和 “椅子”，这两个词分别与餐馆和家庭家具领域有关。最后，鉴于这一 prompt，该模型预测了该例子的（负面）情绪。</p><p>为了将 prompt 生成任务与判别分类任务分开，我们在一个多任务框架内训练我们的模型。根据样本的prompt，PADA被训练成执行两个任务。一个是生成 prompt，由例子领域的DRF集的特征组成，另一个是预测样本的标签。</p><p>对于第一个生成任务，模型接收带有特殊prompt “域：”的例子，这为模型生成 $N_i$ 和 $R_i$ 提供了条件。请注意，$R_i$是一组从 $S_i$ 的DRF集衍生出来的特征，如第4.2节所述，训练的样本被自动注释了它们的 $R_i$。对于第二个判别性任务，模型收到一个 prompt ，由 $N_i$ 和 $R_i$ 组成，其任务是预测 $y_i$。</p><p>按照T5的多任务训练协议，我们对每个任务的样本进行混合。为此，我们定义了一个任务比例混合参数 $α$。 训练集中的每个例子都以 $α$ 的概率形成生成性任务的样本，以 $1-α$ 的概率形成鉴别性任务的样本。$α$ 的值越大，模型对生成性任务的训练就越多。</p><p>PADA以生成的 prompt 为条件进行分类。为了评估这一条件的效果，我们还考虑了一个更简单的PADA变体，它联合执行分类和生成任务，但不使用生成任务的输出作为分类任务的 prompt 。我们把这个变体命名为PADA-NC，以强调判别任务不以生成部分的输出为条件。PADA和PADA-NC之间的区别在图2中得到了强调。</p><p>我们方法的核心是巧妙地选择每个领域的DRF集。我们接下来讨论这些特征和它们的选择过程。</p><h3 id="Domain-Related-Features"><a href="#Domain-Related-Features" class="headerlink" title="Domain Related Features"></a>Domain Related Features</h3><p>对于每个领域，我们定义DRF集，使这些特征为该领域提供一个语义签名。重要的是，如果两个领域有共同的语义，例如餐馆和烹饪领域，我们希望它们的DRFs在语义上是重叠的。由于每个训练样本的 prompt 由其领域的DRF集的特征子集组成，我们也应该决定一个 prompt 生成规则，可以用其相关的特征来注释这些训练样本。</p><p>为了反映该领域的语义，DRF应该经常出现在该领域。此外，相对于所有其他领域，它们在该特定领域中应该是非常普遍的。尽管DRF在一个特定的领域中很突出，但它也可以与其他领域相关。例如，考虑图2中的例子。人质 “这个词与 “Charlie-Hebdo “领域高度相关，而且确实是其DRFs之一。然而，这个词也与 “Sydney-Siege “域相关，这是Rumour Detection数据集的另一个域（Zubiaga等人，2016）。此外，由于这两个领域都与类似的事件有关，前者的DRF集包含恐怖分子的特征，后者的DRF集包含枪手的特征，这并不令人惊讶。<strong>这些特征的相似性促进了我们模型中的参数共享。</strong></p><p>我们对每个源域的DRF集定义如下。让第 $j$ 个源域（$S_j$）的样本（文本）被标记为1，所有其他域（ $S\setminus S_j$ ）的例子被标记为 0。我们首先计算所有 tokens 和这个二元变量之间的相互信息（MI），并选择 MI 得分最高的 $l$ 个token。注意，MI标准可能会促进与（ $S\setminus S_j$ ）高度相关的标记，而不是与 $S_j$ 。因此，我们根据以下条件来过滤 $l$ 个token:</p><script type="math/tex; mode=display">\frac{C_{S\setminus S_j}(n)}{C_{S_j}(n)} \le \rho , C_{S_j}(n) \gt 0</script><p>其中 $C<em>{S_j}(n)$ 是 $S_j$ 中n-gram $n$ 的计数， $C</em>{S\setminus S_j}(n)$ 是在所有源域中除了 $ S_j$ 的 n-gram 计数，$ρ$ 是 n-gram 频率比参数。</p><p>直观地说，$ρ$ 越小，我们就越确定这个n-gram与 $S_j$ 特别相关，与其他领域相比。由于 $S_j$ 中的样本数量远远小于  $S\setminus S_j$  中的样本数量，我们选择 $ρ≥1$ ，但不允许它过大。因此，这个标准允许与 $S_j$ 相关但也与其他源域相关的特征成为 $S_j$ 的DRF集的一部分。我们用 $R_j$ 来表示第 $j$ 个域的DRF集。</p><p>给定一个来自领域 $j$ 的训练样本 $i$ ，我们从 $R_j$ 中选择与该样本最相关的 $m$ 个特征来形成其 prompt。为此，我们计算DRF特征的T5嵌入 和 每个样本的T5嵌入 之间的欧几里得距离。然后，我们根据分数对这个列表进行排序，并选择最重要的 $m$ 个特征。（在这个计算中，我们考虑了T5在其预训练期间学到的非语境嵌入。在我们的实验中，我们只考虑单字（单词）作为DRFs。）</p><p>总而言之，我们针对特定领域的 DRF 集提取和训练样本的 prompt 注释的方法，展示了三个有吸引力的特性。首先，每个样本都有它自己独特的prompt 。其次，我们的 prompt 将每个训练样本映射到其领域的语义空间。最后，特定领域的DRF集可能会在其语义上重叠，要么包括相同的token，要么包括具有类似含义的token。这样，与单独的域名相比，它们提供了一个更细微的领域签名。在推理阶段，模型可以生成一个特定的样本prompt，该提示由不同源域的DRF集的特征组成，这一点后来被使用。</p><h2 id="Experimental-Setup"><a href="#Experimental-Setup" class="headerlink" title="Experimental Setup"></a>Experimental Setup</h2><p>我们的主要模型是PADA：这个多任务模型首先生成域名和领域相关的特征，形成一个prompt，然后用这个prompt来预测任务标签。我们将其与两种类型的模型进行比较。(a) 基于T5的基线，对应于多源DA工作中提出的想法，以及其他最近的最先进的模型；以及(b) 使用PADA特定部分的消融模型，以强调其组成部分的重要性。</p><h3 id="Baseline-Models"><a href="#Baseline-Models" class="headerlink" title="Baseline Models"></a>Baseline Models</h3><ul><li><em>Content-CRF</em> ： 一个为谣言检测而训练的CRF模型。每个预测都以相关的推文以及之前的推文为条件，同时结合了基于内容和社会特征。</li><li><em>Transformer-based Mixture of Experts</em>： 对于每个源域，在该域的训练集上训练一个单独的基于变压器的DistilBERT专家模型（Sanh等人，2019），并在所有源域的数据上训练另一个模型。在测试时，计算这些模型的类别概率的平均值，并选择最高概率的类别。这个模型被Wright和Augenstein（2020）命名为MoE-avg，并证明了在谣言检测方面取得了最先进的性能（该论文将CCRF的重新结果报告为其之前的最先进性能）。</li><li><em>T5-MoE</em> ：一个基于T5的MoE集合模型。对于每个源域，一个单独的预训练的T5模型在该域的训练集上进行微调（即一个领域专家模型）。在推理过程中，模型的最终预测是用Tr-MoE中相同的平均程序决定的。</li><li><em>T5-No-Domain-Adaptation (T5-NoDA)</em> ：一个预先训练好的T5模型，它向PADA（见下文）中使用的同一任务分类器提供信息，以预测任务标签。 在每个DA设置中，该模型是在所有源域的训练数据上训练的。</li><li><em>T5-Domain-Adversarial-Network (T5-DAN)</em> ：一个将T5-NoDA与对抗性领域分类器整合的模型，以学习领域不变的表征（Ganin和Lempitsky，2015）。</li><li><em>T5-Invariant-Risk-Minimization (T5-IRM)</em> ：一个基于T5的模型，它对每个领域有不同的最佳线性分类器的特征分布进行惩罚。IRM（Arjovsky等人，2019）是一个成熟的机器学习基线，用于超越多源分布的泛化（Koh等人，2020）。该模型是在所有源域的训练数据上训练的。</li><li><em>T5-UpperBound (T5-UB)</em> ：一个与T5-NoDA结构相同的域内模型。它在所有领域的训练数据上进行训练，在每个领域的开发数据上进行测试。我们将其性能视为所有DA设置中平均目标性能的上限，对于我们设置中的任何基于T5的模型。</li></ul><h3 id="Ablation-Models"><a href="#Ablation-Models" class="headerlink" title="Ablation Models"></a>Ablation Models</h3><ul><li>PADA-DN 我们的PADA模型的一个简化变体，它只给输入文本分配一个域名作为prompt。由于域名在测试时是未知的，我们为每个测试样本创建多个变体，每个变体都有一个训练域名作为prompt。对于模型的最终预测，我们遵循与Tr-MoE和T5-MoE相同的平均化过程。</li><li>PADA-NC 一个类似于PADA的多任务模型，只是它同时生成特定于样本的域名和DRF prompt并预测任务标签。这个模型不以提示为条件进行任务预测。</li></ul><p><img src="https://i.loli.net/2021/11/23/z4rZXTJYnOytFRE.png" alt=""></p><p>表2显示了我们的结果。 PADA在10个设置中的6个中优于所有的模型，在Rumour Detection和MNLI中分别比T5-NoDA的平均性能提高了3.5%和1.3%，T5-NoDA是不属于我们PADA框架的最佳模型。此外，在10个设置中的9个中，PADA模型是表现最好的模型之一。有趣的是，不执行任何DA的T5-NoDA的性能超过了所有不属于PADA系列的模型，包括MoE模型（平均和大多数模型之间的比较）。</p><p>虽然不同任务之间的性能提升不同，但它们部分源于每个任务中源域和目标域之间的不同性能差距。回顾一下，我们认为T5-UB在谣言检测（82.8%）和MNLI（80.8%）的开发集上的表现，是任何基于T5的模型在所有DA设置中平均目标表现的上限。当考虑到这个上限和T5-NoDA之间的差距时（Rumour Detection为65.8%，MNLI为78.3%），PADA将Rumour Detection的错误率降低了21%，MNLI为52%。事实上，PADA在这两项任务中获得的改进是巨大的。</p><p>虽然不同任务之间的性能提升不同，但它们部分源于每个任务中源域和目标域之间的不同性能差距。回顾一下，我们认为T5-UB在谣言检测（82.8%）和MNLI（80.8%）的开发集上的表现，是任何基于T5的模型在所有DA设置中平均目标表现的上限。当考虑到这个上限和T5-NoDA之间的差距时（Rumour Detection为65.8%，MNLI为78.3%），PADA将Rumour Detection的错误率降低了21%，MNLI为52%。事实上，PADA在这两项任务中获得的改进是巨大的。<br>与MoE相比，PADA的优势并不局限于改进预测。特别是，对于PADA，我们训练一个单一的模型，而对于MoE，我们为每个源域训练一个独特的模型，因此MoE框架中的参数数量随着源域的数量而线性增加。例如，在我们的设置中，Tr-MoE训练了五个DistilBERT模型（每个源域一个，所有源域一个），导致5 - 66M = 330M参数。相比之下, PADA模型保留了T5的220M参数, 而不考虑源域的数量。</p><p>我们的研究结果显示，在10个环境中的9个中，PADA及其变体PADA-DN和PADA-NC优于所有其他模型。特别是，PADA在10个环境中的7个环境中优于非PADA模型，PADA-NC在6个环境中优于这些模型，而PADA-DN在5个环境中优于这些模型。此外，PADA在所有的谣言检测设置中和5个MNLI设置中的3个中优于PADA-DN变体，而其PADA-NC变体在10个设置中的8个中优于PADA-DN。这些结果突出了我们设计选择的重要性。(a) 在特定例子的提示中包括DRFs，使它们能够表达源域和测试例子之间的关系（PADA vs PADA-DN）；以及(b) 利用自回归组件，其中生成的DRF提示被任务分类组件使用（PADA vs PADA-NC）。</p><h3 id="Performance-Shifts-between-Source-and-Target"><a href="#Performance-Shifts-between-Source-and-Target" class="headerlink" title="Performance Shifts between Source and Target"></a>Performance Shifts between Source and Target</h3><p><img src="https://i.loli.net/2021/11/23/x1Lza6UsSTZbhk4.png" alt=""></p><p>当DA方法提高了模型在目标域的性能时，这可能会导致源域和目标域之间的性能差距增加或减少。如果一个模型在其源训练域和未见过的目标域的表现相似，其源域的表现也可以为其未来在这些未见过的域的表现提供一个重要的指示。因此，在我们的设置中，如果未来的目标域是未知的，我们认为这种性能的稳定性是一个理想的属性。</p><p>图3显示了三个热图，描述了每个模型在源域和目标域之间的性能变化。如第6节所述，我们通过计算所有源域开发样本的F1得分和目标域测试集的表现来衡量每个模型的域内性能。然后，我们计算源域和目标域性能指标之间的差异，并对实验中表现最好的模型进行重新定位。总的趋势是明确的：PADA不仅在目标域表现更好，而且它还大大减少了源-目标性能的差距。虽然不是DA模型的T5-NoDA引发了最大的平均绝对性能转变—Rumour Detection为17%，MNLI为4.3%，Aspect Prediction为34%，但PADA的平均绝对性能转变分别为8.7%、3.5%和26%。</p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>我们解决了在训练时不知道目标域的情况下的多源域适应问题。这种设置的有效模型可以应用于任何目标域，对目标域没有任何数据再要求，而且模型参数的数量不会随着源或目标域的数量而增加。我们的算法PADA利用了T5自回归语言模型的提示机制，将测试例子映射到源域所跨越的语义空间。<br>我们对三个任务和十四个多源适应性设置的实验结果表明，与强大的替代方案相比，我们的方法是有效的，同时也表明了模型组件和我们的设计选择的重要性。此外，与MoE范式相比，PADA提供了一个统一的模型，即为每个源域训练一个模型。从直觉上讲，这种方法似乎也更符合认知规律—一个单一的模型试图使自己适应新的输入领域，而不是在每个领域采用一个独立的模型。<br>PADA的提示生成机制自然受到它所训练的源域集的限制。这可能会产生次优的DRF，而这些测试例子来自于与任何源域在语义上极不相关的领域。此外，我们没有直接用主要预测任务来优化提示生成过程，这也可能导致次优的DRF生成。在未来的工作中，我们希望改进我们方法的这些方面，并探索在一个单一模型中容纳多个任务和领域的自然扩展。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;PADA-A-Prompt-based-Autoregressive-Approach-for-Adaptation-to-Unseen-Domains&quot;&gt;&lt;a href=&quot;#PADA-A-Prompt-based-Autoregressive-Approach-</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>二叉树刷题1</title>
    <link href="http://example.com/2021/11/19/%E4%BA%8C%E5%8F%89%E6%A0%91%E5%88%B7%E9%A2%981/"/>
    <id>http://example.com/2021/11/19/%E4%BA%8C%E5%8F%89%E6%A0%91%E5%88%B7%E9%A2%981/</id>
    <published>2021-11-19T11:17:33.000Z</published>
    <updated>2021-11-26T01:55:29.841Z</updated>
    
    <content type="html"><![CDATA[<h1 id="二叉树刷题1"><a href="#二叉树刷题1" class="headerlink" title="二叉树刷题1"></a>二叉树刷题1</h1><p>[TOC]</p><h2 id="树的基础"><a href="#树的基础" class="headerlink" title="树的基础"></a>树的基础</h2><h3 id="102-二叉树的层序遍历"><a href="#102-二叉树的层序遍历" class="headerlink" title="102. 二叉树的层序遍历"></a><a href="https://leetcode-cn.com/problems/binary-tree-level-order-traversal/">102. 二叉树的层序遍历</a></h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123;</span><br><span class="line">        List&lt;List&lt;Integer&gt;&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>) <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">        Queue&lt;TreeNode&gt; queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">        queue.offer(root);</span><br><span class="line">        <span class="keyword">while</span>(!queue.isEmpty())&#123;</span><br><span class="line">            List&lt;Integer&gt; level = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="keyword">int</span> currentLevelSize = queue.size();</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;= currentLevelSize; ++i)&#123;</span><br><span class="line">                TreeNode node = queue.poll();</span><br><span class="line">                level.add(node.val);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(node.left!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    queue.offer(node.left);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>(node.right!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    queue.offer(node.right);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            res.add(level);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="107-二叉树的层序遍历-II"><a href="#107-二叉树的层序遍历-II" class="headerlink" title="107. 二叉树的层序遍历 II"></a><a href="https://leetcode-cn.com/problems/binary-tree-level-order-traversal-ii/">107. 二叉树的层序遍历 II</a></h3><p>从根节点开始搜索，每次遍历同一层的全部节点，使用一个列表存储该层的节点值。</p><p>如果要求从上到下输出每一层的节点值，做法很直观，在遍历完一层节点之后，将存储该层节点值的列表添加到结果列表的尾部。</p><p>但这道题要求从下到上输出每一层，只要对上述操作稍作修改即可；在遍历完一层节点之后，将存储该节点值的列表添加到结果列表的头部。</p><p>为了降低在结果列表的头部添加一层节点值的列表的时间复杂度，结果列表可以使用链表的结构，在链表头部添加一层节点值的列表的时间复杂度是 O(1)O(1)。在 Java 中，由于我们需要返回的 List 是一个接口，这里可以使用链表实现；而 C++ 或 Python 中，我们需要返回一个 vector 或 list，它不方便在头部插入元素（会增加时间开销），所以我们可以先用尾部插入的方法得到从上到下的层次遍历列表，然后再进行反转。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> List&lt;List&lt;Integer&gt;&gt; levelOrderBottom(TreeNode root) &#123;</span><br><span class="line">        List&lt;List&lt;Integer&gt;&gt; res = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>) <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">        Queue&lt;TreeNode&gt; queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">        queue.offer(root);</span><br><span class="line">        <span class="keyword">while</span>(!queue.isEmpty())&#123;</span><br><span class="line">            List&lt;Integer&gt; level = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="keyword">int</span> size = queue.size();</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;size;i++)&#123;</span><br><span class="line">                TreeNode node = queue.poll();</span><br><span class="line">                level.add(node.val);</span><br><span class="line">                TreeNode left = node.left, right =node.right;</span><br><span class="line">                <span class="keyword">if</span>(left!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    queue.offer(left);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>(right!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    queue.offer(right);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            res.add(<span class="number">0</span>, level);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="二叉树的遍历"><a href="#二叉树的遍历" class="headerlink" title="二叉树的遍历"></a>二叉树的遍历</h2><h3 id="前中后序"><a href="#前中后序" class="headerlink" title="前中后序"></a>前中后序</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">preorderTraversal</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        List&lt;Integer&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        preorder(root, res);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">preorder</span><span class="params">(TreeNode root, List&lt;Integer&gt; res)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>) <span class="keyword">return</span>;</span><br><span class="line">        res.add(root.val);</span><br><span class="line">        preorder(root.left, res);</span><br><span class="line">        preorder(root.right, res);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>迭代</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">preorderTraversal</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        List&lt;Integer&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root==<span class="keyword">null</span>) <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">        Deque&lt;TreeNode&gt; stack = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">        TreeNode node = root;</span><br><span class="line">        <span class="keyword">while</span>(!stack.isEmpty() || node!=<span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">while</span>(node!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                res.add(node.val);</span><br><span class="line">                stack.push(node);</span><br><span class="line">                node = node.left;</span><br><span class="line">            &#125;</span><br><span class="line">            node = stack.pop();</span><br><span class="line">            node = node.right;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">inorderTraversal</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        List&lt;Integer&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root==<span class="keyword">null</span>) <span class="keyword">return</span> res;</span><br><span class="line">        Deque&lt;TreeNode&gt; stack = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">        TreeNode node = root;</span><br><span class="line">        <span class="keyword">while</span>(!stack.isEmpty() || node!=<span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">while</span>(node!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                stack.push(node);</span><br><span class="line">                node = node.left;</span><br><span class="line">            &#125;</span><br><span class="line">            node = stack.pop();</span><br><span class="line">            res.add(node.val);</span><br><span class="line">            node = node.right;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">   <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">postorderTraversal</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        List&lt;Integer&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root==<span class="keyword">null</span>) <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">        Deque&lt;TreeNode&gt; stack = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">        TreeNode prev = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">while</span>(root!=<span class="keyword">null</span> || !stack.isEmpty())&#123;</span><br><span class="line">            <span class="keyword">while</span>(root!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                stack.push(root);</span><br><span class="line">                root = root.left;</span><br><span class="line">            &#125;</span><br><span class="line">            root = stack.pop();</span><br><span class="line">            <span class="keyword">if</span>(root.right==<span class="keyword">null</span> || root.right==prev)&#123;</span><br><span class="line">                res.add(root.val);</span><br><span class="line">                prev = root;</span><br><span class="line">                root = <span class="keyword">null</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                stack.push(root);</span><br><span class="line">                root = root.right;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="112-路径总和"><a href="#112-路径总和" class="headerlink" title="112. 路径总和"></a><a href="https://leetcode-cn.com/problems/path-sum/">112. 路径总和</a></h3><h4 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a>DFS</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasPathSum</span><span class="params">(TreeNode root, <span class="keyword">int</span> sum)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(root==<span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(root.left==<span class="keyword">null</span> &amp;&amp; root.right==<span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> root.val == sum;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> hasPathSum(root.left, sum - root.val) || hasPathSum(root.right, sum - root.val);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="BFS"><a href="#BFS" class="headerlink" title="BFS"></a>BFS</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasPathSum</span><span class="params">(TreeNode root, <span class="keyword">int</span> sum)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        Queue&lt;TreeNode&gt; queNode = <span class="keyword">new</span> LinkedList&lt;TreeNode&gt;();</span><br><span class="line">        Queue&lt;Integer&gt; queVal = <span class="keyword">new</span> LinkedList&lt;Integer&gt;();</span><br><span class="line">        queNode.offer(root);</span><br><span class="line">        queVal.offer(root.val);</span><br><span class="line">        <span class="keyword">while</span>(!queNode.isEmpty())&#123;</span><br><span class="line">            TreeNode now = queNode.poll();</span><br><span class="line">            <span class="keyword">int</span> temp = queVal.poll();</span><br><span class="line">            <span class="keyword">if</span>(now.left == <span class="keyword">null</span> &amp;&amp; now.right==<span class="keyword">null</span>)&#123;</span><br><span class="line">                <span class="keyword">if</span>(temp==sum)&#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(now.left != <span class="keyword">null</span>)&#123;</span><br><span class="line">                queNode.offer(now.left);</span><br><span class="line">                queVal.offer(now.left.val + temp);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(now.right !=<span class="keyword">null</span>)&#123;</span><br><span class="line">                queNode.offer(now.right);</span><br><span class="line">                queVal.offer(now.right.val + temp);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="113-路径总和-II"><a href="#113-路径总和-II" class="headerlink" title="113. 路径总和 II"></a><a href="https://leetcode-cn.com/problems/path-sum-ii/">113. 路径总和 II</a></h3><p>问完成一件事情的所有解决方案，一般采用回溯算法（<strong>深度优先遍历</strong>）完成。做回溯算法问题一般先画图，好在这就是一个树形问题，题目已经给我们画好了示意图。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> List&lt;List&lt;Integer&gt;&gt; pathSum(TreeNode root, <span class="keyword">int</span> targetSum) &#123;</span><br><span class="line">        List&lt;List&lt;Integer&gt;&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> res;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Deque&lt;Integer&gt; path = <span class="keyword">new</span> ArrayDeque&lt;&gt;();</span><br><span class="line">        dfs(root, targetSum, path,res);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">dfs</span><span class="params">(TreeNode node, <span class="keyword">int</span> sum, Deque&lt;Integer&gt; path, List&lt;List&lt;Integer&gt;&gt; res)</span></span>&#123;</span><br><span class="line">        <span class="comment">// 递归终止条件1</span></span><br><span class="line">        <span class="keyword">if</span>(node == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 递归终止条件2</span></span><br><span class="line">        <span class="keyword">if</span>(node.val == sum &amp;&amp; node.left==<span class="keyword">null</span> &amp;&amp; node.right==<span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="comment">//当前节点的值还没添加到列表中，所以要先添加，再移除</span></span><br><span class="line">            path.addLast(node.val);</span><br><span class="line">            res.add(<span class="keyword">new</span> ArrayList&lt;&gt;(path));</span><br><span class="line">            path.removeLast();</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        path.addLast(node.val);</span><br><span class="line">        dfs(node.left, sum-node.val, path, res);</span><br><span class="line">        <span class="comment">// 进入左右分支的 path 是一样的，这里不用写下面两行，因为一定会调用到 path.removeLast();</span></span><br><span class="line">        <span class="comment">// path.removeLast();</span></span><br><span class="line">        <span class="comment">// path.addLast(node.val);</span></span><br><span class="line">        dfs(node.right, sum-node.val, path, res);</span><br><span class="line">        path.removeLast();</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 由于先减去了当前非空节点的值，递归终止条件写 sum==0</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">dfs</span><span class="params">(TreeNode node, <span class="keyword">int</span> sum, Deque&lt;Integer&gt; path, List&lt;List&lt;Integer&gt;&gt; res)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 递归终止条件 1：遇到空结点不再递归调用</span></span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 沿途结点必须选择，这个时候要做两件事：1、sum 减去这个结点的值；2、添加到 path 里</span></span><br><span class="line">        sum -= node.val;</span><br><span class="line">        path.addLast(node.val);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 递归终止条件 2：遇到叶子结点，sum 恰好为 0，说明从根结点到叶子结点的路径是一个符合要求的解</span></span><br><span class="line">        <span class="keyword">if</span> (sum == <span class="number">0</span> &amp;&amp; node.left == <span class="keyword">null</span> &amp;&amp; node.right == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// path 全局只有一份，必须做拷贝</span></span><br><span class="line">            res.add(<span class="keyword">new</span> ArrayList&lt;&gt;(path));</span><br><span class="line">            <span class="comment">// 注意：这里 return 之前必须重置</span></span><br><span class="line">            path.removeLast();</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        pathSum(node.left, sum, path, res);</span><br><span class="line">        pathSum(node.right, sum, path, res);</span><br><span class="line">        <span class="comment">// 递归完成以后，必须重置变量</span></span><br><span class="line">        path.removeLast();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="437-路径总和-III"><a href="#437-路径总和-III" class="headerlink" title="437. 路径总和 III"></a><a href="https://leetcode-cn.com/problems/path-sum-iii/">437. 路径总和 III</a></h3><h4 id="法一-深度优先遍历"><a href="#法一-深度优先遍历" class="headerlink" title="法一 深度优先遍历"></a>法一 深度优先遍历</h4><p>穷举所有可能，访问每一个节点node，检测以 node 为起始点且向下延伸的路径有多少种。递归遍历每个节点的所有可能的路径，然后将这些路径数目加起来即为返回结果。</p><ul><li>首先定义 $rootSum(p, val)$ 表示以节点 $p$ 为起点向下且满足路径总和为 $val$ 的路径数目。我们队二叉树上每个节点 $p$ 求出 $rootSum(p,targetSum)$，然后对这些路径数目求和为返回结果</li><li>对节点 $p$ 求 $rootSum(p, targetSum)$时，以当前节点 $p$ 为目标路径的起点递归向下进行搜索。假设当前的节点$p$的值为$val$ ，对左子树和右子树进行递归搜索</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">pathSum</span><span class="params">(TreeNode root, <span class="keyword">int</span> targetSum)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> ret = rootSum(root, targetSum);</span><br><span class="line">        ret += pathSum(root.left, targetSum);</span><br><span class="line">        ret += pathSum(root.right, targetSum);</span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">rootSum</span><span class="params">(TreeNode root, <span class="keyword">int</span> targetSum)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> val = root.val;</span><br><span class="line">        <span class="keyword">if</span>(val == targetSum)&#123;</span><br><span class="line">            ret++;</span><br><span class="line">        &#125;</span><br><span class="line">        ret += rootSum(root.left, targetSum-val);</span><br><span class="line">        ret += rootSum(root.right, targetSum-val);</span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="方法二-前缀和"><a href="#方法二-前缀和" class="headerlink" title="方法二 前缀和"></a>方法二 前缀和</h4><p>上一解法存在许多重复计算，定义节点的前缀和为：由根节点到当前节点的路径上所有节点的和。</p><p>利用先序遍历二叉树，记录下根节点 $root$ 到当前节点 $p$ 的路径上除当前节点以外所有节点的前缀和，在已保存的路径前缀和中查找是否存在前缀和刚好等于当前节点到根节点的前缀和 $curr - targetSum$</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">pathSum</span><span class="params">(TreeNode root, <span class="keyword">int</span> targetSum)</span> </span>&#123;</span><br><span class="line">      Map&lt;Integer, Integer&gt; prefix = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">      <span class="comment">// 前缀和为0的一条路径</span></span><br><span class="line">      prefix.put(<span class="number">0</span>,<span class="number">1</span>);</span><br><span class="line">      <span class="comment">// 前缀和的递归回溯</span></span><br><span class="line">      <span class="keyword">return</span> recursionPathSum(root, prefix, targetSum, <span class="number">0</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">recursionPathSum</span><span class="params">(TreeNode node, Map&lt;Integer,Integer&gt; prefix, <span class="keyword">int</span> target, <span class="keyword">int</span> currSum)</span></span>&#123;</span><br><span class="line">      <span class="keyword">if</span>(node==<span class="keyword">null</span>)&#123;</span><br><span class="line">          <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">int</span> res = <span class="number">0</span>;</span><br><span class="line">      currSum += node.val;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//---核心代码</span></span><br><span class="line">      <span class="comment">// 看看root到当前节点这条路上是否存在节点前缀和加target为currSum的路径</span></span><br><span class="line">      <span class="comment">// 当前节点-&gt;root节点反推，有且仅有一条路径，如果此前有和为currSum-target,而当前的和又为currSum,两者的差就肯定为target了</span></span><br><span class="line">      <span class="comment">// currSum-target相当于找路径的起点，起点的sum+target=currSum，当前点到起点的距离就是target</span></span><br><span class="line">      res += prefix.getOrDefault(currSum - target, <span class="number">0</span>);</span><br><span class="line">      <span class="comment">// 更新路径上当前节点前缀和的个数</span></span><br><span class="line">      prefix.put(currSum, prefix.getOrDefault(currSum, <span class="number">0</span>)+<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 进入下一层</span></span><br><span class="line">      res += recursionPathSum(node.left, prefix, target, currSum);</span><br><span class="line">      res += recursionPathSum(node.right, prefix, target, currSum);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 回到本层，恢复状态，去除当前节点的前缀和数量</span></span><br><span class="line">      prefix.put(currSum, prefix.get(currSum) - <span class="number">1</span>);</span><br><span class="line">      <span class="keyword">return</span> res;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="129-求根节点到叶节点数字之和"><a href="#129-求根节点到叶节点数字之和" class="headerlink" title="129. 求根节点到叶节点数字之和"></a><a href="https://leetcode-cn.com/problems/sum-root-to-leaf-numbers/">129. 求根节点到叶节点数字之和</a></h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">sumNumbers</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> dfs(root, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">dfs</span><span class="params">(TreeNode root, <span class="keyword">int</span> prevSum)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(root==<span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> sum = prevSum * <span class="number">10</span> + root.val;</span><br><span class="line">        <span class="keyword">if</span>(root.left == <span class="keyword">null</span> &amp;&amp; root.right==<span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> sum;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> dfs(root.left, sum) + dfs(root.right, sum);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="100-相同的树"><a href="#100-相同的树" class="headerlink" title="100. 相同的树"></a><a href="https://leetcode-cn.com/problems/same-tree/">100. 相同的树</a></h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isSameTree</span><span class="params">(TreeNode p, TreeNode q)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(p == <span class="keyword">null</span> &amp;&amp; q == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(p==<span class="keyword">null</span> || q==<span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(p.val != q.val)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> isSameTree(p.left, q.left) &amp;&amp; isSameTree(p.right, q.right);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;二叉树刷题1&quot;&gt;&lt;a href=&quot;#二叉树刷题1&quot; class=&quot;headerlink&quot; title=&quot;二叉树刷题1&quot;&gt;&lt;/a&gt;二叉树刷题1&lt;/h1&gt;&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&quot;树的基础&quot;&gt;&lt;a href=&quot;#树的基础&quot; class=&quot;headerl</summary>
      
    
    
    
    
    <category term="LeetCode" scheme="http://example.com/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>Towards a Unified View of Parameter-Efficient Transfer Learning</title>
    <link href="http://example.com/2021/11/17/Towards-a-Unified-View-of-Parameter-Efficient-Transfer-Learning/"/>
    <id>http://example.com/2021/11/17/Towards-a-Unified-View-of-Parameter-Efficient-Transfer-Learning/</id>
    <published>2021-11-17T09:16:12.000Z</published>
    <updated>2021-11-20T07:55:10.006Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Towards-a-Unified-View-of-Parameter-Efficient-Transfer-Learning"><a href="#Towards-a-Unified-View-of-Parameter-Efficient-Transfer-Learning" class="headerlink" title="Towards a Unified View of Parameter-Efficient Transfer Learning"></a>Towards a Unified View of Parameter-Efficient Transfer Learning</h1><p>ICLR2022高分文章</p><p>这篇工作将最近提出的多种Parameter-Efficient的迁移学习方法联系在了一起，提出了一个统一的框架，并探索了这些方法成功的关键因素是什么。</p><p>统一什么？把Adapter、prompt-tuning、LoRA都定义为预训练模型中添加可调整的特定的隐层状态，只是设计的参数维度、修改函数的计算和位置不同。定义成一个统一的框架，顺便还排列组合出几个小变体。</p><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>使通用PLM适应下游任务的最常见方法是微调所有模型参数。然而，这导致每个任务都有一份单独的微调模型参数，当为执行大量任务的模型提供服务时，其成本过高。</p><p>为了缓解这个问题，已经提出了一些轻量级的替代方案，只更新少量的额外参数，同时保持大多数预训练参数的冻结，如：Adapters、prefix tuning 与 prompt tuning、LoRA 。（下文详细介绍他们）</p><p>这些方法都在不同的任务集上表现出与完全微调相媲美的性能，通常是通过更新不到1%的原始模型参数。除了节省参数外，参数有效的调整使其有可能快速适应新的任务，而不会出现灾难性的遗忘（Pfeiffer等人，2021），并且在 out-of-distribution 上往往表现出卓越的稳健性。</p><p>作者接下来针对上面这几种参数有效的方法提出了几个问题：</p><ul><li>这些方法是如何联系的？</li><li>这些方法是否具有对其有效性至关重要的设计要素，这些要素是什么？</li><li>每种方法的有效成分是否可以转移到其他方法中，以产生更有效的变体？</li></ul><h2 id="PRELIMINARIES"><a href="#PRELIMINARIES" class="headerlink" title="PRELIMINARIES"></a>PRELIMINARIES</h2><p>首先看一下现有这些方法在Transformer里的结构是如何：</p><p><img src="https://i.loli.net/2021/11/20/BNFQM3GX546kyiL.png" alt=""></p><ul><li><p>Adapters：在PLM的每一层插入称为适配器的小型神经模块，在微调时只对适配器进行训练。适配器层一般使用$W<em>{down}\in R^{d×r}$的向下投影，将输入 $h$ 投影到瓶颈维度 $r$ 指定的低维空间，然后使用非线性激活函数 $f(\cdot)$，再使用$W</em>{up}\in R^{r×d}$的向上投影，还有一个残差连接。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  h \leftarrow h + f(hW_{dnow})W_{up}    \end{split}\end{equation}</script><p>将两个适配器依次放在变压器的一个层内，一个在多头关注之后，一个在FFN子层之后。</p></li><li><p>prefix tuning 与 prompt tuning ：受通prompt方法的启发，在输入层或隐藏层中预置了额外的 $l$ 个可调整的前缀tokens，在下游任务的微调时只训练这些 soft prompt。具体来说，两组prefix 向量 $P_k , P_v\in R^{l×d}$ 与原始键 $K$ 和值$V$相连接，如图中所示。然后对新的 prefixed key 和值进行多头注意力计算：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  head_i = Attn(x W_{q}^{(i)} , concat(P_k^{(i)}, CW_{k}^{(i)}), concat(P_v^{(i)}, CW_v^{(i)}))    \end{split}\end{equation}</script><p>这其实也于Graphormer等Graph Transformer模型有异曲同工之妙。$P_k$ 和 $P_v$ 分别被分成 $N_h$个头部向量。Prompt-tuning 简化了前缀调整，其只对第一层的输入词嵌入进行预处理；类似工作还包括P-tuning。</p></li><li><p>LoRA ：将可训练的低秩矩阵注入 transformer 层，以近似权重更新。对于一个预训练好的权重矩阵 $W\in R^{d×k}$ LoRA用低秩分解 $W +\Delta W = W +W<em>{down}W</em>{up}$ 表示其更新，其中$W<em>{down}\in R^{d×r},W</em>{up}\in R^{r×k} $ 是可调整的参数。LoRA将这种更新应用于多头注意子层中的 Query 和 Key 投影矩阵，如图1所示。对于多头注意力中的线性投影的特定输入$x$ ，LoRA将投影输出 $h$ 修改为:</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  h \leftarrow  h + s \cdot x W_{down}W_{up}    \end{split}\end{equation}</script><p>其中 $s≥1$ 是可调标量超参数。</p></li></ul><p>其实还有一些参数有效的调整方法像：BitFit 只对预训练模型中的 bias 向量进行微调，以及上一篇文章提到的diff-pruning，它学习一个稀疏的参数更新向量。</p><h2 id="推导-prefix-tuning"><a href="#推导-prefix-tuning" class="headerlink" title="推导 prefix tuning"></a>推导 prefix tuning</h2><p>上文关于 prefix tuning  在注意力 K 和 V上添加可学习的向量来改变注意力模块，这里提出另一种观点：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}  head &=Attn(x W_{q} , concat(P_k, CW_{k}, concat(P_v, CW_v) \\ &= softmax(x W_q concat(P_k, CW_k)^T) \begin{bmatrix} P_v \\ CW_v\\ \end{bmatrix} \\ &= (1-\lambda(x)) softmax(xW_qW_k^TC^T) CW_v + \lambda(x)softmax(xW_qP_k^T) P_v\\& = (1 - \lambda(x)) \underbrace{ \text{Attn}(xW_q, CW_k, CW_v) }_{\text{standard attention}} + \lambda(x) \underbrace{ \text{Attn}(xW_q, P_k, P_v) }_{\text{independent of } C},    \end{split}\end{equation}</script><p>其中 $λ(X)$ 是标量，归一化注意力权重之和：</p><script type="math/tex; mode=display">\begin{equation}\begin{split} \lambda(x) = \frac{\sum_i\exp (xW_qP_k^T)_i}{\sum_i \exp (xW_qP_k^T)_i + \sum_j \exp(xW_qW_k^TC^T)_j}.    \end{split}\end{equation}</script><h2 id="THE-UNIFIED-FRAMEWORK"><a href="#THE-UNIFIED-FRAMEWORK" class="headerlink" title="THE UNIFIED FRAMEWORK"></a>THE UNIFIED FRAMEWORK</h2><p>受 prefix tuning   和  Adapter 之间联系的启发，作者提出了一个总体框架，旨在统一几种最先进的参数有效的调谐方法。</p><p>具体来说，作者把它们看作是学习一个向量 $∆h$，它被应用于各种隐藏表征。形式上，作者把要直接修改的隐藏表征表示为 $h$ ，把计算 $h$ 的PLM子模块的直接输入表示为 $x$。</p><p>为了描述这个修改过程，作者定义了一组设计维度，不同的方法可以通过改变这些维度的值而被实例化。并在表1中说明了Adapters、prefix tuning 和LoRA在这些维度上的情况。</p><p><img src="https://i.loli.net/2021/11/20/45RtsBp9aCEZQlg.png" alt=""></p><ul><li>表中的 Functional Form :是指计算 $∆h$ 的具体函数。所有这些方法的函数形式都类似于proj down → nonlinear → proj up的架构。</li><li>Modified Representation : 指直接修改的隐藏表示形式。</li><li>Insertion Form : 指添加的模块如何插入到网络中。传统上适配器是以 sequential 方式插入某个位置的，其中输入和输出都是 $h$ 。prefix tuning和LoRA 相当于 parallel 插入。</li><li>Composition Function :指修改后的向量 $∆h$ 如何与原始隐藏表征 $h$ 计算，以形成新的隐藏表征。例如，适配器执行简单的加法组合，前缀调整使用门控加法组合，而LoRA通过一个恒定的因子对 $Δh$ 进行缩放，并将其添加到原始隐藏表示中。</li></ul><h2 id="变体组合——通过在不同的方法之间转移设计元素而得到"><a href="#变体组合——通过在不同的方法之间转移设计元素而得到" class="headerlink" title="变体组合——通过在不同的方法之间转移设计元素而得到"></a>变体组合——通过在不同的方法之间转移设计元素而得到</h2><p><img src="https://i.loli.net/2021/11/20/VpIxoeY5yRLZiOF.png" alt=""></p><ul><li>Parallel Adapter 是通过将 prefix tuning 的 parallel 插入转移到 Adapter 的变体。</li><li>Multi-head Parallel Adapter 是使 Adapter 与 prefix tuning 更加相似的进一步措施：应用 Parallel Adapter 来修改头部注意力输出作为 prefix tuning 。这样，变体通过利用多头投影来提高能力</li><li>Scaled Parallel Adapter 是通过将LoRA的组成和插入形式转移到适配器的变体，如图3e所示。</li></ul><h2 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h2><p>下图说明，在数据较为充沛、比较有挑战的任务中，现有的方法 距离 Full FIne-tuning 还有一定差距</p><p><img src="https://i.loli.net/2021/11/20/4kcyzS9rXLFgbNI.png" alt=""></p><h3 id="SEQUENTIAL-OR-PARALLEL"><a href="#SEQUENTIAL-OR-PARALLEL" class="headerlink" title="SEQUENTIAL OR PARALLEL?"></a>SEQUENTIAL OR PARALLEL?</h3><p>Parallel 和  sequential 哪个方式好些？</p><p><img src="https://i.loli.net/2021/11/20/Kxwtri6pOHbU4SG.png" alt=""></p><p> Parallel Adapter在所有情况下都能够击败 Sequential Adapter</p><h3 id="WHICH-MODIFIED-REPRESENTATION-–-ATTENTION-OR-FFN"><a href="#WHICH-MODIFIED-REPRESENTATION-–-ATTENTION-OR-FFN" class="headerlink" title="WHICH MODIFIED REPRESENTATION – ATTENTION OR FFN?"></a>WHICH MODIFIED REPRESENTATION – ATTENTION OR FFN?</h3><p>适配修改放在Transformer哪里比较好？</p><p><img src="https://i.loli.net/2021/11/20/hlqXwRJOBMDVsWv.png" alt=""></p><p><img src="https://i.loli.net/2021/11/20/j7kGyYfVLsQRADE.png" alt=""></p><h3 id="哪个-COMPOSITION-FUNCTION-比较好"><a href="#哪个-COMPOSITION-FUNCTION-比较好" class="headerlink" title="哪个 COMPOSITION FUNCTION 比较好"></a>哪个 COMPOSITION FUNCTION 比较好</h3><p><img src="https://i.loli.net/2021/11/20/BGhesmZN2tREPAr.png" alt=""></p><p>简单composition（Adapter）、门控composition（ prefix tuning ）和缩放composition（LoRA）。</p><p>缩放的 composition 函数，同时也很容易适用。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>(1) Scaled parallel adapter 是修改FFN的最佳变体</p><p>(2) FFN可以在更大的容量下更好地利用修改</p><p>(3) 像 prefix tuning 这样修改头部注意力可以在只有0.1%的参数下实现强大的性能。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Towards-a-Unified-View-of-Parameter-Efficient-Transfer-Learning&quot;&gt;&lt;a href=&quot;#Towards-a-Unified-View-of-Parameter-Efficient-Transfer-Le</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>Generalizing to Unseen Domains: A Survey on Domain Generalization</title>
    <link href="http://example.com/2021/11/14/Generalizing-to-Unseen-Domains-A-Survey-on-Domain-Generalization/"/>
    <id>http://example.com/2021/11/14/Generalizing-to-Unseen-Domains-A-Survey-on-Domain-Generalization/</id>
    <published>2021-11-14T03:02:01.000Z</published>
    <updated>2021-11-21T13:19:00.891Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Generalizing-to-Unseen-Domains-A-Survey-on-Domain-Generalization"><a href="#Generalizing-to-Unseen-Domains-A-Survey-on-Domain-Generalization" class="headerlink" title="Generalizing to Unseen Domains: A Survey on Domain Generalization"></a>Generalizing to Unseen Domains: A Survey on Domain Generalization</h1><p>机器学习系统通常假定训练和测试分布是相同的。为此，一个关键的要求是开发能够泛化到未见过的分布的模型。近年来，Domain generalization 领域泛化（DG），即 out-of- distribution 泛化，吸引了越来越多的兴趣。</p><p>领域泛化处理的是一个具有挑战性的环境，即给定一个或几个不同但相关的领域，目标是学习一个能够泛化到未见过的测试领域的模型。</p><p>本文首次对该领域的最新进展进行了回顾。</p><ul><li>首先，提供了一个领域泛化的正式定义，并讨论了几个相关的领域。</li><li>然后，彻底回顾了与领域泛化相关的理论，并仔细分析了泛化背后的理论。我们将最近的算法分为三类：数据操作、表征学习和学习策略，并对每一类算法详细介绍了几种流行的算法。</li><li>第三，介绍了常用的数据集和应用。</li><li>最后，我们总结了现有的文献并提出了一些未来的潜在研究课题。</li></ul><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>传统的ML模型是基于 i.i.d. 假设进行训练的，即训练和测试数据是完全独立分布的。然而，这一假设在现实中并不总是成立。当训练数据和测试数据的概率分布不同时，由于领域分布的差距，ML模型的性能往往会恶化。收集所有可能领域的数据来训练ML模型是昂贵的，甚至是不可能的。因此，提高ML模型的泛化能力在工业和学术领域都很重要。</p><p>有许多与泛化相关的研究课题，如领域适应、元学习、迁移学习、covariate shift等等。近年来，领域泛化（DG）受到广泛关注。</p><p><img src="https://i.loli.net/2021/11/14/CzTe6qKXc9StOPm.png" alt=""></p><p>如图1所示，领域泛化的目标是从一个或几个不同但相关的领域（即不同的训练数据集）中学习一个模型，该模型将在未见过的测试领域中具有良好的泛化能力。</p><blockquote><p>例如，给定一个由素描、卡通图像和绘画组成的训练集，领域泛化要求训练一个好的机器学习模型，该模型在对来自自然图像或照片的分类中具有最小的预测误差，这些图像显然与训练集中的图像具有不同的分布。</p><p>在过去的几年里，领域泛化在计算机视觉和自然语言处理等各个领域都取得了重大进展。尽管取得了这些进展，但在这一领域还没有一份全面介绍和总结其主要思想、学习算法和其他相关问题的调查报告，以提供对未来的研究见解。</p><p>在本文中，我们提出了第一份关于领域泛化的调查报告，介绍了它的最新进展，特别关注它的公式、理论、算法、数据集、应用和未来研究方向。我们希望这个调查能够为感兴趣的研究者提供一个全面的回顾，并激发在这个领域和相关领域的更多研究。</p></blockquote><ul><li><p>提出了关于领域泛化和相关领域适应的理论分析。</p></li><li><p>通过增加新的类别：特征分解的生成模型、不变的风险最小化、基于梯度运算的方法和其他学习策略来全面总结这些泛化方法。</p></li><li><p>对于所有的类别，我们通过包括更多的相关算法、比较和讨论，扩大了对不同类别方法的分析。</p></li><li><p>扩展了数据集和应用的范围，同时我们也探索了领域通用的评价标准。</p></li></ul><p>本文的组织结构如下。在第二节中提出了领域泛化的问题并讨论了它与现有研究领域的关系。第三节介绍了领域泛化的相关理论。在第四节中，我们详细描述了一些有代表性的DG方法。第五节介绍了应用，第六节介绍了DG的基准数据集。我们在第七节中总结了现有工作的启示，并提出了一些可能的未来方向。最后，我们在第八节中对本文进行总结。</p><h2 id="BACKGROUND"><a href="#BACKGROUND" class="headerlink" title="BACKGROUND"></a>BACKGROUND</h2><h3 id="A-Formalization-of-domain-generalization"><a href="#A-Formalization-of-domain-generalization" class="headerlink" title="A. Formalization of domain generalization"></a><em>A. Formalization of domain generalization</em></h3><p>在本节中，我们介绍本文中使用的符号和定义。</p><p><strong>Definition 1 (Domain) :</strong> X 表示一个非空的输入空间，Y 表示一个输出空间。一个域是由从分布中取样的数据组成的。我们把它表示为 $S = {(x<em>i, y_i)}^n</em>{i=1} \sim P<em>{XY}$ , 其中 $x\in X \subset R^d, y\in Y \subset R$ 表示标签，而 $P</em>{XY}$ 表示输入样本和输出标签的联合分布。X、Y表示相应的随机变量。</p><p><strong>Definition 2 (Domain generalization) :</strong></p><p><img src="https://i.loli.net/2021/11/14/QGw1vmbkEZcjSHD.png" alt=""></p><p>在领域泛化中，我们得到了M个训练（源）的领域 $S<em>{train} = {S^i| i = 1,…,M}$ 其中 $S^i={(x^i_j,y^i_j)}</em>{j=1}^{n<em>i}$ 定义为第 $i$ 个域。每对域的联合分布是不同的：$P</em>{XY}^i \neq P<em>{XY}^j, 1\le i \neq j \le M$ 。域泛化的目标是在 $M$ 个训练域中学习一个稳健的、可泛化的预测函数 $h : X → Y$，以实现对未见过的测试域 $S</em>{test}$ 的最小预测误差（即在训练中不能访问 $S<em>{test}$，$P^{test}</em>{XY}\neq P^i_{XY}  i∈{1,…,M}$）:</p><script type="math/tex; mode=display">min_h E_{(x,y)\in S_{test}} [l(h(x),y)]</script><p>其中 $E$ 是期望， $l(\cdot,\cdot)$ 是loss 。我们在表1中列出了常用的记号。</p><p><img src="https://i.loli.net/2021/11/14/FiE4rGMZsNjq6pb.png" alt=""></p><h3 id="B-Related-research-areas"><a href="#B-Related-research-areas" class="headerlink" title="B. Related research areas"></a><em>B. Related research areas</em></h3><p>有几个研究领域与领域泛化密切相关，包括但不限于：迁移学习、领域适应、多任务学习、多领域学习、元学习、终身学习和 zero-shot 学习。我们在表二中总结了它们与领域泛化的区别，并在下文中简要介绍了它们。</p><p><img src="https://i.loli.net/2021/11/14/MPWAwsXG1t7dBH5.png" alt=""></p><ul><li>Multi-task learning ：联合优化了几个相关任务的模型。通过在这些任务之间共享表征，我们可以使模型在原来的任务上有更好的泛化能力。请注意，多任务学习的目的不是为了加强对新的（未见过的）任务的泛化。特别是，多领域学习是一种多任务学习，它在多个相关领域进行训练，为每个原始领域而不是新的测试领域学习好的模型。</li><li>Transfer learning：在一个源任务上训练一个模型，旨在提高该模型在不同但相关的目标领域/任务上的性能。预训练-微调是转移学习的常用策略，在这种情况下，源域和目标域有不同的任务，目标域在训练中被访问。在DG中，目标域不能被访问，训练和测试任务往往是相同的，而它们的分布是不同的。</li><li>Domain adaptation（DA）：在最近几年也很流行。DA的目的是利用现有的训练源域在给定的目标域上实现性能最大化。DA和DG的区别在于，DA可以接触到目标域的数据，而DG在训练过程中无法看到这些数据。这使得DG比DA更具挑战性，但在实际应用中更现实和有利。</li><li>Meta-learning ：旨在通过学习以前的经验或任务来学习学习者本身，即学会学习。虽然元学习中的学习任务是不同的，但在领域泛化中的学习任务是相同的。元学习是一种通用的学习策略，可以用于DG，通过模拟训练领域中的元训练和元测试任务来提高DG的性能。</li><li>终身学习，或持续学习：关注的是多个连续领域/任务之间的学习能力。它要求模型通过容纳新的知识，同时保留以前学到的经验，随着时间的推移不断地学习。这也与DG不同，因为它可以在每个时间步骤中访问目标域，而且它没有明确处理跨域的不同分布。</li><li>Zero-shot learning：旨在从已看到的类别中学习模型，并对在训练中未看到的类别的样本进行分类。与此相反，一般来说，领域概括研究的问题是训练和测试数据来自相同的类别，但分布不同。</li></ul><p>此外，领域泛化还与 tributionally robust optimization（DRO）有关，其目标是在最坏的分布情况下学习一个模型，希望它能够很好地泛化到测试数据。DRO关注的是优化过程，在领域泛化研究中也可以利用。此外，DG也可以通过数据操作或表征学习方法来完成，这与DRO方法不同。</p><h2 id="THEORY"><a href="#THEORY" class="headerlink" title="THEORY"></a>THEORY</h2><p>在本节中，我们将回顾一些与领域泛化相关的理论。由于领域适应与领域泛化密切相关，我们从领域适应的理论开始。</p><h3 id="A-Domain-adaptation"><a href="#A-Domain-adaptation" class="headerlink" title="A. Domain adaptation"></a><em>A. Domain adaptation</em></h3><p>对于一个二元分类问题，我们把源域上的真实标签函数表示为 $h^{∗s} : X → [0,1]$，目标域上的真标签函数为 $h^{∗t}$。让 $h:X →[0,1]$ 是来自假设空间 $H$ 的任何分类器。然后，源域上两个分类器 $h$ 和 $h’$之间的分类误差可以通过以下方式测量 :</p><script type="math/tex; mode=display">\epsilon^s(h,h') = E_{x\sim P^s_X} [h(x) \neq h'(x)] = E_{x\sim P^s_X} [|h(x) - h'(x)|]</script><p>同样地，我们可以定义 $ε^t$，当取 $x∼P^t_X$ 的期望时。定义 $\epsilon^s(h) := \epsilon^s(h,h^{<em>s})$ 并且 $\epsilon^t(h):=\epsilon^t(h,h^{</em>t})$ 作为分类器 $h$ 在源域和目标域上的风险。</p><p>DG/DA 的目标是使目标风险 $ε^t(h)$ 最小。但由于我们没有任何关于 $h^{∗t}$ 的信息，所以不容易达到这个目标。因此，人们试图用可操作的源风险 $ε^s(h)$来约束目标风险 $ε^t(h)$。 Ben-David等人给出两种风险的界限：</p><script type="math/tex; mode=display">\epsilon^t(h) \leq \epsilon^s(h) +2d_1(P_X^s,P_X^t)+min_{P_{X}\in \{P^s_X,P^t_X\}} E_{x\sim P_X}[|h^{*s}(x) - h^{*t}(x)|]</script><p>其中 $d<em>1 (P_X^s , P_X^t ) := sup</em>{A∈X} |P_X^s [A] - P_X^t [A]| $ 是两个分布之间的总变化，$\mathbf{X}$ 表示 $X$ 上的 σ场。r.h.s 上的第二项衡量的是两个领域中分布的差异，第三项代表的是标签函数的差异。</p><p>然而，总变化是一个强距离（即，它倾向于非常大），可能会使约束（1）松动，并且很难使用有限样本来估计。为了解决这个问题，Ben-David等人开发了另一个约束 ：</p><script type="math/tex; mode=display">\epsilon^t(h) \le \epsilon^s(h) + d_{H\Delta H}(P^s_X,P^t_X) + \lambda_H</script><p>其中，H∆H-divergence 定义为 $d<em>{H∆H}(P_X^s , P_X^t ) := sup</em>{h,h’∈H} |ε^s (h, h’ ) - ε^t (h, h’)|$，取代总变异d1来衡量分布差异，理想联合风险 $λ<em>H : = inf</em>{h∈H} [ε^s (h) + ε^t (h)]$ 衡量 $H$ 在两个领域的预测任务中的复杂性。</p><h3 id="B-Domain-generalization"><a href="#B-Domain-generalization" class="headerlink" title="B. Domain generalization"></a><em>B. Domain generalization</em></h3><ul><li>数据操作，指的是通过对数据的增强和变化使训练数据得到增强。这一类包括数据增强和数据生成两大部分。</li><li>表征学习，指的是学习领域不变特征（Domain-invariant representation learning）以使得模型对不同领域都能进行很好地适配。领域不变特征学习方面主要包括四大部分：核方法、显式特征对齐、领域对抗训练、以及不变风险最小化（Invariant Risk Minimiation, IRM）。特征解耦与领域不变特征学习的目标一致、但学习方法不一致，我们将其单独作为一大类进行介绍。</li><li>学习策略，指的是将机器学习中成熟的学习模式引入多领域训练中使得模型泛化性更强。这一部分主要包括基于集成学习和元学习的方法。同时，我们还会介绍其他方法，例如自监督方法在领域泛化中的应用。</li></ul><h2 id="METHODOLOGY"><a href="#METHODOLOGY" class="headerlink" title="METHODOLOGY"></a>METHODOLOGY</h2><p><img src="https://i.loli.net/2021/11/14/o2gSpyE89cRQkd5.png" alt=""></p><h3 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h3><p>在机器学习（ML）中，我们总是渴望得到更多的训练数据。一个ML模型的泛化性能往往依赖于训练数据的数量和多样性。考虑到一组有限的训练数据，数据处理是产生样本的最便宜和最简单的方法之一，这样可以提高模型的泛化能力。基于数据处理的DG的主要目标是使用不同的数据处理方法来增加现有训练数据的多样性。同时，数据数量也会增加。</p><p>我们将基于数据操作的DG的总体学习目标制定为：</p><script type="math/tex; mode=display">min_h E_{x,y} [l(h(x), y)] + E_{x',y}[l(h(x'),y)]</script><p>其中 x′=mani(x) 表示使用函数mani(-)操纵的数据。基于这个函数上的差异，我们进一步将现有的工作分为两类：数据增强和数据生成。</p><ol><li><p>基于数据增强的DG：数据增量。扩增是训练机器学习模型的最有用的技术之一。典型的增强操作包括翻转、旋转、缩放、裁剪、添加噪声等等。它们已被广泛用于监督学习中，通过减少过拟合来提高模型的泛化性能[117, 34]。毫无例外，它们也可以被采用于DG，其中mani(-)可以被实例化为这些数据增强函数。</p><p><strong>领域随机化</strong>。除了典型的扩增，领域随机化是一种有效的数据扩增技术。它通常通过生成新的数据来完成，这些数据可以在有限的训练样本基础上模拟复杂的环境。随着数据变得更加复杂和多样化，泛化能力可以得到提高。在这里，mani(-)函数被实现为几个手动变换（常用于图像数据），如：改变物体的位置和纹理，改变物体的数量和形状，修改光照和相机视角，以及在数据中加入不同类型的随机噪声。<br>Tobin等人[29]首先使用这种方法从模拟环境中产生更多的训练数据，以便在真实环境中进行概括。类似的技术也被用于[30, 31, 32, 28]以加强模型的泛化能力。Prakash等人[33]在随机放置物体进行数据生成时进一步考虑了场景的结构，这使得神经网络在检测物体时能够学会利用上下文。</p><p><strong>抗辩式数据增强</strong>。抗辩式数据增强的目的是通过增强数据的多样性，同时保证数据的可靠性，来指导增强工作，优化泛化能力。Shankar等人[35]使用贝叶斯网络来模拟标签、行为和输入实例之间的依赖性，并提出了CrossGrad，这是一种谨慎的数据增强策略，沿最大领域变化的方向对输入进行扰动，同时尽可能少地改变类标签。Volpi等人[36]提出了一个迭代程序，用一个虚构的目标领域的例子来增加源数据集，该领域在当前模型下是 “硬 “的，在每次迭代中都会添加对抗性例子，以实现自适应数据增强。Zhou等人[37]用对抗性训练的转换网络进行数据扩充，而不是直接用梯度上升法更新输入。</p></li></ol><p>2) 基于数据生成的DG：数据生成也是一种流行的技术，可以生成多样化和丰富的数据来提高模型的泛化能力。在这里，函数mani(-)可以使用一些生成模型来实现，如变异自动编码器（VAE）[118]，以及生成对抗网络（GAN）[119]。此外，它也可以使用Mixup[120]策略来实现。<br>Rahman等人[38]使用ComboGAN[121]来生成新的数据，然后应用领域差异度量，如MMD[122]来最小化真实图像和生成图像之间的分布分歧，以帮助学习一般代表。Qiao等人[39]利用对抗性训练来创建 “虚构 “而又 “具有挑战性 “的人群，其中Wasserstein Auto-Encoder（WAE）[123]被用来帮助生成保留语义并具有大领域转移的样本。Zhou等人[42]在语义一致的情况下生成了新的分布，然后将源分布和新分布之间的差异最大化。Somavarapu等人[43]引入了一个基于图像风格化的简单转换，以探索跨源的可变性，从而实现更好的泛化，其中AdaIN[124]被用来实现快速风格化的任意风格。<br>除了上述生成模型外，Mixup[120]也是一种流行的数据生成技术。Mixup通过在任何两个实例之间和它们的标签之间进行线性插值，并从Beta分布中抽取权重来生成新数据，这不需要训练生成式模型。最近，有几种方法将Mixup用于DG，通过在原始空间[47, 48]中执行Mixup来生成新的样本；或者在特征空间[49]中，不明确生成原始训练样本。这些方法在流行的基准上取得了很好的性能，同时在概念上和计算上保持简单。</p><h3 id="表征学习"><a href="#表征学习" class="headerlink" title="表征学习"></a>表征学习</h3><p>几十年来，表征学习一直是机器学习的重点[125]，也是领域泛化成功的关键之一。我们将预测函数h分解为 $h = f ◦ g$，其中g是一个表征学习函数，f是分类器函数。表征学习的目标可以被表述为：</p><script type="math/tex; mode=display">min_{f,g} E_{x,g}l(f(g(x)),y) + \lambda l_{reg}</script><p>其中 $l<em>{reg}$ 表示一些正则化项，λ是权衡参数。许多方法都是为了更好地学习具有相应 $l</em>{reg}$ 的特征提取函数 $g$。在本节中，我们根据不同的学习原理将现有的关于表征学习的文献分为两大类：领域不变的表征学习和特征分解。</p><p>1) 基于领域不变表征的DG：[23]的工作从理论上证明，如果特征表征对不同领域保持不变，那么表征就是通用的，可以转移到不同领域（也可以参考第三节）。基于这一理论，人们提出了大量用于领域适应的算法。同样，对于领域泛化来说，目标是减少特定特征空间中多个源域之间的表征差异，使之成为领域不变的，这样学到的模型就能对未见过的领域具有可泛化的能力。按照这一思路，主要有四种方法：基于核的方法、领域对抗性学习、显式特征对齐和不变风险最小化。</p><pre><code>**基于核的方法**是机器学习中最经典的学习范式之一。基于核的机器学习依靠核函数将原始数据转化为高维特征空间，而不需要计算数据在该空间的坐标，只需要计算特征空间中所有对的样本之间的内积。最具代表性的基于核的方法之一是支持向量机（SVM）[126]。对于领域泛化，有很多基于核方法的算法，其中表征学习函数g被实现为一些特征映射φ(-)，这很容易用核函数k(-, -)来计算，如RBF核和Laplacian核。</code></pre><p>   Blanchard等人[24]首次将核方法用于领域泛化，并在[52]中加以扩展。他们采用正半无限核学习，从训练数据中学习一个领域不变的核。Grubinger等人[53]采用转移成分分析法（TCA）[127]来弥补多领域的距离，使之更接近于DG。与TCA的核心思想类似，域不变分量分析（DICA）[25]是使用核进行DG的经典方法之一。DICA的目标是找到一个特征转换核k(-, -)，使特征空间中所有数据之间的分布差异最小。Gan等人[54]采用了与DICA类似的方法，并进一步增加了属性正则化。与处理边际分布的DICA相比，Li等人[55]学习了一种具有领域不变的类条件分布的特征表示。Ghifary等人[56]采用Fisher判别分析法来最小化来自同一类别和同一领域的表示的差异，最大化来自不同类别和不同领域的表示的差异。他们提出了散点成分分析（SCA），使用基于核的方法学习领域不变的表征，其中SCA核考虑了类间和类内的差异。Erfani等人[57]提出了椭圆总结随机化（ESRand），它包括一个随机内核和椭圆数据总结。ESRand将每个领域投射到一个椭圆中以表示领域信息，然后使用一些相似性指标来计算距离。Hu等人[58]提出了多域判别分析法来对DG进行分类核学习，这种方法的粒度更细。Mahajan等人[74]提出使用因果匹配来学习分解表征，他们提出MatchDG来匹配在结果表征下相似的输入，以建立一个不变的分类器。对于领域泛化，一些基于核的方法的理论分析可以从[24, 26]中找到。</p><p>   <strong>领域对抗性学习</strong>。领域对抗性训练被广泛用于学习领域不变的特征。Ganin和Lempitsky[77]和Ganin等人[78]提出了用于领域适应的领域对抗性神经网络（DANN），它对生成器和鉴别器进行对抗性训练。鉴别器被训练来区分领域，而生成器被训练来愚弄鉴别器以学习领域不变的特征表示。Li等人[64]对DG采用了这种想法。Gong等人[79]通过逐渐减少流形空间中的域差异来使用对抗性训练。Li等人[80]提出了一个条件不变的对抗网络（CIAN），用于学习DG的分类对抗网络。类似的想法也被用在[81, 85, 86]中。Jia等人[87]使用单边对抗学习和非对称三倍损失来确保只有来自不同领域的真实面孔是不可区分的，但对于假的面孔则不是。除了对抗性领域分类，Zhao等人[82]通过最小化不同训练领域的条件分布之间的KL分歧，引入了额外的熵正则化，以推动网络学习领域不变的特征。其他一些基于GAN的方法[45, 84, 27]也被提出，在理论上保证了泛化边界。</p><p>   <strong>明确的特征对齐</strong>。这类工作通过明确的特征分布对齐[64, 128, 129]，或特征归一化[130, 131, 132, 62]，将各源域的特征进行对齐，以学习领域不变的表示。<br>   Motiian等人[60]为表征学习引入了一个跨域对比性损失，其中映射的域在语义上是一致的，但又是最大限度的分离。一些方法通过最小化最大平均差异（MMD）[133, 127, 134, 135]、二阶相关[136, 137, 138]、均值和方差（时刻匹配）[129]、Wasserstein距离[128]等，明确地将特征分布分歧最小化，用于域适应或域概括。Zhou等人[128]通过最小化Wasserstein距离，通过最优传输将不同源域的边际分布对齐，实现域不变的特征空间。</p><p>   此外，也有一些工作使用特征归一化技术来提高领域泛化能力[130, 131]。Pan等人[130]将实例归一化（IN）层引入CNN，以提高模型的泛化能力。IN在图像风格转换领域得到了广泛的研究[139, 140, 124]，图像的风格由IN参数反映，即每个特征通道的平均值和方差。因此，IN层[141]可用于消除特定实例的风格差异，以增强概括性[130]。然而，IN是与任务无关的，可能会消除一些鉴别性的信息。在IBNNet中，IN和批量归一化（BN）被平行利用，以保留一些鉴别性信息[130]。在[132]中，BN层被批量归一化（BIN）层所取代，它通过选择性地使用BN和IN来自适应平衡每个通道的BN和IN。Jin等人[62, 63]提出了一个风格归一化和恢复（SNR）模块，以同时确保网络的高泛化和辨别能力。在通过IN进行风格归一化后，进行还原步骤，从残差（即原始特征和风格归一化特征之间的差异）中提炼出与任务相关的鉴别特征，并将其添加到网络中以确保高鉴别力。恢复的想法被扩展到其他基于对齐的方法，以恢复因对齐而丢失的有用的鉴别性信息[142]。</p><p>   <strong>不变风险最小化（IRM）</strong>。Arjovsky等人[88]从另一个角度考虑了领域泛化的表征的领域不变性。他们并不寻求与所有领域的表征分布相匹配，而是强制要求表征空间之上的最优分类器在所有领域都是一样的。其直觉是，预测的理想表征是y的原因，而因果机制不应受到其他因素/机制的影响，因此是领域不变的。</p><p>   对 f 的约束体现了所有域共享相同的表示级分类器的愿望，而目标函数鼓励 f 和 g 实现低源域风险。然而，这个问题很难解决，因为它的约束条件中涉及到一个内部层次的优化问题。然后，作者开发了一个代用问题来学习特征提取器g，这个问题更加实用。</p><p>   其中，一个假的表示级分类器f = 1被认为是，梯度规范项衡量这个分类器的最优性。这项工作还提出了一个可能是强线性假设下的泛化理论，即对于足够多的源域，可以确定基础真理不变的分类器。</p><p>   由于新的概念和容易实现，IRM最近获得了明显的知名度。有一些关于IRM的成功[143]和失败案例的进一步理论分析[92]，而且IRM已经被扩展到其他任务，包括文本分类[91]和强化学习[144]。追求最优表示级分类器的不变性的想法也得到了扩展。Krueger等人[145]通过最小化源域之间的外推风险来促进这种不变性，这基本上是最小化源域风险的方差。Mitrovic等人[146]的目标是在自我监督的设置中学习这样的表征，其中第二域是通过显示各种语义不相关的变化的数据增量构建的。</p><ol><li><p>基于特征解缠的DG：解缠表征学习的目标是学习一个函数，该函数将样本映射到一个特征向量，该向量包含关于不同变化因素的所有信息，每个维度（或维度的一个子集）只包含关于某些因素的信息。基于Disentanglement的领域泛化方法一般将一个特征表示分解为可理解的组合/子特征，其中一个特征是领域共享/不变的特征，另一个是领域特定的特征。基于解缠的DG的优化目标可以总结为：</p><script type="math/tex; mode=display">min_{g_c,g_s,f} E_{x,y}l(f(g_c(x)), y) + \lambda l_{reg}+\mu l_{recon}([g_c(x),g_s(x)],x)</script><p>其中 $g<em>c$ 和 $g_s$ 分别表示领域共享和领域特定的特征表示。 λ，μ是权衡参数。损失 $l</em>{reg} $是一个正则化项，明确鼓励领域共享特征和特定特征的分离，$l_{recon}$表示一个重建损失，防止信息的损失。注意，$[g_c (x), g_s (x)]$表示两种特征的组合/整合（不限于连接操作）。</p><p>根据网络结构和实现机制的选择，基于解缠的DG方法主要可分为两类：多分量分析和生成模型。</p><p><strong>多组份分析</strong>。在多成分分析中，一般来说，使用领域共享和领域特定的网络参数来提取领域共享和领域特定的特征。UndoBias[94]的方法从SVM模型出发，在所有的训练数据上最大化区间分类，以实现领域泛化。他们将第 $i$ 个领域的参数表示为 $w_i = w_0 + ∆_i$，其中 $w_0$ 表示领域共享参数，$∆_i$表示领域特定参数。其他一些方法从不同方面扩展了UndoBias的思想。Niu等人[95]提出使用多视图学习进行领域泛化。他们提出了多视图DG（MVDG）来学习不同视图下的示范性SVM的组合，以实现稳健的泛化。Ding和Fu[96]为每个领域设计了特定领域的网络，并为所有领域设计了一个共享的领域变量网络，以学习不相干的表征，其中采用低秩重建的方式将两类网络以结构化的低秩方式对齐。Li等人[1]将UndoBias的思想扩展到神经网络背景下，并开发了一个用于端到端训练的低秩参数化CNN模型。Zunino等人[97]通过手动比较不同领域的某些区域的注意力热图来学习分解表征。还有其他一些作品采用多成分分析法进行解构[98, 93, 99, 100]。</p><p><strong>生成式模型</strong>。生成模型可用于从数据生成过程的角度进行拆分。这类方法试图从领域层面、样本层面和标签层面制定样本的生成机制。有些工作进一步将输入分解为与类别无关的特征，这些特征包含与特定实例相关的信息。Domain-invariant variational autoencoder (DIVA) [101]将特征分解为domain信息、类别信息和其他信息，这是在VAE框架下学习的。Peng等人[102]将细粒度的领域信息和类别信息进行了分解，这些信息在VAE中被学习。Qiao等人[39]也将VAE用于拆分，他们提出了一个统一特征拆分网络（UFDN），将感兴趣的数据域和图像属性都作为潜在因素来拆分。Liu等人[103]基于数据生成过程的因果观，提出了领域适应和领域泛化的统一解决方案，并为语义表示和泛化误差边界制定了一个识别保证。</p></li></ol><h3 id="学习策略"><a href="#学习策略" class="headerlink" title="学习策略"></a>学习策略</h3><p>除了数据处理和表征学习，DG也被研究在一般的机器学习范式中，分为四类：基于集合学习的DG、基于元学习的DG、基于梯度运算的DG和其他。</p><p>1) 基于集合学习的通用技术：集合学习通常将多个模型（如分类器或专家）结合起来，以增强模型的力量。对于领域泛化，集合学习通过使用特定的网络架构设计和训练策略，利用多个源领域之间的关系来提高泛化能力。他们认为任何样本都可以被看作是多个源域的综合样本，因此整体预测结果可以看作是多个域网络的叠加。</p><p>   Mancini等人[104]提出使用可学习的权重来汇总来自不同来源特定分类器的预测，其中一个领域预测器被用来预测样本属于每个领域的概率（权重）。Segu`等人[61]为不同的源域维护了与域相关的批判性归一化（BN）统计数据和BN参数，而所有其他参数是共享的。在推理中，最终的预测是依赖于域的模型的线性组合，其组合权重是通过测量测试样本的实例归一化统计数据和每个域的累积群体统计数据之间的距离推断出来的。105]的工作提出了不同源域的特定域层，并学习这些层的线性聚合来代表测试样本。Zhou等人[3]提出了领域自适应集合学习（DAEL），其中DAEL模型是由一个跨领域共享的CNN特征提取器和多个特定领域的分类器头组成。每个分类器对自己的领域来说是专家，对其他领域来说则是非专家。DAEL的目的是协同学习这些专家，通过对非专家的教学，鼓励集合体学习如何处理来自<br>   未见过的领域的数据。</p><p>2) 基于元学习的DG：元学习的关键思想是通过基于优化的方法[147]或基于模型的方法[149]，从多个任务中学习通用模型。是通过基于优化的方法[147]、基于度量的学习[148]或基于模型的方法[149]，从多个任务中学习一个通用模型。元学习的思想已经被利用于领域概括。他们将多源领域的数据划分为元训练集和元测试集，以模拟领域转移。用θ表示要学习的模型参数，元学习可以表述为：</p><script type="math/tex; mode=display">       \begin{equation}\begin{split}     \theta^* &= Learn(S_{mte};\phi^*) \\    &= Learn(S_{mte}; MetaLearn(S_{mtrn}))       \end{split}\end{equation}</script><p>   其中 $φ^*=MetaLearn(S<em>{mtrn})$ 表示从元训练集 $S</em>{mtrn}$ 中元学习的参数，然后用于在元测试集 $S_{mte}$ 上学习模型参数θ∗。这两个函数 Learn(-) 和 MetaLearn(-) 要由不同的元学习算法来设计和实现，这相当于一个双级优化问题。梯度更新可以表述为：</p><script type="math/tex; mode=display">   \theta = \theta - \alpha \frac{\partial(l(S_{mte};\theta) + \beta l(S_{mtrn}; \phi)) }{\partial \theta}</script><p>   其中η,β分别是外循环和内循环的学习率。</p><p>   Finn等人[147]提出了模型不可知元学习（MAML）。受MAML的启发，Li等人[12]提出了MLDG（meta-learning for domain generalization），将元学习策略用于DG。MLDG将源域的数据拆分为元训练和元测试，以模拟域转移的情况来学习一般的表征。Balaji等人[13]提出要为分类器学习元正则器（MetaReg）。[14]提出通过设计元优化器对特征提取器进行特征批评训练。Dou等人[108]使用了类似于MLDG的思想，另外还引入了两个互补的损失来明确规范化特征空间的语义结构。Du等人[15]提出了一个信息瓶颈的扩展版本，名为Meta Variational Information Bottleneck（MetaVIB）。他们对来自不同领域的具有相同类别的样本的潜在编码分布之间的Kullback-Leibler（KL）分歧进行正则化，并通过使用随机神经网络来学习生成权重。最近，一些作品也采用了元学习的方式来进行半监督式DG或判别式DG[109, 110, 111, 44]。</p><ol><li>基于梯度操作的DG：除了元学习和集合学习之外，最近的一些工作考虑使用梯度信息来迫使网络学习一般化的表征。Huang等人[112]提出了一种自我挑战的训练算法，旨在通过操纵梯度来学习一般表征。他们迭代地丢弃了在训练数据上激活的主导特征，并迫使网络激活与标签相关的剩余特征。通过这种方式，网络可以被迫从更多的坏情况中学习，这将提高泛化能力。Shi等人[113]提出了一个梯度匹配方案，他们的假设是两个领域的梯度方向应该是相同的，以增强共同表征学习。为此，他们提出将梯度内积（GIP）最大化，以调整各域的梯度方向。通过这种操作，网络可以找到权重，使输入-输出的对应关系在各领域中尽可能接近。GIP可以被表述为：<script type="math/tex; mode=display">L = L_{cls}(S_{train};\theta) - \lambda\frac{2}{M(M-1)} \sum_{i,j}^{i\neq j}G_i\cdot G_j</script>其中 $G_i, G_j$ 是两个域的梯度，可以被 $G = E \frac{\partial l(x,y;\theta)}{\partial\theta}$ 计算得到</li></ol><h3 id="其他学习策略"><a href="#其他学习策略" class="headerlink" title="其他学习策略"></a>其他学习策略</h3><p>还有一些其他的学习策略用于领域泛化。例如，自我监督学习是最近流行的一种学习范式，它从大规模的无标签数据中建立自我监督任务[150]。受此启发，Carlucci等人[114]引入了一个解决拼图的自我监督任务来学习泛化表征。<br>Li等人[116]交替地学习了特征提取器和分类器。首先，他们固定分类器来学习最坏情况下的特征提取器；然后，他们固定特征提取器来学习最坏情况下的分类器。通过这样的偶发训练，最终特征提取器和分类器可以从这些最坏情况中学习到更好的分布外任务的表现。Ryu等人[115]使用随机森林来提高卷积神经网络（CNN）的泛化能力。他们根据随机森林给出的拆分结果的概率质量函数对三联体进行采样，用于通过三联体损失更新CNN的参数。在未来，将有更多的工作用于使用其他学习策略的DG。</p><h2 id="Future-research-challenges"><a href="#Future-research-challenges" class="headerlink" title="Future research challenges"></a>Future research challenges</h2><p>总结了未来在领域泛化方面的一些研究挑战。</p><p>1）<strong><em>Continuous domain generalization</em> 连续的领域泛化</strong>。在许多实际应用中，系统消耗的是具有非平稳统计数据的流媒体数据。在这种情况下，进行连续的领域泛化是非常重要的，它可以有效地更新DG模型以克服灾难性的遗忘并适应新的数据。虽然有一些专注于连续学习的领域适应方法[166]，但只有很少的关于连续DG的调查[167]，而这在实际场景中是有利的。</p><p>2）<strong>对新类别的领域泛化</strong>。现有的DG算法通常假定不同领域的标签空间是相同的。一个更实际、更普遍的设定是支持对新类别的泛化，也就是领域和任务的泛化。这在概念上类似于元学习和 zero-shot 学习的目标。一些工作[65,168]提出了  zero-shot DG，我们期望在这个领域有更多的工作。</p><p>3）<strong>可解释的领域概括</strong>。基于Disentanglement的DG方法将一个特征分解为领域不变/共享和领域特定的部分，为DG提供一些解释。对于其他类别的方法，目前仍然缺乏对DG模型中学习到的特征的语义或特征的深入理解。因果关系[103]可能是理解领域泛化网络并提供解释的一个有前途的工具。</p><p>4）<strong>大规模预训练/自学和DG</strong>：近年来，我们见证了大规模预训练/自学的快速发展，如BERT[169]、GPT- 3[170]和Wav2vec[171]。在大规模数据集上进行预训练，然后根据下游任务对模型进行微调，可以提高其性能，其中预训练有利于学习一般表征。因此，如何设计有用和高效的DG方法来帮助大规模的预训练/自我学习是值得研究的。</p><p>5）<strong>DG的性能评估</strong>：最近的工作[71]指出，在几个数据集上，一些DG方法的性能几乎与基线方法（即经验风险最小化）相同。我们不认为这就是DG在实际应用中没有用处的全部证据。相反，我们认为这可能是由于现在使用的评估方案不合适，或者是领域的差距没有那么大。在更现实的情况下，比如存在明显领域差距的人际关系[63]，DG的改进是巨大的。因此，我们对DG的价值保持肯定，并希望研究人员也能找到更合适的设置和数据集进行研究。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Generalizing-to-Unseen-Domains-A-Survey-on-Domain-Generalization&quot;&gt;&lt;a href=&quot;#Generalizing-to-Unseen-Domains-A-Survey-on-Domain-Genera</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>EANN: Event Adversarial Neural Networks for Multi-Modal Fake News Detection</title>
    <link href="http://example.com/2021/11/12/EANN-Event-Adversarial-Neural-Networks-for-Multi-Modal-Fake-News-Detection/"/>
    <id>http://example.com/2021/11/12/EANN-Event-Adversarial-Neural-Networks-for-Multi-Modal-Fake-News-Detection/</id>
    <published>2021-11-12T06:39:26.000Z</published>
    <updated>2021-11-12T13:28:49.884Z</updated>
    
    <content type="html"><![CDATA[<h1 id="EANN-Event-Adversarial-Neural-Networks-for-Multi-Modal-Fake-News-Detection"><a href="#EANN-Event-Adversarial-Neural-Networks-for-Multi-Modal-Fake-News-Detection" class="headerlink" title="EANN: Event Adversarial Neural Networks for Multi-Modal Fake News Detection"></a>EANN: Event Adversarial Neural Networks for Multi-Modal Fake News Detection</h1><p>社交媒体上的假新闻检测的独特挑战之一是如何识别新出现的事件的假新闻。</p><p>大多数现有的方法很难应对这一挑战，因为它们倾向于学习特定于事件的特征，这些特征不能迁移到看不见的事件。</p><p>事件对抗神经网络(EANN)，它可以提取事件不变的特征，从而有利于对新到达的事件进行假新闻的检测。包括三个主要部分：</p><ul><li>多模态特征提取器：负责从帖子中提取文本和视觉特征</li><li>假新闻检测器：学习用于检测假新闻的可判别表示</li><li>事件鉴别器：去除事件的特定特征，并保留事件间的共享特征</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>最近，社交媒体的激增大大改变了人们获取信息的方式。如今，通过社交媒体消费新闻的人越来越多，社交媒体可以为世界各地发生的事件提供及时、全面的多媒体信息。与传统的文字新闻相比，带有图片和视频的新闻可以提供更好的故事性，吸引更多读者的关注。不幸的是，这也被假新闻所利用，它们通常包含错误的甚至是伪造的图片，以误导读者并获得快速传播。</p><p>假新闻的传播可能造成大规模的负面影响，有时会影响甚至操纵重要的公共事件。例如，在2016年美国总统大选的最后三个月内，为支持两位提名人中的任何一位而产生的假新闻被很多人相信，在Facebook上的分享次数超过3700万次。因此，非常需要一个自动检测器来减轻假新闻造成的严重负面影响。</p><p>到目前为止，各种假新闻检测方法，包括传统学习[6, 15, 29]和基于深度学习的模型[21, 25]，都被利用来识别假新闻。在对不同事件进行充分验证的情况下，现有的深度学习模型由于其卓越的特征提取能力，已经取得了比传统模型更好的性能。<strong>然而，它们仍然无法处理假新闻检测的独特挑战，即检测新出现的和时间关键的事件上的假新闻[27]</strong>。由<strong>于缺乏相应的先验知识，关于这类事件的经过验证的帖子很难及时获得，这导致现有模型的性能不尽人意。</strong>事实上，<strong>现有的模型倾向于捕捉许多事件的特定特征，这些特征在不同的事件中并不共享。</strong>这些<strong>特定的事件特征，虽然能够帮助对已验证的事件进行分类，但会影响对新出现的事件的检测</strong>。出于这个原因，我们<strong>认为学习所有事件中的共享特征将有助于我们从未经核实的帖子中检测出假新闻，而不是捕捉事件的具体特征</strong>。因此，这项工作的目标是设计一个有效的模型，<strong>去除不可转移的特定事件特征，保留所有事件中的共享特征</strong>，以完成识别假新闻的任务。</p><p><strong>要删除事件的具体特征，第一步是要识别它们。对于不同事件的帖子，它们有自己独特的或特定的特征，是不可共享的。这种特征可以通过测量对应于不同事件的帖子之间的差异来检测。</strong>在这里，<strong>帖子可以用学到的特征来表示。因此，识别事件的特定特征等同于测量不同事件的学习特征之间的差异。</strong>然而，这是一个在技术上具有挑战性的问题。首先，由于帖子的学习特征表示是高维的，像平方误差这样的简单指标可能无法估计这种复杂特征表示之间的差异。其次，在训练阶段，特征表示不断变化。这就要求所提出的测量机制能够捕捉到特征表征的变化并持续提供准确的测量。尽管这非常具有挑战性，但<strong>有效估计不同事件上所学特征的差异性是去除事件特定特征的前提。</strong>因此，如何在这种条件下有效地估计异同性是我们必须解决的挑战。</p><p>为了应对上述挑战，提出了一个端到端的框架，称为事件对抗神经网络（EANN），用于基于多模态特征的假新闻检测。受对抗网络的启发，我们在训练阶段加入了<strong>事件判别器来预测事件的辅助标签，而相应的损失可以用来估计不同事件之间特征表示的不相似性。损失越大，不相似性越低。</strong>由于假新闻利用多媒体内容来误导读者并得到传播，我们的模型需要处理多模态的输入。多模态特征表示仍然高度依赖于数据集中的特定事件，不能很好地泛化为识别新来事件的假新闻。</p><p>受到对抗网络的启发。现有的对抗网络通常用于生成能够与观察到的样本相匹配的图像，通过最小化博弈框架。对抗性学习框架已被用于一些任务，如半监督学习的表征[23]、鉴别性的图像特征[20]和领域适应[8, 9]。</p><p><strong>模型还在事件判别器和多模态特征提取器之间建立了一个最小化博弈。特别是，多模态特征提取器被强制要求学习一个事件不变的表征来欺骗判别器。通过这种方式，它消除了对所收集的数据集中特定事件的严格依赖，并对未见过的事件实现了更好的概括能力。</strong></p><h2 id="METHODOLOGY"><a href="#METHODOLOGY" class="headerlink" title="METHODOLOGY"></a>METHODOLOGY</h2><p><img src="https://i.loli.net/2021/11/12/wZ1o2DQRTXMnylm.png" alt=""></p><p>目标是为假新闻检测学习可迁移和可鉴别的特征表示。假新闻检测器和事件判别器都是建立在多模式特征提取器之上的。</p><ul><li>假新闻检测器将学到的特征表示作为输入，预测帖子是假的还是真的。</li><li>事件判别器根据这个潜在的表征来识别每个帖子的事件标签。</li></ul><h3 id="Multi-Modal-Feature-Extractor"><a href="#Multi-Modal-Feature-Extractor" class="headerlink" title="Multi-Modal Feature Extractor"></a>Multi-Modal Feature Extractor</h3><p>文本表征 $R_T$  和 视觉表征 $R_V$， 多模态特征为：$R_F = R_T \oplus R_V \in R^{2p}$</p><h3 id="Fake-News-Detector"><a href="#Fake-News-Detector" class="headerlink" title="Fake News Detector"></a>Fake News Detector</h3><p>它部署了一个带有softmax的全连接层来预测帖子是假的还是真的。假新闻检测器是建立在多模态特征提取器之上的，因此将多模态特征表 $R_F$ 作为输入。我们将假新闻检测器表示为 $G_d(\cdot;θ_d)$，其中 $θ_d$ 代表所有包含的参数。对于第 $i$ 个多媒体帖子，假新闻检测器的输出表示为 $m_i$，是这个帖子是假的概率:</p><script type="math/tex; mode=display">P_{\theta}(m_i) = G_d(G_f(m_i;\theta_f); \theta_d)</script><p>假新闻检测器的目标是识别一个特定的帖子是否是假新闻。我们使用 $Y_d$ 来表示标签集，并采用交叉熵来计算检测损失。</p><script type="math/tex; mode=display">L_d(\theta_f,\theta_d) = -E_{(m,y)\sim(M,Y_d)}[ylog(P_{\theta}(m))+ (1-y)(log(1-P_{\theta}(m)))]</script><p>我们通过寻求最佳参数 $\hat θ_f$和 $\hat θ_d $ 来最小化检测损失函数，这个过程可以表示为：</p><script type="math/tex; mode=display">(\hat\theta_f,\hat\theta_d) = argmin_{\theta_f,\theta_d} L_d(\theta_f,\theta_d)</script><p>如前所述，<strong>假新闻检测的主要挑战之一是训练数据集没有涵盖的事件</strong>。<strong>这就要求我们能够为新出现的事件学习可迁移的特征表示</strong>。<strong>检测损失的直接最小化只有助于检测训练数据集中所包含的事件的假新闻，因为这只捕捉到了特定事件的知识（如关键词）或模式，不能很好地进行推广。</strong></p><p>因此，我们需要使模型能够学习更多的一般特征表示，<strong>以捕捉所有事件中的共同特征</strong>。这样的表征应该是事件不变的，不包括任何事件的特定特征。<strong>为了实现这一目标，我们需要消除每个事件的独特性</strong>。特别是，<strong>我们测量不同事件中特征表征的不相似性，并将其去除，以捕获事件不变的特征表征。</strong></p><h3 id="Event-Discriminator"><a href="#Event-Discriminator" class="headerlink" title="Event Discriminator"></a>Event Discriminator</h3><p>事件判别器是一个神经网络，由两个具有相应激活函数的全连接层组成。它的<strong>目的是根据多模态特征表示，将帖子正确地分类为K个事件之一</strong>。我们将事件判别器表示为 $G_e (R_F; θ_e)$，其中 $θ_e$ 代表其参数。我们用交叉熵来定义事件判别器的损失，并使用 $Y_e$ 来表示事件标签的集合:</p><script type="math/tex; mode=display">L_e(\theta_f, \theta_e) = - E_{(m,y)\sim (M,Y_e)} [\sum_{k=1}^K 1_{[k=y]} log(G_e(G_f(m;\theta_f)); \theta_e)]</script><p>损失最小化的事件判别器的参数 $L_e (-, -)$被写成:</p><script type="math/tex; mode=display">\hat \theta_e = argmin_{\theta_e} L_e(\theta_f,\theta_e)</script><p>上述损失 $L_e (θ_f , \hat θ_e )$<strong>可以用来估计不同事件分布的不相似性</strong>。<strong>大的损失意味着不同事件的分布表征是相似的</strong>，而且学到的特征是事件不变量。因此，为了消除每个事件的唯一性，我们需要通过寻求最佳参数来最大化判别损失。</p><p>上述想法激发了多模态特征提取器和事件判别器之间的最小值博弈。</p><p><strong>一方面，多模态特征提取器试图愚弄事件判别器，使判别损失最大化；另一方面，事件判别器旨在发现包含在特征表示中的事件特定信息，以识别事件。</strong>下一小节将介绍三个部分的整合过程和最终的目标函数。</p><h3 id="Model-Integration"><a href="#Model-Integration" class="headerlink" title="Model Integration"></a>Model Integration</h3><p>在训练阶段，多模态特征提取器 $G_f(-;θ_f)$ 需要与假新闻检测器 $G_d(-;θ_d)$ 合作，使检测损失 $L_d(θ_f , θ_d)$ 最小，从而提高假新闻检测任务的性能。同时，多模态特征提取器 $G_f (-;θ_f)$试图欺骗事件判别器 $G_e (-; θˆe)$，通过最大化事件鉴别损失 $L_e (θ_f, θ_e)$ 来实现事件不变的表示。事件判别器 $G_e (R_F;θ_e)$试图通过最小化事件判别损失来识别基于多模态特征表示的每个事件。我们可以将这个三人游戏的最终损失定义为:</p><script type="math/tex; mode=display">L_{final} (\theta_f,\theta_d,\theta_e) = L_d(\theta_f, \theta_d) - \lambda L_e(\theta_f,\theta_e)</script><p>其中，$λ$ 控制着 【假新闻检测】 和 【事件识别】 的目标函数之间的权衡。在本文中，我们简单地将 $λ$ 设置为1，而不对权衡参数进行调整。对于最小化博弈，我们寻求的参数集是最终目标函数的鞍点:</p><script type="math/tex; mode=display">(\hat \theta_f,\hat \theta_d) = argmin_{\theta_f,\theta_d} L_{final} (\theta_f,\theta_d,\hat \theta_e)</script><script type="math/tex; mode=display">\hat \theta_e = argmax_{\theta_e} L_{final} (\hat \theta_f,\theta_e)</script><p>我们使用随机梯度下降法来解决上述问题。$θ_f$ 根据</p><script type="math/tex; mode=display">\theta_f \leftarrow \theta_f - \eta (\frac{\partial L_d}{\partial \theta_f} - \lambda \frac{\partial L_e}{\partial \theta_f})</script><p>进行更新。这里我们采用[8]中介绍的梯度反转层（GRL）。梯度反转层在前向阶段作为一个识别函数，它将梯度与 $-λ$ 相乘，并在反推阶段将结果传递给前层。GRL可以很容易地加在 多模态特征提取器和事件判别器 之间。</p><p>为了稳定训练过程，我们采用了[8]中的方法来衰减学习率η。</p><script type="math/tex; mode=display">\eta'=\frac{\eta}{(1+\alpha\cdot p)^{\beta}}</script><p>其中 $α=10，β=0.75$，p从0到1线性变化，对应于训练进度。所提出的事件对抗性神经网络（EANN）的详细步骤在算法1中进行了总结。</p><p><img src="https://i.loli.net/2021/11/12/XNKMlT6p2G8ngQE.png" alt=""></p><h2 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h2><p><img src="https://i.loli.net/2021/11/12/PUrRayJ4wCjKsTQ.png" alt=""></p><p>为了进一步分析事件判别器的有效性，我们将EANN-和EANN在微博测试集上用t-SNE[22]学习的文本特征RT定性为图4所示。每个帖子的标签是真实或虚假的。<br>从图4中，我们可以观察到，对于EANN-的方法，它可以学习到可分辨的特征 ，但学到的特征仍然是扭曲在一起的，特别是对于图4a的左边部分。相比之下，EANN模型学习到的特征表征更具可辨识性，而且在图4b所示的不同标签的样本之间有更大的隔离区域。这是因为在训练阶段，事件判别器试图消除特征表征与特定事件之间的依赖关系。在最小化博弈的帮助下，多模态特征提取器可以针对不同的事件学习不变的特征表征，并获得更强大的转移能力来检测新事件的假新闻。EANN-和EANN的比较证明，所提出的方法在事件判别器的作用下可以学习到更好的特征表征，从而取得更好的性能。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;EANN-Event-Adversarial-Neural-Networks-for-Multi-Modal-Fake-News-Detection&quot;&gt;&lt;a href=&quot;#EANN-Event-Adversarial-Neural-Networks-for-Mul</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer</title>
    <link href="http://example.com/2021/11/09/SPoT-Better-Frozen-Model-Adaptation-through-Soft-Prompt-Transfer/"/>
    <id>http://example.com/2021/11/09/SPoT-Better-Frozen-Model-Adaptation-through-Soft-Prompt-Transfer/</id>
    <published>2021-11-09T07:25:44.000Z</published>
    <updated>2021-11-09T13:14:50.422Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SPoT-Better-Frozen-Model-Adaptation-through-Soft-Prompt-Transfer"><a href="#SPoT-Better-Frozen-Model-Adaptation-through-Soft-Prompt-Transfer" class="headerlink" title="SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer"></a>SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer</h1><p>在 《The Power of Scale for Parameter-Efficient Prompt Tuning》的 PROMPTTUNING 方法（学习特定任务的软 prompt，以调节冻结的语言模型来执行下游任务）的基础上，提出了一种新的基于Prompt 的迁移学习方法，称为SPOT：Soft Prompt Transfer。</p><p>SPOT首先在一个或多个源任务上学习 prompt，然后用它来初始化目标任务的 prompt。</p><p>更重要的是，SPOT 大于等于 model-tuning ，同时参数效率更高（最多可减少27,000倍的特定任务参数）。</p><p>进一步对26个NLP任务和160个源-目标任务的组合进行了大规模的任务迁移性研究，并证明了多任务往往可以通过 Prompt Transfer 而相互受益。</p><p>最后，提出了一种简单有效的检索方法，将任务 prompts 解释为任务 embeddings，以识别任务之间的相似性，并预测最可迁移的源任务用于新目标任务。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>越来越大的预先训练的语言模型是获得最佳性能的关键因素。虽然这一趋势不断推动着各种NLP基准的可能性，但这些模型的巨大规模对实际应用提出了重大挑战。对于100B以上的参数模型，为每个下游任务微调和部署一个单独的模型实例将是非常昂贵的。</p><p>为了绕过微调的不可行性，GPT-3 提出了 Prompt design，其中每一个下游任务都被铸成一个语言建模任务，冻结的预训练模型通过对推理时提供的手动文本 prompts 调节来执行不同的任务。</p><p>Brown等人（2020）用一个冻结的GPT-3模型展示了令人印象深刻的 few-shot 性能，尽管其性能高度依赖于 prompt 的选择，并且仍然远远落后于最先进的微调结果。</p><p>最近的工作探索了学习软prompt的方法（</p><ul><li>《Gpt understands, too》</li><li>《Learning how to ask: Querying LMs with mixtures of soft prompts》</li><li>《Prefix-tuning: Optimizing continuous prompts for generation》</li><li>《The power of scale for parameter-efficient prompt tuning》</li></ul><p>），这可以被视为注入语言模型的额外可学习参数。</p><p>PROMPTTUNING 在适应过程中为每个下游任务学习一个小的特定任务 prompt（一个可调整的标记序列，预置在每个样本中），以调节冻结的语言模型来执行该任务。</p><p>引人注目的是，随着模型容量的增加，PROMPTTUNING 与 model-tuning 比较起来，后者在每个下游任务上对整个模型进行微调。然而，在小规模和中等规模的模型（小于11B参数）中，PROMPTTUNING 和 model tuning 之间仍有很大差距。</p><p>例如，对于T5 BASE（220M参数）和T5 XXL（11B参数）模型，在SuperGLUE基准上分别获得了+10.1和+2.4点的平均精度改进。SPOT在所有模型规模上的表现都比model tuning有竞争力或明显更好</p><p><img src="https://i.loli.net/2021/11/09/78b2YLcQWw3xOVg.png" alt=""></p><p>在这些结果的激励下，通过任务 prompts 的视角来研究任务之间的可迁移性。目标是回答以下问题：</p><ul><li><p>(a) 对于一个给定的目标任务，何时将 prompt 初始化为源任务的 prompt 有助于提高性能？</p><p>为了解决(a)，作者使用26个NLP任务和160个源-目标任务的组合对T5模型进行了系统研究。结果表明，任务往往可以通过prompt transfer 而相互受益。</p></li><li><p>(b) 能不能利用任务 prompt，对给定的新目标任务使用哪些源任务做出更有原则的选择？</p><p>为了解决(b)，把学到的任务 prompt 解释为任务嵌入，以构建一个任务的语义空间，并规范任务之间的相似性。设计了一种高效的检索算法，用来测量任务嵌入的相似性，使能够识别那些有可能对给定的新目标任务产生积极迁移性的源任务。</p></li></ul><h2 id="Improving-PROMPTTUNING-with-SPOT"><a href="#Improving-PROMPTTUNING-with-SPOT" class="headerlink" title="Improving PROMPTTUNING with SPOT"></a>Improving PROMPTTUNING with SPOT</h2><p>为了提高PROMPTTUNING的性能，SPOT引入了源 prompt tuning，这是语言模型预训练和目标 prompt tuning 之间的一个中间训练阶段（如图左），在一个或多个源任务上学习prompt（同时仍保持基础模型冻结），然后用来初始化目标任务的prompt。</p><p><img src="https://i.loli.net/2021/11/09/vBU7Mqmlt6JP5Ie.png" alt=""></p><h3 id="Experimental-setup"><a href="#Experimental-setup" class="headerlink" title="Experimental setup"></a>Experimental setup</h3><p>冷冻模型是建立在预先训练好的各种尺寸的T5 checkpoints 之上的。SMALL、BASE、LARGE、XL、XXL，参数分别为60M、220M、770M、3B和11B。在对SPOT的实验中，利用了T5的LM adapted（<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md">prefix LM</a>）版本，发现它更容易为PROMPTTUNING优化。</p><h3 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h3><ul><li>PROMPTTUNING : vanilla prompt tuning approach , 其中针对每个目标任务直接训练独立提示。</li><li>MODELTUNING &amp; MULTI-TASKMODEL TUNING: 将 prompt tuning 方法与MODELTUNING—标准的微调方法进行比较，其中所有的预训练参数都在每个目标任务上分别进行微调。为了进行 apples-to-apples 的比较，我们还包括MULTI-TASKMODEL TUNING，这是一个更有竞争力的基线，首先在SPOT使用的相同混合源任务上微调整个模型，然后在每个目标任务上单独微调。</li></ul><h3 id="Evaluation-datasets"><a href="#Evaluation-datasets" class="headerlink" title="Evaluation datasets"></a>Evaluation datasets</h3><p>在GLUE 和SuperGLUE 基准（每个基准都有8个数据集）的不同任务集上研究下游的性能。</p><h3 id="Data-for-source-prompt-tuning"><a href="#Data-for-source-prompt-tuning" class="headerlink" title="Data for source prompt tuning"></a>Data for source prompt tuning</h3><p>与语言模型的预训练一样，训练数据的选择对于成功的 prompt transfer至关重要。为了研究源训练数据对下游性能的影响，我们比较了一系列不同的源任务。</p><ul><li><strong>A single unsupervised learning task :</strong> 首先考虑在C4数据集上使用 “prefix LM” 目标训练一个 prompt。虽然这个任务是用来预先训练冷冻T5模型的，但它仍然可以帮助学习一个通用的提示。</li><li><strong>A single supervised learning task :</strong> 另外，我们可以使用监督任务来训练 prompt。我们使用MNLI或SQuAD作为单源任务。MNLI被证明对许多句子级别的分类任务有帮助，而SQuAD被发现对QA任务有很好的概括性。</li><li><strong>A multi-task mixture :</strong> 到目前为止，我们一直在对单一来源的任务进行训练提示。另一种方法是多任务训练。在T5的统一文本到文本框架内，这只是相当于将不同的数据集混合在一起。我们探索混合来自不同NLP基准或任务系列的数据集，包括GLUE、SuperGLUE、自然语言推理（NLI）、转述/语义相似性、情感分析、MRQA的问题回答、RAINBOW的常识推理。 我们使用Raffel等人（2020）的例子—比例混合策略，从上述每个NLP基准/任务家族中创建了一个源任务的mixture，人工数据集大小限制为K = 219个训练例子。最后，我们包括C4和上述NLP基准/任务族中的所有标记数据集的混合（55个数据集）。</li></ul><p>遵循 PROMPTTUNING 的观点。，我们使用 CLASS-LABEL 方案(其中 prompt tokens用表示输出类的合并的嵌入来初始化)来初始化提示，并退回到SAMPLEDVOCAB方案以填充任何剩余的提示位置)。</p><h3 id="Effect-of-SPOT"><a href="#Effect-of-SPOT" class="headerlink" title="Effect of SPOT"></a>Effect of SPOT</h3><p><img src="https://i.loli.net/2021/11/09/P79bgkXEKDuZArh.png" alt=""></p><p>我们在图1和表1中比较了SPOT和其他方法的结果。下面，我们对每个发现进行详细的总结和分析。</p><ul><li><strong>SPOT significantly improves performance and stability of PROMPTTUNING:</strong>  表1显示了我们在T5-BASE glue 和SuperGlue基准上的结果。总体而言，结果表明，prompt transfer 为 PROMPTTUNING 提供了一种提高性能的有效手段。我们的烧蚀研究表明，Longer tuning 也是实现我们最佳性能的一个重要因素，并且是对 prompt transfer 的补充。此外，当Longer tuning 被省略时，我们观察到SPOT在不同的运行中提高了稳定性。</li><li><strong>Different source mixtures can lead to performance gains:</strong>  在SPOT方法中，我们可以比较不同来源混合的有效性（见表1）。在GLUE和SuperGLUE上的源 prompt tuning 表现最好，分别获得82.8和73.2的平均分数。有趣的是，C4上的无监督源 prompt tuning（与预训练我们的冻结模型的任务相同）仍然产生了相当大的改进，甚至在SuperGLUE任务中超过了SuperGLUE的源 prompt tuning。此外，使用MNLI或SQuAD作为单一源数据集对GLUE和SuperGLUE都特别有帮助。最后，其他来源的混合也能带来明显的收益，一些NLP基准/任务家族（如NNLI和转述/语义相似性）比其他任务更有利。</li><li><strong>SPOT helps close the gap with MODELTUNING across all model sizes:</strong>  最后，SPOT产生了与强大的MULTI-TASKMODEL TUNING基线相竞争的性能，同时在多任务源 tuning 和目标 tuning 方面参数效率更高；在XXL尺寸下，SPOT获得了91.2的最佳平均得分，<strong>比MULTI-TASKMODELTUNING好+1.1分，尽管其特定任务参数少27000倍。</strong></li></ul><h2 id="Investigating-task-transferability"><a href="#Investigating-task-transferability" class="headerlink" title="Investigating task transferability"></a>Investigating task transferability</h2><p>在确定了 prompt transfer 对 prompt tuning 有帮助之后，我们现在将重点转移到通过任务prompts 的视角来研究任务迁移性。</p><p>为了阐明不同任务之间的可迁移性，我们对26个NLP任务（包括一个无监督的任务）和160个源-目标任务的组合进行了大规模的实证研究。我们证明，在各种情况下，任务可以通过 prompt transfer 来互相帮助，而任务的相似性在决定迁移性方面起着重要作用。</p><p>此外，我们表明，通过将任务prompts解释为任务嵌入，我们可以构建一个任务的语义空间，并制定一个更严格的任务相似性概念。最后，我们提出了一种检索算法，该算法测量任务嵌入的相似性，以选择哪些源任务用于给定的新目标任务（图2，右）。</p><p><img src="https://i.loli.net/2021/11/09/lzYAeRdUOH6rfPh.png" alt=""></p><p>我们学习源任务的prompts，并将早期检查点作为任务嵌入，将最佳检查点保存为源prompts。这些构成了我们prompts库的键和值。给定一个新的目标任务。用户：(i) 计算一个任务嵌入，(ii) 检索一个最佳的源prompt，(iii) 训练一个目标prompt，该prompt以源prompt为初始化。</p><h3 id="Experimental-setup-1"><a href="#Experimental-setup-1" class="headerlink" title="Experimental setup"></a>Experimental setup</h3><p>我们研究了16个源数据集和10个目标数据集的不同集合（见表2）。我们考虑了所有160对可能的源和目标数据集，并从每个源任务转移到每个目标任务。</p><p><strong>Source and target tasks</strong></p><p> 源任务包括一个无监督任务（C4）和15个监督任务，涵盖自然语言推理（NLI）、转述/语义相似性、情感分析、问题回答（QA）和常识推理。所有的源任务都是数据丰富的，或者在以前的工作中已经被证明产生了积极的转移。为了模拟一个真实的场景，我们使用低资源任务（少于1万个训练实例）作为目标任务。这些任务涵盖了上述类型的任务，此外还包括语法可接受性、词义消歧和核心推理的解决。</p><p><strong>Training details</strong></p><p>为了限制计算成本，我们在所有的任务迁移性实验中使用T5 BASE。我们在每个源任务上执行262,144个 prompt tuning 步骤。选择具有最高源任务验证性能的 prompt checkpoint 来初始化不同目标任务的 prompt。由于目标数据集较小，我们只对每个目标任务进行100K的 prompt tuning 步骤。</p><p><strong>Constructing a semantic space of tasks</strong></p><p>由于在具体任务的 prompt tuning 过程中只有 prompt 参数被更新，任务prompt很可能编码了特定的任务知识。这表明，它们可以被用来推理任务的性质及其关系。为了测试这个想法，我们将任务 prompt 解释为任务嵌入，并构建一个任务语义空间。请注意，虽然我们使用源任务的最佳 prompt checkpoint 来迁移到目标任务中，但我们使用早期的 prompt checkpoint 作为我们的任务嵌入。这使得新的目标任务的任务嵌入可以快速计算。在我们的实验中，任务嵌入来自一个固定的prompt checkpoint，即在10K步，为每个任务。我们通过测量它们对应的任务嵌入 $e^1$, $e^2$之间的相似性来估计两个任务 $t^1$,  $t^2$ 之间的相似性，使用以下指标:</p><ul><li><p>COSINE SIMILARITY OF AVERAGE TOKENS: 计算 prompt tokens 的平均集合表示之间的余弦相似度。</p><p>$sim(t^1,t^2) = cos(\frac{1}{L} \sum_i e^1_i, \frac{1}{L}\sum_j e_j^2)$ , 其中 $e^1_i,e^2_j$ 定义为各自的 prompt tokens</p></li><li><p>PER-TOKEN AVERAGE COSINE SIMILARITY: 计算每个 prompt token 对之间的平均余弦相似度 $(e_i^1,e^2_j)$:</p><p>$sim(t^1,t^2) = \frac{1}{L^2} \sum_i\sum_j cos(e_i^1,e_j^2)$</p></li></ul><h3 id="Predicting-and-exploiting-transferability"><a href="#Predicting-and-exploiting-transferability" class="headerlink" title="Predicting and exploiting transferability"></a>Predicting and exploiting transferability</h3><p>利用任务嵌入来预测和利用任务迁移性。具体来说，我们探索了预测对给定目标任务最有利的源任务的方法，然后利用其prompt 来提高目标任务的表现。</p><p>为了扩大我们的源 prompts 集，我们使用了每个源任务上所有三个不同的 prompt tuning运行的 prompt，从而产生了48个源 prompts。给定一个具有任务嵌入的目标任务 $t$ ，我们将所有的源prompts $ρ^s$按其对应的任务嵌入$e^s$ 和目标嵌入$e^t$之间的相似度从高到低排序。</p><p>我们将源prompts 的排序列表表示为 $ρ^{s_r}$，其中r表示排序（r=1,2,…,48）。我们用以下方法进行实验:</p><ul><li>BEST OF TOP-k : 选择前 k 个源prompts，并分别使用它们来初始化目标prompt。这个过程需要对目标任务 t 进行 k 次prompt tuning，每个源prompt一次。然后，最好的单个结果被用来评估这个方法的有效性。</li><li>TOP-k WEIGHTED AVERAGE: 用前k个源prompt 的加权平均数初始化目标prompt $\sum<em>{r=1}^k \alpha_r ρ^{s_r}$，这样我们只对目标任务 $t$ 进行一次提示调整。 权重 $\alpha_r = \frac{sim(e^{s_r, e^t})}{\sum</em>{l=1}^k sim(e^{s_l, e^t})}$， $e^{s_r}$表示相应的任务嵌入 $ρ^{s_r}$</li><li>TOP-k MULTI-TASK MIXTURE: 首先确定 prompt 在前k个 prompt 中的源任务，并将其数据集和目标数据集混合在一起，使用Raffel等人（2020）的例子-比例混合策略。然后，我们在这个多任务混合物上进行源prompt tuning，并使用最后的 prompt checkpoint 来为目标 prompt tuning 进行初始化。</li></ul><p><strong>Evaluation</strong></p><p>我们报告了通过使用上述每一种方法在目标任务中取得的平均分数。对于每个目标任务 $t$ ，我们衡量三个不同的 prompt tuning 运行（导致不同的任务嵌入等）的平均和标准偏差。为了进行比较，我们报告了在对每个目标任务从头开始进行prompt tuning（即没有任何 prompt Transfer）时，比基线的绝对和相对改进。此外，我们还包括通过使用暴力搜索来确定每个目标任务的48个源 prompt 中的最佳 prompt 所取得的谕示结果。</p><h3 id="Effect-of-prompt-based-task-embeddings"><a href="#Effect-of-prompt-based-task-embeddings" class="headerlink" title="Effect of prompt-based task embeddings"></a>Effect of prompt-based task embeddings</h3><p>在这一部分中，我们首先分析我们的任务可迁移性结果。然后，我们论证了使用基于 prompt 的任务嵌入来表示任务、预测和开发任务可迁移性的有效性。</p><p><img src="https://i.loli.net/2021/11/09/bBqas3ylQuUZndf.png" alt=""></p><p><strong>Tasks can help each other via prompt transfer in various scenarios:</strong>  实验结果表明，在许多情况下，将 prompt 从源任务转移到目标任务（SOURCE → TARGET）可以在目标任务上提供显著的增益。</p><p><img src="https://i.loli.net/2021/11/09/e1y6ipCjg5OfaLn.png" alt=""></p><p><strong>Task embeddings capture task relationships :</strong> 图3显示了研究的26个NLP任务的任务嵌入之间的余弦相似性的分层聚类热图，使用的是平均托肯斯的余弦相似性指标。具体来说，类似的任务被归为几个集群，包括问题回答（SQuAD、ReCoRD和DROP；MultiRC和BoolQ）、情感分析（Yelp-2、SST-2和CR）、NLI（MNLI和CB；DocNLI和RTE）、语义相似性（STS-B和CxC）、副词（MRPC和QQP）和常识推理（WinoGrande、HellaSWAG和CosmosQA）。我们注意到，QNLI是由SQuAD数据集建立的NLI任务，与SQuAD没有密切联系；这表明我们的任务模型对任务类型比领域相似性更敏感。有趣的是，它们也捕捉到了ReCoRD对WSC的高可转移性这一非直观的情况。此外，来自同一任务的不同提示的任务嵌入具有很高的相似性分数</p><p><img src="https://i.loli.net/2021/11/09/eEAGvyxmXN8nzlh.png" alt=""></p><p><strong>Correlation between task embedding similarity and task transferability:</strong> 图4显示了目标任务上的相对误差减少是如何作为源和目标任务嵌入之间的相似性的函数而变化的。总的来说，我们发现，在我们研究的四个（10个）目标任务上，任务嵌入的相似性和任务可转移性之间存在明显的正相关，包括STS-B（p &lt; 0.001），CB（p &lt; 0.001，未显示），WSC（p &lt; 0.01），和RTE（p &lt; 0.05），而在其他任务上则不太明显。</p><p><img src="https://i.loli.net/2021/11/09/eVcs79ZDtWYK2JM.png" alt=""></p><p><strong>Task embeddings can be used to predict and exploit task transferability:</strong> 我们在表3中比较了不同方法的结果，以确定哪些源提示可能对给定的目标任务有益。我们发现，对于小的k值(≤9)，使用每键平均COSINE SIMILARITY指标比使用AV-ERAGE TOKENS的COSINE SIMILARITY指标产生更好的结果。我们的结果还表明，BEST OF TOP-k提供了一个预测和利用任务转移性的有效手段。简单地选择源提示，其相关的任务嵌入与目标嵌入具有最高的相似性，使用每键平均COSMINE SIMILARITY度量，比基线有很大的改善（从平均得分74.7到76.7，平均相对误差减少12.1%）。为每个目标任务尝试所有前三名（共48个）的源提示，得到的平均分数为77.5分。随着k值的增大，我们可以保留神谕选择源提示的大部分好处（k=9时平均得分的80%，k=15时平均得分的90%），同时仍然可以消除2/3以上的候选源提示。尽管这种方法需要对目标任务进行K次提示调谐，但与模型调谐相比，提示调谐的成本相对低廉。在k=1的情况下，TOP-k加权平均法与BEST OF TOP-k的平均性能相似，但实现的方差较小。因此，在禁止对tar-get任务进行多次调谐的情况下，这可能是对BEST OF TOP-k的一个有吸引力的替代方案。最后，TOP-k MULTI-TASK MIXTURE也提供了一种获得强大性能的方法，其平均得分为77.8，甚至在k≤3的情况下超过了BEST OF TOP-k。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p><strong>Parameter-efficient transfer learning &amp; lan- guage model prompting</strong></p><p>预训练的语言模型已被证明是改善许多NLP基准的最先进结果的有效手段（Devlin等人，2019；Liu等人，2019b；Yang等人，2019；Lan等人，2020；Raffel等人，2020；Brown等人，2020；He等人，2021）。然而，MODELTUNING（又称微调）—目前将这些模型应用于下游任务的主流方法—可能变得不切实际，因为为每项任务微调所有预训练的参数可能过于昂贵，特别是随着模型规模的不断扩大。</p><p>为了解决这个问题，早期的工作使用了compression技术，如知识分散（Sanh等人，2019；Jiao等人，2020；Sun等人，2020）和模型修剪（Fan等人，2020；Sanh等人，2020；Chen等人，2020），以获得轻型预训练模型。其他工作只更新语言模型的小部分（Zaken等人，2021）或训练特定的任务模块，如适配器（Houlsby等人，2019；Karimi Mahabadi等人，2021）和/或低等级结构（Mahabadi等人，2021；Hu等人，2021），同时保持大部分或全部预训练参数固定。值得注意的是，Brown等人（2020年）使用PROMPTDESIGN的单一冻结的GPT-3模型展示了显著的几次学习性能，其中每个任务都是在推理时间向模型提供手动文本提示，要求它产生一些输出文本。</p><p>此后，一些努力集中在开发基于提示的学习方法，包括精心手工制作的提示（Schick和Schütze，2021）、提示挖掘和转述（Jiang等人，2020b）、基于梯度搜索的改进提示（Shin等人，2020）和自动提示生成（Gao等人，2021）。然而，使用硬性提示被发现是次优的和敏感的，即下游表现和提示格式之间没有明显的相关性，提示的微小变化会导致下游表现的显著差异（Liu等人，2021b）。因此，最近的工作已经转向学习软提示（Liu et al., 2021b; Qin and Eisner, 2021; Li and Liang, 2021; Lester et al., 2021），这可以被看作是注入语言模型的一些额外的可学习参数。我们请读者参考Liu等人（2021a）对基于提示的学习研究的最新调查。</p><p>同时进行的工作（Gu等人，2021）也探讨了 prompt 预训练的有效性。他们的方法使用手工制作的预训练任务，为不同类型的下游任务量身定做，这限制了其对新型下游任务的应用。相比之下，我们使用现有的任务作为源任务，并表明即使在源任务和目标任务之间存在不匹配（如任务类型、输入/输出格式）的情况下，prompt transfer也能带来好处。他们的工作也集中在 few-shot 的设置上，而我们是在较大的数据集背景下工作。此外，我们研究了任务的可迁移性，并证明任务往往可以通过 prompt transfer 来互相帮助，而任务 prompt 可以被解释为任务嵌入，以正式确定任务的相似性，从而确定哪些任务可以互相受益。</p><p><strong>Task transferability</strong></p><p>我们还建立在现有的关于NLP的任务迁移性的工作上（Phang等人，2019；Wang等人，2019a；Liu等人，2019a；Talmor和Berant，2019；Pruksachatkun等人，2020；Vu等人，2020；Poth等人，2021）和计算机视觉（Zamir等人，2018；Achille等人，2019；Yan等人，2020）。之前的工作表明，从数据丰富的源任务（Phang等人，2019年）、需要复杂推理和推理的任务（Pruksachatkun等人，2020年）或与目标任务相似的任务（Vu等人，2020年）中有效转移。也有人努力预测任务之间的可转移性（Bingel和Søgaard，2017；Vu等人，2020；Poth等人，2021）。Vu等人（2020）使用来自输入文本或语言模型对角线Fisher信息矩阵的任务嵌入，而Poth等人（2021）探索基于适配器的方法。在这里，我们对T5的使用使我们能够更好地对任务空间进行建模，因为每个任务都被投到一个统一的文本到文本的格式中，并且在不同的任务中使用相同的模型（没有特定的任务成分）。此外，基于提示的任务嵌入相对来说更容易获得。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>在本文中，我们研究了在提示调谐背景下的转移学习。我们表明，规模对于PROMPTTUNING与MODEL-TUNING的性能相匹配是没有必要的。我们的SPOT方法在不同的模型规模下与MODEL-TUNING的性能相匹配，甚至超过了MODEL-TUNING的性能，同时参数效率更高（最多可减少27,000倍的特定任务参数）。我们对任务转移性的大规模研究表明，在各种情况下，任务可以通过提示转移而相互受益。最后，我们证明，任务提示可以被解释为任务嵌入，以正式确定任务之间的相似性。我们提出了一种简单而有效的检索方法，以衡量任务的相似性，从而确定哪些源任务可以给一个新的目标任务带来好处。从整体上看，我们希望我们的工作能够促进对基于提示的迁移学习的更多研究。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;SPoT-Better-Frozen-Model-Adaptation-through-Soft-Prompt-Transfer&quot;&gt;&lt;a href=&quot;#SPoT-Better-Frozen-Model-Adaptation-through-Soft-Prompt-</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>The Power of Scale for Parameter-Efficient Prompt Tuning</title>
    <link href="http://example.com/2021/11/08/The-Power-of-Scale-for-Parameter-Efficient-Prompt-Tuning/"/>
    <id>http://example.com/2021/11/08/The-Power-of-Scale-for-Parameter-Efficient-Prompt-Tuning/</id>
    <published>2021-11-08T03:04:04.000Z</published>
    <updated>2021-11-08T11:14:12.897Z</updated>
    
    <content type="html"><![CDATA[<h1 id="The-Power-of-Scale-for-Parameter-Efficient-Prompt-Tuning"><a href="#The-Power-of-Scale-for-Parameter-Efficient-Prompt-Tuning" class="headerlink" title="The Power of Scale for Parameter-Efficient Prompt Tuning"></a>The Power of Scale for Parameter-Efficient Prompt Tuning</h1><p><a href="https://zhuanlan.zhihu.com/p/428512183">https://zhuanlan.zhihu.com/p/428512183</a></p><p>在这项工作中，作者探索了 “prompt tuning”，这是一种简单而有效的机制，用于学习 “软提示”，以调节冻结的语言模型来执行特定的下游任务。</p><p>与GPT-3使用的离散文本提示不同，软提示是通过反向传播来学习的，并且可以进行调整以合并来自任何数量的标注样本的信号。</p><p>此端到端学习方法在很大程度上超过了GPT-3的 few-shot learning。更值得注意的是，通过使用T5对模型规模的消减，表明，随着规模的扩大，prompt tuning变得更有竞争力：当模型超过数十亿个参数时，该方法 “缩小了差距”，与model tuning （其中调整了所有模型权重）的强大性能相匹配。</p><p>这一发现尤其重要，因为共享和服务大型模型的成本很高，而将一个冻结模型重用于多个下游任务的能力可以减轻这一负担。</p><p>此方法可以看作是对最近提出的  “prefix tuning”，以软提示为条件的冻结模型对领域转移的鲁棒性有好处，并能实现有效的 “prompt ensembling”。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>随着预训练的大型语言模型的广泛成功，出现了一系列的技术来适应这些通用模型的下游任务。ELMo提出冻结预训练的模型，并学习其每层表征的特定任务加权。</p><p>然而，自GPT和BERT以来，主流的适应技术是模型调整（或 “微调”），即在适应期间调整所有的模型参数。</p><p>最近，GPT3 表明，prompt design （或 “priming”）在通过文本提示来调节冻结的GPT-3模型的行为方面是惊人的有效。提示通常由一个任务描述和/或几个典型的例子组成。这种回到 “冻结 “预训练模型的做法很有吸引力，尤其是在模型规模不断扩大的情况下。与其说每个下游任务都需要一个单独的模型副本，不如说一个通用模型可以同时为许多不同的任务服务。</p><p>不幸的是，基于提示的适应有几个关键的缺点。任务描述容易出错，需要人的参与，而且提示的有效性受限于模型输入中能容纳多少条件文本。</p><p>因此，下游任务的质量仍然远远落后于 model tuning。例如，GPT-3 175B 在SuperGLUE上的几率性能比微调的T5-XXL低17.5分（71.8比89.3），尽管使用了16倍的参数。</p><p>最近提出了几项自动化 prompt 设计。</p><p>《 AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts》</p><p>虽然这种技术优于人工提示设计，但相对于 model tuning 而言，仍有差距。</p><p>《Prefix-tuning: Optimizing continuous prompts for generation》 提出 “prefix tuning” 并 在生成性任务中表现出很强的效果。这种方法冻结了模型参数，并将调整过程中的误差反向传播到预置在编码器堆栈中每一层的前缀激活，包括输入层。</p><p>《WARP: Word-level Adversarial ReProgramming》 通过将可训练的参数限制在 masked 语言模型的输入和输出子网络上，简化了这个方法，并在分类任务上显示了合理的结果。</p><p>本文提出了 prompt tuning 作为适应语言模型的进一步简化方法。我们冻结了整个预训练的模型，只允许每个下游任务有额外的 $k$ 个可调整的标记被预加到输入文本中。这种 “软提示 “是端到端的训练，可以浓缩来自完整的标记数据集的信号，使我们的方法能够胜过  few-shot prompts，并缩小与模型调整的质量差距。如图1所示，随着规模的扩大，提示调谐变得更具竞争力。</p><p><img src="https://i.loli.net/2021/11/08/Ws5EYPU7JOTMItl.png" alt=""></p><p>同时，由于一个预训练模型被循环用于所有下游任务，因此我们保留了冻结模型的有效服务优势</p><p><img src="https://i.loli.net/2021/11/08/VGpNzwImrZFR7oE.png" alt=""></p><p>与Prefix-tuning 和 AutoPrompt 虽然都使用 “软提示”，但本文是第一个表明单独的 prompt tuning（没有中间层前缀或特定任务的输出层）就足以与 model tuning 相媲美。</p><p>主要贡献：</p><ul><li>提出 prompt tuning，并证明其在大型语言模型体系中与模型调谐的竞争力。</li><li>消融了许多设计选择，并证明质量和稳健性随着规模的扩大而提高。</li><li>表明在领域转移问题上，prompt tuning 优于 model tuning。</li><li>提出 “prompt ensembling”并显示其有效性。</li></ul><h2 id="2-Prompt-Tuning"><a href="#2-Prompt-Tuning" class="headerlink" title="2 Prompt Tuning"></a>2 Prompt Tuning</h2><p>按照T5的 “text-to-text”的方法，将所有的任务作为文本生成。我们现在不是将分类建模为给定某种输入的输出类别的概率$Pr(y|X)$ ，其中 $X$ 是一系列 tokens，$y$ 是一个单一的类别标签，而是将其建模为条件生成，其中 $Y$ 是代表一个类别标签的一系列 tokens。T5将分类建模为 $Pr_θ(Y|X)$，参数由构成其编码器和解码器的 transformer 的权重 $θ$ 决定。</p><p>Prompting 是指在生成 $Y$ 的过程中为模型添加额外的信息作为条件的方法。</p><p>通常情况下，提示是通过在输入的 $X$ 上预置一系列标记 $P$ 来完成的，这样模型就能最大限度地提高正确 $Y$ 的可能性，即$Pr_θ(Y |[P ; X ])$，同时保持模型参数 $θ$ 的固定。在GPT-3中，prompt tokens的表示，$P={p_1, p_2, …, p_n}$，是模型嵌入表的一部分，由冻结的 $θ$ 作为参数。</p><p> 因此，寻找最佳 prompt 需要通过人工搜索或无差别搜索方法来选择提示符。</p><p>Prompt tuning 消除了提示 $P$ 被 $θ$ 参数化的限制；相反，提示有它自己的专用参数 $θ_P$，可以被更新。prompt design 涉及从固定的冻结嵌入词汇中选择提示标记，而 Prompt tuning 可以被认为是使用固定的特殊标记的提示，其中只有这些提示标记的嵌入可以被更新。</p><p>新的条件生成现在是 $Pr_{\theta;\theta_P} (Y| [P;X])$，可以通过反向传播使 $Y$ 的可能性最大化来训练，同时只对 $θ_P$ 应用梯度更新。</p><p>给定一系列的 n个 tokens，${x_1, x_2,…, x_n}$，T5做的第一件事就是嵌入这些tokens，形成一个矩阵 $X_e\in R^{n\times e}$，其中 $e$ 是嵌入空间的维度。</p><p>soft-prompts 被表达为一个参数 $P_e\in R^{p\times e}$ 其中 $p$ 是prompt长度。</p><p>然后，prompt 与嵌入的输入相连接，形成一个单一的矩阵 $[P_e; X_e]\in R^{(p+n)×e}$，然后像平常一样流经编码器-解码器。我们的模型被训练为最大化 $Y$ 的概率，但只有 prompt 参数 $P_e$被更新。</p><h3 id="Design-Decisions"><a href="#Design-Decisions" class="headerlink" title="Design Decisions"></a>Design Decisions</h3><p>有许多可能的方法来初始化 prompt 表征。最简单的是从头开始训练，使用随机初始化。一个更复杂的选择是将每个 prompt token 初始化为一个从模型词汇中提取的嵌入。</p><p>从概念上讲，soft-prompt  以与输入前的文本相同的方式调节冻结网络的行为，因此，类似于单词的表述可能作为一个好的初始化点。对于分类任务，第三个选择是用列举输出类别的嵌入来初始化 prompt，类似于 “verbalizers”。</p><p>由于我们希望模型在输出中产生这些 tokens，用有效的目标 tokens 的嵌入来初始化prompt ，应该使模型将其输出限制在合法的输出类别中。</p><p>另一个设计考虑是 prompt 的长度。我们方法的参数成本是 $EP$ ，其中E是 token 嵌入维度，P是 prompt 长度。prompt越短，必须调整的新参数就越少，所以我们的目标是找到一个表现良好的最小长度。</p><h3 id="Unlearning-Span-Corruption"><a href="#Unlearning-Span-Corruption" class="headerlink" title="Unlearning Span Corruption"></a>Unlearning Span Corruption</h3><p>T5 模型的预训练任务是 Span Corruption，模型被要求去重构被打乱的句子</p><p>与GPT-3等自回归语言模型不同，我们试验的 T5 模型使用编码器-解码器架构，并对 Span Corruption 目标进行预训练。具体来说，T5的任务是 “重建 “输入文本中被屏蔽的span ，这些跨度被 tokens 为独特的哨兵符号。目标输出文本由所有被屏蔽的跨度组成，用哨兵标记分开，再加上最后一个哨兵标记。</p><p>例如。从文本 “Thank you for inviting me to your party last week”中，我们可以构建一个预训练的例子，其中输入是 “Thank you ⟨X⟩ me to your party ⟨Y⟩ week”，目标输出是”⟨X⟩ for inviting ⟨Y⟩ last ⟨Z⟩”。</p><p>虽然Raffel等人（2020年）发现这种架构和预训练目标比传统的语言建模更有效，但我们假设这种设置并不适合产生一个可以通过prompt tuning而随时控制的冻结模型。特别是，一个专门针对 Span Corruption 进行预训练的T5模型，如T5.1.1，从未见过真正自然的输入文本（不含哨兵标记），也从未被要求预测真正自然的目标。</p><p>事实上，由于T5的 Span Corruption 预处理的细节，每个预训练目标都会以哨兵开始。虽然这种输出哨兵的 “非自然 “倾向很容易通过微调来克服，但我们怀疑，由于解码器的先验因素无法调整，仅通过提示就很难推翻它。</p><p>考虑到这些问题，我们在三种情况下试验了T5模型。</p><ul><li>(1) “Span Corruption”。我们使用预先训练好的现成的T5作为我们的冻结模型，并测试其为下游任务输出预期文本的能力。</li><li>(2) “Span Corruption + Sentinel”。我们使用相同的模型，但在所有的下游目标中预置一个哨兵，以便更接近于预训练中看到的目标。</li><li>(3) “LM Adaptation”。我们继续T5的自我监督训练，进行少量的附加步骤，但使用Raffel等人（2020）所讨论的 “LM “目标；给定一个自然文本 prefix 作为输入，该模型必须产生自然文本的延续作为输出。</li></ul><p>最重要的是，这种适应性只发生一次，产生一个单一的冻结模型，我们可以在任何数量的下游任务中重复使用，进行prompt tuning。</p><p>通过LM adaptation，我们希望将T5 “快速 “转变为一个与GPT-3更相似的模型，GPT-3总是输出真实的文本，并且作为一个 “few-shot learner”，对提示有良好的反应。与从头开始的预训练相比，这种后期转变的成功率并不明显，而且据我们所知，以前也没有人研究过这种情况。因此，我们对各种长度的适应进行了实验，最高可达10万步。</p><h2 id="3-Results"><a href="#3-Results" class="headerlink" title="3 Results"></a>3 Results</h2><p>实验设定如下：</p><p><strong>预训练模型：</strong>T5 v1.1 from small to XXL。</p><p><strong>默认设置：</strong>采用经过额外 100k steps 的 LM Adaption 以后的 T5 模型 + 100 tokens 的 prompt。</p><p><strong>评测数据集：</strong>采用 SuperGLUE 基准的全量数据，将数据集重定义为 text-to-text 的形式（但是并不会加上 task name 的前缀），每一个 task 单独训练一个 prompt，训练步数为 30K，最后报告 SuperGLUE 的 dev set 的结果。</p><p><strong>基线模型：</strong>1）Model Tuning：每个 task 分别微调一个 T5 模型；2） Model Tuning（Multi-Task）：多个 task 一起训练，为了区分每个 task，会加上 task name 的前缀。</p><p>实验结果如下：</p><p><img src="https://i.loli.net/2021/11/08/gyFpB84RMnjEA3I.png" alt=""></p><p>随着模型参数的增加，Prompt Tuning 的效果越来越好，当 T5 模型参数达到 XXL 时，Prompt Tuning 的效果追平了 Model Tuning 和 Model Tuning（Multi-Task）。同时，Prompt Tuning 的效果远远超过了与 T5 同参数级别的 GPT-3 in context learning 的效果。</p><p><strong>prompt tokens 对 prompt tuning 的影响：</strong>在一般模型大小情况下，prompt tokens 越多，确实效果越好，但是当 token 超过 20 以后，增益就越来越小，对于超大模型的情况，即使是单个 prompt token，也能达到和 20 个 token 以上的 prompt 相近的效果。</p><p><strong>prompt token 的初始化：</strong>1. 随机初始化；2. 从 T5 词表中 5000 个常见单词中采样；3. 用类标签来初始化，标签是多个 token 时，则取均值，当类标签都用完后，剩下的 prompt token 用方法 2 初始化。类标签初始化在各种尺寸的模型上都表现最好，但是不同初始化策略在各种尺寸模型上表现差异很大，当尺寸变 XXL 后，这种差异就会消失。</p><p><strong>预训练任务对 prompt tuning 的影响：</strong>Span Corruption 任务导致了 prompt tuning 表现很差，即使加了 Sentinel 也没法缓解，而 LM Adaptation 设定下随着模型尺寸增大则 prompt tuning 表现越来越好。当然，当尺寸变为 XXL 后，这种影响也会消失。</p><p><strong>LM Adaptation steps 对 prompt tuning 的影响：</strong>LM Adaptation steps 越多，效果越好，这也说明 T5 需要进一步预训练才行。当然，当尺寸变为 XXL 后，这种影响也会消失。</p><h2 id="4-Comparison-to-Similar-Approaches"><a href="#4-Comparison-to-Similar-Approaches" class="headerlink" title="4 Comparison to Similar Approaches"></a>4 Comparison to Similar Approaches</h2><p>比较的一个重要轴是每种方法所需的特定任务参数的数量，如图4所示。在具有可学习参数的方法中，prompt tuning是参数效率最高的，对于超过10亿个参数的模型，需要不到0.01%的特定任务参数。</p><p><img src="https://i.loli.net/2021/11/08/NHCwWmDvGKdOY9o.png" alt=""></p><p><strong>“prefix tuning”</strong>：学习在每个 transformer 层预置的前缀序列。这类似于学习 transformer 的激活，这些激活在每个网络层都是固定的。与此相反，Prompt Tuning 使用单一的提示表示，被预置到嵌入式输入。除了重新引用较少的参数外，我们的方法允许 transformer 更新中间层的任务表征，就像输入实例的背景一样。他们的工作建立在GPT-2 和 BART 的基础上，而我们的工作则以T5为基础，研究了随着模型大小的增加，设计选择的性能和稳健性的变化。当使用BART时，prefix tuning 包括编码器和解码器网络上的前缀，而 Prompt Tuning 只需要编码器上的提示。</p><p><strong>与 P-tuning 的区别：</strong>P-tuning 的 soft tokens 需要考虑插入位置，同时采取的策略是 LM+Prompt Tuning，而 Prompt Tuning 则是直接插入在 prefix 位置，同时固定了 LM。同时 Prompt Tuning 相比 Model Tuning 的好处在于不会太过拟合在目标任务上，拥有更好的泛化性。</p><p><strong>“adapters”</strong>，即在冻结的预训练网络层之间插入小型瓶颈层。adapters 提供了另一种减少特定任务参数的手段，Houlsby等人（2019年）在冻结BERT-Large并只增加2-4%的额外参数时，实现了接近全模型调谐的GLUE性能。Pfeiffer等人（2020年）在一个多语言文本中使用多个适配器，明确地将语言理解与任务规范分开，与我们的方法类似。</p><p>adapters 和 Prompt Tuning 之间的一个核心区别是这些方法如何改变模型行为。</p><p>adapters 通过允许重写任何给定层的激活来修改作用于输入表征的实际函数，该功能由神经网络参数化。</p><p>Prompt Tuning  通过保留固定的函数和增加新的输入表示来修改行为，这些表示会影响后续输入的处理。</p><h2 id="5-Resilience-to-Domain-Shift"><a href="#5-Resilience-to-Domain-Shift" class="headerlink" title="5 Resilience to Domain Shift"></a>5 Resilience to Domain Shift</h2><p><strong>通过冻结核心语言模型参数，prompt tuning可防止模型修改其对语言的一般理解。相反，prompt表示间接调整输入的表示。</strong></p><p><strong>这减少了模型通过记忆特定的词汇线索和虚假的相关关系来 overfit 数据集的能力。</strong>这一限制表明，prompt tuning可能会提高对domain shifts 的稳健性，在这种情况下，输入的分布在训练和评估之间有所不同。</p><p>我们在两个任务上研究了  zero-shot  的领域转移：问题回答（QA）和转述检测（paraphrase detection）。对于问答，使用MRQA 2019关于泛化的共享任务。这项任务以统一的格式收集提取的QA数据集，并测试在 “域内 “数据集上训练的模型在评估 “域外 “数据集时的表现。在我们的实验中，我们在SQuAD上训练，并在每个域外数据集上进行评估。</p><p><img src="https://i.loli.net/2021/11/08/v2mIghVcnt6FCUk.png" alt=""></p><p>作为对结构域转移稳健性的第二个测试，我们探索了来自GLUE的两个释义检测任务之间的迁移。第一个任务是QQP(Iyer等人，2017年)，它询问来自社区问答网站Quora的两个问题是否是“重复的”。</p><p><img src="https://i.loli.net/2021/11/08/odZXCSlN2ceIExk.png" alt=""></p><h2 id="6-Prompt-Ensembling"><a href="#6-Prompt-Ensembling" class="headerlink" title="6 Prompt Ensembling"></a>6 Prompt Ensembling</h2><p><strong>Prompt 集成：</strong>prompt tuning 的另一个好处在于可以在保存一份 LM 模型拷贝情况下，同时训练多个 prompt，并实现集成。作者在 SuperGLUE 上训练了 5 个 prompt，并用多数投票法进行集成，表现优于单一 prompt。</p><p><strong>总结</strong></p><p>Prompt Tuning 的做法是添加可训练的 prefix，同时固定 LM，只训练 prefix，采用 Prompt Tuning 的方式可以在 T5 超大模型和全量数据的情况下，追平 fine-tuning 的效果。</p><p>实验发现采用 prompt tuning 的方式在小模型的情况容易受到 prompt 长度，初始化策略，预训练任务等影响，并不稳定，也没法超过 fine-tuning 的效果。</p><p>作者没有探索少量数据 + 超大模型情况下和 fine-tuning 的效果比较。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;The-Power-of-Scale-for-Parameter-Efficient-Prompt-Tuning&quot;&gt;&lt;a href=&quot;#The-Power-of-Scale-for-Parameter-Efficient-Prompt-Tuning&quot; class=</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>二分查找典型问题(二、三)</title>
    <link href="http://example.com/2021/11/08/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E5%85%B8%E5%9E%8B%E9%97%AE%E9%A2%98-%E4%BA%8C%E3%80%81%E4%B8%89/"/>
    <id>http://example.com/2021/11/08/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E5%85%B8%E5%9E%8B%E9%97%AE%E9%A2%98-%E4%BA%8C%E3%80%81%E4%B8%89/</id>
    <published>2021-11-08T02:13:14.000Z</published>
    <updated>2021-11-18T12:12:06.642Z</updated>
    
    <content type="html"><![CDATA[<h1 id="二分查找典型问题-二、三"><a href="#二分查找典型问题-二、三" class="headerlink" title="二分查找典型问题(二、三)"></a>二分查找典型问题(二、三)</h1><p><a href="https://leetcode-cn.com/leetbook/read/learning-algorithms-with-leetcode/x4e1p5/">https://leetcode-cn.com/leetbook/read/learning-algorithms-with-leetcode/x4e1p5/</a></p><h2 id="二、二分答案"><a href="#二、二分答案" class="headerlink" title="二、二分答案"></a>二、二分答案</h2><p>二分答案意思是：题目要我们找的是一个整数，并且这个整数我们知道它可能的最小值和最大值。此时可以考虑用二分查找算法找到这个的目标值</p><h3 id="69-Sqrt-x"><a href="#69-Sqrt-x" class="headerlink" title="69. Sqrt(x)"></a><a href="https://leetcode-cn.com/problems/sqrtx/">69. Sqrt(x)</a></h3><p>分析：这个题要求计算一个非负整数的平方根，返回值是一个整数。当平方根是浮点数事，需向下取整</p><h4 id="法一：暴力解法"><a href="#法一：暴力解法" class="headerlink" title="法一：暴力解法"></a>法一：暴力解法</h4><p>输入 8 返回的是 2 ， 因为 3 的平方等于 9 大于 8，因此【结果只保留整数的部分，小数部分将被舍去】。要求我们从1开始找，找到最后一个平方以后小于等于 x 的那个数。</p><p>假设 s 表示从 1 开始的那个数：</p><ul><li>如果 s 平方以后小于 x， 暂时放过</li><li>如果 s 平方以后等于 x ，直接返回</li><li>如果 s 平方以后大于 x，说明 s-1 是题目要求，返回 s-1</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">mySqrt</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">       <span class="comment">// 特判</span></span><br><span class="line">       <span class="keyword">if</span> (x &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">           <span class="keyword">return</span> x;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= x; ++i) &#123;</span><br><span class="line">           <span class="keyword">if</span> (i == x / i) &#123;</span><br><span class="line">               <span class="keyword">return</span> i;</span><br><span class="line">           &#125; <span class="keyword">else</span> <span class="keyword">if</span> (i &gt; x / i) &#123;</span><br><span class="line">               <span class="keyword">return</span> i - <span class="number">1</span>;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;参数出错&quot;</span>);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>注意：如果判别条件写成 <code>s * s == x</code> ，会发生整型溢出，应该写成 <code>s = x / s</code> ，判别条件 <code>s * s &gt; x</code> 也是类似这样写。</p><p>复杂度分析：</p><p>时间复杂度：O(x)，最坏情况下暴力解法会一直尝试到 x/2，O(x/2)=O(x)；<br>空间复杂度：O(1)，只使用到常数个变量。</p><h4 id="法二，二分查找"><a href="#法二，二分查找" class="headerlink" title="法二，二分查找"></a>法二，二分查找</h4><p>如果一个数的平方大于 <code>x</code> ，这个数就一定不是我们要找的平方根。于是，可以通过逼近的方式找到平方根。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">mySqrt</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (x == <span class="number">0</span>) &#123;</span><br><span class="line">           <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">int</span> left = <span class="number">1</span>;</span><br><span class="line">       <span class="keyword">int</span> right = x / <span class="number">2</span>;</span><br><span class="line">       <span class="keyword">while</span> (left &lt; right)&#123;</span><br><span class="line">           <span class="comment">// 写完分支以后调整为向上取整</span></span><br><span class="line">           <span class="keyword">int</span> mid = (left + right + <span class="number">1</span>) / <span class="number">2</span>;     </span><br><span class="line">           <span class="keyword">if</span> (mid &gt; x / mid) &#123;</span><br><span class="line">               <span class="comment">// mid 以及大于 mid 的数一定不是解，下一轮搜索的区间为 [left, mid - 1]</span></span><br><span class="line">               right = mid - <span class="number">1</span>;</span><br><span class="line">           &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">               left = mid;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> left;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h3 id="287-寻找重复数"><a href="#287-寻找重复数" class="headerlink" title="287. 寻找重复数"></a><a href="https://leetcode-cn.com/problems/find-the-duplicate-number/">287. 寻找重复数</a></h3><ul><li>要找 一个整数，这个整数有明确的范围 (1到n)之间，因此可以使用 二分查找。</li><li>每一次猜一个数，然后遍历整个数组，进而缩小搜索区间，然后确定重复的是哪个数。</li><li>不是在输入数组上直接使用二分查找，而是在数组 $[1,…,n]$ （有序数组）上使用二分查找。</li></ul><p>理解题意：</p><ul><li>n + 1个整数，放在长度为 n 的数组里，根据抽屉原则，至少有一个数组重复</li><li>找重复，最容易想到的是哈希表</li><li>但题目要求，0(1)空间</li><li>找找一个有范围的整数可以用二分查找</li><li>快慢指针</li></ul><p>二分查找的思路是先猜一个数（有效范围  $[left,…,right]$ 里位于中间的数 mid），然后统计原始数组中，小于等于mid 的元素个数cnt</p><ul><li>如果cnt严格大于mid，根据抽屉原则，重复元素在 $[left, mid]$ 里</li><li>否则，重复元素就在  $[mid+1,..right]$</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findDuplicate</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = len - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> num : nums)&#123;</span><br><span class="line">                <span class="keyword">if</span>(num &lt;= mid)&#123;</span><br><span class="line">                    cnt+=<span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 根据抽屉原理，小于等于4的个数如果严格大于4个，此时重复元素一定出现在 [1...4]里</span></span><br><span class="line">            <span class="keyword">if</span>(cnt &gt; mid)&#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1300-转变数组后最接近目标值的数组和"><a href="#1300-转变数组后最接近目标值的数组和" class="headerlink" title="1300. 转变数组后最接近目标值的数组和"></a>1300. <a href="https://leetcode-cn.com/problems/sum-of-mutated-array-closest-to-target/">转变数组后最接近目标值的数组和</a></h3><p>一句话解题：</p><ul><li>使用二分法确定一个整数 threshold （就是提中说的value），使得这个 threshold下，【转变后的数组】的和最接近目标值 target</li><li>转变的规则是：严格大于threshold的元素变成 threshold，那么 threshold 越大，【转变后的数组】的和越大，这是单调性。（注意说得具体一点是：单调不减，因为有些情况下，阈值扩大后，和可能不变）</li></ul><p>这道题比较麻烦的是求和以后可能不等于 target，所以让我们求【最接近的方案】，这个烦人的根源是 value 的取值一定得是整数，正是因为题目说 value 的整数，并且【答案不一定是arr中的数字】，因此依然可以使用二分查找法确定这个整数值。</p><p><img src="https://i.loli.net/2021/11/11/9czC2Jl61mZ7RaS.png" alt=""></p><p>做题的时候，会发现判别条件很不好写，因为【怎么衡量接近】，度量这个【最接近】的量不好选，因为此需要考虑别的方案；</p><p>最接近的情况是：选定了一个value求和以后，恰恰好等于target。不过更有可能出现的情况是：value选得小了，接近程度变大，而value选得大了，接近程度变小。</p><p><img src="https://i.loli.net/2021/11/11/dpaIjifs3KPLk8C.png" alt=""></p><p>代码一</p><p>如果选择一个阈值 value，使得它对应的 sum 是第 1 个大于等于target的，那么目标值可能在 value 也可能在 value - 1</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findBestValue</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len=arr.length;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> num: arr)&#123;</span><br><span class="line">            right = Math.max(right, num);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> num : arr)&#123;</span><br><span class="line">                sum += Math.min(num, mid);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 计算第一个使得转变后数组的和大于等于target的阈值 threshold</span></span><br><span class="line">            <span class="keyword">if</span>(sum &lt; target)&#123;</span><br><span class="line">                <span class="comment">// 严格小于的不一定是解</span></span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 比较阈值线分别定在 left-1 和left 的时候与target的接近程度</span></span><br><span class="line">        <span class="keyword">int</span> sum1 = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> sum2 = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> num:arr)&#123;</span><br><span class="line">            sum1 += Math.min(num, left-<span class="number">1</span>);</span><br><span class="line">            sum2 += Math.min(num, left);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(target - sum1 &lt;= sum2 - target)&#123;</span><br><span class="line">            <span class="keyword">return</span> left - <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  如果选择一个阈值 value ，使得它对应的 sum 是最后 1 个小于等于 target 的阈值，那么目标值可能在 value 也可能在 value + 1。</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findBestValue</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 注意：</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> num : arr) &#123;</span><br><span class="line">            right = Math.max(right, num);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left + <span class="number">1</span>) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">int</span> sum = calculateSum(arr, mid);</span><br><span class="line">            <span class="comment">// 计算最后 1 个使得转变以后数组的和小于等于 target 的阈值 threshold</span></span><br><span class="line">            <span class="keyword">if</span> (sum &gt; target) &#123;</span><br><span class="line">                <span class="comment">// 大于等于的就不是解，threshold 太大了，下一轮搜索区间是 [left, mid - 1]</span></span><br><span class="line">                right = mid - <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 下一轮搜索区间是 [mid, right]</span></span><br><span class="line">                left = mid;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 比较阈值线分别定在 left 和 left + 1 的时候与 target 的接近程度</span></span><br><span class="line">        <span class="keyword">int</span> sum1 = calculateSum(arr, left);</span><br><span class="line">        <span class="keyword">int</span> sum2 = calculateSum(arr, left + <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 注意：这里必须加绝对值，因为有可能出现 sum1 == sum2 &lt; target 的情况</span></span><br><span class="line">        <span class="keyword">if</span> (Math.abs(target - sum1) &lt;= Math.abs(sum2 - target)) &#123;</span><br><span class="line">            <span class="keyword">return</span> left;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> left + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">calculateSum</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> threshold)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> num : arr) &#123;</span><br><span class="line">            sum += Math.min(num, threshold);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="三、判别条件复杂的二分查找问题"><a href="#三、判别条件复杂的二分查找问题" class="headerlink" title="三、判别条件复杂的二分查找问题"></a>三、判别条件复杂的二分查找问题</h2><p>在上一节【二分答案】中看到的问题是，根据目标变量具有的单调性质编写判别函数。</p><p>还有一类问题是这样的：目标变量和另一个变量有相关关系（一般而言是线性关系），目标变量的性质不好推测，但是另一个变量的性质相对容易推测。</p><p>这样的问题的判别函数通常会写成一个函数的形式。</p><h3 id="875-爱吃香蕉的珂珂"><a href="#875-爱吃香蕉的珂珂" class="headerlink" title="875. 爱吃香蕉的珂珂"></a><a href="https://leetcode-cn.com/problems/koko-eating-bananas/">875. 爱吃香蕉的珂珂</a></h3><p>思路分析：</p><ul><li>根据题意可以知道：珂珂吃香蕉的速度越小，耗时越多。反之，速度越大，耗时越小，这是题目的单调性</li><li>我们要找的速度，因为题目限制了一小时内只能选择一堆香蕉吃，因此速度最大值就是这几堆香蕉中，数量最多的那一堆。速度的最小值是1，其实还可以再分析一下下界是多少，由于二分搜素的时间复杂度很低，严格的Fenix不是很有必要。</li><li>还是因为一小时内只能选择一堆香蕉吃，因此：<strong>每堆香蕉吃完的耗时 = 这堆香蕉的数量/ 一小时吃香蕉的数量</strong>。根据题意，这里的 / 在不能整除的时候需向上取整</li></ul><p>注意：当二分查找算法猜测的速度恰好使得珂珂在规定的实际内吃完香蕉的时候，还应该去尝试更小的速度是不是还可以保证在规定的时间内吃完香蕉</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">minEatingSpeed</span><span class="params">(<span class="keyword">int</span>[] piles, <span class="keyword">int</span> h)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = piles.length;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> maxVal = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> pile: piles)&#123;</span><br><span class="line">            maxVal = Math.max(maxVal, pile);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 速度最小的时候，耗时最长</span></span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 速度最大的时候，耗时最短</span></span><br><span class="line">        <span class="keyword">int</span> right = maxVal;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(calculateSum(piles, mid) &gt; h)&#123;</span><br><span class="line">                <span class="comment">//耗时太多，说明速度太慢，下一轮搜索区间是 [mid+1, right]</span></span><br><span class="line">                left = mid +<span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">calculateSum</span><span class="params">(<span class="keyword">int</span>[] piles, <span class="keyword">int</span> speed)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> pile : piles)&#123;</span><br><span class="line">            <span class="comment">// 向上取整可以这样写</span></span><br><span class="line">            sum += (pile + speed -<span class="number">1</span>) / speed;</span><br><span class="line">            <span class="comment">// 还可以这样</span></span><br><span class="line">            <span class="comment">// if (pile % speed == 0)&#123;</span></span><br><span class="line">            <span class="comment">//     sum += pile / speed;</span></span><br><span class="line">            <span class="comment">// &#125;else&#123;</span></span><br><span class="line">            <span class="comment">//     sum += pile / speed + 1;</span></span><br><span class="line">            <span class="comment">// &#125;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>时间复杂度：</p><ul><li>时间复杂度： $O(Nlog \ max(piles))$, 这里 N 表示数组 piles 的长度、我们在 $[1,max\ piles]$ 里使用二分查找，定位最小速度，而每一次执行判别函数的时间复杂度是 $O(N)$</li></ul><h3 id="410-分割数组的最大值"><a href="#410-分割数组的最大值" class="headerlink" title="410. 分割数组的最大值"></a><a href="https://leetcode-cn.com/problems/split-array-largest-sum/">410. 分割数组的最大值</a></h3><ul><li>动态规划的写法其实是穷举：按照长度、前缀、枚举最后一个划分，记录每一步结果。细节比较多，需要作图+仔细讨论边界情况，并且属性二维数组、三层for循环的写法；</li><li>本题的二分查找思路：查找一个有范围的整数，关于在利用单调性逼近这个整数。</li></ul><p>题意分析：各自和的最大值最小：</p><ul><li>由于数组是确定的，其中一组得分多，相应的另一组分到的值就少，所以对于任意一种拆分（切成 m 段），这 m 段可以取最大值 val</li><li>需要找一种拆分，使得这个最大值 val 的值是所有分成 m 段拆分里值最小的那个；</li></ul><h4 id="方法1：动态规划"><a href="#方法1：动态规划" class="headerlink" title="方法1：动态规划"></a>方法1：动态规划</h4><p>枚举所有的分割情况，例如题目中的输入数组【7，2，5，10，8】分割成两个非空连续子数组，可以有以下四种方式：</p><ul><li><code>[7, | 2, 5, 10, 8]</code>；</li><li><code>[7, 2, | 5, 10, 8]</code>；</li><li><code>[7, 2, 5, | 10, 8]</code>；</li><li><code>[7, 2, 5, 10, | 8]</code>。</li></ul><p>比较容易想到的递归结构是：</p><ul><li>找到最后一个分割，求出这个分割的连续子数组的和，与之前的分割取最大值。</li><li>枚举最后一个分割，找出所有最大值中最小的那一个</li></ul><p>整个过程稍微有一些繁琐，但是思想是直接的：对于所有长度的 前缀区间（题目中的关键信息是子数组连续，所以考虑前缀区间），枚举所有可能的分割，并记录每一步的结果，递推完成计算。以题目中的示例 [7, 2, 5, 10, 8] 为例：</p><p>先考虑 所有前缀区间 分割成 1 个非空连续子数组的情况：</p><ul><li>[7] 分割成 1 个非空连续子数组的和，就是整个数组的和 7，下同；</li><li>[7, 2] 分割成 1 个非空连续子数组的和，就是整个数组的和 9；</li><li>[7, 2, 5] 分割成 1 个非空连续子数组的和，就是整个数组的和 14；</li><li>[7, 2, 5, 10] 分割成 1 个非空连续子数组的和，就是整个数组的和 24；</li><li>[7, 2, 5, 10, 8] 分割成 1 个非空连续子数组的和，就是整个数组的和 32；</li></ul><p>再考虑 所有前缀区间 分割成 2 个非空连续子数组的情况：</p><ul><li>[7] 不能分割成 2 个非空连续子数组的和；</li><li>[7, 2] 分割成 2 个非空连续子数组，只有 1 种分割情况：[7, | 2] ，其中「[7] 分割成 1 个非空连续子数组」的情况我们在第 1 步计算过；</li><li>[7, 2, 8] 分割成 2 个非空连续子数组，有 2 种分割情况：</li><li>[7, | 2, 8] ，其中「[7] 分割成 1 个非空连续子数组」的情况我们在第 1 步计算过；</li><li>[7, 2, | 8] ，其中「[7, 2] 分割成 1 个非空连续子数组」的情况我们在第 2 步计算过；</li></ul><p>分析到这里，可以把递推结构形式化描述成如下：</p><p><strong>第一步：定义状态</strong></p><p>$dp[i][k]$ 表示：将前缀区间 $[0, i]$ 被分成 k 段的各自和的最大值的最小值记为 $dp[i][k]$，那么前缀区间 $[0, j]$ （这里 j &lt; i） 被分成 k - 1 段各自和的最大值的最小值为 $dp[j][k - 1]$。</p><p>即：第一维是第 <code>k</code> 个分割的最后一个元素的下标 <code>i</code> ，第二维是分割的总数 <code>i</code>。</p><p><strong>第二步：推导状态转移方程</strong></p><script type="math/tex; mode=display">dp[i][k] = max(dp[j][k-1],rangeSum(j+1,i))</script><p>这里 $rangeSum(j + 1, i)$ 表示数组 $nums[j + 1..i] $的区间和，它可以先计算出所有前缀和，然后以 O(1) 的方式计算出区间和。</p><p>上面的状态转移方程中，j 的值需要枚举。我们画图分析：</p><p><img src="https://i.loli.net/2021/11/14/XpFodj7RkH4G8nC.png" alt=""></p><ul><li>由于区间 [0, j] 一定要分成 k - 1 个非空连续子数组；</li><li>j 的意义是：第 k - 1 个分割的最后一个元素的下标；</li><li>而下标 k - 1 的前面（不包括 k - 1），一共有 k - 1 个元素（这一条只要是下标从 0 开始均成立）；</li><li>故 j 的枚举从 k - 2 开始，到 i - 1 结束，因为第 k 个分割至少要有 1 个元素。</li></ul><p><strong>第三步：思考初始化</strong></p><ul><li>由于要找最小值，初值赋值成为一个不可能达到的很大的值；</li><li>分割数为 1 ，即不分割的情况，所有的前缀和就是依次的状态值。</li></ul><p><strong>第 4 步：思考输出</strong></p><p>$dp[len][k]$，根据状态定义，这是显然的。</p><p>下面给出题目中的示例 [7, 2, 5, 10, 8] 的状态计算表，为了更突出一般性，把 m 设置成为数组的长度 5：</p><p><img src="https://i.loli.net/2021/11/14/HvYxSOThwA7slU1.png" alt=""></p><p>编码的思考路径：</p><p>我们按照阶段、状态和选择进行分析，依次把三层循环写下来：</p><ul><li>阶段：依次计算长度为 1 的区间、长度为 2 的区间，直到题目要求的长度为 m 的区间；</li><li>状态：前缀区间 [0, i] 的状态值，由于 i 要被分成 k 份，前缀区间里至少要有 k 个元素，最小前缀区间 k 个元素的最后一个元素的下标为 k - 1，故 i 从 k - 1 开始到 len - 1；</li><li>选择：枚举第 k - 1 个分割的最后一个元素的下标，根据上面的分析，从 k - 2 到 i - 1。</li></ul><h4 id="方法2：二分查找"><a href="#方法2：二分查找" class="headerlink" title="方法2：二分查找"></a>方法2：二分查找</h4><p>题目关键字：【非负整数数组】和【连续】这两个信息</p><p>与69题、287题：可以用于查找一个有范围的整数，就能想到是不是可以使用二分查找去解决</p><p>挖掘单调性：使用二分查找的一个前提是【数组具有单调性】，我们就去想想有没有单调性可以挖掘，不难发现：</p><ul><li>如果设置【数组各自和的最大值】很大，那么必然导致分割数很小</li><li>如果设置【数组各自和的最大值】很小，那么必然导致分割数很大</li></ul><p>可以通过调整【数组各自和的最大值】来达到：使得分割数恰好为 m 的效果。这里要注意一个问题：</p><p>如果某个数组各自的最大值恰好使得分割数为 m，此时不能放弃搜索，因为我们要使得这个最大值 最小化，此时还应该继续尝试缩小这个数组各自和的最大值，使得分割数超过 m，超过 m 的最后一个使得分割数为 m 的数组各自和的最大值就是我们要找的最小值。</p><p>举个例子：</p><p>例如：（题目中给出的示例）输入数组为【7，2，5，10，8】，m=2。如果设置数组各自和的最大值为 21那么每个的是【7，2，5，|10，8】，此时 m =  2，此时这个值太大，尝试一点一点缩小：</p><ul><li>设置 数组各自和的最大值 为 20，此时分割依然是 [7, 2, 5, | 10, 8]，m = 2；</li><li>设置 数组各自和的最大值 为 19，此时分割依然是 [7, 2, 5, | 10, 8]，m = 2；</li><li>设置 数组各自和的最大值 为 18，此时分割依然是 [7, 2, 5, | 10, 8]，m = 2；</li><li>设置 数组各自和的最大值 为 17，此时分割就变成了 [7, 2, 5, | 10, | 8]，这时 m = 3。</li></ul><p><code>m</code> 变成 <code>3</code> 之前的值 <strong>数组各自和的最大值</strong> <code>18</code> 是这个问题的最小值，所以输出 <code>18</code>。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">splitArray</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> m)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> max = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">         <span class="comment">// 计算「子数组各自的和的最大值」的上下界</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> num :nums)&#123;</span><br><span class="line">            max = Math.max(max, num);</span><br><span class="line">            sum += num;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用【二分查找】确定一个恰当的【子数组各自的和的最大值】</span></span><br><span class="line">        <span class="keyword">int</span> left = max;</span><br><span class="line">        <span class="keyword">int</span> right = sum;</span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span> splits = split(nums, mid);</span><br><span class="line">            <span class="keyword">if</span>(splits &gt; m)&#123;</span><br><span class="line">                <span class="comment">// 如果分割数太多，说明【子数组各自的和的最大值】太小，此时需要将【子数组各自的和的最大值】调大</span></span><br><span class="line">                <span class="comment">// 下一轮搜索的区间是【mid+1，right】</span></span><br><span class="line">                left = mid+<span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">split</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> maxIntervalSum)</span></span>&#123;</span><br><span class="line">        <span class="comment">// 至少是一个分割</span></span><br><span class="line">        <span class="keyword">int</span> splits = <span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 当前区间的和</span></span><br><span class="line">        <span class="keyword">int</span> curIntervalSum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> num : nums)&#123;</span><br><span class="line">            <span class="comment">// 尝试加上当前遍历的这个数，如果加上去超过了【子数组各自的和的最大值】，就不加这个数，另起炉灶</span></span><br><span class="line">            <span class="keyword">if</span> (curIntervalSum + num &gt; maxIntervalSum)&#123;</span><br><span class="line">                curIntervalSum = <span class="number">0</span>;</span><br><span class="line">                splits++;</span><br><span class="line">            &#125;</span><br><span class="line">            curIntervalSum += num;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> splits;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1011-在-D-天内送达包裹的能力"><a href="#1011-在-D-天内送达包裹的能力" class="headerlink" title="1011. 在 D 天内送达包裹的能力"></a><a href="https://leetcode-cn.com/problems/capacity-to-ship-packages-within-d-days/">1011. 在 D 天内送达包裹的能力</a></h3><p>对于左边界而言，由于我们不能「拆分」一个包裹，因此船的运载能力不能小于所有包裹中最重的那个的重量，即左边界为数组 weights 中元素的最大值。</p><p>对于右边界而言，船的运载能力也不会大于所有包裹的重量之和，即右边界为数组 weights 中元素的和。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">shipWithinDays</span><span class="params">(<span class="keyword">int</span>[] weights, <span class="keyword">int</span> days)</span> </span>&#123;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span> maxVal = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> w : weights)&#123;</span><br><span class="line">            maxVal = Math.max(maxVal, w);</span><br><span class="line">            sum+=w;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> left = maxVal;</span><br><span class="line">        <span class="keyword">int</span> right = sum;</span><br><span class="line">        <span class="comment">// int left = Arrays.stream(weights).max().getAsInt(), right = Arrays.stream(weights).sum();</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span> cur = cal(weights, mid);</span><br><span class="line">            <span class="keyword">if</span> (cur &lt;= days) &#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">cal</span><span class="params">(<span class="keyword">int</span>[] weights, <span class="keyword">int</span> mid)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> need = <span class="number">1</span>, cur = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> weight : weights) &#123;</span><br><span class="line">            <span class="keyword">if</span> (cur + weight &gt; mid) &#123;</span><br><span class="line">                ++need;</span><br><span class="line">                cur = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            cur += weight;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> need;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1482-制作-m-束花所需的最少天数"><a href="#1482-制作-m-束花所需的最少天数" class="headerlink" title="1482. 制作 m 束花所需的最少天数"></a><a href="https://leetcode-cn.com/problems/minimum-number-of-days-to-make-m-bouquets/">1482. 制作 m 束花所需的最少天数</a></h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">minDays</span><span class="params">(<span class="keyword">int</span>[] bloomDay, <span class="keyword">int</span> m, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> len = bloomDay.length;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(m * k &gt; len)&#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> maxVal = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> minVal = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> b : bloomDay)&#123;</span><br><span class="line">            maxVal = Math.max(b, maxVal);</span><br><span class="line">            minVal = Math.min(b, minVal);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> left = minVal;</span><br><span class="line">        <span class="keyword">int</span> right = maxVal;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span> cur = cal(bloomDay ,mid, m, k);</span><br><span class="line">            <span class="keyword">if</span>(cur &gt;= m)&#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">cal</span><span class="params">(<span class="keyword">int</span>[] bloomDay, <span class="keyword">int</span> mid, <span class="keyword">int</span> m, <span class="keyword">int</span> k)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> curNum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> f = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> b:bloomDay)&#123;</span><br><span class="line">            <span class="keyword">if</span>(b &lt;= mid)&#123;</span><br><span class="line">                f++;</span><br><span class="line">                <span class="keyword">if</span>(f==k)&#123;</span><br><span class="line">                    curNum++;</span><br><span class="line">                    f=<span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                f=<span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> curNum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="LCP-12-小张刷题计划"><a href="#LCP-12-小张刷题计划" class="headerlink" title="LCP 12. 小张刷题计划"></a><a href="https://leetcode-cn.com/problems/xiao-zhang-shua-ti-ji-hua/">LCP 12. 小张刷题计划</a></h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">minTime</span><span class="params">(<span class="keyword">int</span>[] time, <span class="keyword">int</span> m)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(time.length &lt; m)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = Integer.MAX_VALUE;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(check(time, m, mid))&#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">check</span><span class="params">(<span class="keyword">int</span>[] time, <span class="keyword">int</span> m, <span class="keyword">int</span> limit)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> cur = <span class="number">0</span>, sum = <span class="number">0</span>, max = <span class="number">0</span>, day = <span class="number">1</span>; <span class="comment">// 当前遍历的题目，当前组的总耗时，当前组的最大耗时，需要的天数</span></span><br><span class="line">        <span class="keyword">while</span>(cur &lt; time.length)&#123;</span><br><span class="line">            sum += time[cur];</span><br><span class="line">            max = Math.max(max, time[cur]);</span><br><span class="line">            <span class="keyword">if</span>(sum - max &gt; limit)&#123;<span class="comment">// 当前组总耗时减去组内最大耗时仍超出限制，则需开启额外一天</span></span><br><span class="line">                day ++;</span><br><span class="line">                <span class="keyword">if</span>(day &gt; m)&#123;<span class="comment">// 超出总天数m，无法完成分配</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                sum = time[cur]; <span class="comment">// sum和max更新为新组的值</span></span><br><span class="line">                max = time[cur];</span><br><span class="line">            &#125;</span><br><span class="line">            cur++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>; <span class="comment">// 能遍历完所有题目即完成了分配</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;二分查找典型问题-二、三&quot;&gt;&lt;a href=&quot;#二分查找典型问题-二、三&quot; class=&quot;headerlink&quot; title=&quot;二分查找典型问题(二、三)&quot;&gt;&lt;/a&gt;二分查找典型问题(二、三)&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://leetcode-cn</summary>
      
    
    
    
    
    <category term="LeetCode" scheme="http://example.com/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>Parameter-Efficient Transfer Learning with Diff Pruning</title>
    <link href="http://example.com/2021/11/05/Parameter-Efficient-Transfer-Learning-with-Diff-Pruning/"/>
    <id>http://example.com/2021/11/05/Parameter-Efficient-Transfer-Learning-with-Diff-Pruning/</id>
    <published>2021-11-05T06:23:11.000Z</published>
    <updated>2021-11-14T08:18:23.949Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Parameter-Efficient-Transfer-Learning-with-Diff-Pruning"><a href="#Parameter-Efficient-Transfer-Learning-with-Diff-Pruning" class="headerlink" title="Parameter-Efficient Transfer Learning with Diff Pruning"></a>Parameter-Efficient Transfer Learning with Diff Pruning</h1><p>Diff pruning 使参数有效的迁移学习在新任务中得到良好的扩展。</p><p>该方法学习了一个特定于任务的 “diff “向量，该向量对原始预训练的参数进行了调整。在训练过程中，这个 diff 向量通过对L0-norm惩罚的可微调近似来适应性地修剪，以鼓励稀疏性。</p><p>随着任务数量的增加，diff pruning仍然具有参数有效，因为它只需要为每个任务存储一个小的diff向量。由于它不需要在训练期间访问所有任务，因此它在任务以流形式设置中很有吸引力。</p><p>在GLUE基准测试中，diff pruning可以与微调基线的性能相媲美，而每个任务只需修改0.5%的预训练模型参数，与流行的修剪方法相比，其扩展性更强。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>针对特定任务对预训练的深度网络进行微调是当代NLP的主流模式，在一系列自然语言理解任务中取得了最先进的结果。</p><p>虽然这种方法简单明了，在经验上也很有效，但很难扩展到多任务、内存受限的情况下（例如设备上的应用），因为它需要为每个任务运送和存储一整套模型参数。</p><p>由于这些模型是通过自监督的预训练来学习可推广的、与任务无关的语言表征，因此为每个任务微调整个模型似乎特别浪费。</p><p>提高参数有效的一种流行方法：</p><ul><li>《Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning》</li><li>《Poor Man’s BERT: Smaller and Faster Transformer Models》</li><li>《Masking as an Efficient Alternative to Finetuning for Pretrained Language Models》</li><li>《Adaptive Sparsity by Fine-Tuning》</li></ul><p>是为每个任务学习较小的压缩模型。这种方法面临着严重的稀疏性/性能权衡，并在每个任务中保留大量非零参数（例如10%-30%）。多任务学习和基于特征的迁移允许每个任务进行更有效的参数迁移学习。</p><p>多任务学习和 feature-based 的迁移允许每个任务的参数效率更高的迁移学习：</p><ul><li>《Multi-Task Deep Neural Networks for Natural Language Understanding》</li><li>《BAM! Born-Again Multi-Task Networks for Natural Language Understanding》</li><li>《BERT and PALs: Projected attention layers for efficient adaptation in multi-task learning》</li><li>《Sentence- BERT: Sentence Embeddings using Siamese BERT-Networks》</li></ul><p>这些方法在共享模型的基础上训练少量附加参数（例如线性层）。然而，多任务学习通常需要在训练期间访问所有任务，以防止灾难性遗忘，而 feature-based 的迁移学习（例如，基于任务不可知的句子表示）通常通过微调表现得更好</p><p>一个有吸引力的中间地带是为特定任务 finetune 基础模型的扩展。这种方法在保持基于特征的转移的任务模块化的同时，还能抓住 finetune 的训练优势。</p><p>例如，Adapters 使用较小的特定任务模块，在模型的层之间插入这种方法不需要在训练期间访问所有的任务，针对现实环境，随着新的任务流到达（</p><p>《Parameter-efficient transfer learning for nlp》</p><p>《Adapter- Fusion: Non-Destructive Task Composition for Transfer Learning》</p><p>）发现，适配器层可以在GLUE基准上匹配完全微调的BERT的性能，而每个任务需要3.6%的额外参数（平均）。</p><p>Diff pruning 是对预训练模型的一个新的扩展，目的是为了更参数有效的迁移学习。Diff pruning 不是修改模型的结构，而是通过一个特定任务的 diff 向量扩展基础模型。</p><p>为了学习这个向量，我们将特定任务的模型参数重新参数化为 $θ<em>{task} = θ</em>{pretrained} + δ<em>{task}$，其中预训练的参数向量 $θ</em>{pretrained}$是固定的，特定任务的 diff 向量 $δ_{task}$ 是微调的。差异向量用L0-norm惩罚的可微调近似值进行重构，以鼓励稀疏性。</p><p>Diff pruning 可以变得非常有效的参数，因为它只需要为每个任务存储 diff 向量的非零位置和权重。存储共享预训练模型的成本保持不变，并在多个任务中分摊。在GLUE基准上，Diff pruning 可以匹配完全微调的BERT基线的性能，而每个任务只微调0.5%的预训练参数。随着任务数量的增加，diff pruning在所需的存储量方面优于流行的基于剪枝的方法。</p><h2 id="Background-Transfer-Learning"><a href="#Background-Transfer-Learning" class="headerlink" title="Background: Transfer Learning"></a>Background: Transfer Learning</h2><p>NLP中的迁移学习大多使用 pretrain finetune 范式，它从预训练的模型中为所有任务初始化一个模型参数子集，然后根据特定的任务目标进行调整。预训练目标包括上下文预测、自动编码、机器翻译，以及最近的语言建模的变种目标。</p><p>这里我们考虑将转移学习应用于多个任务。 我们考虑的是具有潜在的未知任务集（可能以流的形式到达）的设置，其中每个任务 $τ∈T$ 有一个相关的训练集 $D<em>{\tau} = {x</em>{\tau}^{(n)}, y<em>{\tau}^{(n)}}</em>{n=1}^{N}$。对于所有任务，目标是产生（可能是捆绑的）模型参数 $θ_τ$，使经验风险最小化，</p><script type="math/tex; mode=display">min_{\theta_{\tau}} \frac{1}{N} \sum_{n=1}^N C(f_{\tau}(x_{\tau}^{(n)}; \theta_{\tau}), y_{\tau}^{(n)}) + \lambda R(\theta_{\tau})</script><p>其中，$f<em>τ(\cdot;θ</em>τ)$ 是一个关于输入的参数化函数（例如神经网络），$C(\cdot,\cdot)$是一个损失函数（例如交叉熵）1，R（-）是一个具有超参数 $λ$ 的操作性正则器。</p><p>我们可以通过简单地学习每个任务的独立参数来使用 pretrain finetune 方法。然而，预训练模型的巨大规模使得这种方法的参数非常不方便。例如，广泛采用的模型，如BERT-BASE 和 BERT-LARGE，分别有1.1亿和3.4亿个参数，而他们同代的模型有数十亿的参数数。 解决这种参数效率低下的经典方法是通过联合训练，针对多个任务训练一个共享模型（连同特定任务的输出层）。然而，多任务学习的通常表述要求事先知道任务集 $T$，以防止灾难性的遗忘，这使得它不适合于任务集未知或任务流到来的应用。</p><h2 id="Diff-Pruning"><a href="#Diff-Pruning" class="headerlink" title="Diff Pruning"></a>Diff Pruning</h2><p>Diff pruning 将特定任务的微调表述为学习一个 diff 向量 $δ_τ$，该向量被添加到预先训练的模型参数 $θ$ 中，该参数保持固定。我们首先对特定任务的模型参数进行重新参数化，</p><script type="math/tex; mode=display">\theta_{\tau} = \theta + \delta_{\tau}</script><p>这导致了下面的经验风险最小化问题，</p><script type="math/tex; mode=display">min_{\delta_{\tau}} L(D_{\tau}, f_{\tau} , \theta+\delta_{\tau}) + \lambda R(\theta+\delta_{\tau})</script><p>为了简洁起见，我们将 $L(D<em>τ, f</em>τ, θ_τ)$ 定义为:</p><script type="math/tex; mode=display">L(D_{\tau} , f_{\tau},\theta_{\tau}) = \frac{1}{N} \sum_{n=1}^N C(f_{\tau}(x_{\tau}^{(n)};\theta_{\tau}), y_{\tau}^{(n)})</script><p>这种微不足道的重新参数化表明，存储预训练参数 $θ$ 的成本在不同的任务中被分摊，而新任务的唯一边际成本是 diff向量。如果我们能将 $δ$ 正则化，使其稀疏，从而使  $||\delta_{\tau}||_0 &lt;&lt; ||\theta||_0$，那么随着任务数量的增加，这种方法可以变得更具有参数效率。我们可以用差值向量的L0-norm惩罚来指定这一目标,</p><script type="math/tex; mode=display">R(\theta + \delta_{\tau}) = ||\delta_{\tau}||_0 = \sum_{i=1}^d 1 \ \ \{\delta_{\tau,i} \neq 0\}</script><h3 id="Differentiable-approximation-to-the-L0-norm"><a href="#Differentiable-approximation-to-the-L0-norm" class="headerlink" title="Differentiable approximation to the L0-norm"></a>Differentiable approximation to the L0-norm</h3><p>这个正则器很难优化，因为它是不可微分的。为了近似这个L0目标，我们采用了一种基于梯度的学习方法，即使用一个宽松的掩码向量进行L0稀疏度学习《Learning Sparse Neural Networks through L0 Regularization》</p><p>这种方法包括将 binary vector 放宽到连续空间，然后与密集的权重向量相乘，以确定在训练中应用多少权重向量。训练结束后，掩码被制成确定性的，并且很大一部分 diff 向量为零。</p><p>为了应用这种方法，我们首先将 $δ_τ$ 分解成一个二进制掩码向量，再乘以一个密集向量。</p><script type="math/tex; mode=display">\delta_{\tau} = z_{\tau} \odot w_{\tau} , \ \ \ z_{\tau} \in \{0,1\}, \ w_{\tau}\in R^d</script><p>我们现在对真实目标进行下限，并对关于 $z<em>τ$ 的期望进行优化，其分布 $p(z</em>τ; α<em>τ)$ 初始是伯努利，并引入参数 $α</em>τ$。</p><script type="math/tex; mode=display">min_{\alpha_{\tau}, w_{\tau}} E_{z_{\tau} \sim p(z_{\tau};\alpha_{\tau})} [L(D_{\tau}, f_{\tau},\theta + \delta_{\tau}) + \lambda||\delta_{\tau}||_0]</script><p>这个目标仍然因为 $z_τ$ 的离散性而变得复杂，但是这个期望为经验上有效松弛提供了一些指导。</p><p>遵循先前的工作《Learning Sparse Neural Networks through L0 Regularization》，将 $z<em>τ$ 放宽到连续空间 $[0, 1]^d$，并采用拉伸的  Hard-Concrete 分布，这样就可以使用路径梯度估计器。具体来说，$z</em>τ$ 现在被定义为来自均匀分布的样本 $u$ 的一个确定性和（次）可微函数。</p><script type="math/tex; mode=display">    \begin{equation}\begin{split}  u &\sim U(0, 1)\\ s_{\tau} &= \sigma(logu - log(1-u) + \alpha_{\tau}) \\ \hat s_{\tau} &= s_{\tau} \times (r - l ) + l \\ z_{\tau} &= min (1,max(0, \hat s_{\tau}))    \end{split}\end{equation}</script><p>这里 $l<0, r>1$是两个常数，用来将 $s_τ$ 拉伸到区间 $(l,r)^d$，然后用 $min(1, max(0, \cdot))$ 操作将它夹在 $[0, 1]^d$中。在这种情况下，我们有一个预期L0-norm的可微闭式表达。</p><script type="math/tex; mode=display">E[||\delta_{\tau}||_0] = \sum_{i=1}^d \sigma(\alpha_{\tau,i} - log\frac{-l}{r})</script><p>因此，最终的优化问题由以下方式给出，</p><script type="math/tex; mode=display">min_{\alpha_{\tau}, w_{\tau}} E_{u\sim U[0,1]} [L(D_{\tau}, f_{\tau}, \theta + z_{\tau} \odot w_{\tau})] + \lambda \sum_{i=1}^d \sigma(\alpha_{\tau} - log\frac{-l}{r})</script><p>为了减少符号的混乱，我们把没有经过预训练的特定任务输出层的参数归入θ。我们现在可以利用路径梯度估计器来优化关于 $α<em>τ$ 的第一项，因为期望不再依赖于它。 训练后，我们通过对 $u$ 采样一次以获得 $z</em>τ$（即 不一定是二进制向量，但由于钳位函数的原因，其维数非常多，恰好为零），然后设置 $δ<em>τ = z</em>τ \odot w_τ $</p><h3 id="L0-ball-projection-with-magnitude-pruning-for-sparsity-control"><a href="#L0-ball-projection-with-magnitude-pruning-for-sparsity-control" class="headerlink" title="L0-ball projection with magnitude pruning for sparsity control"></a>L0-ball projection with magnitude pruning for sparsity control</h3><p>微分 L0 正则化使我们能够实现高稀疏率。然而，最理想的是设置一个精确的稀疏率，特别是考虑到需要参数预算的应用。由于正则化系数 $λ$ 是某个 $η$ 的约束条件 $E[||δ_τ||_0]&lt; η$ 的拉格朗日乘数，原则上可以通过搜索不同的 $λ$ 值来实现。 然而，我们发现通过训练后投影到目标 L0-ball 上实现精确的稀疏率更有效率，而且经验上也更有效。</p><p>《Structured Pruning of Large Language Models》</p><p>具体来说，我们对 diff 向量 $δ<em>τ$ 使用 magnitude pruning 幅度修剪，通过在 $δ</em>τ$ 中只保留前  $t \%\times d$ 的值来达到稀疏率 $t\%$。注意，与标准的 magnitude pruning 不同，这是基于diff向量值的幅度而不是模型参数。我们发现，在固定非零掩码的情况下进一步微调 $δ_τ$ 以保持良好的性能是很重要的，这也是 magnitude pruning 中经常出现的情况。由于这种通过投射到L0-ball 上的参数效率可以在没有自适应 diff puning的情况下应用，这样的方法将作为我们在实证研究中的基线之一。</p><h3 id="Structured-Diff-Pruning"><a href="#Structured-Diff-Pruning" class="headerlink" title="Structured Diff Pruning"></a>Structured Diff Pruning</h3><p>为了使diff pruning能够适应模型结构，我们考虑了一个结构化的扩展，其中包括维度之间的依赖性。假设，这种方法可以让模型学会在局部区域修改参数，而不是独立处理每个参数。修改正则器，首先将参数索引分为G组 $ {g(1),…,g(G)}$，其中 $g(j)$ 是由组 $g(j)$ 支配的参数指数的子集。</p><p>然后，为每个组 $g(j)$ 引入一个标量 $z^j<em>τ$（及相关参数 $α^j</em>τ$），并将索引 $i\in g(j)$的特定任务参数分解为 $δ<em>j = z</em>{τ,i} - z^j<em>τ - w</em>{τ,i}$</p><p>然后，期望的L0范数由下式给出:</p><script type="math/tex; mode=display">    \begin{equation}\begin{split}      E[||\delta_{\tau}||_0] &= \sum_{j=1}^G \sum_{i\in g(j)} E[1\ \{z_{\tau,i}\ \cdot z_{\tau}^g > 0\}] \\     &= \sum_{j=1}^G \sum_{i\in g(j)} \sigma(\alpha_{\tau,i} - log\frac{-l}{r}) \cdot \sigma(\alpha_{\tau}^j - log\frac{-l}{r})    \end{split}\end{equation}</script><p>我们可以像以前一样用基于梯度的优化训练。一个组中的参数被正则器所鼓励，共同被移除。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Model-and-datasets"><a href="#Model-and-datasets" class="headerlink" title="Model and datasets"></a>Model and datasets</h3><p>为了评估，使用GLUE基准以及SQuAD抽取式问题回答数据集。按照Adapters，在GLUE任务的以下子集上测试。</p><ul><li>多类型自然语言推理（MNLI），目标是预测两个句子之间的关系是包含关系、矛盾关系还是中性关系（我们在 $MNLI<em>m$ 和 $MNLI</em>{mm}$上进行测试，分别对匹配/不匹配的领域进行测试）；</li><li>Quora问题对（QQP），一个分类任务，预测两个问题是否语义等同；</li><li>问题自然语言推理（QNLI），必须预测一个句子是否是问题的正确答案。</li><li>Stanford Sentiment Treebank (SST-2)，一个预测电影评论情绪的句子分类任务；</li><li>Corpus of Linguistic Acceptability (CoLA)，其目标是预测一个句子在语言上是否可以接受。</li><li>语义文本相似性基准（STS-B），必须预测两个句子之间的相似性等级；</li><li>微软研究院转述语料库（MRPC），目标是预测两个句子是否在语义上等同；</li><li>识别文本关联（RTE），必须预测第二个句子是否被第一个句子所包含。</li></ul><p>该基准对CoLA使用Matthew’s correlation，对STS-B使用Spearman，对MRPC/QQP使用F1 score，对MNLI/QNLI/SST- 2/RTE使用accuracy。</p><h3 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h3><p>将结构化和非结构化的 diff pruning 变体与以下基线进行比较：</p><ul><li>Full finetuning：像往常一样对BERT-LARGE进行完全微调</li><li>Last layer finetuning：仅微调倒数第二层(连同最终输出层)</li><li>Adapters：该研究在预训练模型的每一层之间训练特定任务的瓶颈层，通过改变瓶颈层的大小，可以对参数效率进行控制。</li><li>Non-adaptive diff pruning：在magnitude pruning的基础上进行diff pruning（即，通过通常的微调获得$\theta<em>{\tau}$，设置 $\delta</em>{\tau} = \theta<em>{\tau} - \theta$，然后应用magnitude pruning，再对 $\delta</em>{\tau}$ 进行额外的微调）。对于diff pruning，我们将目标稀疏率设置为0.5%</li></ul><p><img src="https://i.loli.net/2021/11/07/pa6jXZ2JvkinyCO.png" alt=""></p><h3 id="Structured-vs-Non-structured-Diff-Pruning"><a href="#Structured-vs-Non-structured-Diff-Pruning" class="headerlink" title="Structured vs. Non-structured Diff Pruning"></a>Structured vs. Non-structured Diff Pruning</h3><p><img src="https://i.loli.net/2021/11/07/rGCJ4qX1cn9d2R8.png" alt=""></p><p>结构化Diff Pruning 为每个组引入了一个额外的掩码，这鼓励了对整个组进行 pruning。这比传统的组稀疏技术的限制性要小，这些技术被用于L0-norm松弛，迫使一个组中的所有参数共享同一个掩码。然而，我们仍然期望整个组更经常地被pruning 掉，这可能会使学习过程偏向于完全消除或将非零 diff 聚在一起。在表3中，我们确实发现，结构化的差异修剪导致的微调模型更有可能使整个组与它们的预训练值（零差异）没有变化。</p><h3 id="Task-specific-Sparsity"><a href="#Task-specific-Sparsity" class="headerlink" title="Task-specific Sparsity"></a>Task-specific Sparsity</h3><p><img src="https://i.loli.net/2021/11/07/bP5n9iavNcFOJqB.png" alt=""></p><p>预训练模型的不同层被认为是对不同信息的编码。鉴于每个任务可能会招募不同种类的语言现象嵌入到隐藏层中，我们假设 diff pruning 将通过特定任务的微调来修改预训练模型的不同部分。图2显示了每个任务中不同层的非零 diff 参数的百分比。我们发现，不同的任务确实修改了网络的不同部分，尽管有些任务之间存在一些质量上的相似性，例如QNLI和QQP（都必须对问题进行编码），以及MRPC和STS-B（都必须预测句子间的相似性）。嵌入层对所有任务的修改都很稀疏。虽然稀疏性分布的一些变化是由于简单的随机性造成的，但我们确实观察到在同一任务的多次运行中存在一定程度的一致性。</p><h3 id="Effect-of-L0-ball-projection"><a href="#Effect-of-L0-ball-projection" class="headerlink" title="Effect of L0-ball projection"></a>Effect of L0-ball projection</h3><p><img src="https://i.loli.net/2021/11/07/8MiDzmJnkHr1TGp.png" alt=""></p><p>应用 magnitude pruning 幅度修剪来投影到L0-ball上是实现精确稀疏目标的关键。如表4所示，我们观察到通过此方法在性能上几乎没有损失。我们重申，使用固定掩码进行微调至关重要，即使对于不应用幅度修剪的方法也是如此。</p><p><img src="https://i.loli.net/2021/11/07/YsyAIgbN5el4njO.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Parameter-Efficient-Transfer-Learning-with-Diff-Pruning&quot;&gt;&lt;a href=&quot;#Parameter-Efficient-Transfer-Learning-with-Diff-Pruning&quot; class=&quot;h</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>谣言、虚假信息综述</title>
    <link href="http://example.com/2021/11/03/A-Survey-on-Natural-Language-Processing-for-Fake-News-Detection/"/>
    <id>http://example.com/2021/11/03/A-Survey-on-Natural-Language-Processing-for-Fake-News-Detection/</id>
    <published>2021-11-03T09:48:08.000Z</published>
    <updated>2021-11-18T02:26:32.227Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="谣言、虚假信息综述"><a href="#谣言、虚假信息综述" class="headerlink" title="谣言、虚假信息综述"></a>谣言、虚假信息综述</h1><hr><h1 id="A-Survey-on-Natural-Language-Processing-for-Fake-News-Detection"><a href="#A-Survey-on-Natural-Language-Processing-for-Fake-News-Detection" class="headerlink" title="A Survey on Natural Language Processing for Fake News Detection"></a>A Survey on Natural Language Processing for Fake News Detection</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>虚假新闻检测是自然语言处理（NLP）中的一个关键但具有挑战性的问题。<strong>社交网络平台的迅速崛起不仅带来了信息可及性的大幅提高，而且也加速了假新闻的传播。因此，假新闻的影响越来越大，有时甚至延伸到线下世界，威胁到公共安全。鉴于海量的网络内容，自动检测假新闻是一个实用的NLP问题，对所有在线内容提供商都有用，以减少人类检测和防止假新闻传播的时间和精力。</strong>在本文中，我们描述了假新闻检测所涉及的挑战，也描述了相关任务。我们系统地回顾和比较了为该任务开发的任务描述、数据集和NLP解决方案，还讨论了它们的潜力和局限性。基于我们的见解，我们概述了有希望的研究方向，包括更精细、详细、公平和实用的检测模型。我们还强调了假新闻检测和其他相关任务之间的区别，以及NLP解决方案对假新闻检测的重要性。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>自动假新闻检测是评估新闻中 claims（声明，主张） 的真实性的任务。这是一个新的但关键的NLP问题，因为传统的新闻媒体和社交媒体对社会中的每个人都有巨大的社会政治影响。例如，对假新闻的曝光会导致对某些政治候选人的无效、疏远和嘲讽（Balmas，2014）。假新闻甚至与威胁公共安全的真实世界的暴力事件有关（例如，比萨门（Kang和Goldman，2016））。检测假新闻是NLP可以帮助的一个重要的应用，因为它也对技术如何在教育公众的同时促进验证 claims 的真实性产生了更广泛的影响。</p><p><strong>这项任务的传统解决方案是请专业人员，如记者，根据以前说过的或写过的事实，对照证据来检查 claims。然而，这样做既费时又费力。</strong>例如，PolitiFact 需要三位编辑来判断一条新闻的真伪。随着互联网社区和信息传播速度的快速增长，互联网内容的自动假新闻检测已经引起了人工智能研究界的兴趣。<strong>自动假新闻检测的目标是减少人类检测假新闻的时间和精力，帮助我们停止传播假新闻。随着计算机科学子领域的发展，如机器学习（ML）、数据挖掘（DM）和NLP，假新闻检测的任务已从不同角度得到研究。</strong></p><p>在本文中，我们从NLP的角度调查了自动假新闻检测。概括地说，我们介绍了假新闻检测的技术挑战，以及研究人员如何定义不同的任务并制定ML解决方案来解决这个问题。我们讨论了每项任务的优点和缺点，以及潜在的陷阱和弊端。更具体地说，我们对假新闻检测的研究工作进行了概述，并对其任务定义、数据集、模型构建和性能进行了系统的比较。我们还讨论了这个方向上的未来研究的指导方针。本文还包括一些其他方面，如社会参与分析。我们的贡献有三个方面。</p><ul><li>对用于自动检测假新闻的自然语言处理解决方案进行了首次全面调查。</li><li>系统地分析了假新闻检测如何与现有的NLP任务保持一致，并讨论了问题的不同公式的假设和值得注意的问题。</li><li>对现有的数据集、NLP方法和结果进行了分类和总结，为对这个问题感兴趣的新研究人员提供了第一手的经验和易懂的介绍。</li></ul><h2 id="2-Related-Problems"><a href="#2-Related-Problems" class="headerlink" title="2 Related Problems"></a>2 Related Problems</h2><h3 id="2-1-Fact-Checking"><a href="#2-1-Fact-Checking" class="headerlink" title="2.1. Fact-Checking"></a>2.1. Fact-Checking</h3><p>事实核查的任务是评估政治家、专家学者等公众人物提出的主张的真实性。许多研究者并不区分假新闻检测和事实核查，因为它们都是为了评估 claims主张 的真实性。一般来说，<strong>假新闻检测通常专注于新闻事件，而事实核查则更广泛。</strong> Thorne和Vlachos（2018）对这一主题进行了全面的回顾。</p><h3 id="2-2-Rumor-Detection"><a href="#2-2-Rumor-Detection" class="headerlink" title="2.2. Rumor Detection"></a>2.2. Rumor Detection</h3><p><strong>谣言检测并没有一个一致的定义</strong>。最近的一项调查（Zubiaga等人，2018）将谣言检测定义为将个人主张分为谣言和非谣言，其中谣言被<strong>定义为在发布时由未经核实的信息片段组成的声明</strong>。<strong>换句话说，谣言必须包含可以验证的信息，而不是主观的意见或感觉。</strong></p><h3 id="2-3-Stance-Detection"><a href="#2-3-Stance-Detection" class="headerlink" title="2.3. Stance Detection"></a>2.3. Stance Detection</h3><p>立场检测是指从文本中评估作者在辩论中站在哪一边的任务。它与假新闻检测不同，因为它不是针对真实性，而是针对一致性。<strong>立场检测可以是假新闻检测的一个子任务，因为它可以应用于搜索文本的证据</strong>（Ferreira和Vlachos，2016）。PHEME，假新闻数据集之一，有与新闻相关的推文，捕捉到用户信任或不信任的行为。</p><h3 id="2-4-Sentiment-Analysis"><a href="#2-4-Sentiment-Analysis" class="headerlink" title="2.4. Sentiment Analysis"></a>2.4. Sentiment Analysis</h3><p>情感分析是一项提取情感的工作，例如顾客对一家餐厅的好感或负面印象。与谣言检测和假新闻检测不同的是，情感分析不是为了对主张进行客观验证，而是为了分析个人情感。</p><h2 id="3-Task-Formulations"><a href="#3-Task-Formulations" class="headerlink" title="3. Task Formulations"></a>3. Task Formulations</h2><p>在第2节中，我们比较了与假新闻检测有关的问题，以确定本调查的范围。在本调查中，<strong>假新闻检测的一般目标是识别假新闻，定义为看似新闻的虚假故事，包括在谣言检测中被判断为可以验证的信息的谣言。</strong>特别是，我们专注于文本内容的假新闻检测。输入可以是文本，从简短的声明到整个文章。输入与使用的数据集有关（见第4节），而且还可以附加附加信息，如发言人的身份。<br>有不同类型的标签或评分策略用于假新闻检测。在大多数研究中，假新闻检测被表述为一个分类或回归问题，但分类的使用更为频繁。</p><h3 id="3-1-Classification"><a href="#3-1-Classification" class="headerlink" title="3.1. Classification"></a>3.1. Classification</h3><p>最常见的方法是将假新闻的检测制定为一个二元分类问题。<strong>然而，将所有的新闻分为两类（假的或真的）是很困难的，因为存在着新闻部分是真的和部分是假的情况。</strong>为了解决这个问题，增加额外的类别是常见的做法。主要是为既不完全真实也不完全虚假的新闻设置一个类别，或者设置两个以上的真实度作为附加类别。当使用这些数据集时，预期的输出是多类标签，而这些标签是作为独立的标签学习的，具有i.i.d的假设（Rashkin等人，2017；Wang，2017）。<br>假新闻分类器取得良好性能的条件之一是有足够的标签数据。然而，要获得可靠的标签需要大量的时间和人力。因此，人们提出了半/弱监督和无监督的方法（Rubin和Vashchilko，2012；Bhattacharjee等人，2017）。</p><h3 id="3-2-Regression"><a href="#3-2-Regression" class="headerlink" title="3.2. Regression"></a>3.2. Regression</h3><p>虚假新闻检测也可以被表述为一项回归任务，其输出是真实性的数字分数。Nakashole和Mitchell（2014）采用了这种方法。通常情况下，评估是通过计算预测分数和地面真实分数之间的差异或使用Pearson/Spearman Correlations来完成。然而，由于可用的数据集有离散的地面真实分数，这里的挑战是如何将离散的标签转换成数字分数。</p><h2 id="4-Datasets"><a href="#4-Datasets" class="headerlink" title="4. Datasets"></a>4. Datasets</h2><p>数据集地址集合：</p><p> <a href="https://www.sohu.com/a/377489976_787107">https://www.sohu.com/a/377489976_787107</a></p><p><a href="https://www.zhihu.com/question/264356019/answer/1327236489">https://www.zhihu.com/question/264356019/answer/1327236489</a></p><p><img src="https://i.loli.net/2021/11/03/bJIfvLDMoyYBFsc.png" alt=""></p><p>自动假新闻检测的一个重要挑战是数据集的可用性和质量。我们将公共假新闻数据集分为三类：</p><ul><li>claims :  是一个或几个句子，包括值得验证的信息（表2中有一个样本）</li><li>整篇文章 : 是由许多相互关联的句子组成，构成信息的整体。</li><li>社交网络服务（SNS）数据，在长度上与 claims 相似，但以账户和帖子的结构化数据为特征，包括大量的非文本数据。</li></ul><h3 id="4-1-Claims"><a href="#4-1-Claims" class="headerlink" title="4.1. Claims"></a>4.1. Claims</h3><p>POLITIFACT、CHANNEL4.COM、 SNOPES 是三个来源的新闻中的人工标注的短文，这些短文是人工收集和标注的。编辑们从各种场合，如辩论、竞选、Facebook、Twitter、采访、广告等，精心挑选了这些说法。许多数据集是基于这些网站创建的。</p><p>Vlachos和Riedel（2014）发布了第一个公开的假新闻检测数据集，收集了来自POLITIFACT和CHANNEL4.COM的数据。这个数据集有221条声明，其中有制作日期、说话人和URL，以及五分制的真实性标签。EMERGENT（Ferreira和Vlachos，2016）也是早期的声称-验证数据集的工作。它是在事实核查的背景下进行立场分类，包括带有一些支持或反对文本的主张。这个数据集可以改善事实核查，条件是提供一些与 Claims 有关的文章。</p><p>Vlachos只包括221项索赔，Emergent只包括300项索赔，因此将其用于基于机器学习的评估是不切实际的。这些天来，有许多索赔的数据集被公布，它们可以作为前两者的改进版使用。</p><p>最近一个用于假新闻检测的基准数据集是LIAR（Wang，2017）。这个数据集与Vlachos和Riedel（2014）一样从Politifact收集数据，但包括12,836个真实世界的短文，每个声明都被标记为六级真实性。该数据集中还包括关于主题、政党、背景和发言人的信息。对于来自Politifact文章的数据集，Rashkin等人（2017）也发表了大型数据集。他们也收集了来自PunditFact（Politifact的衍生网站）的文章。</p><p>Fever 是一个为事实核查提供相关证据的数据集。在这一点上，它与EMERGENT相似。Fever包含185,445个由维基百科数据生成的说法。每个声明都被标记为支持、反驳或信息不足。他们还标注了他们使用维基百科中的哪些感性内容作为证据。Fever使我们有可能开发出一个能够与证据一起预测主张的真实性的系统，尽管来自维基百科的事实和证据的类型可能仍然表现出与现实世界的政治运动的一些主要风格差异。</p><h3 id="4-2-Entire-Article-Datasets"><a href="#4-2-Entire-Article-Datasets" class="headerlink" title="4.2. Entire-Article Datasets"></a>4.2. Entire-Article Datasets</h3><p>有几个假新闻检测的数据集可以预测———整个文章是真的还是假的。例如，FAKENEWSNET（Shu等人，2017a；Shu等人，2017b；Shu等人，2018）是一个正在进行的假新闻研究的数据收集项目。它包括基于BuzzFeed和PolitiFact的假新闻文章的标题和正文。它还收集了来自Twitter的这些文章的社会参与信息。<br>BS DETECTOR4是从一个名为BS Detector的浏览器扩展中收集的，表明其标签是BS Detector的结果，而不是人类注释者。BS Detec- tor通过检查人工编制的不可靠域名列表，搜索有问题的网页上的所有链接，以寻找不可靠来源的参考。</p><h3 id="4-3-Posts-On-Social-Networking-Services"><a href="#4-3-Posts-On-Social-Networking-Services" class="headerlink" title="4.3. Posts On Social Networking Services"></a>4.3. Posts On Social Networking Services</h3><p>BUZZFEEDNEWS收集了9家新闻机构在Facebook上的2282个帖子。每个帖子都由5名BuzzFeed记者进行事实核查。这个数据集的优势在于，文章是从左倾和右倾组织的两边收集的。BUZZFEEDNEWS有两个丰富的版本。Potthast等人（2017）通过添加链接文章等数据对其进行了丰富，而BUZZFACE（Santia和Williams，2018）则通过Facebook上与新闻文章相关的160万条评论来扩展BuzzFeed数据集。</p><p>SOME-LIKE-IT-HOAX（Tacchini等人，2017）由32个Facebook页面的15500个帖子组成，也就是组织的公开资料（14个阴谋论和18个科学组织）。这个数据集是根据发布者的身份而不是帖子级别的注释来标注的。这种数据集的一个潜在隐患是，这种标签策略可能导致模型学习每个发布者的特征，而不是假新闻的特征。</p><p>PHEME（Zubiaga等人，2016）和CREDBANK(Mitra and Gilbert, 2015)是两篇文章。PHEME包含9个有新闻价值的事件的330条twitter线程（一个人的一系列连接tweet），标记为真或假。CREDBANK包含覆盖96天的6000万条推文，被分组为1049个事件，有一个30维的真实性标签向量。每个事件都由30名人类注释者以5分的李克特量表对其真实性进行评分。他们将30个评分串联起来作为一个向量，因为他们发现很难将其简化为一个一维的分数。</p><p>如上所述，这些数据集是为验证推文的真实性而创建的。因此，它们只限于少数主题，并且可能包括与新闻没有关系的推文。因此，这两个数据集对于假新闻的检测并不理想，它们更多地被用于谣言检测。</p><h2 id="5-Methods"><a href="#5-Methods" class="headerlink" title="5. Methods"></a>5. Methods</h2><p>我们介绍假新闻的检测方法。像往常一样，我们首先将输入文本预处理成合适的形式（5.1.）。如果数据集有整个文章的长度，可以使用修辞学方法作为手工制作的特征提取之一（5.3.）。如果数据集有EMERGENT或FEVER这样的证据，我们可以使用5.4.中的方法来收集输出的证据。</p><h3 id="5-1-Preprocessing"><a href="#5-1-Preprocessing" class="headerlink" title="5.1. Preprocessing"></a>5.1. Preprocessing</h3><p>预处理通常包括标记化、词干化和概括化或加权词。为了将标记化的文本转换为特征，经常使用术语频率-反向文档频率（TF-IDF）和语言学查询和单词计数（LIWC）。对于单词序列，通常使用预先学习的单词嵌入向量，如word2vec（Mikolov等人，2013）和GloVe（Pennington等人，2014）。</p><p>当使用整个文章作为输入时，一个额外的预处理步骤是从原始文本中识别中心主张。Thorne等人（2018）使用TF- IDF和DrQA系统（Chen等人，2017）对句子进行排名。这些操作与子任务密切相关，如单词嵌入、命名实体识别、消歧义或核心参考解析。</p><h3 id="5-2-Machine-Learning-Models"><a href="#5-2-Machine-Learning-Models" class="headerlink" title="5.2. Machine Learning Models"></a>5.2. Machine Learning Models</h3><p>如第3节所述，现有的研究大多使用监督方法，而半监督或无监督的方法则较少使用。在本节中，我们主要通过几个实际的例子来描述分类模型。</p><h4 id="5-2-1-Non-Neural-Network-Models"><a href="#5-2-1-Non-Neural-Network-Models" class="headerlink" title="5.2.1. Non-Neural Network Models"></a>5.2.1. Non-Neural Network Models</h4><p>Support Vector Machine (SVM) 和 Naive Bayes Clas- sifier (NBC) 是经常使用的分类模型（Conroy等人，2015；Khurana和Intelligentie，2017；Shu等人，2018）。这两种模型在结构上有很大不同，它们通常都被用作基线模型。Logistic回归（LR）（Khurana和Intelligentie，2017；Bhattacharjee等人，2017）和决策树，如Ran- dom Forest Classifier（RFC）（Hassan等人，2017）也被偶尔使用。</p><h4 id="5-2-2-Neural-Network-Models"><a href="#5-2-2-Neural-Network-Models" class="headerlink" title="5.2.2. Neural Network Models"></a>5.2.2. Neural Network Models</h4><p>循环神经网络（RNN）在自然语言处理中非常流行，特别是长短时记忆（LSTM），它解决了梯度消失的问题，因此它可以捕获较长期的依赖关系。在第6节中，许多基于LSTM的模型在LIAR和FEVER上都有很高的准确性。此外，Rashkin等人（2017）建立了两个LSTM模型，将文本作为简单的词嵌入输入到一边，并作为LIWC特征向量输入到另一边。在这两种情况下，它们都比NBC和MaxEntropy(MaxEnt)模型更准确，尽管只是轻微的。</p><p>卷积神经网络（CNN）也被广泛使用，因为它们在许多文本分类任务中都很成功。Wang（2017）使用了一个基于Kim的CNN（Kim，2014）的模型，将最大池的文本代表与双向LSTM的元数据代表连接起来。CNN也被用于提取具有各种元数据的特征。例如，Deligiannis等人（2018）将新闻和出版商之间的关系图样数据作为CNN的输入，并用它们评估新闻。</p><p>Karimi等人（2018）提出了多源多类假新闻检测框架（MMFD），其中CNN分析索赔中每个文本的局部模式，LSTM分析整个文本的时间依赖性，然后通过全连接网络传递所有最后的隐藏输出的连接。 这个模型利用了两种模型的特点，因为LSTM对长句子的效果更好。</p><p>注意力机制经常被纳入神经网络以获得更好的性能。Long等人（2017）使用了一个注意力模型，该模型结合了说话人的名字和语句的主题，首先关注特征，然后将加权向量送入LSTM。这样做使准确率提高了约3%（表3）。Kirilin和Strube（2018）使用了一个非常类似的注意机制。Pham（2018）使用了记忆网络，它是一种基于注意力的神经网络，也分享了注意力机制的想法。</p><h3 id="5-3-Rhetorical-Approach"><a href="#5-3-Rhetorical-Approach" class="headerlink" title="5.3. Rhetorical Approach"></a>5.3. Rhetorical Approach</h3><p>修辞结构理论（RST），有时与矢量空间模型（VSM）相结合，也被用于假新闻检测（Rubin等人，2015b；Della Vedova等人，2018；Shu等人，2017b）。RST是一个故事连贯性的分析框架。通过定义文本单元的语义作用（例如，一个句子代表环境、证据和目的），这个框架可以系统地识别基本思想，并分析输入文本的特点。然后根据其连贯性和结构来识别假新闻。为了用RST解释结果，VSM被用来将新闻文本转换成向量，在高维RST空间中与真新闻和假新闻的中心进行比较。向量空间的每个维度表示新闻文本中修辞关系的数量。</p><h3 id="5-4-Collecting-Evidence"><a href="#5-4-Collecting-Evidence" class="headerlink" title="5.4. Collecting Evidence"></a>5.4. Collecting Evidence</h3><p>基于RTE（识别文本蕴涵）（Dagan等人，2010）的方法经常被用来收集和利用证据。RTE是识别句子之间关系的任务。通过使用RTE方法从数据源（如新闻文章）收集支持或反对输入的句子，我们可以预测输入是否正确。基于RTE的模型需要文本证据进行事实核查，因此这种方法只有在数据集包括证据时才能使用，如FEVER和Emergent。</p><hr><h1 id="The-Future-of-False-Information-Detection-on-Social-Media-New-Perspectives-and-Trends"><a href="#The-Future-of-False-Information-Detection-on-Social-Media-New-Perspectives-and-Trends" class="headerlink" title="The Future of False Information Detection on Social Media: New Perspectives and Trends"></a>The Future of False Information Detection on Social Media: New Perspectives and Trends</h1><p>社交媒体上虚假信息的大量传播已经成为一种全球性的风险，隐性地影响着公众舆论，威胁着社会/政治发展。因此，虚假信息检测（FID）已成为近年来风起云涌的研究课题。作为一个前景广阔、发展迅速的研究领域，我们发现很多人已经为FID的新研究问题和方法付出了努力。因此，有必要对FID的新研究趋势做一个全面的回顾。我们首先简要回顾了FID的文献历史，在此基础上，我们提出了几个新的研究挑战和技术，包括<strong>早期检测、多模态数据融合检测和解释式检测</strong>。我们进一步研究了FID中各种人群智能的提取和使用，这为解决FID的挑战铺平了道路。最后，我们对FID的开放性问题和未来的研究方向提出了自己的看法，如<strong>模型对新事件的适应性/通用性、对新型机器学习模型的接纳、人群智慧的聚合、检测模型中的对抗性攻击和防御等等</strong>。                         </p><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><blockquote><p>社会化媒体平台（如Twitter1、Facebook2、新浪微博3）彻底改变了信息的传播模式，大大提高了信息传播的速度、数量和种类。然而，社交媒体为事实和虚假信息的快速传播提供了便利。根据奈特基金会最近的一项调查4，美国人估计，他们在社交媒体上看到的新闻有65%是假新闻。此外，虚假信息通常在社交网络中传播得更快、更深、更广。</p></blockquote><p>利用社交媒体传播误导性信息的敌对行为构成了一种政治威胁[8]。例如，在2016年美国总统大选期间，有多达529种不同的低可信度言论在推特上传播[73]，约有1900万个恶意机器人账户发布或转发了支持特朗普或克林顿的帖子，这有可能影响了选举。2018年，《科学》杂志发表了关于 “假新闻 “的主题期刊，他们报道说，假 statements 声明可以引起人们的恐惧和惊讶的感觉，这有助于社会恐慌。例如，一段名为索马里人 “被推入浅坟 “埃塞俄比亚的虚假视频，引起了埃塞俄比亚两个种族之间的暴力冲突；一条网上的虚假信息，暗示希腊已经取消了转机限制，导致希腊警察与移民发生了冲突。上述例子表明，虚假信息的泛滥对社会信息传播的生态构成了严重威胁[91]。社交媒体用户每天都会接触到大量关于各种主题的信息。对用户来说，判断每条信息的可信度是不现实的，也是不可行的[140]。因此，检测社交媒体上的虚假信息是非常迫切的。</p><p>随着新媒体时代的到来，多模态的社交媒体帖子已经逐渐成为社交媒体的主流。因此，随着人工智能（AI）的快速发展，未来的网络虚假信息将超越文字，大规模地扩展到高质量和可操控的信息材料，如图像、视频和音频[8]。例如，DeepFakes[44, 56]利用深度学习模型创建了真实人物的音频和视频，说和做他们从未说过或做过的事情，这使得虚假信息越来越逼真，越来越难以辨别。虽然自动虚假信息检测不是一个新现象，但目前它已经引起了越来越多的公众关注。</p><p>为了便于理解和解释网络和社交媒体上的虚假信息，Kumar等人[89]根据其意图和知识对虚假信息进行总结和分类。</p><p><img src="https://i.loli.net/2021/11/03/kq1zrQMWtJ9jx2U.png" alt=""></p><p>按照意图，false information 可以分为错误信息 misinformation 和虚假信息 disinformation，错误信息是指在事件演变过程中产生的虚假信息，或者是在知识更新过程中产生的虚假信息，没有误导的目的[87，150]；虚假信息是指为了某种目的而故意误导他人的虚假信息[36，166]。根据知识，虚假信息可以被认为是基于意见的，它表达了用户的主观意见，描述了一些没有独特基础真相的情况，而基于事实的，是捏造或违背绝对基础真相的信息[172]。此外，相关文献中还有一些类似的术语，<strong>如谣言、假新闻。谣言一词通常指的是在发布时未经核实的信息</strong>[204]。<strong>因此，谣言可能会被证明是真的或假的。与谣言不同的是，假新闻一词被广泛用于指那些故意的、可验证的虚假新闻文章</strong>[162]。我们根据其意图对这些术语进行分类，如图1所示。尽管上述术语之间有区别，但它们都涉及到虚假信息的传播，并有能力或意图影响一些用户。因此，本调查坚持这些术语的定义，并从技术角度回顾了社交媒体上虚假信息检测（FID）的发展。</p><blockquote><p>近年来，在FID方面有很多努力。根据现有FID方法中使用的特征类型，我们将其分为<strong>四类：基于内容的方法、基于社会环境的方法、基于特征融合的方法和基于深度学习的方法</strong>。<strong>基于内容的检测方法</strong>主要利用从社交帖子中提取的文本或视觉特征进行二元分类（真实或虚假）。<strong>基于社会环境的方法</strong>一般依赖于丰富的用户之间的互动特征，如评论、转贴、关注等。<strong>基于特征融合的方法</strong>综合利用了内容特征和社会环境特征。此外，<strong>基于深度学习的方法</strong>主要通过神经网络学习信息的潜在深度表示。</p></blockquote><p>尽管过去几年对FID做了很多研究，但仍有许多遗留问题需要解决。<strong>首先，现有的FID方法大多利用内容或传播特征，并且通常在虚假信息的整个生命周期中工作良好，这可能导致早期检测的性能不佳。</strong>由于虚假信息可能在短短几分钟内产生严重影响，因此在早期阶段检测它们是至关重要的。第二，随着多模态帖子在社交网络上传播的增加，传统的基于文本的检测方法已不再可行，在更复杂的情况下，利用图像或视频进行FID是有益的。第三，目前的检测方法只给出了声明是否为假的最终结果，但缺乏做出决定的理由。对于揭穿不准确的信息并防止其进一步传播，给出一个令人信服的解释是非常重要的。</p><p>本文旨在深入调查与FID方法有关的最新发展。目前已经有一些关于FID的调查[39, 162, 201, 204]。Zhou等人[201]从<strong>基于知识、基于风格、基于传播和基于可信度</strong>等四个角度研究假新闻，并总结了心理学和社会科学的相关检测方法。Zubiaga等人[204]专注于谣言分类系统，研究了现有的识别疑似谣言、收集谣言相关帖子、检测帖子立场和评估目标事件可信度的方法。同样，Fernandez等人[39]将错误<strong>信息检测分为四个阶段：错误信息识别、传播、验证和驳斥。</strong>他们相应地组织了现有的在线错误信息检测系统。Shu等人[162]从数据挖掘的角度将检测模型分为基于新闻内容的模型和基于社会背景的模型，并总结了虚假新闻检测算法的评估测量方法。我们的调查与其他相关调查的区别如下：</p><ul><li>上述调查对基于深度学习的虚假信息检测方法关注甚少。然而，在过去的三年里，深度学习模型已经被广泛地应用于FID。为了给检测方法提供一个最新的全面调查，我们调查并交叉比较了最近基于深度学习的方法。</li><li>本文回顾了近年来在FID领域出现的新问题和新技术，如早期检测、多模态数据融合检测和解释式检测等。此外，我们的论文从人群智能的角度调查了这些新问题和有前途的工作，研究了利用人群智能促进FID的潜力。</li><li>人工智能的发展提高了FID模型的性能，因此数据集已经变得和算法一样重要。本文为未来的研究人员梳理了自2015年以来广泛使用的开放数据集，这些数据集被现有的调查所忽视。</li></ul><p>与现有研究大多使用帖子内容不同，基于人群智能的方法旨在检测基于聚合的用户意见、猜想和证据的虚假信息，这是人类与帖子互动过程中注入的隐性知识（如帖子的发布、评论和转贴）。最重要的是，我们工作的主要贡献包括：</p><ul><li>基于对FID的简要文献回顾，我们集中讨论了它的最新研究趋势，包括对新事件的模型通用性、早期检测、基于多模态融合的检测和解释式检测。</li><li>我们对基于人群智能的FID方法进行了调查，包括FID中人群智能的范围，基于人群智能的检测模型，以及人机混合融合模型。</li><li>我们进一步讨论了FID的开放性问题和有前途的研究方向，如模型对新事件的适应性/通用性，拥抱新型机器学习模型，以及FID模型中的对抗性攻击和防御。</li></ul><p>本文的其余部分组织如下。我们在第2节中对现有的FID工作进行了简要的文献回顾。然后，我们在第3节调查了FID的几个新的研究趋势。在第4节中，我们强调了基于人群智能的检测，然后在第5节中介绍了FID的开放问题和未来方向。最后，我们在第6节中总结了本文。</p><h2 id="2-A-BRIEF-LITERATURE-REVIEW"><a href="#2-A-BRIEF-LITERATURE-REVIEW" class="headerlink" title="2 A BRIEF LITERATURE REVIEW"></a>2 A BRIEF LITERATURE REVIEW</h2><p>本调查主要关注检测在社交网络上传播的虚假或不准确的说法，因此我们首先给出虚假信息检测问题的一般定义。</p><ul><li>对于一个具体的声明 $s$ ，它包含一组相关的 $n$ 个帖子 $P={p_1, p_2, …, p_n }$ 和一组相关的用户 $U={u_1,u_2,…,u_m}$.每个 $p_i$ 由一系列代表帖子的属性组成，包括文字、图片、评论数量等。每个 $u_i$ 由一系列描述用户的属性组成，包括姓名、注册时间、职业等。</li><li>让 $E = {e1,e2,…,en}$ 指的是 $m$ 个用户和 $n$ 个帖子之间的互动。每个 $e_i$ 被定义为 $e_i = {p_i,u_j,a,t}$，代表一个用户 $u_j$ 在时间 $t$ 通过行动 $a$ （发帖、转帖或评论）与帖子 $p_i$互动。</li></ul><p>定义2.1。false information错误信息检测：给定具有帖子集 $P$、用户集 $U$ 和参与集 $E$ 的语句 $s$，错误信息检测任务是学习预测函数 $F(S)\to {0，1}$</p><p>在下文中，我们对现有的FID技术进行了简要的文献回顾，分为四大类型，即基于内容、基于社会环境、基于特征融合和基于深度学习的方法，如表1（前三种类型）和表2（最后一种类型）所总结的。此外，我们还对现有的几个在线FID工具进行了总结，这些工具对于减轻虚假信息的影响和防止其进一步传播具有重要意义。</p><h3 id="2-1-Content-based-Methods"><a href="#2-1-Content-based-Methods" class="headerlink" title="2.1 Content-based Methods"></a>2.1 Content-based Methods</h3><p>对于一个具体的事件，其微博一般是由一段文字来描述，往往与几张图片或视频相关。基于内容的方法主要是基于特定的写作风格或虚假文章的耸人听闻的标题，如词汇特征、句法特征和主题特征[143]。例如，Castillo等人[16, 17]发现，高可信度的推文有更多的URL，而且文本内容长度通常比低可信度的推文长。</p><p>许多研究利用词法和句法特征来检测虚假信息。例如，Qazvinian等人[136]发现，语篇（POS）是FID的一个可区分的特征。Kwon等人[90]发现一些类型的情感是机器学习分类器的明显特征，包括积极的情感词（如爱、好、甜）、否定词（如不、不、永不）、认知行动词（如原因、知道）和推断行动词（如可能、也许）。然后，他们提出了一个周期性的时间序列模型来识别真实推文和虚假推文之间的关键语言差异。此外，Pérez-Rosas等人[128]总结了真实和虚假内容的语言学特征的差异，可以分为五类。”Ngrams”、”标点符号” “心理语言学特征”、”可读性 “和 “句法”。基于上述特征，使用线性SVM来识别虚假信息。Rashkin等人[141]总结了不可信的新闻内容的语言风格。具体来说，他们发现第一/第二人称代词在低可信度信息中使用的频率更高，夸张的词汇也是如此。</p><p>词汇特征有时不能完全反映虚假信息的特征，因为它的位置性。因此，许多研究为FID引入了语义特征，如话题、情感和写作风格。例如，Potthast等人[134]利用不同的写作风格来检测虚假声明。同样地，Horne等人根据假新闻文章在标题风格上与真实新闻文章有很大不同的观察，提出了一个FID模型。Hu等人[68]提出了一个利用情感信息检测低可信度社交帖子的框架。Ito等人[70]将Latent Dirichlet Allocation（LDA）主题模型引入到推文可信度的评估中，他们提出了推文主题特征和用户主题特征，用于检测虚假信息。</p><h3 id="2-2-Social-Context-based-Methods"><a href="#2-2-Social-Context-based-Methods" class="headerlink" title="2.2 Social Context-based Methods"></a>2.2 Social Context-based Methods</h3><p>传统的基于内容的方法是孤立地分析单个微博或主张的可信度，忽略了不同微博和事件之间的高度关联性。此外，大量的人类内容互动数据（发帖、评论、转帖、评级和标签等）为FID提供了丰富的参考信息。具体来说，基于社会环境的方法可以进一步分为基于帖子和基于传播的方法。</p><p>(1) Post-based features</p><p>基于帖子的方法主要依靠用户的帖子来表达他们对特定事件的情绪或意见。许多研究通过分析用户的可信度[95, 118]或立场[63, 116]来检测虚假信息。例如，Shu等人[164]从用户档案中探索出对FID真正有用的特征，从而减少检测过程中特征提取的负担。具体来说，他们发现性格外向和随和的用户不太可能受到虚假信息的影响。此外，Guess等人[57]指出，保守派更倾向于在Facebook中分享虚假帖子。Long等人[104]发现，在基于内容的检测方法中应用用户档案（如党派、验证信息和位置）可以提高其在FID上的表现。他们提出了一个混合检测模型，分别提取新闻内容的主题特征和用户属性特征。此外，Tacchini等人[170]发现，有不准确信息的社交帖子通常比真实的事实有更多的赞。因此，他们使用逻辑回归（LR）模型和众包算法，在用户喜欢的基础上检测虚假信息。</p><p>(2) Propagation-based features</p><p>基于传播的方法将帖子和事件的可信度作为一个整体进行评估[14]，这些方法通常关注信息传播网络的构建和可信度的传播。<br>一些研究通过分析其传播模式来检测虚假信息。例如，Ma等人[107]发现，社会环境的特征会随着时间的推移而逐渐改变。因此，他们提出了一个DSTS模型来描述FID的社会背景特征的时间模式，该模型将信息传播序列划分为固定长度的片段，然后从每段帖子中提取基于内容和社会背景的特征，最后用SVM进行分类。Liu等人[102]构建了基于异质用户特定属性的信息传播网络，用于识别虚假信息的特殊传播结构。Kim等人[79]提出了一个贝叶斯非参数模型来描述新闻文章的传播特征，该模型联合利用文章的主题和用户兴趣来进行FID。此外，Wu等人[186]观察到虚假信息通常先由普通用户发布，然后由一些意见领袖转发，最后由大量的普通用户传播。然而，真相往往是由一些意见领袖发布，然后由大量用户直接传播。基于这一观察，他们提出了一个用于FID的混合SVM分类器，该分类器对信息传播结构、主题信息、用户属性等共同建模。<br>此外，许多研究还通过构建特定的树状或网络结构来检测虚假信息。例如，Ma等人[108]将谣言相关的微博传播建模为传播树，他们提出了一种基于内核的方法来捕捉这些传播树之间的模式，以实现FID。此外，Gupta等人[62]构建了一个包含用户、帖子和事件的可信度传播网络来模拟虚假信息的传播过程。Jin等人[74]提出了一个连接微博、子事件和事件的三层可信度传播网络，用于信息可信度验证。</p><h3 id="2-3-Feature-Fusion-based-Methods"><a href="#2-3-Feature-Fusion-based-Methods" class="headerlink" title="2.3 Feature Fusion-based Methods"></a>2.3 Feature Fusion-based Methods</h3><p>基于内容的检测方法主要从写作风格、词汇和句法特征方面来识别真实和非真实的主张之间的差异，而基于社会背景的检测方法主要利用从信息传播过程中提取的特征。由于两类方法应用的特征可以互补[145]，最近许多研究者开始研究基于特征融合的新方法。例如，Vedova等人[30]利用了用户和帖子之间的互动信息，以及帖子的文本信息。具体来说，他们对社交帖子进行词干分析，并将每个帖子表示为单词的TF-IDF向量。之后，他们利用用户的喜欢行为来描述社会背景特征，与Tacchini等人的工作类似[170]，最后通过整合这两种信号来识别虚假信息。为了利用传统的内容特征（如词汇或句法特征），Volkova等人[176]将来自新闻内容的心理语言学信号和来自社会环境的作者观点作为FID中不同分类器的输入数据。此外，Shu等人[165]进一步探讨了出版商、新闻作品和用户之间的社会关系。他们提出了一个名为TriFN的通用检测框架，通过非负矩阵分解（NMF）算法对新闻内容、社会互动和新闻发布者之间的内在关系进行建模，用于识别低可信度信息。</p><h3 id="2-4-Deep-Learning-based-Methods"><a href="#2-4-Deep-Learning-based-Methods" class="headerlink" title="2.4 Deep Learning-based Methods"></a>2.4 Deep Learning-based Methods</h3><p>基于深度学习的方法旨在自动抽象出虚假信息数据的高层表示。目前，大多数工作主要利用递归神经网络[106]和卷积神经网络[195]进行FID，如表2所示。在下文中，我们首先总结了广泛使用的深度学习模型，主要包括。</p><p><img src="https://i.loli.net/2021/11/03/stFAr7x1CKzyXME.png" alt=""></p><ul><li>卷积神经网络（CNN）。CNN是典型的前馈神经网络之一，有三种层，即卷积层、池化层和全连接层[135]。在卷积层中，多个滤波器（核）与输入向量卷积，生成特征图。之后，池化层降低特征图的维度，以加速网络的训练过程。通过多次卷积和池化操作，CNN可以从输入中捕获局部和全局特征。最后，CNN通过全连接层（如Softmax）输出分类结果。可以看出，FID模型可以通过调整过滤器的大小来捕捉词与词、短语与短语之间的内容特征。</li><li>图卷积网络（Graph Convolutional Network，GCN）。GCN是一种处理图数据的神经网络，由卷积层和全连接层组成，可以有效捕捉图的结构特征[29, 83]。每个卷积层的隐藏状态矩阵由一个特殊矩阵的非线性变化得到，该矩阵是该图的相邻矩阵与其上一层的隐藏状态矩阵和权重矩阵的乘积。</li><li>递归神经网络（RNN）。RNN可以有效地捕捉连续数据的特征，通过同一隐含层的神经元之间的信息传输，节省了以前的计算。社交网络帖子显然具有时间性特征，因此FID模型可以将帖子的交互数据划分为连续的片段，并通过RNN捕获其顺序性特征。然而，Glorot等人[49]发现，RNN可能存在梯度消失的问题，这使得它不具备长期记忆。因此，长短时记忆（LSTM）[65]和门控递归单元（GRU）[25]，一种具有门控机制的RNN，被广泛用于NLP中。LSTM增加了一个存储单元来存储当前网络状态，然后通过输入门、遗忘门和输出门的协调来控制信息流。虽然GRU没有引入额外的存储单元，但它可以通过一个复位门和一个更新门控制当前的存储。</li><li>递归神经网络（RvNN）。RvNN与RNN类似，它将数据结构展开，可用于分析数据的分层结构[135]，如语法分析树。该模型由根节点、左叶节点和右叶节点组成。此外，每个节点都从直接的左、右子节点中学习它的表示方法，该方法是递归计算的，直到所有节点都被遍历。</li><li>自动编码器（AE）。AE是一个无监督的学习模型，包括编码和解码阶段[64]。在编码阶段，输入数据通过多个隐藏层转化为潜伏向量，在解码阶段将其重构为原始数据。通过最小化重建误差，AE尽可能地学习了输入的表示。与AE相比，变异自动编码器（VAE）约束了编码阶段，成为一个生成模型[82]。编码阶段的隐藏层通过从特定的分布（如高斯分布）中取样来学习潜变量，然后将其输入到解码阶段以生成现实的样本。</li><li>生成式对抗网络（GAN）。GAN是一种生成性神经网络，由生成器和鉴别器组成[51]。在反向传播的迭代过程中，鉴别器区分其输入是来自真实的数据集还是由生成器生成的虚假样本，而生成器则根据数据集的抽样分布生成真实的样本以混淆鉴别器。他们最终实现了纳什均衡，也就是说，生成器和鉴别器的性能不能再有任何提高。</li><li>注意机制。注意机制通常被用来描述神经网络对输入序列的注意分布[7]。它计算当前输入序列和输出向量之间的匹配度，目的是捕捉输入的关键信息。匹配度越高，注意力分数就越高。因此，检测方法可以利用注意力机制来找到这些对FID贡献较大的词或短语。</li></ul><p>许多现有的研究利用深度神经网络，通过将相关帖子建模为时间序列数据来学习虚假信息的潜在文本表征。例如，Ma等人[106]提出了一个基于RNN的检测模型，该模型捕获了连续的用户评论流的时间-语言特征。Li等人[94]认为帖子流的前向和后向序列都传达了丰富的交互信息，因此他们提出了用于FID的双向GRU方法。Liu等人[101]认为假新闻和真新闻的传播模式存在差异，他们利用CNN和GRU对传播路径进行分类，以识别低可信度信息。Yu等人[194]认为帖子的时间序列特征有助于对事件进行准确建模，他们提出了FID的ACAMI模型。该模型使用event2vec（建议学习事件相关表征）和注意力机制来提取事件的时间和语义表征，然后使用CNN来提取高层次特征，用于对虚假微博帖子进行分类。</p><p>一些方法将文本信息和社会背景信息（如用户回应、用户或网站简介）结合起来作为深度神经网络的数据输入。例如，Guo等人[60]提出了一个分层的神经网络，将用户、帖子和传播网络的信息作为数据输入。此外，他们利用注意力机制来估计FID中特征的不同贡献。Ruchansky等人[144]的工作提出了一个基于RNN的检测模型，该模型结合了新闻内容、用户反应和源用户的特征来促进FID的性能。Ma等人[111]提出了一个基于GAN的检测模型，其目的是捕捉低频但有效的假推文迹象。生成器（基于GRU的seq2seq模型）试图生成有争议的观点，使推文的观点分布更加复杂，而判别器（基于RNN）试图从增强的样本中识别虚假信息的稳健特征。</p><p>也有一些使用图神经网络进行FID的工作，如GCNs。他们通常利用神经网络来分析社交帖子的传播结构，然后为分类器提取信息传播模式的高级表示。例如，Monti等人[117]提出了一个基于GCN的FID模型，它整合了推文内容、传播结构、用户资料和用户社会关系（关注和被关注）。考虑到原始推文和所有相关推文，即评论和转发，检测模型将每条推文作为节点，推文传播路径和用户关系作为边，建立一个特定事件图。之后，他们使用GCN来识别那些低可信度的推文，其中包含两个卷积层和两个全连接层。此外，Dong等人[33]提出了一个基于GCN的检测模型，名为GCNSI，它利用图卷积网络来检测多个虚假信息源。</p><h2 id="3-NEW-TRENDS-IN-FALSE-INFORMATION-DETECTION"><a href="#3-NEW-TRENDS-IN-FALSE-INFORMATION-DETECTION" class="headerlink" title="3 NEW TRENDS IN FALSE INFORMATION DETECTION"></a>3 NEW TRENDS IN FALSE INFORMATION DETECTION</h2><p>在回顾了关于FID的传统研究后，本节调查了这一领域的几个新的研究趋势，包括早期检测、通过多模态数据融合检测和解释性检测。</p><h3 id="3-1-Early-Detection"><a href="#3-1-Early-Detection" class="headerlink" title="3.1 Early Detection"></a>3.1 Early Detection</h3><p>虚假信息很容易被社交网络上的大量用户传播，在很短的时间内造成严重影响[14, 46]。因此，对虚假信息的早期检测成为一个重要的研究课题。然而，大多数现有的研究（基于内容和社会背景的方法）通过假设他们拥有所有的生命周期数据来检测虚假信息。他们依赖于几个聚合特征，如内容特征和传播模式，这需要一定数量的帖子来训练强大的分类器。虚假信息开始时的可用数据非常有限，以至于在早期阶段检测它很有挑战性。最近，有一些针对早期FID的努力。</p><p>传统的机器学习方法通常会分析帖子早期传播中的用户交互信息，手动提取大量的特征，最后用分类器（如SVM、随机森林）来评估其中的可信度。例如，Liu等人[100]发现，在少量的数据中，来源的可靠性、用户的多样性和证据信号，如 “我看到 “和 “我听到”，对FID有很大的影响。此外，Qazvinian等人[136]观察到，在推文传播的早期阶段，用户倾向于表达自己的信念（如支持或质疑）。因此，合理利用信息中的用户信念，对早期发现虚假信息大有裨益。为了解决数据缺乏的问题，从相关事件中借用知识进行FID将是另一种有用的方法。例如，Sampson等人[149]提出了一种通过利用隐性链接（如标签链接、网络链接）从相关事件中获取额外信息的突发性FID方法。实验结果表明，当可用的文本或互动数据较少时，这种隐性链接明显有助于正确识别突发的不真实声明。</p><p>许多检测方法利用深度学习模型对虚假信息进行早期检测。基于深度学习的检测方法通常使用神经网络来自动提取社会环境特征，并通过利用注意力机制找到FID的关键特征。例如，Liu等人[99]观察到只有少数帖子对FID有很大贡献。为了选择这些关键内容，他们提出了一个基于注意力的检测模型，该模型通过每个帖子的注意力值来评估其重要性。此外，实验结果表明，正确使用注意力机制有利于早期发现虚假信息。同样，Chen等人[20]发现，在信息传播的不同时期，用户倾向于对不同的内容进行评论（例如，从惊讶到质疑）。基于这一观察，提出了一个基于RNN的深度注意模型，有选择地学习连续帖子的时间性隐藏表征，以实现早期FID。Yu等人[195]利用一个基于CNN的模型从帖子序列中提取关键特征，并学习它们之间的高层次互动，这有利于用相对较少的互动数据识别虚假推文。Nguyen等人[123]也利用CNN来学习每条推文的潜在表征，相应地获得推文的可信度。然后，他们通过汇总事件开始时所有相关推文的预测，来评估目标事件是否是一条虚假信息。更重要的是，Liu等人[101]发现大多数用户在信息传播的早期过程中没有评论就转发源推文，这隐含着利用用户评论进行早期FID的一些延迟。因此，他们提出了一个传播路径分类模型，名为PPC，该模型联合使用CNN和GRU来提取转发路径中用户的局部和全局特征。</p><h3 id="3-2-Detection-by-Multimodal-Data-Fusion"><a href="#3-2-Detection-by-Multimodal-Data-Fusion" class="headerlink" title="3.2 Detection by Multimodal Data Fusion"></a>3.2 Detection by Multimodal Data Fusion</h3><p>传统的FID方法专注于文本内容和传播结构。然而，社交媒体帖子也包含丰富的视觉数据，如图片和视频，而这种多模态数据往往被忽视。图片和视频比纯文本信息更吸引用户，因为它们可以生动地描述目标事件。<br>图像处理的巨大进步，如AE、VAE和GAN（如第2.4节所述），证明了图像可以很容易地被编辑和修改，使假图像的生成更加容易。因此，分析多模态数据之间的关系并开发基于融合的模型可以成为FID的一个有前途的方法[14]。社交媒体上的虚假信息中主要有三种假图像，包括图像篡改、图像不匹配和图像混合。</p><h3 id="3-3-Explanatory-False-Information-Detection"><a href="#3-3-Explanatory-False-Information-Detection" class="headerlink" title="3.3 Explanatory False Information Detection"></a>3.3 Explanatory False Information Detection</h3><p>大多数基于深度学习的FID方法在输出决策结果时，往往不会呈现做出决策的原因，它们利用预先训练好的分类器来识别测试集中的可疑事件[14]。然而，找到支持决策的证据碎片将有利于揭穿虚假信息并防止其进一步扩散。因此，解释型FID已经成为另一个趋势性的研究课题。现有的解释性FID研究主要集中在两个方面：一是探索实用的可解释性检测模型（模型的解释），二是解释其结果（结果的解释）。</p><h4 id="Interpretation-of-models"><a href="#Interpretation-of-models" class="headerlink" title="Interpretation of models"></a>Interpretation of models</h4><p>关于可解释的FID模型的研究主要集中在利用概率图模型（PGM）和知识图（KG）。</p><ul><li>概率图模型（PGM）。PGM使用图来表示相关变量（节点）的联合概率分布，由贝叶斯网络和马尔科夫网络组成，前者使用有向无环图来模拟变量之间的因果关系，后者使用无向图来模拟变量之间的互动[38]。节点的关系可以通过条件独立性来解释。基于概率图的检测模型可以同时描述用户、社交帖子和人际交往内容的特征，并根据显性交互数据近似推断出隐性信息的可信度。此外，广泛使用的近似推理算法是变分推理、信念传播和蒙特卡洛抽样[84]。</li><li>知识图谱（KG）。KGs以图的形式描述现实世界中的实体以及它们之间的关系。具体来说，KG包含各种领域的知识，定义了实体的可能类别和关系，并允许任何实体之间有潜在的关联[127]。此外，还有一些权威的知识库，如Freebase17、Wikidata18、DBpedia19、谷歌的知识图谱20。这些知识库包含了数以百万计的实体和声明，为FID提供了参考。检测方法可以通过知识提取、融合和完成来检查社交媒体帖子的事实。</li></ul><p>具体来说，Shi等人[157]提出了一种基于KG的事实核查方法。它首先通过从知识图中提取类似实体的元路径来分析帖子的语义信息。之后，该方法在收集到的事实状态中挖掘出异质连接模式，用于事实核查。此外，Gad-Elrab等人[47]提出了ExFaKT，为候选事实提供人类可理解的解释，它结合了来自文本内容和知识图谱的语义证据。ExFaKT使用Horn规则（一阶谓词逻辑的一个子集）将目标事实重写为多个易于解释的事实，以便进一步进行FID。Popat等人[131]的工作提出了一个概率模型，将FID的内容感知和趋势感知评估算法统一起来。具体来说，他们对事件相关文章之间的相互作用进行建模，以产生适当的用户可解释的解释，包括语言特征、立场和来源的可靠性。Yang等人[191]提出了一种无监督的FID方法，称为UFD。该方法利用贝叶斯网络来模拟真相和用户意见的完整生成过程。UFD将新闻文章的真实性和用户的声誉视为潜在变量，然后利用用户之间的社交活动来提取他们对新闻可信度的观点。</p><h4 id="Interpretation-of-results"><a href="#Interpretation-of-results" class="headerlink" title="Interpretation of results"></a>Interpretation of results</h4><p>对结果的解释主要是指决策过程的可视化，或对事实的分析。虽然基于深度学习的方法极大地提高了FID的性能，但深度模型的内在机制并不能很好地解释。因此，研究人员利用其他辅助信息进行解释性的FID。</p><p>由于注意机制中的注意程度可以表征输入的每一部分的重要性[19]，几个基于深度学习的检测方法通过注意程度的可视化来解释其分类。例如，Chen等人[20]将他们的模型识别的一些虚假索赔的注意力分布可视化，发现大多数与事件相关的词被赋予的程度低于表达用户怀疑、愤怒和其他情绪的词。Dong等人[32]提出了一个名为DUAL的基于注意力的FID模型，该模型分别使用GRU来提取文本特征和DNN来提取社会环境特征。他们将两个隐藏层的注意力矩阵可视化，有效地描述了识别真假帖子时每个隐藏层的注意力程度分布。同样地，Popat等人[133]使用双向LSTM分别提取源主张和外部相关帖子的特征，然后结合注意力机制来学习虚假信息的表示。他们还将其关注度可视化，显示许多信号词如 “勉强真实”、”证据 “和 “揭示 “被赋予较高的关注度。此外，他们使用主成分分析（PCA）来可视化他们的模型所学习的文本特征向量，发现真实和虚假说法的文本表示可以被适当地分开。</p><p>ClaimVerif[200]是一个在线解释信息可信度评价系统，它将给定主张的立场、观点、来源可信度等因素考虑在内，以提供有效证据。在识别网上的虚假主张时，ClaimVerif使用谷歌搜索抓取相关文章，分析原始信息和转帖信息的文本特征，最后输出源信息的可信度，以及人类可理解的证据。类似地，CredEye[132]通过分析在线相关文章来确定一个给定的说法是否是假的。解释的依据是这些文章的语言风格、立场和来源声誉。此外，Yang等人[190]提出了一个可解释的FID框架，名为XFake，它全面分析了声明的属性（如主题、说话人和背景）、语义特征和语言特征。XFake通过可视化界面显示几个支持性的例子，并以集合树的形式显示推理过程。</p><h2 id="4-CROWD-INTELLIGENCE-BASED-DETECTION"><a href="#4-CROWD-INTELLIGENCE-BASED-DETECTION" class="headerlink" title="4 CROWD INTELLIGENCE-BASED DETECTION"></a>4 CROWD INTELLIGENCE-BASED DETECTION</h2><p>现有的研究表明，帖子的内容特征仍然是FID的首要任务。由于社交帖子是由用户产生、互动和消费的，它将在帖子的编辑、评论和转发中摄入各种人类智能（如意见、立场、质疑、证据提供）。在社交媒体帖子的传播过程中，所谓的人群智慧[58, 96, 185]也会以集体的方式被聚集起来。正如Castillo等人[16]所说，一个有希望的假设是，在社交媒体环境中存在一些内在的信号，有助于评估信息的可信度。Ma等人[110]也发现，Twitter支持基于聚合的用户意见、猜想和证据碎片的虚假信息的 “自我检测”。虽然，如何在FID中利用人群智能仍然是一个开放的问题。在第4节中，我们试图通过提炼和介绍人群智能在FID系统中的几种不同使用形式来解决这个问题，如图2所示。</p><p><img src="https://i.loli.net/2021/11/03/oeDcFSqNtObuY1J.png" alt=""></p><h3 id="4-1-Crowd-Intelligence-in-False-Information"><a href="#4-1-Crowd-Intelligence-in-False-Information" class="headerlink" title="4.1 Crowd Intelligence in False Information"></a>4.1 Crowd Intelligence in False Information</h3><p>在FID中，人群智能是指在信息产生和传播过程中，来自社交媒体用户智慧的聚合线索或社会信号。在本小节中，我们总结了FID中人群智能的含义和使用方式。<br>我们从社会背景、集体知识和集体行为等三个方面来描述人群智能。</p><ul><li>Social contexts. 源用户和传播者之间的社会关系和互动有助于理解信息的确定性。例如，Kim等人[80]认为用户的标记可以间接反映推文的可信度，所以他们使用PGM来生成人与内容的互动过程，推断推文的真实性。Zhao等人[199]发现群众在评论中对真实性的质疑或询问是低可信度信息的指示性信号，他们使用正则表达式从用户评论中提取上述信号进行FID。此外，Wu等人[188]认为类似的话题可能会在类似的人群中传播，所以他们对传播者进行编码，以捕捉他们的社会接近性，从而识别虚假信息。</li><li>Collective knowledge. 群众提供的收集的证据对推断信息的可信度很有用。例如，Lim等人[97]利用用户对目标事件在线证据的支持或反对来检测不准确的言论。Rayana等人[142]认为用户的评分和评论是对帖子可信度的真实评价，所以他们提出了一个名为SpEagle的检测框架，从集体线索和关系数据（信息传播网络）中提取特征。此外，Qian等人[138]提出了一种用于FID的人群知识转移方法，其中利用了历史上真/假说法中的人群反应知识（如背景特征和行为特征）。</li><li>Collective behaviors. 在很多情况下，虽然个人行为不能很好地描述信息的可信度，但一群用户的聚合行为往往能揭示更多信息。这可能是指人群互动模式，行为或意见偏离多数[88]，观点冲突，等等。例如，经常参与低可信度信息的生产和传播的用户有行为偏差，例如，在短时间内发布几个意见，或在一个固定的时间间隔后与内容互动。基于上述观察，Kumar等人[88]通过贝叶斯模型推断出回复者及其评论的可信度。此外，Jin等人[76]发现同一事件下的相关推文包含支持和反对的意见（通过LDA主题模型分析），他们利用这些冲突的观点来建立FID的可信度传播网络。</li></ul><p>在调查了现有的FID研究后，我们提炼出四种不同的人群智能使用方式，如下所示。</p><ul><li>群体学习模型。它主要使用特征工程和代表学习将人群智能融入到FID模型中。</li><li>人群行为建模。它使用图或概率模型对人群行为和互动进行建模，以推断信息的可信度。</li><li>群众的知识转移。学习到的FID模型通常在新事件上不能很好地发挥作用。这种方式解决了如何将人群知识从现有的事件转移到新的事件。</li><li>人机混合模型。考虑到人类智能和机器智能的互补性，这种方式集中于开发用于FID的混合人机模型。</li></ul><p>前面三种方式的一个共同特点是，人群智能是以隐性方式使用的，没有明确的人类输入。具体来说，人群智能被表示为统计学上的人类行为模式，作为学习模型的特征或参数使用。然而，最后一种方式是基于明确的人类输入，例如使用众包进行数据标记。</p><p><img src="https://i.loli.net/2021/11/03/WmdofhkMb9NeV2p.png" alt=""></p><h3 id="4-2-Implicit-Crowd-Intelligence-Models"><a href="#4-2-Implicit-Crowd-Intelligence-Models" class="headerlink" title="4.2 Implicit Crowd Intelligence Models"></a>4.2 Implicit Crowd Intelligence Models</h3><p>在本节中，我们介绍了关于将隐式人群智能用于FID的开创性研究，特别关注4.1节中描述的前三种方式，如表4中总结的那样。</p><p>(1) 群体学习模型。在该模型中，群体智能被表示为训练分类器以检测虚假信息的特征。这已被证明对早期的 FID 很有用。例如，刘等人。 [100] 尝试使用来自 Twitter 数据的人群线索来解决实时虚假索赔揭穿的问题，包括人们的意见、证人账户的统计数据、对事件的聚合信念、网络传播等。赵等人。 [199] 观察到，在决定是否相信此消息之前，有些人愿意质疑或询问 Twitter 中声明的真实性。特别是，他们发现使用探究思维有助于及早发现虚假信息。<br>社会关系和交互也是 FID 特征学习中广泛使用的群体智能。例如，吴等人。 [188] 假设相似的消息通常会导致相似的信息传播轨迹。他们提出了一种社交媒体用户嵌入方法来捕捉社交接近度和社交网络结构的特征，在此基础上利用 LSTM 模型对信息传播路径进行分类并识别其真实性。拉亚娜等人。 [142] 应用集体意见线索和相关数据来检测虚假信息。<br>通过利用发布虚假帖子的用户行为与发布真实事实的用户行为不同的人群情报来识别虚假信息也很有帮助。陈等人。 [22] 提出了一种无监督学习模型，该模型结合了 RNN 和自动编码器，以将低可信度信息与其他真实声明区分开来。此外，谢等人。 [189] 观察到垃圾评论攻击与其评分模式密切相关，这与正常评论者的行为模式不同。因此，他们提出了一种基于其时间行为模式的垃圾评论检测方法，为群体学习模型的 FID 提供了参考。</p><p>(2) 人群行为建模。在这个模型中，集体的人群行为（人群智能的一种类型）被建模为图或概率模型来推断信息的可信度。Hooi等人[66]发现，欺诈性账户经常在短时间内呈现他们的评级（评级分数满足偏斜分布）。群众智慧的特点是贝叶斯推理模型，它可以估计一个用户的行为与相关社区的行为有多大偏差。他们通过测量行为偏差的程度来推断用户评级的可信度。同样，Kumar等人[88]提出了一个贝叶斯检测模型，该模型结合了聚合的人群智慧，如用户的行为属性、评级的可靠性和产品的优良性。通过对异常行为的惩罚，它可以推断出评级平台的信息可信度。<br>一些研究利用聚合的人群行为建模来促进虚假信息的早期检测。例如，Ma等人[110]假设回复者倾向于询问谁支持或否认给定的事件，并表达他们对更多证据的渴望。因此，他们提出了两个树状结构的递归神经网络（RvNN），用于有效的虚假推文表征学习和早期检测，可以对用户回复结构进行建模，并学习捕捉FID的聚合信号。</p><p>(3) 群众的知识转移。<strong>现有的FID模型在新出现的和时间紧迫的事件上仍然表现不佳。换句话说，现有的FID模型通常捕捉到许多与事件相关的特征，而这些特征在其他事件中并不常见。</strong>因此，<strong>有必要学习并将从现有众包数据中获得的共享知识转移到新的事件中</strong>。Wang等人[182]的工作提出了一个利用可转移特征识别新产生的虚假事件的检测模型，名为事件对抗神经网络（EANN ），它包括三个部分，即 “特征提取器”、”事件判别器 “和 “假新闻检测器”。<strong>EANN使用事件判别器来学习与事件无关的共享特征，并在模型训练中减少事件特定特征的影响。</strong><br>群众知识转移模型也有助于早期FID。例如，Qian等人[138]提出了一个生成性条件变异自动编码器，从历史上用户对真实和虚假新闻文章的评论中捕捉用户反应模式。换句话说，当虚假信息传播的早期阶段没有社会互动数据时，人群智能被利用来产生对新文章的反应，以提高模型的检测能力。Wu等人[187]还探讨了历史众包数据中的知识是否能对新出现的虚假社交媒体帖子的检测有所帮助。他们观察到，内容相似的社交帖子往往会导致类似的行为模式（如好奇心、询问）。因此，他们建立了一个稀疏表示模型来选择共享特征并训练与事件无关的分类器。</p><h2 id="5-OPEN-ISSUES-AND-FUTURE-DIRECTIONS"><a href="#5-OPEN-ISSUES-AND-FUTURE-DIRECTIONS" class="headerlink" title="5 OPEN ISSUES AND FUTURE DIRECTIONS"></a>5 OPEN ISSUES AND FUTURE DIRECTIONS</h2><p>尽管研究人员已经为解决FID系统的上述挑战做出了越来越多的努力，但仍有一些开放性的问题需要在未来进行研究，如下所述。</p><h3 id="1-Cognitive-mechanisms-of-false-information"><a href="#1-Cognitive-mechanisms-of-false-information" class="headerlink" title="(1) Cognitive mechanisms of false information."></a>(1) Cognitive mechanisms of false information.</h3><p>人们对虚假信息的认知机制的研究对于虚假社交媒体帖子的检测和反驳具有很好的指导作用[87]，尤其是基于群体智能的检测方法。几部作品对社交媒体平台上的低可信帖子进行了分析，以研究虚假信息能够快速广泛传播的原因。莱万多夫斯基等人。 [92] 认为打击虚假信息需要在技术和心理学的背景下进行科学研究，因此他们提出了一种称为“技术认知”的跨学科解决方案。此外，他们将用户面对虚假信息的认知问题分为影响效应、熟悉度逆火效应、矫枉过正逆火效应和世界观逆火效应四类，为研究用户对虚假信息的感知奠定了基础。 93]。正如 Acerbi [1] 总结的那样，不准确信息的快速传播在于它们包含满足用户认知偏好的特定内容。为了探索虚假信息的认知特征，他们通过将认知偏好编码为“威胁”、“厌恶”、“社交”、“名人”等部分，进一步分析了真假新闻文章的偏好分布。未来一个有价值的研究点是将虚假和真实的信息与具有认知吸引力的特征进行比较，或者评估与认知偏好相关的特征如何促进信息病毒式传播。</p><p>除了在数据分析层面研究认知机制外，我们还可以从人脑认知功能的角度来学习这种机制。神经科学的进步为研究虚假信息的认知机制提供了一个很好的途径。正如Poldrack等人[130]所说，利用脑电图（EEG）、脑磁图（MEG）、功能性磁共振成像（fMRI）和其他脑成像工具可以推动我们了解人脑如何形成社会行为。此外，Adolphs[3]已经确定了参与社会认知调控的神经结构，如扣带皮层、海马体和基底前脑。Arapakis等人[6]利用脑电图记录来测量用户对新闻文章的兴趣，实验结果显示，额叶α不对称性（FFA）可以客观地评价用户对媒体内容的偏好。为了解释信息病毒的机制，Scholz等人[151]提出了一个基于fMRI数据的神经认知框架来评估用户在Facebook上分享信息的意愿。如果我们能够理解虚假信息的认知机制，就可以把更多的精力放在探索揭穿信息最大化的方法上，从而找到针对虚假信息的有力对策。</p><h3 id="2-Lack-of-standard-datasets-and-benchmarks"><a href="#2-Lack-of-standard-datasets-and-benchmarks" class="headerlink" title="(2) Lack of standard datasets and benchmarks."></a>(2) Lack of standard datasets and benchmarks.</h3><p>尽管研究人员在FID方面做了大量的工作，但仍然缺乏像ImageNet[31]这样的视觉对象识别基准数据集。数据集作为一种资源，与FID的算法同样重要。然而，收集虚假信息是一个耗时耗力的过程，这导致了权威基准的缺乏。<br>我们总结了2015年以来的公开数据集，如表6所示，其数据收集自新浪微博（如RUMDECT，Meida_Weibo）、Twitter（如。MediaEval、PHEME、RUMOUREVAL）和其他社交平台，以及snopes.com、politifact.com（例如Emergent、BuzzFeedWebis、LIAR、Declare、FakeNewsNet）和其他事实核查网站。然而，这些数据集的注解方法、数据维度以及真假陈述的比例都不一样，这给研究人员公平评估其模型性能带来了一定的挑战。Shu等人[162]总结了广泛使用的FID的评价指标，现有的评价指标仍然是精度、召回率、F1得分、准确率等机器学习模型评价指标。在FID中，我们需要定义一些更实用的评价指标。例如，在政治选举中，我们会更关注虚假声明是否被更全面地识别出来（即更关注召回率而不是精度），所以用F1得分来评价检测模型的性能并不是很合适。在未来的研究中，需要标准的数据集和实用的评价指标来比较各种FID算法，促进FID方法的发展。</p><h3 id="3-Model-adaptivity-generality-to-new-events"><a href="#3-Model-adaptivity-generality-to-new-events" class="headerlink" title="(3) Model adaptivity/generality to new events"></a>(3) Model adaptivity/generality to new events</h3><p><strong>FID方法应该识别未见过的、新出现的事件，因为系统的现有数据可能与新出现的事件的内容不同。然而，现有的方法倾向于提取事件的特定特征，而这些特征很难与新事件共享[</strong>204]。正如Tolosi等人[173]所说，<strong>基于特征工程的检测方法很难检测到不同领域（如政治、犯罪、自然灾害）的虚假信息，因为不同事件的特征变化很大。因此，模型的通用性或适应性对于提高FID模型的稳健性相当重要。</strong>Zubiaga等人[206]指出，<strong>依赖于领域的特征分布可能会限制模型的泛化能力。</strong>由于大多数特征的分布直接对应于事件，FID模型的性能将受到影响。尽管我们在第4.2节中讨论了一些人群知识转移模型[138, 182, 187]，但还有更多的东西需要研究。在其他领域（如情感分类[50]和图像识别[103]）成功使用的转移学习模型[59, 126]，可以被用来设计领域适应性的FID模型。使用基于GAN的判别器[182]是另一种有前途的方法，以建立具有共享特征的通用FID模型。</p><h3 id="4-Embracing-of-novel-machine-learning-models"><a href="#4-Embracing-of-novel-machine-learning-models" class="headerlink" title="(4) Embracing of novel machine learning models."></a>(4) Embracing of novel machine learning models.</h3><p>FID过程从本质上讲就是学习分类器，以识别给定主张的可信度。我们发现，许多研究建立了深度学习模型[20, 72, 101, 106, 123, 144, 195]来提高自动事实核查的性能。然而，仍有更多可以探索的地方。在下文中，我们将介绍几个有代表性的例子，它们利用先进的机器学习技术来进行FID。</p><ul><li>Multi-task learning.  多任务学习[109]旨在通过使用相关任务中包含的领域知识来提高模型的泛化性能。现有的方法通过对任务的相关性进行建模，如特征共享、子空间共享和参数共享等，来寻找多个任务之间的共同点，作为促进每个任务学习效果的一些补充知识。例如，Ma等人[109]认为FID任务与立场分类任务高度相关，所以他们提出了一个神经多任务学习框架，以更好地进行事实核查。在权重共享的机制下，他们提出了两个基于RNN的多任务结构来联合训练这两个任务，这可以为谣言表征提取普通以及特定任务的特征。在这项工作的启发下，我们可以研究FID和其他任务之间的联系和协作，并进一步设计基于多任务学习的算法来提高FID模型性能。</li><li>Few-shot learning.  [183]致力于解决数据稀缺的问题，利用少数监督信息来识别未见过的类的样本。现有的少量学习方法通常将其训练程序分解为多个元任务学习程序，类似于元学习[43]，从不同任务的数据中提取可转移的知识。因此，这允许只用少量的标记数据对新类进行分类。据我们所知，在FID中应用的少量学习方法较少，因此我们可以从其他相关领域学习，如文本分类。为了提高分类器的归纳和泛化能力，Geng等人[48]提出了一个基于动态路由算法的分类架构，称为归纳网络，它从少数样本中学习泛化的类级表示。归纳网络主要包含一个编码器模块、一个归纳模块和一个关系模块。具体来说，编码器模块生成样本和查询表征，然后归纳模块利用一个转换矩阵将样本级表征映射到类级表征。最后，关系模块计算出查询和每个类别之间的匹配度。这项工作表明，Few-shot learning在NLP中有很大的潜力，我们可以继续研究基于Few-shot learning的FID方法。</li><li>Semi-supervised models.大多数现有的FID工作集中在监督分类上，他们通常通过大量的标记数据（例如，假的或不假的）来训练分类器识别虚假信息。然而，在很多情况下，我们只有少量的标记数据。半监督模型经常被用来处理标签稀少的问题。例如，Guacho等人[55]提出了一种半监督的FID方法，它利用基于张量分解的文本嵌入来捕捉社交帖子的全局和局部特征。在构建所有帖子的K-近邻（K-NN）图后，他们使用信念传播算法将已知的标签传播到图中，以获得事件的最终可信度。此外，图神经网络的发展也为半监督检测模型的研究提供了机会。GNN，如DeepWalk[129]、LINE[171]和node2vec[54]，利用不同的采样算法来生成节点序列，然后通过跳格模型学习每个节点或传播路径的表示。他们在损失函数中引入了一阶接近性（两个相邻节点之间相似性的表征）和二阶接近性（两个节点之间结构相似性的表征），以确保神经网络能够充分提取图的特征。特别是GCNs[83]，如第2节所讨论的，在相邻的卷积层中通过图形的拉普拉斯矩阵的非线性变换来传递信息。每个卷积层只计算一阶接近度，所以GCN可以通过多个卷积层学习节点或传播路径的高级特征表示。特别是，GNN能够通过明确的图正则化方法[184]平滑标签信息，用于图的半监督学习。因此，FID模型可以建立信息传播图，并结合GNNs来检测虚假信息。</li><li>Unsupervised models. 如果能够直接建立可靠的无监督检测模型，对于快速驳斥虚假信息具有重要意义。无监督模型可以从人与内容的互动（如发布或转发社交媒体帖子）和人与人的互动（如关注或提及某些用户）来评估帖子的可信度。一方面，GAN和VAE的进步为无监督的FID模型带来了新的可能性。另一方面，PGMs仍然可以在FID中发挥重要作用。例如，Chen等人[22]从用户的发帖行为中判断一个帖子是否是假的。这种无监督的方法利用AE来学习一个人最近的发帖和他们的评论的潜在代表。当其重建误差收敛时，该模型可用于评估新帖子的可信度。如果模型的重建误差超过一定的阈值，这个帖子可能是一个假消息。Yang等人[191]将新闻真实性和用户可信度视为潜在变量，并利用用户评论来推断他们对新闻真实性的看法。换句话说，新闻的真实性取决于用户意见的可信度，而意见的可信度则依赖于用户的声誉。他们利用贝叶斯网络对互动过程进行建模，在没有任何标记数据的情况下推断出新闻文章的真实性。实际上，用户的意见可能会受到其他用户的影响，而且他们对不同主题的虚假信息的识别能力也是不同的。在使用PGM时可以进一步考虑这些条件。</li><li>Hybrid learning models. 混合学习模型的发展，结合了线性模型和深度学习模型，已经成为人工智能领域新的研究趋势，即显性特征和潜在特征的结合使用。它利用了两类学习模型的互补性。例如，Wide &amp; Deep[24]是一个表现良好的推荐系统框架，其中Wide部分提取显性特征，Deep部分学习非线性的潜性特征。在FID中也有初步的混合学习模型。Yang等人[192]提出了用于检测虚假信息的TI-CNN模型，该模型在融合显性和隐性特征空间的基础上，对文本和视觉信息进行整体训练。此外，Zhang等人[197]提出了一个基于贝叶斯深度学习的FID模型，该模型使用LSTM来编码索赔和用户评论，并利用贝叶斯模型来推断分类结果。由于混合学习模型仍处于早期阶段，在这个方向上还需要进一步的研究，如概率图模型和深度学习模型的融合。</li></ul><h3 id="5-Adversarial-attack-and-defense-in-FID-models"><a href="#5-Adversarial-attack-and-defense-in-FID-models" class="headerlink" title="(5) Adversarial attack and defense in FID models."></a>(5) Adversarial attack and defense in FID models.</h3><p>基于深度学习的FID模型有助于有效提高事实核查的性能。然而，Szegedy等人[169]已经证明，训练有素的神经网络可能无法抵御对抗性攻击，这意味着在输入向量中添加一些小的扰动会使模型得到错误的结果[4]。现有的FID研究很少强调深度模型的鲁棒性，这些模型可能被对抗性攻击所欺骗。</p><p>虽然很少有关于FID模型中对抗性攻击和防御的研究，但关于其他任务（如图像分类[52，169]、语音识别[15]、文本分类[86]和强化学习[10]）的相关工作已经被调查。有几项工作侧重于对抗性攻击对模型的影响。例如，Dai等人[28]提出了一种基于强化学习（RL）的图数据的对抗性攻击方法，该方法通过增加或减少图中的边的数量来学习最佳攻击策略。为了生成通用的文本对抗性扰动，Behjati等人[9]提出了一种基于梯度投影的攻击方法。Jia等人[71]通过在问题中添加不会对人类理解造成困难的句子或短语来攻击问答系统。</p><p>以上攻击研究可以指导FID模型的对抗性攻击防御研究。Zhou等人[202]进一步将FID模型的对抗性攻击分为事实失真、主客体交换和原因混淆。为了抵御对抗性攻击，他们进一步提出了一个众包知识图谱来及时收集新闻事件的事实。Qiu等人[139]将防御方法分为三类，包括修改数据（如对抗性训练、梯度隐藏）、修改模型（如正则化、防御性蒸馏）和使用辅助工具（如防御-GAN[148]）。无论是对模型的攻击还是对数据的操作，都对FID系统的稳健性提出了更高的要求。因此，在FID的对抗性攻击和防御方面仍有更多的工作要做。</p><h3 id="6-Explanatory-detection-models"><a href="#6-Explanatory-detection-models" class="headerlink" title="(6) Explanatory detection models."></a>(6) Explanatory detection models.</h3><p>提供决策结果的证据或解释可以增加用户对检测模型的信任。尽管关于解释型FID模型的工作很少，但在其他相关领域，如推荐系统，解释的应用已经被研究过。</p><p>可解释的推荐，提供关于为什么推荐一个项目的解释，在最近几年引起了越来越多的关注[198]。它可以提高用户对推荐系统的接受度、信任度和满意度，增强系统的说服力。例如，Chen等人[23]提出了一种基于atten- tive神经网络的可视觉解释的推荐方法，以模拟用户对图像的注意力。用户可以通过提供个性化和直观的视觉亮点来理解产品被推荐的原因。Catherine等人[18]研究了如何在外部知识图谱的支持下产生可解释的推荐，他们提出了一个个性化的PageRank程序，将项目和知识图谱实体一起排名。Wang等人[181]的工作提出了一个基于强化学习（RL）的模型诊断性解释推荐系统，它可以灵活地控制解释的呈现质量。最重要的是，这种可解释推荐系统所使用的方法可以启发我们设计更好的可解释FID系统。</p><p>从更高的角度来看，机器学习模型已经在不同的应用领域（超越了推荐系统和FID）提供了突破性进展。尽管取得了巨大的成功，我们仍然缺乏对其固有行为的理解，例如分类器是如何得出一个特定的决定的。这导致了可解释机器学习（IML）研究方向的激增。IML使机器学习模型有能力以人类可理解的术语进行解释或呈现[2, 34]。Du等人[35]定义了两种类型的可解释性：模型级解释和预测级解释。模型级解释，为增加模型本身的透明度，可以阐明机器学习模型的内部工作机制。预测层面的解释有助于揭示特定输入和模型输出之间的关系。对于FID来说，它更关注预测层面的解释，它可以说明一个决定是如何得出的（使用来源的可靠性、证据和立场等要素）。构建预测级可解释模型的一个代表性方案是采用注意力机制，它被广泛用于解释序列模型（如RNN）做出的决策结果。我们还应该研究植根于IML的其他方法，以提高FID系统的可解释性。</p><h3 id="7-Aggregation-of-crowd-wisdom"><a href="#7-Aggregation-of-crowd-wisdom" class="headerlink" title="(7) Aggregation of crowd wisdom."></a>(7) Aggregation of crowd wisdom.</h3><p>如何聚合人群智慧对FID系统来说非常重要，因为人群贡献的数据往往有噪音。大多数用户的意见可以有效地用于识别虚假信息，但也存在真理掌握在少数人手中的情况。因此，未来仍有必要探索FID的人群智慧的聚合和优化方法。</p><p>我们可以从真相发现系统中学习。随着利用人类智慧从相互冲突的多源数据中提取可靠信息的能力，真相发现已经成为一个越来越重要的研究课题。对于FID，我们也有关于一个事件的多个帖子，目标是识别这个事件的真相。因此，这两个研究问题有相似之处，我们可以借用真相发现系统的知识来促进FID的研究。例如，Liu等人[98]提出了一种专家验证辅助的图像标签真相发现方法，旨在尽可能地从嘈杂的众包标签中推导出正确的标签。特别是，它以人机协作的方式利用了一种半监督学习算法，可以最大限度地发挥专家标签的影响，减少专家的努力。Zhang等人[196]提出了一个名为 “TextTruth “的基于概率图的真相发现模型，它通过全面学习关键因素（一组关键词）的可信度来选择高度可信的问题答案。TextTruth以无监督的方式将答案提供者的可信度和答案因素的可信度一起推断出来。Yin等人[193]提出了一个以无监督的方式进行人群智慧聚合的模型，称为标签感知自动编码器（Label-Aware Autoencoders，LAA），它提取了多源标签的基本特征和模式，并通过一个分类器和一个重构器推断出可信的标签。为了解决同一信息源在不同主题上具有不同可信度的挑战，Ma等人[105]提出了一种名为FaitCrowd的众包数据聚合方法。FaitCrowd通过在概率贝叶斯模型上对问题内容和发布者的答案进行建模，共同学习问题的主题分布、答案提供者的基于主题的知识和真实答案。</p><h3 id="8-Propagation-by-social-bots"><a href="#8-Propagation-by-social-bots" class="headerlink" title="(8) Propagation by social bots."></a>(8) Propagation by social bots.</h3><p>现有的FID研究集中在索赔的内容和发布模式上。然而，对发布和传播帖子的 “账户 “的特征并没有很好的调查。最近，人们已经做出了一些努力来研究虚假信息像病毒一样迅速传播的根本原因。例如，Shao等人[155]对2016年美国总统选举期间的1400万条推文进行了详细分析，他们观察到<br>“社交机器人 “显然促进了虚假信息的快速传播。社交机器人通常指的是一种计算机算法或软件程序，为了某种目的而模仿人类的互动行为（例如，生产内容、关注其他账户、转发帖子等）[40]。这些恶意的机器人账户在虚假推文传播的早期阶段异常活跃。此外，在对社交机器人的社会互动和情感互动进行建模后，Stella等人[167]发现，他们增加了负面和暴力内容在社交网络上的曝光。</p><p>以上发现表明，抑制社交机器人可以成为缓解虚假信息传播的一个有前景的方法。一些研究者分析了社交机器人的行为模式并提出了一些检测方法。例如，Ferrara等人[40]将现有的社交机器人检测方法分为四类，包括基于图的模型、众包、基于特征的模型和混合模型。Almaatoug等人[5]设计了一种社交机器人检测方法，该方法结合了内容属性、社交互动和个人资料属性。同样，Minnich等人[115]提出了BotWalk检测方法，该方法利用几个特征来区分用户和机器人账户，如元数据、内容、时间信息和网络互动。Cresci等人[27]对社交机器人的集体行为进行了穿透性分析，并介绍了一种用于垃圾邮件检测的社会指纹技术。特别是，他们利用数字DNA技术来描述所有账户的集体行为，然后他们提出了一种受DNA启发的方法来识别真实账户和垃圾邮件。Cresci等人[26]也利用集体账户的特征来检测恶意的机器人。由于社交机器人促进了低可信度声明的传播和负面内容的曝光[155, 167]，未来的工作可以将FID与社交机器人检测相结合，为快速驳斥虚假信息提供新的解决方案。</p><h3 id="9-False-Information-Mitigation"><a href="#9-False-Information-Mitigation" class="headerlink" title="(9) False Information Mitigation."></a>(9) False Information Mitigation.</h3><p> 有效的FID是预防虚假信息的一部分，也需要科学研究来减少虚假信息的影响，这属于虚假信息缓解的研究范畴。一些著作对虚假信息缓解和干预的方法进行了回顾。例如，Sharma等人[156]从信息扩散的角度总结了三种缓解方法，即 “去污” “竞争级联 “和 “多阶段干扰”。Shu等人[159]将现有的缓解策略分为 “用户识别”、”网络规模估计 “和 “网络干预”。由于每个用户在虚假信息的传播中扮演着不同的角色，如意见领袖、监护人、恶意传播者和旁观者，因此有必要采取灵活的缓解措施。例如，意见领袖和监护人适合被推荐使用事实信息，以帮助传播真相[175]，而恶意账户或机器人应被遏制[122]。正如Ozturk等人[125]曾经说过的，在Twitter上用事实核查信息展示虚假信息，有助于减少虚假信息的持续传播。基于这一观察，Budak等人[13]提出了多运动独立级联模型，它包含一个虚假信息的运动和一个真实信息的运动。此外，我们还可以利用多变量霍克斯过程[37]来模拟外部干预影响下的虚假信息的传播动态。</p><p>在未来的研究中，FID可以与上述缓解策略相结合，在防止社交网络上的虚假信息传播方面探索出更多有前景的工作。此外，Sundar[168]曾经证实，社交帖子中存在的来源归属改善了用户对在线信息的可信度和质量的看法。因此，来源归属和因果推理[158]也可以用来指导社交媒体上虚假信息的检测。</p><hr><h1 id="Detection-and-Resolution-of-Rumours-in-Social-Media-A-Survey"><a href="#Detection-and-Resolution-of-Rumours-in-Social-Media-A-Survey" class="headerlink" title="Detection and Resolution of Rumours in Social Media: A Survey"></a>Detection and Resolution of Rumours in Social Media: A Survey</h1><p><strong>尽管人们越来越多地使用社交媒体平台来收集信息和新闻，但其未经审核的性质往往导致谣言的出现和传播，即在发布时未经核实的信息项目。</strong>同时，<strong>社交媒体平台的开放性提供了研究用户如何分享和讨论谣言的机会</strong>，并探索如何利用自然语言处理和数据挖掘技术自动评估其真实性。在这篇文章中，我们介绍并讨论了两种在社交媒体上流传的谣言：一种是长期流传的谣言，另一种是<strong>在突发事件等快节奏事件中催生的新出现的谣言</strong>，这些报道是零散发布的，在早期阶段往往是未经核实的状态。我们概述了对社交媒体谣言的研究，最终目标是开发一个由四个部分组成的谣言分类系统<strong>：谣言检测、谣言跟踪、谣言立场分类和谣言真实性分类</strong>。我们深入研究了科学文献中提出的开发这四个组成部分的方法。我们总结了迄今为止在开发谣言分类系统方面所做的努力和取得的成就，并在结论中对未来在社会媒体挖掘中检测和解决谣言的研究途径提出建议。</p><h2 id="INTRODUCTION-1"><a href="#INTRODUCTION-1" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>社会媒体平台越来越多地被用作收集信息的工具，例如，社会问题（Lazer等人，2009年），以及在突发新闻事件中了解最新进展（Phuvipadawat和Murata，2010年）。之所以能做到这一点，是因为这些平台使任何拥有互联网连接设备的人都能实时分享他们的想法和/或发布他们可能目睹的正在发生的事件的最新情况。因此，社交媒体已经成为记者（Diakopoulos等人，2012；Tolmie等人，2017）以及普通公民（Hermida，2010）的有力工具。然而，虽然社交媒体提供了前所未有的信息来源，但由于平台缺乏系统性的努力来调节帖子，也导致了错误信息的传播（Procter等人，2013b；Webb等人，2016），然后需要额外的努力来确定其来源和真实性。与突发新闻故事相关的更新往往是零散发布的，这就造成了这些更新中很大一部分在发布时未经核实，其中一些可能后来被证明是错误的（Silverman 2015a）。在没有权威声明证实或驳斥一个正在进行的谣言的情况下，据观察，社交媒体用户往往会通过一个集体的、主观间的感觉制造过程来分享他们自己对谣言真实性的想法（Tolmie等人，2018），这可能会导致谣言背后的真相曝光（Procter等人，2013a；Li和Sakamoto，2015）。</p><p>然而，尽管社交媒体具有这种明显的稳健性<strong>，但其日益增长的产生谣言的趋势促使人们开发一些系统，这些系统通过收集和分析用户的集体判断</strong>（Lukasik等人，2016），能够通过加速感知过程来减少谣言的传播（Derczynski和Bontcheva 2014）。<strong>谣言检测系统可以在早期阶段识别出真实性不确定的帖子，可以有效地用来警告用户，其中的信息可能是虚假的</strong>（Zhao等人，2015）。同样，一个汇总了用户发布的不断变化的集体判断的谣言分类系统可以帮助跟踪谣言的真实性状态，因为它被暴露在这个集体感知的过程中（Metaxas等人，2015）。在这篇文章中，我们概述了开发这样一个谣言分类系统所需的组件，并讨论了到目前为止为建立该系统所做的努力的成功。</p><h3 id="1-1-Defining-and-Characterising-Rumours"><a href="#1-1-Defining-and-Characterising-Rumours" class="headerlink" title="1.1 Defining and Characterising Rumours"></a>1.1 Defining and Characterising Rumours</h3><p>谣言的定义。最近研究文献中的出版物使用了彼此不同的谣言的定义。例如，最近的一些工作<strong>将谣言错误地定义为被认为是虚假的信息</strong>（如Cai等人（2014）和Liang等人（2015）），而大多数文献将谣言定义为 “<strong>流通中的未经核实的、工具性的信息声明</strong>“（DiFonzo和Bordia，2007）。在我们的文章中，<strong>我们采用了谣言的定义特征，即它们在发布时是未经核实的</strong>，这与主要词典给出的定义是一致的，比如《牛津英语词典》将谣言定义为 “目前流传的不确定或可疑的故事或报告 “1，或者《梅里亚姆-韦伯斯特词典》将其定义为 “目前没有已知权威机构证明其真实性的声明或报告 “2。 这种未经核实的信息可能被证明是真的，或部分或完全错误；或者，它也可能仍未解决。因此，在这篇文章中，我们坚持这个流行的谣言定义，将其归类为 “<strong>在发布时其真实性尚未得到验证的流通信息</strong>“。这个定义的选择与近期社会媒体研究的一些文献不同；但是，它与主要的字典和社会科学的一个长期研究领域相一致（Allport和Postman 1946；Donovan 2007）。<strong>谣言可以被理解为一个尚未被验证的信息，因此它的真实价值在流传过程中仍未得到解决。当没有证据支持它，或者没有来自权威来源（如那些有信誉的人）或在特定背景下可能有可信度的来源（如目击者）的正式确认时，谣言就被定义为未经证实。</strong></p><p>谣言类型。许多不同的因素可用于按类型对谣言进行分类，包括其最终的真实性价值（真实、虚假或未解决）（Zubiaga等人，2016c）或其可信度（例如，高或低）（Jaeger等人，1980）。另一个按类型对谣言进行分类的尝试是Knapp（1944），他提出了三种类型的谣言的分类法。(1) “白日梦 “谣言：即导致一厢情愿的谣言；(2) “无聊 “谣言：即增加焦虑或恐惧的谣言；以及(3) “楔子驱动 “谣言：即产生仇恨的谣言。当涉及到开发一个谣言分类系统时，主要决定要利用的方法的因素是它们的时间特征：</p><ul><li>突发新闻中出现的新谣言。在突发新闻中出现的谣言通常是以前没有被观察到的。因此，谣言需要被自动检测出来，而且考虑到系统可用的训练数据可能与后来观察到的数据不同，谣言分类系统需要能够处理新的、未见过的谣言。在这些情况下，早期检测和解决谣言是至关重要的，需要实时处理帖子流。在突发新闻中出现的谣言的一个例子是，当一个可疑的恐怖分子的身份被报道时。谣言分类系统可能已经观察到其他类似的疑似恐怖分子的案件，但案件和涉及的名字很可能会有所不同。因此，在这些情况下，谣言分类器的设计需要考虑新案例的出现，以及它们可能带来的新词汇。</li><li>长时间讨论的长期谣言。有些谣言可能流传了很长时间，但其真实性却没有得到确定的证实。尽管（或可能是因为）很难确定实际的真相，这些谣言还是会引起人们巨大的、持续的兴趣。例如，关于奥巴马是穆斯林的传言就是如此。虽然这个说法没有证据，但似乎没有任何证据能让大家满意地推翻它。3 对于像这样的谣言，一个谣言分类系统可能不需要检测谣言，因为它可能是先验的。此外，该系统可以利用历史上关于该谣言的讨论来对正在进行的讨论进行分类，其中词汇的差异性要小得多，因此建立在旧数据上的分类器仍然可以用于新数据。与新出现的谣言相比，对于长期存在的谣言，处理通常是回顾性的，所以帖子不一定需要实时处理。</li></ul><p>在整篇文章中，我们提到了这两种类型的谣言，描述了不同的访问者如何处理每一种谣言。</p><h3 id="1-2-Studying-Rumours-From-Early-Studies-to-Social-Media"><a href="#1-2-Studying-Rumours-From-Early-Studies-to-Social-Media" class="headerlink" title="1.2 Studying Rumours: From Early Studies to Social Media"></a>1.2 Studying Rumours: From Early Studies to Social Media</h3><p>简史。谣言和相关现象已经被从许多不同的角度进行了研究（Donovan 2007），从心理学研究（Rosnow和Foster 2005）到计算分析（Qazvinian等人2011）。传统上，研究人们对谣言的反应是非常困难的，因为这将涉及到在谣言展开时实时收集反应，假设参与者已经被招募了。为了克服这一障碍，All-port（Allport and Postman 1946, 1947）在战时谣言的背景下进行了早期调查。他提出了研究谣言的重要性，强调 “有新闻价值的事件很可能会滋生谣言”，”流通中的谣言数量会随着主题对相关个人的重要性而变化，同时与相关主题有关的证据的模糊性。这使他提出了一个尚待回答的动机问题。”谣言可以被科学地理解和控制吗？” (Allport and Postman 1946)。他在1947年的实验（Allport and Postman 1947）揭示了一个关于谣言流通和信仰的有趣事实。他研究了美国总统富兰克林-D-罗斯福如何消除关于美国海军在1941年日本袭击珍珠港时遭受损失的谣言。研究表明，在总统发表讲话之前，69%的本科生认为损失比官方公布的要大；但五天后，在总统发表讲话的同时，只有46%的同等学生认为这一说法是真的。这项研究揭示了一个有声望的人发表的官方声明在影响社会对谣言准确性的看法方面的重要性。</p><p>早期的研究集中在不同的目标上。一些工作研究了决定谣言传播的因素，例如，包括谣言的可信度对其后续传播的影响，其中可信度是指谣言可能被视为真实的程度。Prasad(1935)和Sinha(1952)的早期研究认为，在自然灾害的背景下，可信度不是影响造谣的一个因素。然而，最近，Jaeger等人（1980）发现，当可信度较高时，谣言的传播更为频繁。此外，Jaeger等人（1980年）和Scanlon（1977年）发现，接受者认为谣言的重要性是决定它是否被传播的一个因素，最不重要的谣言被传播得更多。</p><p>互联网上的流言。互联网的广泛采用使自然环境下的谣言研究进入了一个新的阶段（Bordia 1996），并且随着社交媒体的出现而显得尤为重要，它不仅为分享信息提供了强大的新工具，而且也便于从大量的参与者那里收集数据。例如，Takayasu等人（2015）利用社交媒体研究了2011年日本地震期间流传的谣言的扩散情况，该谣言称地震后的雨水可能包括有害的化学物质，并导致人们被警告要携带雨伞。作者研究了早期报道该谣言的推文以及后来报道该谣言的推文的转发（RTs）情况。虽然他们的研究显示，后来的更正推文的出现减少了报告虚假谣言的推文的传播，但分析仅限于一个谣言，并没有为理解社交媒体中谣言的性质提供足够的洞察力。然而，他们的案例研究确实展示了一个对社会有重要影响的谣言的例子，因为市民们都在关注有关地震的最新动态，以保持安全。</p><p>社会媒体中的谣言。近年来，社交媒体作为研究谣言的一个来源已经得到了重视，这是因为它是收集与谣言相关的大型数据集的一个有趣的来源，而且，除其他因素外，其巨大的用户群和分享的便利性使其成为谣言滋生的沃土。研究普遍发现，由于用户在分享意见、猜想和证据时具有众包的自我修正特性，Twitter在驳斥不准确信息方面表现良好。例如，Castillo等人（2013）发现，在2010年智利地震的案例中，支持和驳斥虚假谣言的推文比例为1：1（每条支持的推文对应一条驳斥的推文）。Procter等人（2013b）在分析2011年英格兰骚乱期间的虚假谣言时得出了类似的结论，但他们指出，任何自我纠正的效果都很缓慢。相反，在他们对2013年波士顿马拉松爆炸案的研究中，Starbird等人（2014）发现Twitter用户在区分真相和骗局方面做得并不好。在研究三种不同的谣言时，他们发现支持虚假谣言的推文所占比例分别为44：1、18：1和5：1。Zubiaga等人（2016c）进一步深入研究了谣言传播和支持的时间方面，描述了对九个突发新闻事件中的谣言的分析。这项研究的结论是，虽然总体趋势是用户在早期阶段支持未经核实的谣言，但随着时间的推移，会转向支持真实的谣言和驳斥虚假的谣言。因此，社交媒体聚合大量用户社区的判断的能力（Li和Sakamoto 2015）促使人们进一步研究机器学习方法，以改善谣言分类系统。尽管谣言和错误信息的传播给此类系统的开发带来了挑战，但将开发过程分解成更小的组成部分并利用合适的技术，在开发有效系统方面取得了令人鼓舞的进展，这些系统可以帮助人们在评估从社交媒体收集的信息的真实性方面做出决定。</p><h3 id="1-3-Scope-and-Organisation"><a href="#1-3-Scope-and-Organisation" class="headerlink" title="1.3 Scope and Organisation"></a>1.3 Scope and Organisation</h3><p>这篇调查文章的起因是<strong>人们越来越多地使用Facebook或Twitter等社交媒体平台来发布和发现信息。虽然我们承认它们在收集独家信息方面的作用毋庸置疑，但它们的开放性、缺乏节制以及信息可以随时随地发布的便利性，无疑给信息质量保障带来了很大的问题。考虑到谣言的传播可能带来的不安和潜在的危害，近年来，开发处理谣言的数据挖掘工具的动机越来越强烈。</strong>这篇调查文章旨在深入研究谣言对开发用于收集社交媒体信息的数据挖掘应用所带来的这些挑战，并总结迄今为止在这个方向上的努力。</p><p>我们在第2节继续这一调查，研究社交媒体给众多领域带来的机会，同时也引入了必须处理谣言的新挑战。接着是对谣言分类系统的分析，我们首先描述了将谣言数据集放在一起的不同方法，以便进行进一步的实验；第3节描述了数据集的生成，首先是访问社交媒体API的方法，然后概述了收集和注释从社交媒体收集的数据的方法。我们在第4节中总结了对社交媒体中谣言的扩散和动态的特征和理解的研究结果。之后，我们在第5节中描述了构成谣言分类系统的组件。然后，在随后的章节中进一步描述这些组件并讨论现有的方法；第6节中的谣言检测系统、第7节中的谣言追踪系统、第8节中的谣言立场分类以及第9节中的真实性分类。我们在第10节中继续列举并描述了现有的处理谣言分类的应用和相关应用。最后，我们在第11节中总结了到目前为止的成就，并概述了未来的研究方向。</p><h2 id="2-SOCIAL-MEDIA-AS-AN-INFORMATION-SOURCE-CHALLENGES-POSED-BY-RUMOURS"><a href="#2-SOCIAL-MEDIA-AS-AN-INFORMATION-SOURCE-CHALLENGES-POSED-BY-RUMOURS" class="headerlink" title="2 SOCIAL MEDIA AS AN INFORMATION SOURCE: CHALLENGES POSED BY RUMOURS"></a>2 SOCIAL MEDIA AS AN INFORMATION SOURCE: CHALLENGES POSED BY RUMOURS</h2><p>社会媒体越来越多地被一系列专业人士和公众所利用，成为了解最新发展和时事的信息来源（Van Dijck 2013；Fuchs 2013）。社交媒体的使用已经在许多不同的领域被发现是有用的；我们在下面描述一些最值得注意的使用。</p><p>新闻收集。社交媒体平台在新闻传播方面显示出巨大的潜力，在突发新闻报道方面有时甚至超过了专业新闻机构（Kwak等人，2010）。除其他外，这使人们能够从目击者和广泛的用户那里获得最新信息，这些用户可以获得潜在的独家信息（Diakopoulos等人，2012；Starbird等人，2012）。为了利用社交媒体平台的这一特点，研究人员研究了新闻收集工具的发展（Zubiaga等人，2013年；Diakopoulos等人，2012年；Marcus等人，2011年），分析了用户生成内容（UGC）在新闻报道中的使用（Hermida和Thurman，2008年；Tolmie等人，2017年），并探索了社交媒体催生合作和公民新闻的潜力，包括对社交媒体上发布的报道进行合作核查（Hermida，2012年；Spangenberg和Heise，2014年）。</p><p>突发事件和危机。近年来，社会媒体在紧急情况和危机中的使用也大幅增加（Imran等人，2015；Castillo，2016；Procter等人，2013a），其应用包括从目击者那里获得报告或找到寻求帮助的人。人们发现社交媒体对不同情况下的信息收集和协调非常有用，包括紧急情况（Yates和Paquette 2011；Yin等人2012；Procter等人2013a）、抗议活动（Trottier和Fuchs 2014；Agarwal等人2014）和自然灾害（Vieweg等人2010；Middleton等人2014）。</p><p>公共舆论。研究人员也在利用社交媒体来收集用户对一系列社会问题的看法，然后将其汇总以衡量公众意见（Murphy等人，2014）。研究人员试图清理社交媒体数据（Gao等人，2014），并试图摆脱人口偏见（Olteanu等人，2016），以了解社交媒体如何塑造社会对问题、产品、人物等的看法。古德曼等人（2011）。人们发现，社交媒体对于衡量选举期间的民意（Anstead和O’Loughlin 2015），以及网上意见对组织声誉（Sung和Lee 2015）或对健康项目的态度（Shi等人2014）等方面的影响都很有用。</p><p>金融/股票市场。社交媒体也已经成为了解金融界和股票市场最新发展的重要信息来源。例如，推文中表达的情绪被用来预测股市反应（Azar和Lo 2016），收集投资者在社交媒体上发布的意见（Chen等人，2014）或分析社交媒体帖子对品牌和产品的影响（李等人，2015）。</p><p>由于社交媒体作为信息来源的潜力越来越大，其传播错误信息和未经证实的主张的倾向已经引起了许多研究。研究考察了用户的可信度认知（Westerman等人，2014），也评估了用户依赖社交媒体收集新闻等信息的程度（Gottfried and Shearer，2016）。因此，社交媒体中存在的谣言和有问题的说法所带来的困难导致了人们对建立谣言分类系统的技术的兴趣，并通过促进用户收集准确的信息来缓解这一问题。谈到谣言分类系统的发展，有两个主要用例需要考虑。</p><ul><li>处理长期存在的流言。在这种情况下，被追踪的谣言是预先知道的，并且社交媒体被作为收集意见的来源而加以挖掘。这个用例可能适用，例如，当想要追踪公众意见，或者当诸如潜在的收购等谣言在金融领域被长期讨论时。</li><li><strong>处理新出现的传言。当某些事件或话题被追踪时，新的谣言突然出现。这种用例可能适用于新闻收集和紧急情况，在这种情况下，信息被零散地发布并需要被核实，或者其他突然出现的谣言，例如那些预计会对股票市场产生影响的政治决定。</strong></li></ul><h2 id="3-DATA-COLLECTION-AND-ANNOTATION"><a href="#3-DATA-COLLECTION-AND-ANNOTATION" class="headerlink" title="3 DATA COLLECTION AND ANNOTATION"></a>3 DATA COLLECTION AND ANNOTATION</h2><p>本节介绍了用于收集社交媒体数据的不同策略，这些数据能够研究谣言，以及收集数据注释的方法。</p><h3 id="3-1-Access-to-Social-Media-APIs"><a href="#3-1-Access-to-Social-Media-APIs" class="headerlink" title="3.1 Access to Social Media APIs"></a>3.1 Access to Social Media APIs</h3><p>访问、收集和存储社交媒体平台数据的最佳方式通常是通过应用编程接口（API）（Lomborg和Bechmann，2014年）。API是易于使用的界面，通常伴随着描述如何请求感兴趣的数据的文档。它们被设计成可以被其他应用程序访问，而不是为人设计的网络接口；API提供了一套定义明确的方法，应用程序可以调用这些方法来请求数据。例如，在一个社交媒体平台上，可能需要检索某个特定用户发布的所有数据或包含某个关键词的所有帖子。</p><p>在使用API之前，关键的第一步是阅读其文档，了解其方法和限制。事实上，每个社交媒体平台都有自己的局限性，当想要开发一个利用社交媒体数据的谣言分类系统时，这是关键。用于研究谣言的三个关键平台是Twitter、新浪微博和Facebook；这里我们简要讨论一下这三个平台的特点和局限。</p><ul><li>Twitter提供了使用其API的详细文档4，它可以访问REST API以从其数据库中获取数据，也可以访问流式API以实时获取数据。在注册了一个Twitter应用程序5后，该程序将生成一组密钥，用于通过OAuth认证访问API，然后开发人员将有机会使用一系列方法（”端点”）来收集Twitter数据。这些端点中最慷慨的是可以访问整个推文流中随机抽样的1%；要获得更大比例的数据通常需要付费。为了确保收集到全面的推文，最好是通过流媒体API实时收集推文；同样，从这个API免费收集的推文数量有1%的限制。使用Twitter的API的主要优点是它是最开放的，这可能部分解释了为什么它被最广泛地用于研究；主要的注意事项是它主要被设计用来收集实时或最近的数据，因此收集比过去几周更早的数据更具挑战性。推特在收集每条推文时都会提供一系列元数据，包括推文语言、地点（如有）等，以及发布推文的用户的详细信息。</li><li>新浪微博是中国最流行的微型博客平台，它提供的API与Twitter有许多相似之处。然而，对它的一些方法的访问是不公开的。例如，搜索API需要先与管理员联系以获得批准。此外，新浪微博提供的一系列方法只能通过其REST API访问，它缺乏一个官方的流媒体API来检索实时数据。要通过新浪微博的流媒体API检索实时数据，必须使用第三方供应商，如Socialgist。7,8 与Twitter一样，新浪微博为每个帖子提供一组元数据，包括帖子的信息和用户的详细信息。</li><li>Facebook提供了一个记录在案的API，以及一套适用于多种编程语言和平台的软件开发工具包，使得利用其数据开发应用程序变得容易。与Twitter的API类似，Facebook也需要注册一个应用程序10来生成访问API所需的密钥。与Twitter相比，Facebook用户发布的大部分内容都是私密的，因此无法访问发布的具体内容，除非用户是认证账户的 “朋友”。获取Facebook上的帖子的变通方法通常是从所谓的Facebook页面收集数据，这些页面是由组织、政府、团体或协会创建的公开页面。与Twitter不同的是，从这些Facebook页面获取历史数据是可能的；但是，访问仅限于在这些页面上发布的内容。脸谱网提供的每个帖子的元数据更加有限，需要向API提出额外的请求才能获得这些数据。</li></ul><h2 id="4-CHARACTERISING-RUMOURS-UNDERSTANDING-RUMOUR-DIFFUSION-AND-FEATURES"><a href="#4-CHARACTERISING-RUMOURS-UNDERSTANDING-RUMOUR-DIFFUSION-AND-FEATURES" class="headerlink" title="4 CHARACTERISING RUMOURS: UNDERSTANDING RUMOUR DIFFUSION AND FEATURES"></a>4 CHARACTERISING RUMOURS: UNDERSTANDING RUMOUR DIFFUSION AND FEATURES</h2><p>最近的许多研究都关注了社会媒体中谣言的出现和传播的特点。从这些研究中得到的启示反过来也可以为谣言分类系统的发展提供参考。其中一些研究集中于对某一特定谣言的广泛分析，而另一些研究则是对较大的谣言集进行更广泛的分析。</p><p>对围绕谣言的话语进行研究是为了考察围绕谣言的讨论以及谣言随时间的演变。一些研究着眼于定义一个方案，以对谣言的反应类型进行分类。Maddock等人（2015年）研究了谣言的起源和随时间的变化，从而确定了对谣言的七种行为反应：错误信息、猜测、纠正、质疑、对冲、不相关或中立/其他。同样，Procter等人（2013b）提出，对谣言的反应可以分为四种类型，即支持、否认、呼吁提供更多信息和评论。还有人研究了谣言，以了解人们对谣言的反应。通过研究在中国微博平台上传播的谣言，Liao和Shi（2013）确定了七种类型的用户（名人、认证、大众媒体、组织、网站、网络明星和普通人）的干预，他们以七种不同的方式（提供信息、发表意见、情感状态、感性陈述、询问性陈述、指导性陈述和离题陈述）做出贡献。在另一项研究中，Zubiaga等人（2016c）研究了由Twitter上的谣言报道引发的 “对话”（即由回复关系连接的一系列推文），发现社交媒体用户的普遍倾向是支持和传播谣言，而不考虑其真实性价值。这包括声誉高的用户，如新闻机构，他们倾向于在谣言的早期阶段支持谣言，稍后在需要时发布更正声明。在早期的研究中，Mendoza等人（2010）发现了谣言支持和真实性之间的强烈关联，表明大多数用户支持真实的谣言，而更多的用户否认虚假的谣言。尽管这些研究之间存在明显的矛盾，但值得注意的是，Mendoza等人（2010年）研究了谣言的整个生命周期，因此汇总导致了良好的相关性；相反，Zubiaga等人（2016c）专注于谣言的早期再行动，表明用户在谣言的早期阶段确定真实性方面存在问题。利用Reddit的谣言数据，也发现了不同用户之间的差异，表明有三个不同的用户群体：一般支持虚假谣言的用户，一般反驳虚假谣言的用户，以及一般对虚假谣言开玩笑的用户（Dang等人，2016a）。也有人认为，更正通常是由新闻机构发布的，它们有时会被广泛传播（Takayasu等人，2015；Arif等人，2016；Andrews等人，2016），特别是如果这些更正来自志同道合的账户（Hannak等人，2014），偶尔甚至会导致原始帖子的删除或取消分享（Frias-Martinez等人，2012）。然而，更正并不总是具有与原始谣言相同的效果（Lewandowsky等人，2012；Shin等人，2016；Starbird等人，2014），这加强了开发处理新出现的谣言分类系统的必要性。</p><p>其他研究也关注了促使谣言传播的因素。谣言的传播通常取决于用户之间的关系强度，谣言更有可能在网络中的强关系中传播（Cheng等人，2013）。其他对谣言时间模式的研究表明，在社交媒体（Kwon等人，2013年；Kwon和Cha，2014年；Lukasik等人，2015年b）和互联网上的其他平台（Jo，2002年），谣言的流行度往往会随着时间的推移而波动，但在谣言流行度消退后又有可能被重新讨论。</p><p>研究还考察了谣言的出现。通过使用谣言理论方法来研究导致表达对追踪谣言的兴趣的因素，Oh等人（2013）认为缺乏官方来源和个人参与是最重要的因素，而其他因素，如焦虑，则不那么重要。海报的可信度和谣言的吸引力也被认为是促成谣言传播的因素（Petty和Cacioppo 2012）。Liu等人（2014）强化了这些发现，认为个人参与是最重要的因素。Chua等人（2016）分析了Twitter上的具体谣言信息，发现拥有较大粉丝网络的成熟用户的推文传播最广。</p><p>虽然许多研究都探讨了谣言的传播，但对这些研究的详尽分析并不在本调查文章的范围内，而是侧重于有关检测和解决谣言的方法的发展研究。要阅读更多关于研究谣言扩散的研究，我们推荐Serrano等人（2015）和Walia和Bhatia（2016）的调查。</p><h2 id="5-RUMOUR-CLASSIFICATION-SYSTEM-ARCHITECTURE"><a href="#5-RUMOUR-CLASSIFICATION-SYSTEM-ARCHITECTURE" class="headerlink" title="5 RUMOUR CLASSIFICATION: SYSTEM ARCHITECTURE"></a>5 RUMOUR CLASSIFICATION: SYSTEM ARCHITECTURE</h2><p>谣言分类系统的结构可以有轻微的变化，这取决于具体的使用情况。这里我们定义了一个典型的谣言分类系统的架构，它包括一个完整系统所需的所有组件；然而，正如我们在下面的描述中指出的，根据需求，其中一些组件可以省略。谣言分类系统通常从确定某条信息未被证实开始（即谣言检测），最后确定该条信息的估计可信度值（即可信度分类）。从谣言检测到真实性分类的整个过程是通过以下四个部分进行的（见图1）。</p><p><img src="https://i.loli.net/2021/11/04/Z9NnoAdsbS8t1EX.png" alt=""></p><ul><li>谣言检测。首先，一个谣言分类系统必须确定一条信息是否构成谣言。谣言检测组件的典型输入可以是社交媒体的帖子流，然后一个二元分类器必须确定每个帖子是被视为谣言还是非谣言。这个组件的输出是帖子流，其中每个帖子都被标记为谣言或非谣言。这个组件对于识别新出现的谣言很有用；但是，在处理先验已知的谣言时，它就没有必要了。</li><li>谣言追踪。一旦确定了一个谣言，或者因为它是先验的，或者因为它是由谣言检测组件确定的，谣言跟踪组件就会收集和过滤讨论该谣言的帖子。谣言的输入可以是一个帖子或描述它的句子，也可以是一组关键词，这个组件监测社交媒体以找到讨论该谣言的帖子，同时剔除不相关的帖子。该组件的输出是讨论该谣言的帖子的集合。</li><li>立场分类。当谣言追踪组件检索与谣言相关的帖子时，立场分类组件确定每个帖子对谣言的真实性的定位。将一组与同一谣言相关的帖子作为输入，它为每一个帖子输出一个标签，这些标签一般从预定义的立场类型集合中选择。这个组件对于促进后续处理真实性分类的组件的任务很有用。但是，如果公众的立场被认为是没有用的，例如，仅仅依靠专家的输入或权威来源的验证的情况下，它可以被省略。</li><li>真实性分类。最后的真实性分类组件试图确定谣言的实际真相价值。它可以使用在谣言跟踪组件中收集到的帖子集，以及在立场分类组件中产生的立场标签作为输入。它可以选择从其他来源，如新闻媒体，或其他网站和数据库中收集额外的数据。该组件的输出可以只是预测的真值，但它也可以包括上下文，如URL或其他数据源，以帮助最终用户通过与相关来源的双重检查来评估分类器的可靠性。</li></ul><p>在下面的章节中，我们将更详细地探讨这四个组成部分，到目前为止用于实施这些组成部分的方法以及迄今取得的成就。</p><h2 id="6-RUMOUR-DETECTION"><a href="#6-RUMOUR-DETECTION" class="headerlink" title="6 RUMOUR DETECTION"></a>6 RUMOUR DETECTION</h2><h3 id="6-1-Definition-of-the-Task-and-Evaluation"><a href="#6-1-Definition-of-the-Task-and-Evaluation" class="headerlink" title="6.1 Definition of the Task and Evaluation"></a>6.1 Definition of the Task and Evaluation</h3><p>谣言检测任务是指系统必须从一组社交媒体帖子中确定哪些帖子是报告谣言的，因此是在传播有待核实的信息。请注意，一条推文构成谣言的事实并不意味着它以后会被认为是真的或假的，而是意味着它在发布的时候是未经核实的。从形式上看，该任务将社交媒体帖子的时间线TL={t1,…,t|TL|}作为输入，分类器必须确定这些帖子中的每一个，ti，是谣言还是非谣言，从Y={R,NR}中分配标签。因此，该任务通常被表述为一个二元分类问题，其性能通过计算目标类别（即谣言）的精度、召回率和F1分数来评估。</p><h3 id="6-2-Datasets"><a href="#6-2-Datasets" class="headerlink" title="6.2 Datasets"></a>6.2 Datasets</h3><p>唯一公开的数据集是PHEME的谣言和非谣言数据集，其中包括与5个突发新闻故事相关的1972个谣言和3830个非谣言的集合（Zubiaga等人，2016b）。</p><h3 id="6-3-Approaches-to-Rumour-Detection"><a href="#6-3-Approaches-to-Rumour-Detection" class="headerlink" title="6.3 Approaches to Rumour Detection"></a>6.3 Approaches to Rumour Detection</h3><p>尽管人们对分析社交媒体中的谣言和建立工具来处理之前已经确定的谣言越来越感兴趣（Seo等人，2012；Takahashi和Igata，2012），但在自动谣言检测方面的工作却很少。谣言检测方面的一些工作（Qazvinian等人，2011年；Hamidian和Diab，2015年，2016年）仅限于寻找先验的谣言。他们用一组预定义的谣言（例如，奥巴马是穆斯林）来喂养分类器，将新的推文分类为与已知的谣言之一有关或无关（例如，我认为奥巴马不是穆斯林将与谣言有关，而奥巴马正在与一群穆斯林交谈则不是）。像这样的方法对于长期存在的谣言是很有用的，在这种情况下，需要的是识别与追踪已经确定的谣言有关的推文；在这篇调查文章中，我们把这项任务称为谣言追踪，因为被监测的谣言是已知的，但帖子流需要被过滤。仅仅依靠谣言追踪是不够的，因为在快节奏的背景下，<strong>如突发新闻，会出现新的、未见过的谣言，而与尚未被发现的谣言相关的具体关键词并不是预先知道的。为了处理这个问题，分类器将需要学习可概括的模式，以帮助在新出现的事件中识别谣言。</strong></p><p>第一个解决新谣言检测的工作是Zhao等人（2015）的工作。他们的方法建立在这样的假设上：谣言会引起怀疑论者的推文，他们会质疑或询问谣言的真实性；如果一条信息有一些相关的询问推文，那么就意味着该信息是谣言的。作者创建了一个由五个正则表达式（例如”（那个|这个|它）是真的吗”）组成的人工策划列表，用于识别询问性推文。然后，这些询问的推文按相似度进行聚类，每个聚类最终被视为一个候选谣言。他们用召回率来评估是不可行的，而只用精确度来评估。</p><p>相比之下，Zubiaga等人（2016b，2017）提出了另一种方法，在整个突发新闻故事中学习上下文，以确定一条推文是否构成谣言。他们的假设是，由于缺乏上下文，单单一条推文可能不足以知道其背后的故事是否是谣言。此外，他们避免了对询问性推文的依赖，他们认为并非所有的谣言都会引发，因此可能导致低召回率，因为没有引发询问性推文的谣言会被遗漏。他们的上下文学习方法依靠条件随机场（CRF）作为顺序分类器，学习事件中的报道动态，这样分类器就可以根据事件中迄今为止的情况，对每条新推文确定其是否是谣言。他们的方法导致了比Zhao等人（2015）的基线分类器更高的性能，也改善了一些作为基线的非序列分类器。在这种情况下，该分类器也被评估为召回率，取得了最先进的结果。</p><p>Tolosi等人（2016年）对不同事件中的谣言进行特征分析，发现很难区分谣言和非谣言，因为不同事件的特征变化很大。Zubiaga等人（2016b）解决了推特层面的这些发现，表明通过利用事件的背景，可以实现普遍性。<br>McCreadie等人（2015年）研究了使用众包平台来识别社交媒体中的谣言和非谣言的可行性，发现注释者取得了很高的注释者之间的一致性。他们还将谣言分为六个不同的类型。未经证实的信息、有争议的信息、错误的信息/虚假的信息、报道、有关联的争议和有意见的。然而，他们的工作仅限于对谣言和非谣言的众包注解，他们没有研究自动谣言检测系统的发展。这项研究的数据集没有公开提供。<br>然而，其他的工作被贴上了谣言检测的标签，专注于确定社交媒体上发布的信息是真的还是假的，而不是早期检测未经核实的信息，因此我们在第9节关于真实性分类中讨论。<br>技术现状。谣言检测的最先进方法是Zubiaga等人（2017）提出的方法，它利用与特定事件相关的早期帖子的上下文来确定一条推文是否构成谣言。</p><h2 id="11-DISCUSSION-SUMMARY-AND-FUTURE-RESEARCH-DIRECTIONS"><a href="#11-DISCUSSION-SUMMARY-AND-FUTURE-RESEARCH-DIRECTIONS" class="headerlink" title="11 DISCUSSION: SUMMARY AND FUTURE RESEARCH DIRECTIONS"></a>11 DISCUSSION: SUMMARY AND FUTURE RESEARCH DIRECTIONS</h2><p>随着社交媒体渗透率的提高，关于开发谣言检测和验证工具的研究变得越来越受欢迎，它使普通用户和专业从业人员能够实时收集新闻和事实，但也带来了未经核实的信息传播的副作用。这篇调查文章总结了科学文献中关于发展谣言分类系统的研究，对社会媒体谣言进行了定义和定性，并描述了发展其四个主要组成部分的不同方法。(1) 谣言检测，(2) 谣言追踪，(3) 谣言立场分类，以及(4) 谣言真实性分类。在这样做的过程中，该调查为这些组件的开发提供了一个技术现状的指导。该调查特别关注在社交媒体上流传的谣言的分类。大多数一般方面，如谣言的定义和分类架构，都是可以推广到新闻文章等类型的。然而，为四个部分中的每一部分描述的具体方法通常是为社交媒体设计的，不一定直接适用于其他体裁。在下文中，我们将回顾迄今为止所取得的进展，现有系统的缺点，概述对未来研究的建议，并评论谣言分类系统对其他类型的误导性信息的适用性和通用性，这些信息也在社交媒体中传播。</p><p>自从社交媒体作为信息和新闻收集的平台激增以来，检测和解决谣言的研究有了很大进展。一系列的研究采取了非常不同的方法来理解和描述社会谣言，而这种多样性有助于阐明谣言分类系统的未来发展。在构成谣言分类系统的所有四个组成部分中，已经进行了重新搜索，尽管大多数都集中在管道的最后两个组成部分，即谣言立场分类和真实性分类。尽管如本调查所示，该研究领域取得了实质性进展，但我们也表明，这仍然是一个需要进一步研究的开放性研究问题。我们在下一节中研究主要的开放性研究挑战。</p><h3 id="11-1-Open-Challenges-and-Future-Research-Directions"><a href="#11-1-Open-Challenges-and-Future-Research-Directions" class="headerlink" title="11.1 Open Challenges and Future Research Directions"></a>11.1 Open Challenges and Future Research Directions</h3><p>近年来，谣言分类的研究主要集中在管道的后期阶段，即谣言的立场分类和真实性分类。这些都是至关重要的阶段；然而，如果不执行前面的检测谣言和跟踪与这些谣言相关的帖子的任务，它们就不能被使用。后者在以前的工作中通常被跳过，要么把这些组件的开发留给未来的工作，要么假设谣言和相关帖子是由人输入的。我们认为，未来的研究应该集中在谣言的检测和跟踪上，以避免完全依赖人在回路中的情况，从而减轻这些初始任务。在这个方向上的进一步研究将能够开发出完全自动化的谣言分类系统。</p><p>谣言检测的研究应该从在谣言的特定背景下测试最先进的事件检测技术开始。除了事件检测系统所做的，谣言检测系统还需要确定检测到的事件是否构成谣言。如果只使用其内容，确定一个单独的社交媒体帖子是否报告了一个谣言是具有挑战性的。最近的研究表明，使用上下文（Zubiaga等人，2017年）和互动（Zhao等人，2015年）可以提供帮助，这些都是值得详细探索的方向。</p><p>对谣言追踪系统的研究是有限的，而且研究人员经常假设用于收集与谣言有关的帖子的关键词是先验的。社交媒体的一个明显的问题是用户之间使用不一致的词汇，例如，用户可能不明确地使用杀戮或射击来指称同一事件。在扩大数据收集方面的研究仍处于起步阶段，通过技术（如伪相关性反馈）使用查询扩展方法，还有待详细探讨，但初步研究显示了其潜力</p><p>谣言分类系统发展的一个重要限制是缺乏公开可用的数据集。除了我们在本调查中列出的最近发表的数据集，我们鼓励研究人员发布他们自己的数据集，以便对不同的数据集进行进一步研究，从而使科学界能够相互比较他们的方法。</p><p>虽然许多人试图自动确定谣言的真实性价值，但鉴于分类器不可避免地会出现错误，仅仅输出真实性的最终决定的系统可能并不总是足够的。为了使真实性分类器的输出更加可靠，我们认为系统需要提供更丰富的输出，其中还包括决策的原因（Procter等人，2013b）。真实性分类器不仅输出自动确定的真实性分数，而且还链接到可以证实这一决定的来源，这将更加稳健，因为它将使用户能够评估分类器决定的可靠性，并且—如果发现想要忽略它。例如，可以通过使用立场分类器的输出来丰富真实性分类器的输出，选择一些支持和反对的观点，作为摘要呈现给用户。鉴于实现完全准确的真实性分类器是一个不太可能的目标，我们认为这个方向的研究应该特别关注寻找信息源，以促进终端用户对谣言的真实性做出自己的判断。</p><p>现有真实性分类系统的另一个注意事项是，它们侧重于确定真实性，而不考虑谣言是否得到解决。在谣言尚未解决的情况下，真实性分类任务就变成了预测任务，由于缺乏支持系统决策的证据，这对终端用户来说可能并不可靠。由于谣言具有未经证实的立场，确定其真实性很难，或者需要权威来源的参与，未来的研究应该研究谣言真实性确定的时间性，可能会尝试在找到证据后很快确定真实性。<br>谈到立场分类，最近的工作表明，利用社交媒体流和对话中的上下文来开发个人帖子立场的最先进分类器是有效的。然而，这个方向的研究仍处于起步阶段，还需要更多的研究来最好地利用这种背景来最大化立场分类器的性能。谣言分类的研究主要依赖于社交媒体帖子的内容，而从用户元数据和互动中提取的进一步信息可能有助于提高分类器的性能。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;谣言、虚假信息综述&quot;&gt;&lt;a href=&quot;#谣言、虚假信息综述&quot; class=&quot;headerlink&quot; title=&quot;谣言、虚假信息综述&quot;&gt;&lt;/a&gt;谣言、虚假信息综述&lt;/h1&gt;&lt;hr&gt;
&lt;h1 id=&quot;A-Survey-on-Natural</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>Learn Continually, Generalize Rapidly, Lifelong Knowledge Accumulation for Few-shot Learning</title>
    <link href="http://example.com/2021/10/30/Learn-Continually-Generalize-Rapidly-Lifelong-Knowledge-Accumulation-for-Few-shot-Learning/"/>
    <id>http://example.com/2021/10/30/Learn-Continually-Generalize-Rapidly-Lifelong-Knowledge-Accumulation-for-Few-shot-Learning/</id>
    <published>2021-10-30T03:32:36.000Z</published>
    <updated>2021-11-29T13:19:21.348Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Learn-Continually-Generalize-Rapidly-Lifelong-Knowledge-Accumulation-for-Few-shot-Learning"><a href="#Learn-Continually-Generalize-Rapidly-Lifelong-Knowledge-Accumulation-for-Few-shot-Learning" class="headerlink" title="Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning"></a>Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning</h1><p>随着时间的推移不断扩展知识，并利用这些知识迅速归纳到新的任务中，这是人类语言智能的一个关键特征。</p><p>现有的追求快速泛化到新任务的模型（如few-shot learning），大多是在固定的数据集上进行单次训练，无法动态地扩展其知识；而持续学习算法则不是专门为快速泛化设计的。</p><p>作者提出了一个新的学习设置，即 “ Continual Learning of Few-Shot Learners”（CLIF），以在一个统一的设置中解决这两种学习设置的挑战。</p><p>CLIF假设一个模型从一连串不同的NLP任务中依次学习，积累知识以提高对新任务的概括能力，同时也保留了之前学习的任务的性能。</p><p>本文研究了在持续学习设置中泛化能力是如何受到影响的，评估了一些持续学习算法，并提出了一种新颖的<strong>带有正则化的Adapter的双级超网络</strong>。</p><p><img src="https://i.loli.net/2021/10/30/3o5X2RKGhSDivAP.png" alt=""></p><p>挑战：模型在一连串的NLP任务中学习（逐一到达；不重复访问），然后在以下方面进行评估：（1）对新的（few-shot learning）任务的泛化；以及（2）保留其在解决已见任务上的性能。</p><blockquote><p>作者认为此类任务与LifeLong的区别:</p><p>此任务研究了NLP模型是否可以在一连串的任务中不断积累可归纳的知识，并迅速学习归纳到新的任务。</p><p>相关的工作是希望从连续到达的任务中学习，被称为持续学习（CL），主要关注的是当模型在新任务中被持续更新时，保留在所见任务中的表现。在后续的分析中，发现，现有的大多数CL方法几乎不利于模型的泛化能力，即使它们被证明可以缓解灾难性遗忘。</p></blockquote><hr><h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><h3 id="The-CLIF-Problem"><a href="#The-CLIF-Problem" class="headerlink" title="The CLIF Problem"></a>The CLIF Problem</h3><p>我们假设有一个NLP模型 $f$ 随着时间的推移在不同的任务上不断地训练（即持续学习），然后通过少量的例子迅速概括到许多未见过的任务（即few-shot适应）</p><p>在持续学习阶段，模型遇到一个有序的 $N_u$ 上游任务列表 : $[T_u^1,…,T^{N_u}_u]$ ，其中每个任务有自己的训练集和测试集。</p><p>为了测试连续选了的模型 $f$ 的 few-shot 学习能力，在一组单独的 $N<em>v$ 少量任务 ${T_v^i}</em>{i=1}^{N_v} $ 上对其进行adapt ，其中每个未见的任务只有几个训练样本。</p><p>在CLIF中，除了传统的CL目标是保持在所见任务上的性能外，在CLIF中，保持可概括的知识以在训练结束时获得更好的few-shot learning性能也是至关重要的。 </p><h3 id="Evaluation-Protocol"><a href="#Evaluation-Protocol" class="headerlink" title="Evaluation Protocol"></a>Evaluation Protocol</h3><p>如图所示，针对CLIF设置评估方法有三个主要方面：few-shot性能、最终性能和即时性能。</p><p><img src="https://i.loli.net/2021/10/30/b2W3lNhOngXmQv9.png" alt=""></p><ul><li><em>Few-shot Performance</em>: 首先，在一组未见过的任务上苹果持续训练的模型 $f$,  在上游任务 $T^1<em>u,…,T_u^{N_u}$ 训练结束后，用几个标注的样本对每个任务 $T_v^i$ 进行微调。因此，我们可以评估 few-shot 的泛化能力。把一个任务 $T_v^i$ 的 few-shot accuracy 记为 $s</em>{FS}^i = F(Y<em>v^i, \hat Y_v^i)$, 其中 $\hat Y_v^i$ 是对任务 $T</em>{v}^i$ 的测试样本进行预测， $Y<em>v^i$ 是真实标签。$F$ 是度量函数如accuracy。 记录所有few-shot 任务，例如：$s</em>{FS}= \frac{1}{N<em>v} \sum</em>{i=1}^{N<em>v} s</em>{FS}^i$ 。 还计算了在每个 few-shot 任务上单独训练的模型的相对改进 $\Delta_{FS}$</li><li><em>Instant Performance</em> : 在模型完成对上游任务 $T<em>u^i$ 的学习后，立即评估其性能，在模型$f$ 将任务 $j$ 学习为 $\hat Y</em>{u}^{i,j}$ 之后，记录在任务 $T<em>u^i$ 的测试集上的预测。 Instant performance 在任务 $T_u^i$ 上被定义为 $s</em>{inst.}^i = F(Y<em>u^i,\hat Y_u^{i,i})$ 。例如，模型 $f$ 在 $T_u^1$ 和 $T_u^2 $ 的数据上训练之后，在 $T_u^3$ 上进一步训练之前评估 $f$ 在 $T_u^2$ 上的性能。因此，$f$ 在 $T_u^2 $ 上的表现可以告诉我们，模型将其知识从学习 $T_u^1 $ 转移到学习 $T_u^2 $  的情况 —— 使用 $f$ 仅只在 $T_u^2 $ 上训练时的表现作为参考。我们计算所有上游任务的 Instant performance，$s</em>{inst.} = \frac {1}{N<em>u} \sum</em>{i=1}^{N<em>u} s</em>{inst.}^i $  ，此外还计算了相对于在每个上游任务上单独训练的改进 $\Delta_{inst.}$， 以表明上学学习的好处。</li><li><em>Final Performance</em> ：评估 $f$ 在对上游任务的持续学习结束时的表现，以了解模型 $f$ 在学习解决更多任务后对任务知识的遗忘程度。一个任务 $T<em>u^i $ 的最终 accuracy 被定义为 $F(Y_u^i,\hat Y_u^{i,N_u})$ 。同样地，我们报告了所有任务的平均最终准确度，记为 $s</em>{final} = \frac{1}{N} \sum<em>{i=1}^{N_u} s</em>{final}^i$。遗忘可以被量化为 $s<em>{inst} - s</em>{final}$ 。</li></ul><h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h3><p>CLIF 的设置对现有的 few-shot learning 方法来首特别具有挑战，大多数 few-shot 学习方法假定所有任务的上游数据集总是可用的，并且没有按时序去学习。因此，上游的任务可以在多任务学习的环境下共同学习。然而，CLIF问题采用的是持续学习的设置，即任务是按顺序访问的，没有重新访问。因此，依靠从任务分布中随机抽样的方法并不适用。</p><h3 id="Tasks-and-Data-Streams"><a href="#Tasks-and-Data-Streams" class="headerlink" title="Tasks and Data Streams"></a>Tasks and Data Streams</h3><p>为了将CLIF挑战推向一个更实际的设置，考虑了一组多样化的NLP任务来进行CL和few-shot learning。我们考虑了两个数据集的组合，被称为CLIF-26和CLIF-55任务：</p><p><img src="https://i.loli.net/2021/10/30/kdeHY4IvltMXyBw.png" alt=""></p><p>将CLIF-26中每个GLUE任务中的训练样本数量限制在10,000个，以避免数据集过度失衡。对于CLIF-55，每类使用90个样本进行连续学习。</p><p>在CLIF-26和CLIF-55的 few-shot 学习任务中，如果没有指定的话，每类使用 k=16 个样本，并在实验中包括更多的 k 的设置。由于GLUE的测试标签没有公开，仅报告了验证集的性能。</p><hr><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>首先介绍我们研究中的 baseline。然后，我们介绍一些现有的持续学习和持续元学习的方法。最后，提出了一个新颖的正则化双级适配器生成框架，以更好地解决CLIF问题。</p><h3 id="Base-NLP-Models"><a href="#Base-NLP-Models" class="headerlink" title="Base NLP Models"></a>Base NLP Models</h3><h4 id="BART-and-BART-Adapter"><a href="#BART-and-BART-Adapter" class="headerlink" title="BART and BART-Adapter"></a>BART and BART-Adapter</h4><p>由于将CLIF问题中的NLP任务制定为统一的文本到文本格式，我们使用预先训练好的语言模型（LM）作为模型f的架构，并在训练期间对整个模型进行微调。</p><p>我们还包括Adapter训练，作为对整个BART模型进行微调的一种改变。这里，适配器是插在BART每层之后的两层MLPs。</p><p>给出 transformer的第 $l$ 层的输出 $h_l$ , adapter的输出被计算为 $h_l’ = h_l + f_l^a(h_l)$， 其中 $f_l^a$ 是在 $l$ 层的adapter。只有adapter在训练中被学习，BART模型被frozen。</p><h4 id="Hyper-Networks-for-Adapter-Generation"><a href="#Hyper-Networks-for-Adapter-Generation" class="headerlink" title="Hyper-Networks for Adapter Generation"></a>Hyper-Networks for Adapter Generation</h4><p>除了BART和BART适配器之外，还使用考虑HyperNetwork（HNet）架构。HyperNetwork 记为 $g$ ，将任务表示 $z$ 作为输入，并生成另一个预测模型的模型参数，记为 $f$ 来解决该任务。在 few-shot learning 中，$z$ 通常被计算为任务的训练实例的平均表示，即 任务的平均表示: $z = \frac{1}{|D<em>{tr}^i|} \sum</em>{(x<em>j,y_j)\in D</em>{tr}^i} f<em>e(x_j, y_j) $ ，其中 $D</em>{tr}^i$ 是任务 $T^i$ 的训练集，$f_e$ 是encoder。</p><p>我们使用一个BART编码器作为 $f_e$，并将 $x$ 和标签 $y$ 的文本格式串联起来，得到任务表示 $z$。</p><h3 id="Baseline-Learning-Algorithms"><a href="#Baseline-Learning-Algorithms" class="headerlink" title="Baseline Learning Algorithms"></a>Baseline Learning Algorithms</h3><h4 id="Single-Task-Learning"><a href="#Single-Task-Learning" class="headerlink" title="Single Task Learning"></a>Single Task Learning</h4><p>为了了解基础模型在没有任何知识转移的情况下对上游任务的参考性能，应用了单一任务学习（STL）方法，该方法在每个任务的数据集上单独地训练和测试模型 $f$。</p><p>在这种情况下，我们忽略了CLIF问题的顺序性，所以我们可以用这个STL的性能来评估不同的持续方法（下面介绍）的有效性。理想情况下，一个有效的 CL 算法应该具有比 STL 结果更好的几率准确性，这意味着它积累了并有效地迁移了知识，用于学习。</p><p>同样地，为了了解 few-shot 任务的参考性能，我们在没有任何上游训练的情况下，为每个 few-shot 任务学习一个模型 $f$ ，这样我们就可以用这种性能来评估CLIF方法对泛化能力的改善程度。</p><h4 id="Continual-Learning-Algorithms"><a href="#Continual-Learning-Algorithms" class="headerlink" title="Continual Learning Algorithms"></a>Continual Learning Algorithms</h4><p>作为一种简单的基线方法，我们使用 Vanilla 表示简单地在上游任务上按顺序训练模型 $f$。</p><p>具体来说，它在 $T_u^i$ 上训练模型 $f$，直到其性能收敛，然后在 $T_u^{i+1}$ 的数据上不断训练 $f$。</p><p>请注意，CL 中不允许访问先前任务的数据，还考虑在实验中考虑 CL 算法，例如 EWC、MbPA++和 meta-MbPA。</p><p>EWC 正则化了训练过程中重要模型参数的变化，MbPA++ 方法对存储在内存中的几个训练样本执行测试 test-time 调整。 meta-MbPA 方法包括快速适应元学习目标。</p><h4 id="Hyper-Networks-for-CL"><a href="#Hyper-Networks-for-CL" class="headerlink" title="Hyper-Networks for CL"></a>Hyper-Networks for CL</h4><p>《Continual learning with hypernetworks》 提出了 hypernetwork-based continual learning。其中减轻灾难性遗忘的高级想法是惩罚超网络在其学习新任务时为先前任务生成的模型权重的改变。虽然原始工作生成模型的整个参数，但我们仅通过生成适配器的权重来使其适应 PTLMs。 将这种方法记为 HNet-Reg。</p><p>具体来说，当模型刚刚完成学习任务 $T<em>{u}^{i-1}$ 并且在持续学习阶段学习任务 $T_u^i$ 之前，我们存储当前超网络为所有先前任务 $T_u^1…T_u^{i=1}$ 生成的适配器权重，记为 ${\hat\theta_1^{i-1},\hat\theta_2^{i-1},…,\hat\theta</em>{i-1}^{i-1}}$ ，其中生成是通过超网络 $h$ 应用于先前任务 $1,..,{i-1}$ 的存储任务表示来控制的，记为 $M = {z_h^1,…,z_h^{i-1}}$ 。在这里，任务 $T_u^i$ 的任务表示 $z_i$ 在学习任务之前随机初始化，并在学习任务时联合优化。</p><p>然后，在学习 $T_u^i$  的每一步中，我们随机抽样一个先验任务 $T_u^j \ \ (j &lt; i)$ 来规范超网络学习。 它惩罚在当前步骤 $\theta_j$ 生成的适配器权重与预先计算的权重之间的 $l_2$ 距离，例如 $||\theta_j-\hat \theta_j^{i-1}||_2^2$</p><p>因此，避免了超网络 g 在持续学习阶段过多地改变其先前任务的输出，从而更好地保证学习模型的知识积累。</p><h4 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h4><p>EWC 和 HNET-Reg 不是为 CLIF 问题精心设计的，CLIF还试图在持续学习后改进对未知任务的 few-shot 泛化。 虽然 MbPA 和 meta-MbPA 中的 test-time 适应可能有利于 few-shot learning，但这些工作并未研究这种能力。 此外，由于这两种算法存储了先前训练任务的真实数据，因此不适用于无法再访问来自早期任务的数据的隐私敏感应用，这是持续学习中的典型场景。</p><h3 id="Our-Extension-Bi-level-Hypernetworks-for-Adapters-with-Regularization"><a href="#Our-Extension-Bi-level-Hypernetworks-for-Adapters-with-Regularization" class="headerlink" title="Our Extension: Bi-level Hypernetworks for Adapters with Regularization"></a>Our Extension: Bi-level Hypernetworks for Adapters with Regularization</h3><p>受用于 few-shot 和 CL的超网络方法的启发，我们将基于超网络的CL方法扩展到CLIF。我们提出了一种新的方法，即带有正则化的双级超网络Adapters（BiHNet+Reg），该方法学习使用双级任务表示来生成Adapters权重，以便在一连串的任务中学习快速适应模型，同时通过正则化来减轻遗忘效应。</p><p>方法由三个组件组成：</p><p><img src="https://i.loli.net/2021/10/30/usniAeI3GEyHXL7.png" alt=""></p><ul><li>Context Predictor 上下文预测器，从训练实例中生成双级任务表征（即高资源和few-shot表征）</li><li>Adapter-Wise Hypernetworks 超网络，根据任务表征生成适配器的权重；</li><li>Regularization 正则化项，阻止所见任务的权重变化以避免遗忘</li></ul><h4 id="Context-Predictor"><a href="#Context-Predictor" class="headerlink" title="Context Predictor"></a>Context Predictor</h4><p>为每个任务 $t$ 生成两个任务表征，分别在高资源和 few-shot 的情况下为其建模，表示为 $z_h^t$ 和 $z_f^t$，用frozne BART编码器。高资源表征用于鼓励持续学习过程中的知识转移；few-shot 任务表征帮助我们在 few-shot learning 模仿 few-shot任务，以获得更好的泛化，类似于元学习。</p><p>然后，高资源任务表示被计算为任务 $t$ 中所有样本的上下文向量的平均值。 记为：$z<em>h^t = \frac{1}{|D_t|} \sum</em>{(x_i,y_i)\in D_t} R(x_i, y_i)$</p><p>然而，few-shot 任务表示 $z<em>f^t$ 使用有限数量 K 个采样样本 $z_f^t = \frac{1}{k} \sum</em>{(x<em>i,y_i)\in \Tau(D_t, K)} R(x_i, y_i)$， 其中$\Tau(D_t,K)$ 是在 $D_t$ 中采样K 个样本。请注意，在不断的学习过程中，上游任务的高资源表征被长期储存在一个记忆模块中，$M={z_h^t| t\in {\Tau_u^i}</em>{i=1}^{N_u}}$ 。在few-shot的学习阶段，我们设定 K为给定的样本的数量，因此对于任何任务，$z_h=z_f$。</p><h4 id="Adapter-Wise-Hypernetworks"><a href="#Adapter-Wise-Hypernetworks" class="headerlink" title="Adapter-Wise Hypernetworks"></a>Adapter-Wise Hypernetworks</h4><p>使用超网络 $g$ 来生成frozen BART模型 $f$ 的各层之间的适配器的权重。</p><p>在训练过程中，使用高资源和采样的任务表征 $z_h^t$ 和 $z_f^t$ 来产生适配器权重 分别记为 $\theta_t^h$ 和 $\theta_t^f$。我们对这两个适配器的预测损失进行了优化。</p><h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p>HyperNetwork是模型中唯一可训练的部分，对生成的适配器施加正则化以减轻遗忘。</p><p>虽然 BiHNet 被训练为从高资源和低资源任务表示生成适配器，但发现仅存储和正则化来自高资源任务表示的输出就足够了。</p><h4 id="Summary-and-Highlights"><a href="#Summary-and-Highlights" class="headerlink" title="Summary and Highlights"></a>Summary and Highlights</h4><p>总而言之，提出的方法首先生成了双级任务表征，用于训练具有正则化项的适配超网络，以避免随时间推移而遗忘。</p><p>与基于重放记忆的CL方法（例如MbPA）不同，我们的方法不存储任何真实的训练实例。相反，它使用任务表示来存储记忆，因此允许该方法应用于对隐私敏感的场景中。</p><hr><h2 id="Results-and-Analysis"><a href="#Results-and-Analysis" class="headerlink" title="Results and Analysis"></a>Results and Analysis</h2><p>在本节中，将讨论两个主要研究问题：</p><ul><li>考虑到潜在的灾难性遗忘，与离线设置相比，模型如何在CL设置中长期积累可推广的知识？</li><li>持续学习的方法是否能减少对所见任务的表现和可归纳知识的灾难性遗忘。</li></ul><p>作者在试验了各种模型架构的组合和学习算法。通过其模型结构和应用的CL算法来说明一种方法，例如BART-Vanilla, BiHNet-EWC。</p><p><img src="https://i.loli.net/2021/10/30/M9NOlJoS64AYhGy.png" alt=""></p><h3 id="Examining-Knowledge-Accumulation"><a href="#Examining-Knowledge-Accumulation" class="headerlink" title="Examining Knowledge Accumulation"></a>Examining Knowledge Accumulation</h3><p>在这一节中，提出了对模型在离线和CL设置中获得可归纳知识的能力的分析。</p><p>我们注意到BiHNet方法，对应于学习生成适配器，应与BiHNet-Single和BART-Adapter-Single进行比较，后者是零知识基线，其学习生成或从随机初始化中学习适配器；</p><p>同样，BART方法应与BART-Single进行比较。重点是确定CLIF的挑战，并将方法论的讨论留在下一小节。</p><h4 id="问题1：来自上游任务的知识是否有助于模型在在线学习和持续学习设置中的few-shot泛化？"><a href="#问题1：来自上游任务的知识是否有助于模型在在线学习和持续学习设置中的few-shot泛化？" class="headerlink" title="问题1：来自上游任务的知识是否有助于模型在在线学习和持续学习设置中的few-shot泛化？"></a>问题1：来自上游任务的知识是否有助于模型在在线学习和持续学习设置中的few-shot泛化？</h4><p>看表2，在CLIF-26和CLIF-55数据集上，我们看到BiHNet-MTL在 few-shot 情况下的表现比零知识基线要好0.4%和1.0%，这意味着在标准的离线学习设置中，上游任务对few-shot 情况下的泛化有帮助。</p><p>对于BART模型，我们注意到BART-MTL在Clif-55数据集上比BART-Single提高了2.5%。然而，我们注意到CLIF-26的情况正好相反。鉴于在这些模型中整个BART参数都被优化了，我们假设BART-MTL可能受到了预训练的BART模型本身的知识遗忘的影响；而在适配器和BiHNet模型中，BART模型被冻结了。</p><p>因此，在本节的其余部分，我们更关注 侧重于BiHNet方法。</p><h4 id="问题2：模型的泛化能力是如何随时间变化的？"><a href="#问题2：模型的泛化能力是如何随时间变化的？" class="headerlink" title="问题2：模型的泛化能力是如何随时间变化的？"></a>问题2：模型的泛化能力是如何随时间变化的？</h4><p><img src="https://i.loli.net/2021/10/30/GJhbKFcu3sAHVYj.png" alt=""></p><p>我们专注于BiHNet-Vanilla和BART-Vanilla方法，并回答三个子问题。</p><ul><li><p><strong>知识是否在上游任务中被单调地积累？</strong>与两个零知识基线相比，我们注意到BiHNet-Vanilla普遍提高了即时准确率（在CLIF-26上为6.5%，在CLIF-55上为8.9%）和少数次准确率（在CLIF-55上为0.8%），但在CLIF-26上的few-shot准确率除外（-0.4%）。这些结果在一定程度上证实了积极的知识积累。在图4中，我们绘制了模型依次访问每个上游训练任务时在CLIF-26上的few-shot精度。我们注意到BiHNet-Vanilla的几率并没有单调地增加，这意味着这些上游学习任务之间的干扰或对可概括的知识的遗忘。</p></li><li><p><strong>任务的顺序是否重要？</strong>图5显示了在CLIF-26上不同任务顺序下的方法性能。我们通过增加和减少与few-shot学习任务的相关性来排列任务，其中相关性被定义为模型从单一上游任务转移时的few-shot准确性。结果显示，在这两个顺序中，BiHNet-Vanilla的竞争力都不如BART- Adapter-Single。这意味着在持续学习中，如果没有CL算法，知识积累就不那么稳健。</p><p><img src="https://i.loli.net/2021/11/29/og8kGxFzUNDpP7H.png" alt=""></p></li></ul><h4 id="问题3：模型的灾难性遗忘是否阻碍了其知识积累？"><a href="#问题3：模型的灾难性遗忘是否阻碍了其知识积累？" class="headerlink" title="问题3：模型的灾难性遗忘是否阻碍了其知识积累？"></a>问题3：模型的灾难性遗忘是否阻碍了其知识积累？</h4><p>在表2中，我们看到Vanilla和MTL方法的最终准确率之间存在明显的差异（大约20分），这验证了当训练实例不是i.i.d.时对所见任务性能的灾难性遗忘。然而，我们发现MTL和Vanilla训练之间的差距对于 few-shot 学习性能是接近的，其中BART-Vanilla甚至比BART-MTL更好，这可能是充分遗忘缓解过拟合的一个积极结果（王等人，2020）。<strong>这表明灾难性遗忘对泛化能力的影响与它对所见任务表现的影响相比</strong>，程度较小。</p><h3 id="Effect-of-Continual-Learning-Algorithms"><a href="#Effect-of-Continual-Learning-Algorithms" class="headerlink" title="Effect of Continual Learning Algorithms"></a>Effect of Continual Learning Algorithms</h3><p>有了对前面问题的认识，我们现在分析基线持续学习算法和所提出的方法是否有助于知识积累和提高模型的（few-shot）f泛化能力。</p><h4 id="问题1：持续学习算法能缓解灾难性遗忘吗？"><a href="#问题1：持续学习算法能缓解灾难性遗忘吗？" class="headerlink" title="问题1：持续学习算法能缓解灾难性遗忘吗？"></a>问题1：持续学习算法能缓解灾难性遗忘吗？</h4><p> 从表2中，我们注意到MbPA++、meta-MbPA、EWC在CLIF-26上明显比 BART-Vanilla 或 BiHNetVanilla 提高了最终准确率，这证实了对缓解灾难性遗忘的积极作用。 在CLIF-55上，其特点是训练任务更多，每个任务的样本更少，我们发现基线CL算法未能提高最终的准确性。对于基于记忆的方法，如MbPA++和meta-MbPA，这可能是因为对存储的例子有明显的过度拟合。相比之下，BiHNet-Reg在两个数据集中都很有效。</p><h4 id="问题2：缓解灾难性遗忘能更好地保留泛化能力吗？"><a href="#问题2：缓解灾难性遗忘能更好地保留泛化能力吗？" class="headerlink" title="问题2：缓解灾难性遗忘能更好地保留泛化能力吗？"></a>问题2：缓解灾难性遗忘能更好地保留泛化能力吗？</h4><p>通过比较BiHNet-Vanilla和BiHNet-Reg的 few-shot 准确性，我们发现在两个数据集上，few-shot 准确性和即时准确性分别提高了1.9%和4.2%。从图5中，我们看到BiHNet-Reg在默认的和递减的相关性顺序中优于BiHNet-Vanilla；而我们观察到BiHNet-Reg在递增的相关性顺序中出现了异常。从图4中，我们看到随着BiHNet-Reg学习更多的上游任务，few-shot的学习精度提高得更加稳定。</p><h4 id="问题3：BiHNet-REG比HNet-REG有改进吗？"><a href="#问题3：BiHNet-REG比HNet-REG有改进吗？" class="headerlink" title="问题3：BiHNet-REG比HNet-REG有改进吗？"></a>问题3：BiHNet-REG比HNet-REG有改进吗？</h4><p>BiHNet-Reg与HNet-Reg（Oswald等人，2020）的主要区别是：（1）few-shot 任务表征；（2）用上下文预测器推断任务表征，而不是将其作为可训练的嵌入学习。作为一项消融研究，我们逐步替换掉BiHNet中的两个组件，如表3所示。我们看到，在两个数据集上，去掉 few-shot 任务表示会导致 few-shot 准确率下降1.08和0.33个点；而用可训练的任务嵌入取代上下文预测器会导致最终准确率明显下降10个点以上。我们注意到，在CLIF-26上，可训练的em-beddings的几率略高1.5分，但在CLIF-55上则低了2.3分，因为它有更多的上游训练任务。</p><p><img src="https://i.loli.net/2021/11/29/qG7y1uslxtZdrNM.png" alt=""></p><h4 id="问题4：敏感度分析：模型如何在不同数量的few-shot训练样本下执行。"><a href="#问题4：敏感度分析：模型如何在不同数量的few-shot训练样本下执行。" class="headerlink" title="问题4：敏感度分析：模型如何在不同数量的few-shot训练样本下执行。"></a>问题4：敏感度分析：模型如何在不同数量的few-shot训练样本下执行。</h4><p>图6总结了不同方法在CLIF-26上每类不同数量的训练实例下的几率表现。我们观察到BiHNet-Reg总是能达到最好的性能，而且当训练集较小时，改进更为显著。</p><p><img src="https://i.loli.net/2021/11/29/IhUeVGqH96WvJRQ.png" alt=""></p><p>讨论。我们的研究结果表明，与类似的适配器学习框架（BiHNet-Single和BART-Adapter-Single）相比，BiHNet-Reg可以有效地提高知识积累的时间。然而，BiHNet-Reg并不能与BART-Single的几次学习准确性相媲美。我们认为这是由于适配器的模型容量有限，与整个transformer的微调相比。这为改进与PTLM微调兼容的持续学习算法开辟了未来的工作。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Continual-Learning"><a href="#Continual-Learning" class="headerlink" title="Continual Learning"></a>Continual Learning</h3><p>CL文献中涉及的主要挑战是克服解决灾难性的遗忘。一般来说，现有的持续学习方法包括基于记忆和生成重放的方法（Robins，1995；Lopez-Paz和Ranzato，2017；Shin等人，2017）、基于正则化的方法（Kirkpatrick等人，2017；Nguyen等人，2018）和基于模型扩展的方法（Shin等人，2017）。最近，持续学习在NLP领域引起了关注（Sun等人，2020；Wang等人，2019b；Huang等人，2021）。</p><h3 id="Continual-Meta-Learning"><a href="#Continual-Meta-Learning" class="headerlink" title="Continual Meta Learning"></a>Continual Meta Learning</h3><p>有文献研究了NLP应用之外的持续元学习，对问题有各种定义。</p><p>一些前期工作目的是开发一种算法，当早期任务的少数训练实例在测试时再次可用时，可以快速恢复以前的性能。</p><p>Caccia等人（2020）提出了一种设置，即模型访问一连串可能重新出现的任务，并测量在线累积性能作为衡量标准。Antoniou等人（2020）假设模型访问了一连串的  few-shot 照片的分类任务，而测试任务由训练时看到的类组成。Jerfel等人（2019）的问题设置与我们的问题设置最为相关，我们的问题设置可以更好地在新任务上进行  few-shot 照片的学习，但只针对图像分类任务进行研究，任务数量少得多。据我们所知，我们的工作是第一个研究在不同的NLP任务中对大规模转化器模型进行少数次学习的持续知识积累。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>我们提出了 “少数学习者的持续学习”（CLIF）挑战，以模拟学习者在一连串的NLP任务中不断积累（可归纳的）知识，同时保持其在所见任务中的表现的情景。我们提出了评估协议来研究现有的持续学习算法的性能，并介绍了我们的方法BiHNet-Reg。我们展示了建立一个NLP系统的潜力，该系统通过持续的训练，可以完成更多的任务，并且在掌握新任务方面变得更有效率。未来的工作包括将我们的工作扩展到数据分布可能不断变化的任务无关的场景，以及研究用新出现的数据不断完善大规模预训练模型的算法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Learn-Continually-Generalize-Rapidly-Lifelong-Knowledge-Accumulation-for-Few-shot-Learning&quot;&gt;&lt;a href=&quot;#Learn-Continually-Generalize-R</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>Pattern Exploiting Training (PET)</title>
    <link href="http://example.com/2021/10/28/Pattern-Exploiting-Training-PET/"/>
    <id>http://example.com/2021/10/28/Pattern-Exploiting-Training-PET/</id>
    <published>2021-10-28T02:31:26.000Z</published>
    <updated>2021-10-30T03:39:47.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pattern-Exploiting-Training-PET"><a href="#Pattern-Exploiting-Training-PET" class="headerlink" title="Pattern Exploiting Training (PET)"></a>Pattern Exploiting Training (PET)</h1><p>介绍PET范式，可用于半监督或无监督训练。</p><p>这篇主要关注两篇相同作者的文章：</p><p>《Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference》</p><p>《It’s Not Just Size That Matters：Small Language Models Are Also Few-Shot Learners》</p><p>首先看到一个问题比较好：<strong>BERT在预训练时学习到的知识或者说参数我们在fine-tunning的时候都有用到吗？</strong></p><p>答案是不是的。</p><p>BERT的预训练其中一个任务是MLM，就是去预测被 【MASK】掉的token，采用的是拿bert的最后一个encoder（base版本，就是第12层的encoder输出，下图左图蓝色框）作为输入，然后接全连接层，做一个全词表的softmax分类（这部分就是左图的红色框）。但在finetuing的时候，我们是把MLM任务的全连接层抛弃掉，在最后一层encoder后接的初始化层来做具体下游任务。</p><p><img src="https://i.loli.net/2021/10/28/c31HAsXB5QbPklt.png" alt=""></p><p>MLM目标是预测 输入时被挑选的15%的单词，所以在BERT的最后一层（如BERT-base版本就是第12层）的token的embedding后会接一个【embedding维度，词表大小】的全连接矩阵，做token的预测，这个全连接矩阵就是MLM层参数</p><p>问题是，<strong>能不能通过某些巧妙的设计，把MLM层学习到的参数也利用上？</strong></p><blockquote><p>注意，Prompt设计的这种完形填空和MLM任务是有区别的，二者虽然都是都是词分类，但是候选集不同，MLM的候选词是整个词库，prompt是verbalizer里的词。Prompt使用MLM层把其他的词给忽略掉。</p></blockquote><p>答案当然是可以的，请继续往下看。</p><p>现在举一个二分类的例子，输入一条汽车论坛的评论，输出这个评论是属于【积极】or【消极】。但问题是现在我每个类别只有10个labeled数据，1K条unlabeled数据。怎么训练model？</p><p>直接做有监督训练?样本量太少，会过拟合。应该优先采用半监督学习的方法，如UDA、MixText这种，而PET采用的是另外一种巧妙的设计思想。</p><p>对于”I love this movie”这句输入，可以在后面加上Prompt也就是Pattern：”the movie is <em>_</em>“，组成如下这样一句话：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">I love this movie, the movie is ___</span><br></pre></td></tr></table></figure><p>然后让预训练模型用表示情感的答案（例如”great”、”terrible”等）做完形填空，最后再将该答案转换为情感分类的标签。这里定义一个<strong>verblizer</strong>作为映射函数，把label【great】映射为+，把label【terrible】映射为- 。</p><p>这样一来，我们就可以通过构造合适的「模板」，控制模型的输出空间，从而训练一个完全无监督的预训练模型来解决各种各样的下游任务。<strong>BERT预训练时的MLM层的参数能利用上</strong>。而且，<strong>即使model没有进行fine tunning，这个model其实就会含有一定的准确率</strong>！</p><p>Pattern和verblizer，就是一个PVP（pattern-verbalizer pairs）。</p><h2 id="Prompt-Notation"><a href="#Prompt-Notation" class="headerlink" title="Prompt Notation"></a>Prompt Notation</h2><p>设 $M$ 是被mask的语言模型，$V$ 是它的词表，$[MASK]$ 也包含在词表中。令 $L$ 为目标分类任务 A 的一组标签。</p><p>我们把任务 A 的输入写成一串短语 $x=(s_1,…,s_k)$，其中 $s_i\in V$.</p><p>例如如果 $A$ 是文本推理（两个话虽然句子），$k=2$</p><p>我们将pattern定义为一个函数 $P$, 它以 $x$ 为输入，输出一个短语或句子 $P(x)\in V^*$, 其中正好包含一个MASK标记，也就是说，它的输出可以被看作是一个完形填空问题。</p><p>此外，将 verbalizer 定义为一个注入函数 $v:L\to V$, 它将每个标签映射到 $M$ 的词表中的一个词。</p><p>令 $p=(P,v)$  是 PVP（pattern-verbalizer pairs）</p><p>我们假设可以访问较小的训练集 $T$ 和 通常大得多的一组无标签数据 $D$ 。</p><p>对于每个恰好包含一个MASK标签和 $w\in V$ 的序列 $z\in V^*$, 用 $M(w|z)$ 表示语言模型在掩码位置赋予$w$ 的非标准化分数。给定某个输入 $x$，我们将标签 $l\in L$的得分定义为:</p><script type="math/tex; mode=display">s_p(l|x) = M(v(l) | P(x))</script><p>并使用Softmax获得标签上的概率分布 :</p><script type="math/tex; mode=display">q_p(l|x) = \frac{e^{s_p(l|x)}}{\sum_{l'\in L} e^{s_p(l'|x)}}</script><h2 id="Auxiliary-Language-Modeling"><a href="#Auxiliary-Language-Modeling" class="headerlink" title="Auxiliary Language Modeling"></a>Auxiliary Language Modeling</h2><p>只有几个训练示例可用，可能会发生灾难性的遗忘。</p><p>由于现在是用MLM做分类任务，所以可以引入无标注数据一起训练！</p><p>举个简单的例子，下图样例1是labeled数据，我们利用pattern把它改写后，对 __ 部分做完形填空预测（即MLM任务）。</p><p>样例2是一个unlabeled数据，我们就不对  __ 部分做预测，而是对被【MASK】做预测。这里的【MASK】可以采用BERT的方法，随机对句子的15%token进行【MASK】。</p><p><img src="https://i.loli.net/2021/10/28/daK2yEYcXFSoN9u.png" alt=""></p><p>训练时两个损失联合训练：</p><script type="math/tex; mode=display">L = (1-\alpha) \cdot L_{CE} + \alpha \cdot L_{MLM}</script><p>由于 $L<em>{MLM} $ 通常比 $L</em>{CE}$ 大得多，在初步实验中，发现$α=10^{-4}$的值能给出良好的结果</p><p>这样做的好处是，能让model更适应于当前的任务，有点像<strong>在预训练模型上继续根据任务的domain和task继续做预训练，然后再做fine-tunning呢？</strong></p><h2 id="Combining-PVPs"><a href="#Combining-PVPs" class="headerlink" title="Combining PVPs"></a>Combining PVPs</h2><p>引入一个问题，<strong>怎么评价我们的pattern定义得好不好？</strong></p><p>我们可以造两个pattern，又可以造两个verblizer。其实一共有4个PVP。我们怎么衡量哪一个PVP训练完后在测试集上的效果最好？</p><p>答案是我们也不知道，因为<strong>我们不能站在上帝视角从一开头就选出最佳的PVP，同样由于是小样本学习，也没有足够的验证集让我们挑选最佳的PVP</strong>。既然如此，解决方式就是<strong>知识蒸馏</strong>。</p><p>具体的，我们用20个labeled数据训练4个PVP模型，然后拿这四个PVP模型对1K条unlabeled数据进行预测，预测的结果用下式进行平均。</p><script type="math/tex; mode=display">s_M(l|x) = \frac{1}{Z} \sum_{p\in P} w(p) \cdot s_p(l|x)</script><p>其中 $Z$ 保持概率和为1， $s_p(l|x)$ 就是单个PVP模型对样本预测的概率分布，$w(p)$ 就是PVP的权重。</p><p>有uniform和weighted两种方式，uniform就是所有PVP的权重都为1，weighted就是把每个PVP的权重设置为它们在训练集上的准确率。最后还要对上式进行<strong>temperature=2</strong>的软化。</p><p>这就是在做知识的蒸馏。<strong>何谓知识的蒸馏？</strong>经过这样处理后，噪声减少了，利用多个PVP平均的思想把某些本来单个PVP预测偏差比较大的进行平均后修正。</p><p>这样子，利用训练好的PVPs所有1K条unlabeled数据打上soft label，再用这1K条打上软标签的数据进行传统的有监督训练，训练完的model应用于下游任务的model。</p><blockquote><p>注意哦，这里就可以用<strong>轻量的模型</strong>来做fine tuning了哦，因为从20条labeled数据扩充到1K条有带有soft label的数据，labeled数据量大大增加，这时候轻量级的模型也能取得不错的结果，而且轻量模型对轻量部署、高并发等场景更加友好。</p></blockquote><p>下图就是所有的流程，再总结一下步骤就是</p><p><img src="https://i.loli.net/2021/10/28/iyWJmp32kv8uQeU.png" alt=""></p><ul><li>第一步先定义PVPs，然后对每对PVP用labeled数据进行单独的训练，该步可以加入上面提到的Auxiliary Language Modeling一起训练</li><li>第二步：用训练好的PVPs，对unlabled数据进行预测，并知识蒸馏，得到大量的soft label；</li><li>第三步：用第二步得到的带有soft label的data，用传统的fine tuning方法训练model。</li></ul><h2 id="IPET"><a href="#IPET" class="headerlink" title="IPET"></a>IPET</h2><p>将所有单个模型的知识提炼到单个分类器C中意味着它们不能相互学习。由于一些 pattern 的表现(明显地)比其他模式差，因此最终模型的训练集 $T_C$可能包含许多标记错误的示例。</p><p>在每个PVP训练的过程中，互相之间是没有耦合的，就是没有互相交换信息，IPET的意思就是想通过迭代，不断扩充上面训练PVP的数据集。</p><p>这里简单举个例子，现在有20个labeled数据，1K个unlabeled数据，定义5个PVP，</p><p>第一轮，利用20个labeled数据分别训练PVP，第二轮，用第2~4个PVP来预测这1K unlabeled数据，然后选一些模型预测概率比较高的加入到第一个PVP的训练集上，同样用第1、3、4、5个PVP来训练这1K条，然后也将这部分加入到第2个PVP的训练集中，然后再训练一轮，训练后，重复，这样每一轮每个PVP的训练样本不断增多，而且PVP之间的信息也发生了交互。</p><p><img src="https://i.loli.net/2021/10/28/lzncwRWb5ovF93e.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://i.loli.net/2021/10/28/faoicY2BVnuyI9l.png" alt=""></p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><h3 id="Combining-PVPs-1"><a href="#Combining-PVPs-1" class="headerlink" title="Combining PVPs"></a>Combining PVPs</h3><p>作者发现不同PVP之间可能有很大的性能差别，如下图min就是最差的PVP，max就是最好的PVP，可以观察到它们之间的差别就很大。但是又不能站在上帝视角从一开始就选择最好的PVP，所以办法就是做commind PVPs，即上面所提到的知识蒸馏，而且发现蒸馏后会比采用单个最好的PVP效果还要好，并且发现uniform和weighted两个方法效果差不多。</p><p><img src="https://i.loli.net/2021/10/28/UkDHrpN1St9q8hR.png" alt=""></p><h3 id="Auxiliary-Language-Modeling-1"><a href="#Auxiliary-Language-Modeling-1" class="headerlink" title="Auxiliary Language Modeling"></a>Auxiliary Language Modeling</h3><p>labeled数据越少，auxiliary task的提升效果越明显。</p><p><img src="https://i.loli.net/2021/10/28/JtsafNYc1BxU9V2.png" alt=""></p><h3 id="Iterative-PER"><a href="#Iterative-PER" class="headerlink" title="Iterative PER"></a>Iterative PER</h3><p>iPET的效果，因为iPET是迭代多轮，每一轮每个PVP的训练集都会增大，从图可以看到每一轮的模型效果都是越来越好的。</p><p><img src="https://i.loli.net/2021/10/28/hVICaP1rKk62fdZ.png" alt=""></p><h3 id="In-Domain-Pretraining"><a href="#In-Domain-Pretraining" class="headerlink" title="In-Domain Pretraining"></a>In-Domain Pretraining</h3><p>这里讨论了一个问题：PET效果比有监督训练好，是不是因为PET在大量无标签上打上软标签，扩大了有标签数据集？</p><p>然后作者做了一个实验，有监督训练时，先在所有数据集上进行继续预训练（这一步作者认为相当于把无标签数据也加进来了），然后再fine funing。实验结果表明，即使这样，有监督效果也离PET有一定距离。</p><p><img src="https://i.loli.net/2021/10/28/Ftm58XIxSDTbj9Z.png" alt=""></p><h2 id="It’s-Not-Just-Size-That-Matters：Small-Language-Models-Are-Also-Few-Shot-Learners"><a href="#It’s-Not-Just-Size-That-Matters：Small-Language-Models-Are-Also-Few-Shot-Learners" class="headerlink" title="It’s Not Just Size That Matters：Small Language Models Are Also Few-Shot Learners"></a>It’s Not Just Size That Matters：Small Language Models Are Also Few-Shot Learners</h2><p>这篇论文是上篇论文的延伸，其实没有太多新的工作，主要是下面提到的处理多个token的mask，这篇论文主要PK GPT3，不断diss GPT3有多少的不环保。</p><h2 id="PET-with-Multiple-Masks"><a href="#PET-with-Multiple-Masks" class="headerlink" title="PET with Multiple Masks"></a>PET with Multiple Masks</h2><p>PET要定义pattern和verblizer，还拿那汽车评论场景举例，我们能不能定义一个verbilzer，它把不同label映射到长度不一的token，如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">定义 Pattern: s。真__!  这里s代表原始输入。  </span><br><span class="line">定义Verbilzer,  v(积极)&#x3D;好,   V(消极)&#x3D;不好</span><br><span class="line">样例 x:   保养贵，配件贵，小毛病多，还有烧机油风险。(label为消极)</span><br><span class="line">pattern(x) &#x3D; 保养贵,配件贵,小毛病多,还有烧机油风险，真__!</span><br></pre></td></tr></table></figure><p>因为verbilzer把标签映射到长度不一致的token，那我们究竟定义长度为多少的下划线<em>_</em>，来让model进行完形填空。答案是用最长的那个，例如这里最长的是”不好”，长度为2，所以就挖空两个下划线来让模型做完形填空预测。</p><p>做Inference时，</p><ul><li>$p(\text{积极}|x) =$ 第一个下划线__ 模型预测到token为好的概率。</li><li>$p(\text{消极}|x)= $  就麻烦一些，先让模型对两个下划线，进行预测，看是第一个下划线预测token为不，还是第二个下划线预测token为好的概率高一些，把高的那个token先填上去，再重新预测剩下的。举个例子，假如模型预测第一个下划线token为不的概率是0.5，第二个下划线token为好的概率为0.4，即先把不填上第一个下划线，然后再用模型重新预测第二个token为好的概率，假如为0.8，即  $p(\text{消极}|x)= 0.5*0.8=0.4$  </li></ul><p>做train时，就不考虑这么细致了，具体的，取上面的例子为例，</p><ul><li>$p(\text{积极}|x) =$ 第一个下划线__，模型预测到token为好的概率，跟inference是一样的</li><li>$p(\text{消极}|x)= 0.5*0.4 = 0.2$ ，这里就不分成两步，一步KO，目的是一次前向计算就算完，避免训练过慢。  </li></ul><p>最后，采用的损失函数也跟第一篇的不一样，这里用的是hinge loss，详细的请看论文。</p><script type="math/tex; mode=display">\sum_{y'\in Y_x} max(0; 1-log\hat q_p (y|x) + log\hat q_p(y'|x))</script><h3 id="Unlabeled-Data-Usage"><a href="#Unlabeled-Data-Usage" class="headerlink" title="Unlabeled Data Usage"></a>Unlabeled Data Usage</h3><p>还是下面这幅图，这里讨论了unlabeled数据的利用。</p><p>在PET利用到unlabeled数据的有三个地方：</p><ul><li>第一处：PET的第二步，用PVPs对unlabeled数据进行知识蒸馏，给数据打上soft label，然后第三步利用这些软标签训练一个模型；</li><li>第二处：PET的第一步，假如用的是iPET的话，每一个generation都会把部分的无标签数据打上标签，加入到PVP的训练集；</li><li>第三处：PET的第一步，假如采用的是Auxiliary Language Modelling辅助训练，也会引入无标签数据。</li></ul><p>首先，讨论上面的第一点，究竟能不能直接用PET训练的第一步的PVPs来做预测，这样就不用给unlabeled数据打软标签了（因为虽然说unlabeled数据比labled数据容易获得，但某些场景下unlabeled数据也有可能是拿不到的），答案是可以的，大家看下表的倒数两列，发现不用PET训练的第二、第三步，直接采用第一步训练好的PVPs来做下游应用的预测，效果也是OK的。</p><p><strong>只不过，这样做的话，你应用于下游任务的时候就是一堆PVP模型，而不是单一个模型了，这样对轻量部署不是很友好</strong>。</p><p><img src="https://i.loli.net/2021/10/28/SwGde9gKBF5t1xb.png" alt=""></p><p>还讨论了上面的第二处，发现iPET训练过程中，每一个generation从unlabeled数据中挑选部分加入到PVP的训练集，能让PVP收敛更快，减少不稳定性。</p><p><img src="https://i.loli.net/2021/10/28/SeqEI7XiB2rgCVl.png" alt=""></p><h3 id="Model-Type"><a href="#Model-Type" class="headerlink" title="Model Type"></a>Model Type</h3><p>不同预训练模型的影响，像BERT这种双向的语言模型会比GPT这种单向的要好，因为假如采用的是单向的语言模型，那么pattern的下划线__部分只能放在句子末尾进行预测。</p><p><img src="https://i.loli.net/2021/10/28/73phCBVNdwszAtf.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Pattern-Exploiting-Training-PET&quot;&gt;&lt;a href=&quot;#Pattern-Exploiting-Training-PET&quot; class=&quot;headerlink&quot; title=&quot;Pattern Exploiting Training (P</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>Parameter-Efficient Transfer Learning for NLP</title>
    <link href="http://example.com/2021/10/23/Parameter-Efficient-Transfer-Learning-for-NLP/"/>
    <id>http://example.com/2021/10/23/Parameter-Efficient-Transfer-Learning-for-NLP/</id>
    <published>2021-10-23T13:48:54.000Z</published>
    <updated>2021-10-24T06:27:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Parameter-Efficient-Transfer-Learning-for-NLP"><a href="#Parameter-Efficient-Transfer-Learning-for-NLP" class="headerlink" title="Parameter-Efficient Transfer Learning for NLP"></a>Parameter-Efficient Transfer Learning for NLP</h1><p>微调大型预训练模型是 NLP 中一种有效的传输机制。 但是，在存在很多下游任务的情况下，微调参数效率低下：每项任务都需要一个全新的模型。</p><p>作为替代方案，作者建议使用Adapter进行迁移。原始网络的参数保持不变，实现了高度的参数共享。</p><p>Adapter提供紧凑且可扩展的模型；它们只为每个任务添加几个可训练的参数，并且可以添加新任务，而无需重新访问以前的任务。</p><ul><li><p>紧凑型模型：在每个任务中使用少量附加参数解决多个任务的模型。</p></li><li><p>可扩展模型：可以增量训练以解决新任务，而不会忘记以前的任务。</p></li></ul><p>自然语言处理中最常见的两种迁移学习技术是 feature-based 和 fine-tuning。</p><ul><li>feature-based的转移涉及预训练实值嵌入向量。 这些嵌入可能在单词、句子或段落级别。 然后将嵌入提供给自定义的下游模型。</li><li>fine-tuning 涉及从预先训练的网络复制权重并在下游任务上调整它们</li></ul><p>feature-based 和 fine-tuning 都需要为每个任务设置一组新的权重。 如果网络的较低层在任务之间共享，则fine-tuning参数效率更高。 然而，提出的adapter tuning方法的参数效率更高。</p><p><img src="https://i.loli.net/2021/10/24/rQJmoCbIKV8hFN3.png" alt=""></p><p>x 轴显示每个任务训练的参数数量； 这对应于解决每个额外任务所需的模型大小的边际增加。</p><p>adapter tuning 训练少两个数量级的参数来，同时获得与fine-tuning 相似的性能。</p><p>adapter 是在预训练网络层之间添加的新模块。 基于adapter tuning与 feature-based/fine-tuning在以下方面有所不同。</p><p>考虑参数为$w$ 的函数 $\phi_w(x)$ (神经网络)。</p><p>Feature-based 将 $\phi_w$ 与新函数组合 $X_v$ 在一起: $X_v(\phi_w(x))$ , 然后，仅训练新的、特定于任务的参数 $v$</p><p>Fine-tuning 为每个新任务调整原始参数 $w$ , 限制紧凑性。</p><p>对于adapter tuning，定义了新函数 $\psi<em>{w,v}(x)$，其中参数 $w$ 从预训练中复制过来。初始参数 $v_0$ 设置为使新函数类似于原始函数：$\psi</em>{w,v<em>0}\approx \phi_w(x) $ 。 在训练期间，只有 $v$ 被调整。 定义 $\psi</em>{w,v}$ 通常涉及向原始网络添加新层 $\phi_{w}$</p><p>如果选择  $|v|≪|w|$，结果模型需要 $∼|w|$ 多任务的参数。 由于 w 是固定的，模型可以扩展到新任务而不影响以前的任务。</p><p>adapter tuning 几乎与完全Fine-tuning 的BERT的性能相当，但仅使用3%的特定于任务的参数，而微调使用100%的特定于任务的参数。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>为了实现这些特性，提出了一个新的瓶颈适配器模块。 当执行深层网络的普通fine-tuning时，对网络的顶层进行修改。这是必需的，因为上游和下游任务的标签空间和损失不同。</p><p>Adapter模块执行更通用的架构修改，以将预先训练的网络重新用于下游任务。</p><p> 特别是，Adapter调整策略涉及将新层注入原始网络。 原始网络的权重不变，而新的Adapter层是随机初始化的。 在标准Fine-tuning中，新的顶层和原始权重是共同训练的。 相比之下，在适配器调整中，原始网络的参数被冻结，因此可能被许多任务共享。</p><p>适配器模块有两个主要功能：参数数量较少 和 near-identity 的初始化。</p><p>与原始网络的层相比，适配器模块很小。 这意味着当添加更多任务时，总模型大小增长相对缓慢。</p><p>Adapter模型的稳定训练需要 near-identity 的初始化；下图实验证明初始化很重要</p><p><img src="https://i.loli.net/2021/10/24/cifQn7Gb2vejPyM.png" alt=""></p><p>横坐标为初始化分布的标准差</p><h3 id="Instantiation-for-Transformer-Networks"><a href="#Instantiation-for-Transformer-Networks" class="headerlink" title="Instantiation for Transformer Networks"></a>Instantiation for Transformer Networks</h3><p><img src="https://i.loli.net/2021/10/24/R7zDTM84XbqAfuI.png" alt=""></p><p>为了限制参数的数量，提出了一种瓶颈结构。</p><p>Adapter首先将原始的d维特征投影到较小的维度 m，然后应用非线性再投影回d维。</p><p>每层添加的参数总数(包括偏置)为 $2md+d+m$ 。</p><p>通过设置 $m≪d$ ，限制每个任务添加的参数数量，使用的参数大约是原始模型参数的0.5−8%。</p><p>瓶颈维度 m 提供了一种在性能和参数效率之间进行权衡的简单方法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Parameter-Efficient-Transfer-Learning-for-NLP&quot;&gt;&lt;a href=&quot;#Parameter-Efficient-Transfer-Learning-for-NLP&quot; class=&quot;headerlink&quot; title=&quot;Pa</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>二分查找基本思想：减而治之</title>
    <link href="http://example.com/2021/10/23/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3%EF%BC%9A%E5%87%8F%E8%80%8C%E6%B2%BB%E4%B9%8B/"/>
    <id>http://example.com/2021/10/23/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3%EF%BC%9A%E5%87%8F%E8%80%8C%E6%B2%BB%E4%B9%8B/</id>
    <published>2021-10-23T01:35:22.000Z</published>
    <updated>2021-11-01T02:23:17.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="二分查找基本思想：减而治之"><a href="#二分查找基本思想：减而治之" class="headerlink" title="二分查找基本思想：减而治之"></a>二分查找基本思想：减而治之</h1><p><img src="https://i.loli.net/2021/10/23/rHePAwZuEWGXUgh.jpg" alt=""></p><p>这里「减」是「减少问题」规模的意思，治是「解决」的意思。「减治思想」从另一个角度说，是「排除法」，意即：每一轮排除掉一定不存在目标元素的区间，在剩下 可能 存在目标元素的区间里继续查找。每一次我们通过一些判断和操作，使得问题的规模逐渐减少。又由于问题的规模是有限的，我们通过有限次的操作，一定可以解决这个问题。</p><p>可能有的朋友听说过「分治思想」，「分治思想」与「减治思想」的差别就在于，我们把一个问题拆分成若干个子问题以后，应用「减治思想」解决的问题就只在其中一个子问题里寻找答案。</p><h2 id="二分查找算法的应用范围"><a href="#二分查找算法的应用范围" class="headerlink" title="二分查找算法的应用范围"></a>二分查找算法的应用范围</h2><h3 id="在有序数组中进行查找一个数（二分下标）"><a href="#在有序数组中进行查找一个数（二分下标）" class="headerlink" title="在有序数组中进行查找一个数（二分下标）"></a>在有序数组中进行查找一个数（二分下标）</h3><p>这里「数组」和「有序」是很重要的，我们知道：数组具有 随机访问 的特性，由于数组在内存中 连续存放，因此我们可以通过数组的下标快速地访问到这个元素。如果数据存放在链表中，访问一个元素我们都得通过遍历，有遍历的功夫我们早就找到了这个元素，因此，在链表中不适合使用二分查找。</p><h3 id="在整数范围内查找一个整数（二分答案）"><a href="#在整数范围内查找一个整数（二分答案）" class="headerlink" title="在整数范围内查找一个整数（二分答案）"></a>在整数范围内查找一个整数（二分答案）</h3><p>如果我们要找的是一个整数，并且我们知道这个整数的范围，那么我们就可以使用二分查找算法，逐渐缩小整数的范围。这一点其实也不难理解，假设我们要找的数最小值为 0，最大值为 N，我们就可以把这个整数想象成数组 [0, 1, 2,…, N] 里的一个值，这个数组的下标和值是一样的，找数组的下标就等于找数组的值。这种二分法用于查找一个有范围的数，也被称为「二分答案」，或者「二分结果」，也就是在「答案区间」里或者是「结果区间」里逐渐缩小目标元素的范围；</p><p> 在我们做完一些问题以后，我们就会发现，其实二分查找不一定要求目标元素所在的区间是有序数组，也就是说「有序」这个条件可以放宽，半有序数组或者是山脉数组里都可以应用二分查找算法。</p><p>旋转数组和山脉数组有什么样的特点呢？可以通过当前元素附近的值推测出当前元素一侧的所有元素的性质，也就是说，旋转和山脉数组的值都有规律可循，元素的值不是随机出现的，在这个特点下，「减治思想」就可以应用在旋转数组和山脉数组里的一些问题上。我们可以把这两类数组统一归纳为部分有序数组。</p><h2 id="二分查找算法的两种思路"><a href="#二分查找算法的两种思路" class="headerlink" title="二分查找算法的两种思路"></a>二分查找算法的两种思路</h2><p>思路 1：在循环体中查找元素 （先介绍）；<br>思路 2：在循环体中排除目标元素一定不存在的区间。</p><ul><li>如果这个二分查找的问题比较简单，在输入数组里不同元素的个数只有 1 个，使用思路 1 ，在循环体内查找这个元素；</li><li>如果这个二分查找的问题比较复杂，要你找一个可能在数组里不存在，或者是找边界这样的问题，使用思路 2 ，在循环体内排除一定不存在目标元素的区间会更简单一些。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 「力扣」第 704 题：二分查找</span></span><br><span class="line">  <span class="comment">// 循环体中</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">search</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = len - <span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 目标元素可能存在在区间 [left, right]</span></span><br><span class="line">        <span class="keyword">while</span> (left &lt;= right) &#123;</span><br><span class="line">            <span class="comment">// 推荐的写法是 int mid = left + (right - left) / 2;</span></span><br><span class="line">            <span class="keyword">int</span> mid = (left + right) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span> (nums[mid] == target) &#123;</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] &lt; target) &#123;</span><br><span class="line">                <span class="comment">// 目标元素可能存在在区间 [mid + 1, right]</span></span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 目标元素可能存在在区间 [left, mid - 1]</span></span><br><span class="line">                right = mid - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="comment">// 排除</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">search1</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target)</span></span>&#123;</span><br><span class="line">      <span class="keyword">int</span> len = nums.length;</span><br><span class="line">      <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">int</span> right = len - <span class="number">1</span>;</span><br><span class="line">      <span class="comment">// 目标元素可能存在在区间 [left, right]</span></span><br><span class="line">      <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">          <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">          <span class="keyword">if</span> (nums[mid] &lt; target)&#123;</span><br><span class="line">              <span class="comment">// 下一轮搜索区间是 [mid+1, right]</span></span><br><span class="line">              left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">              <span class="comment">// 下一轮搜索区间是 [left, mid]</span></span><br><span class="line">              right = mid;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">if</span> (nums[left] == target)&#123;</span><br><span class="line">          <span class="keyword">return</span> left;</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="二分查找的细节（重点）"><a href="#二分查找的细节（重点）" class="headerlink" title="二分查找的细节（重点）"></a>二分查找的细节（重点）</h2><h3 id="细节-1：循环可以继续的条件"><a href="#细节-1：循环可以继续的条件" class="headerlink" title="细节 1：循环可以继续的条件"></a>细节 1：循环可以继续的条件</h3><p>while (left &lt;= right) 表示在区间里只剩下一个元素的时候，我们还需要继续查找，因此循环可以继续的条件是 left &lt;= right，这一行代码对应了二分查找算法的思路 1：在循环体中查找元素。</p><h3 id="细节-2：取中间数的代码"><a href="#细节-2：取中间数的代码" class="headerlink" title="细节 2：取中间数的代码"></a>细节 2：取中间数的代码</h3><p>取中间数的代码 int mid = (left + right) / 2; ，严格意义上是有 bug 的，这是因为在 left 和 right 很大的时候，left + right 有可能会发生整型溢出，这个时候推荐的写法是：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br></pre></td></tr></table></figure><p>这里要向大家说明的是 /2 这个写法表示 下取整。这里可能有的朋友有疑问：这里取中间位置元素的时候，为什么是取中间靠左的这个位置，能不能取中间靠右那个位置呢？答案是完全可以的。先请大家自己思考一下这个问题，我们放在细节 3 说。</p><p>有些朋友可能会看到 int mid = (left + right) &gt;&gt; 1; 这样的写法，这是因为整数右移 1 位和除以 2（向下取整）是等价的，这样写的原因是因为位运算比整除运算要快一点。但事实上，高级的编程语言，对于 / 2 和除以 2 的方幂的时候，在底层都会转化成为位运算，我们作为程序员在编码的时候没有必要这么做，就写我们这个逻辑本来要表达的意思即可，这种位运算的写法，在 C++ 代码里可能还需要注意优先级的问题。</p><p>在 Java 和 JavaScript 里有一种很酷的写法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> mid = (left + right) &gt;&gt;&gt; <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>这种写法也是完全可以的，这是因为 &gt;&gt;&gt; 是无符号右移，在 left + right 发生整型溢出的时候，右移一位由于高位补 0 ，依然能够保证结果正确。如果是写 Java 和 JavaScript 的朋友，可以这样写。在 Python 语言里，在 32 位整型溢出的时候，会自动转成长整形，这些很细枝末节的地方，其实不是我们学习算法要关注的重点。</p><p>我个人认为这几种种写法差别不大，因为绝大多数的算法面试和在线测评系统给出的测试数据，数组的长度都不会很长，遇到 left + right 整型溢出的概率是很低的，我们推荐大家写 int mid = left + (right - left) / 2;，让面试官知道你注意了整型溢出这个知识点即可。</p><h3 id="细节-3：取中间数可不可以上取整"><a href="#细节-3：取中间数可不可以上取整" class="headerlink" title="细节 3：取中间数可不可以上取整"></a>细节 3：取中间数可不可以上取整</h3><p>我们在「细节 2」里介绍了 int mid = (left + right) / 2; 这个表达示里 / 2 这个除号表示的含义是下取整。很显然，在区间里有偶数个元素的时候位于中间的数有 22 个，这个表达式只能取到位于左边的那个数。一个很自然的想法是，可不可以取右边呢？遇到类似的问题，首先推荐的做法是：试一试就知道了，刚刚我们说了实证的精神，就把</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> mid = (left + right + <span class="number">1</span>) / <span class="number">2</span>;</span><br><span class="line"><span class="comment">// 或</span></span><br><span class="line"><span class="keyword">int</span> mid = left + (right - left + <span class="number">1</span>) / <span class="number">2</span>;</span><br></pre></td></tr></table></figure><p>因为我们的思路是根据中间那个位置的数值决定下一轮搜索在哪个区间，每一轮要看的那个数当然可以不必是位于中间的那个元素，靠左和靠右都是没有问题的。</p><p>甚至取到每个区间的三分之一、四分之一、五分之四，都是没有问题的。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> mid = left + (right - left) / <span class="number">3</span>;</span><br><span class="line"><span class="comment">// 或</span></span><br><span class="line"><span class="keyword">int</span> mid = left + <span class="number">4</span> * (right - left) / <span class="number">5</span>;</span><br></pre></td></tr></table></figure><p>一般而言，取位于区间起点二分之一处，首先是因为这样写简单，还有一个更重要的原因是：取中间位置的那个元素在平均意义下效果最好。这一点怎么理解呢？</p><p>在没有任何「<strong>先验知识</strong>」的情况下，在搜索区间里猜中间位置是最好的。</p><h1 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h1><h2 id="35-搜索插入位置"><a href="#35-搜索插入位置" class="headerlink" title="35. 搜索插入位置"></a><a href="https://leetcode-cn.com/problems/search-insert-position/">35. 搜索插入位置</a></h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">searchInsert</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = len-<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> mid=<span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 目标元素可能存在在区间 [left, right]</span></span><br><span class="line">        <span class="comment">// 区间里只剩下一个元素的时候，我们还需要继续查找</span></span><br><span class="line">        <span class="keyword">while</span>(left &lt;= right)&#123;</span><br><span class="line">            mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span> (nums[mid] == target)&#123;</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(nums[mid] &lt; target)&#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                right = mid -<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 分别处理如下四种情况</span></span><br><span class="line">        <span class="comment">// 目标值在数组所有元素之前  [0, -1]</span></span><br><span class="line">        <span class="comment">// 目标值等于数组中某一个元素  return mid;</span></span><br><span class="line">        <span class="comment">// 目标值插入数组中的位置 [left, right]，return  right + 1</span></span><br><span class="line">        <span class="comment">// 目标值在数组所有元素之后的情况 [left, right]， return right + 1</span></span><br><span class="line">        <span class="keyword">return</span> right + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">searchInsert1</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> length = nums.length;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (nums[length - <span class="number">1</span>] &lt; target) &#123;</span><br><span class="line">            <span class="keyword">return</span> length;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = length;</span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left)/ <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &lt; target)&#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="34-在排序数组中查找元素的第一个和最后一个位置"><a href="#34-在排序数组中查找元素的第一个和最后一个位置" class="headerlink" title="34. 在排序数组中查找元素的第一个和最后一个位置"></a><a href="https://leetcode-cn.com/problems/find-first-and-last-position-of-element-in-sorted-array/">34. 在排序数组中查找元素的第一个和最后一个位置</a></h2><ul><li>不可以找到target后向两边扩散（线性查找），这样的话时间复杂度为 $O(N)$</li><li>应该使用两次二分查找，先找target第一次出现的位置，再找target最后一次出现的位置，注意分类讨论，并且把分类讨论的结果合并。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] searchRange(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target) &#123;</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line">        <span class="keyword">if</span>(len==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;-<span class="number">1</span>,-<span class="number">1</span>&#125;;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = len-<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(target &gt; nums[right] || target&lt;nums[left])&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;-<span class="number">1</span>,-<span class="number">1</span>&#125;;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> firstPosition = findFirstPosition(nums, target);</span><br><span class="line">        <span class="keyword">if</span>(firstPosition == -<span class="number">1</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;-<span class="number">1</span>,-<span class="number">1</span>&#125;;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> lastPosition = findLastPosition(nums, target);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;firstPosition, lastPosition&#125;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">findFirstPosition</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = nums.length-<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">            <span class="comment">// 小于一定不是解</span></span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &lt; target)&#123;</span><br><span class="line">                <span class="comment">// 下一轮搜索区间是 [mid+1, right]</span></span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="comment">// nums[mid] &gt; target, 下一轮搜索区间是[left ,mid]</span></span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(nums[left] == target)&#123;</span><br><span class="line">            <span class="keyword">return</span> left;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">findLastPosition</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = nums.length - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right-left+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &gt; target)&#123;</span><br><span class="line">                <span class="comment">// 下一轮搜索区间是[left, mid-1]</span></span><br><span class="line">                right = mid -<span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="comment">// 下一轮搜索区间是[mid, right]</span></span><br><span class="line">                left = mid;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>findFirstPosition() </p><p>情况 ① ：当 nums[mid] &lt; target 时</p><ul><li>mid 一定不是 target 第一次出现的位置；</li><li>由于数组有序，mid 的左边一定比 nums[mid] 还小，因此 mid 的左边一定不是 target 第一次出现的位置；</li><li>mid 的右边比 nums[mid] 还大，因此 mid 的右边有可能存在 target 第一次出现的位置。<br>因此下一轮搜索区间是 [mid + 1..right]，此时设置 left = mid + 1；</li></ul><p>情况 ② ：当 nums[mid] == target 时</p><ul><li>mid 有可能是 target 第一次出现的位置；</li><li>mid 的左边也有可能是 target 第一次出现的位置；</li><li>mid 的右边一定不是 target 第一次出现的位置。<br>因此下一轮搜索区间在 [left..mid]，此时设置 right = mid。</li></ul><p>情况 ③ ：当 nums[mid] &gt; target 时</p><ul><li>mid 一定不是 target 第一次出现的位置；</li><li>mid 的右边也一定不是 target 第一次出现的位置；</li><li>mid 的左边有可能是 target 第一次出现的位置，因此下一轮搜索区间在 [left..mid - 1]，此时设置 right = mid - 1。</li></ul><p>重点在这里：把情况 ② 和情况 ③ 合并，即当 nums[mid] &gt;= target 的时候，下一轮搜索区间是 [left..mid]，此时设置 right = mid - 1。这样做是因为：只有当区间分割是 [left..mid] 和 [mid + 1..right] 的时候，while(left &lt; right) 退出循环以后才有 left == right 成立。</p><p>findLastPosition() 也可以类似分析，这里省略。</p><p>在本题解中，while(left &lt; right) 只表示退出循环以后有 left == right 成立，不表示搜索区间为左闭右开区间，本题解以及我的其它题解中，对循环不变量的定义均为：在 nums[left..right] 中查找目标元素。</p><h2 id="153-寻找旋转排序数组中的最小值"><a href="#153-寻找旋转排序数组中的最小值" class="headerlink" title="153. 寻找旋转排序数组中的最小值"></a><a href="https://leetcode-cn.com/problems/find-minimum-in-rotated-sorted-array/">153. 寻找旋转排序数组中的最小值</a></h2><h3 id="二分法"><a href="#二分法" class="headerlink" title="二分法"></a>二分法</h3><p>旋转排序数组，几乎就是有序的数组，可以通过比较特定位置的元素的值判断达到减治的效果（逐渐缩小搜索区间）</p><p>很自然的，会看 <strong>中间数</strong> （位于待搜索区间中间位置的元素），由于不是有序数组，因此不能称之为中位数。</p><p>另外，待搜索区间头和尾的元素位置特殊的元素，有两个比较自然的思路是：</p><ul><li>思路1：看看当前搜索区间的 <strong>左边界</strong> 和 <strong>中间数</strong>，是不是可以缩小搜索区间的范围</li><li>思路2：看看当前搜索区间的 <strong>右边界</strong> 和 <strong>中间数</strong>，是不是可以缩小搜索区间的范围</li></ul><p>要想清楚不妨举几个例子：</p><p>例1：$[1,2,3,4,5]$</p><p>例2：$[2,3,4,5,1]$</p><p>这两个例子的 <strong>中间数</strong> 都比左边界大，但 旋转排序数组 的<strong>最小值</strong>  一个在中间数的左边，一个在右边，因此思路1不合适。</p><p>针对思路2，依然写两个例子，这两个例子分别是 <strong>中间数比右边界大</strong> 和 <strong>中间数比右边界小</strong>，看看能不能推导出一般化的结论。</p><p>例3：$[7,8,9,10,11,12,1,2,3]$</p><p>中间数 11 比右边界 3 大，因此中间数左边的数（包括中间数）都不是 旋转排序数组的最小值，因此下一轮搜索的区间是 $[mid+1, right]$ ，将下一轮搜索的左边界设置成中间数位置 +1，即 $left = mid+1$</p><p>例4：$[7,8,1,2,3]$</p><p>中间数 1 比右边界3小，说明中间数到右边界是递增的，那么中间数右边的（不包括中间数）一定不是 旋转数组的最小值，可以排除，但中间数有可能是整个数组中的最小值，就如本例，因此， 在下一轮搜索区间是 $[left,mid]$，于是把右边界设置为 $right=mid$</p><p>从例 3 和例 4 可以看出，不论中间数比右边界大，还是中间数比右边界小，我们都可以排除掉将近一半的元素，把原始问题转换成一个规模更小的子问题，这正是「减而治之」思想的体现，因此思路 2 可行。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findMin</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = len-<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &gt; nums[right])&#123;</span><br><span class="line">                left = mid +<span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="comment">// 因为题目说 可以假设数组中不存在重复元素</span></span><br><span class="line">                <span class="comment">// 此时一定有 nums[mid] &lt; nums[right]</span></span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> nums[left];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="分治法"><a href="#分治法" class="headerlink" title="分治法"></a>分治法</h3><p>分治法是将原问题划分成若干与原问题同结构且规模更小的子问题，等到这些子问题解决了以后，原问题也得到了</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findMin</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line">        <span class="keyword">return</span> findMin(nums, <span class="number">0</span>, len-<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">findMin</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(left == right)&#123;</span><br><span class="line">            <span class="keyword">return</span> nums[left];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(left+<span class="number">1</span> == right)&#123;</span><br><span class="line">            <span class="keyword">return</span> Math.min(nums[left], nums[right]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 这一步是关键</span></span><br><span class="line">        <span class="keyword">if</span>(nums[left] &lt; nums[right])&#123;</span><br><span class="line">            <span class="keyword">return</span> nums[left];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(nums[mid] &lt; nums[right])&#123;</span><br><span class="line">            <span class="comment">// 右边是顺序数组， [mid +1, right] 这个区间里的元素可以不看</span></span><br><span class="line">            <span class="keyword">return</span> findMin(nums, left, mid);</span><br><span class="line">        &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> findMin(nums, mid+<span class="number">1</span>, right);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="154-寻找旋转排序数组中的最小值-II"><a href="#154-寻找旋转排序数组中的最小值-II" class="headerlink" title="154. 寻找旋转排序数组中的最小值 II"></a><a href="https://leetcode-cn.com/problems/find-minimum-in-rotated-sorted-array-ii/">154. 寻找旋转排序数组中的最小值 II</a></h2><p>有序数组可能存在重复值</p><h3 id="二分法-1"><a href="#二分法-1" class="headerlink" title="二分法"></a>二分法</h3><ul><li>当中间数比右边界表示的数大的时候，中间数一定不是目标数</li><li>当中间数比右边界表示的数小的时候，中间数就可能是目标数</li><li>当中间数比有边界表示的数相等时：此时只把右边界排除掉就好</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findMin</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = len - <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) /<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &gt; nums[right])&#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span>(nums[mid] &lt; nums[right])&#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="keyword">assert</span> nums[mid] == nums[right];</span><br><span class="line">                right -- ;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> nums[left];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="分治法-1"><a href="#分治法-1" class="headerlink" title="分治法"></a>分治法</h3><p>分治法将原问题划分成若干与原问题同结构且规模更小的子问题，等到这些子问题解决了以后，原问题也得到了解决。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findMin</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line">        <span class="keyword">return</span> findMin(nums, <span class="number">0</span>, len-<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">findMin</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(left==right)&#123;</span><br><span class="line">            <span class="keyword">return</span> nums[right];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(left+<span class="number">1</span> == right)&#123;</span><br><span class="line">            <span class="keyword">return</span> Math.min(nums[left] , nums[right]);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(nums[left] &lt; nums[right])&#123;</span><br><span class="line">            <span class="keyword">return</span> nums[left];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 分治边界</span></span><br><span class="line">        <span class="keyword">int</span> mid = left+(right-left)/<span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span>(nums[mid] == nums[right])&#123;</span><br><span class="line">            <span class="keyword">return</span> findMin(nums, left, right-<span class="number">1</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(nums[mid] &lt; nums[right])&#123;</span><br><span class="line">            <span class="keyword">return</span> findMin(nums, left, mid);</span><br><span class="line">        &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> findMin(nums, mid+<span class="number">1</span>, right);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="33-搜索旋转排序数组"><a href="#33-搜索旋转排序数组" class="headerlink" title="33. 搜索旋转排序数组"></a><a href="https://leetcode-cn.com/problems/search-in-rotated-sorted-array/">33. 搜索旋转排序数组</a></h2><p>题中数组不存在重复元素。</p><p>根据示例 [4, 5, 6, 7, 0, 1, 2] ，自己手写几个旋转数组。不难发现：将待搜索区间从中间一分为二，位于中间的元素 nums[mid] 一定会落在其中一个有序区间里。需要分类讨论。</p><p><img src="https://i.loli.net/2021/10/29/6HSdnsJcp7wYvtb.png" alt=""></p><p><strong>中间元素和右边界的关系</strong> 为例，其它情况类似。由于不存在重复元素，<strong>所以它们的关系不是大于就是小于</strong>。</p><p><strong>关键</strong>：把比较好些的判断（<code>target</code> 落在有序的那部分）放在 <code>if</code> 的开头考虑，把剩下的情况放在 <code>else</code> 里面。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">search</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line">        <span class="keyword">if</span> (len == <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = len - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="comment">// 根据分支的逻辑将中间数改成上取整</span></span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left + <span class="number">1</span>) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &lt; nums[right])&#123;</span><br><span class="line">                <span class="comment">// 此时 [mid..right] 有序</span></span><br><span class="line">                <span class="keyword">if</span>(nums[mid] &lt;= target &amp;&amp; target &lt;= nums[right]) &#123;</span><br><span class="line">                    <span class="comment">// 如果 target 的值落在这个区间里, 下一轮搜索区间是[mid..right],此时设置left = mid</span></span><br><span class="line">                    left = mid;</span><br><span class="line">                &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">                    <span class="comment">// 否则，下一轮搜索区间是 [left..mid-1] 此时设置 right = mid - 1</span></span><br><span class="line">                    right = mid - <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="comment">// 此时 nums[mid] &gt;= nums[right] 注意此时 mid 可能与right重合</span></span><br><span class="line">                <span class="comment">// 数组前半个部分有序，即[left..mid] 有序， 为了与上一个分支的逻辑一致，认为[left .. mid-1]</span></span><br><span class="line">                <span class="keyword">if</span>(nums[left] &lt;= target &amp;&amp; target &lt;= nums[mid-<span class="number">1</span>])&#123;</span><br><span class="line">                    <span class="comment">// 如果target的值落在区间 [left..mid-1] 里，设置right = mid -1</span></span><br><span class="line">                    right = mid - <span class="number">1</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">                    <span class="comment">// 否则，下一轮搜索区间是 [mid..right] 此时设置 left = mid</span></span><br><span class="line">                    left = mid;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (nums[left] == target)&#123;</span><br><span class="line">            <span class="keyword">return</span> left;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4-寻找两个正序数组的中位数"><a href="#4-寻找两个正序数组的中位数" class="headerlink" title="4. 寻找两个正序数组的中位数"></a><a href="https://leetcode-cn.com/problems/median-of-two-sorted-arrays/">4. 寻找两个正序数组的中位数</a></h2><p>解题核心思想：</p><ul><li>使用二分查找确定两个有序数组的 【分割线】，中位数就由分割线左右两侧的元素决定；</li><li>分割线满足这样的性质：左右两边元素个数相等（这里忽略两个数组长度之和奇偶性的差异）</li><li>分割线左边所有元素 小于等于 分割线右边所有算是</li><li>由于分割线两边元素个数相等，移动分割线就会有【此消彼长】的现象，所以使用二分法去定位</li></ul><p>这条分割线的特点是：</p><ul><li>当数组的总长度为偶数的时候，分割线左右的数字个数总和相等；当是奇数时，分割线左边数字比右边仅仅多1</li><li>分割线左边的所有元素都小于等于分割线右边的所有元素</li></ul><p>如果找到这条分割线，那么中位数可以确定下来，同样得分奇偶性：</p><ul><li>当数组总长度为偶数时，中位数就是分割线左边最大值与分割线右边最小值的平均数</li><li>当是奇数时，中位数就是分割线左边的最大值，因此在数组长度为奇数时，中位数就是分割线左边的最大值。</li></ul><p>因为两个数组本别是有序数组，因此，我们只需要判定交叉的关系中，是否满足左边依然小于等于右边即可，即</p><ul><li>第1个数组分割线左边的第1个数小于等于第2个数组分割线右边的第一个数</li><li>第2个数组分割线左边的第1个数小于等于第1个数组右边的第1个数</li></ul><p>通过不断缩减搜索区间确定分割线的位置</p><ul><li>当数组总长度为偶数时，左边一共有 $\frac{len(nums1) + len(nums2)}{2}$ 个元素</li><li>当数组总长度为奇数时，左边一共有 $\frac{len(nums1) + len(nums2)}{2}+1$个元素</li></ul><p>奇数的时候是除以2向下取整，所以计算左边元素总数的时候就得 +1。也可以向上取整</p><script type="math/tex; mode=display">\frac{len(nums1) + len(nums2) +1}{2}</script><p>这里用到了一个小技巧，把下取整，修改为上取整的时候，只需要在被除数的部分，加上除数减 1 即可</p><p>这样问题就转化为，我们在其中一个数组找到 $i$ 个元素，则另一个数组的元素个数就一定是 $\frac{len(nums1) + len(nums2) +1}{2} - i$</p><p>于是怎么找到 $i$ 是要解决的问题。</p><p>找 i 个元素，我们通常的做法是找索引为 i的元素，因为下标是从 0 开始编号，因此编号为 i 的元素，就刚刚好前面有 i 个元素。因此，i 就是第 1 个数组分割线的右边的第 1 个元素。</p><p>下面我们来看怎么找 i，需要分类讨论。</p><p>情况1：如下图，此时分割线左边元素比右边多1，但是第一个数组分割线比右边第一个数6小于第二个数组分割线左边第一个数8，说明第一个数组左边的数少了，分割线要右移。</p><p><img src="https://i.loli.net/2021/11/01/mtGepBKROfFNsxH.png" alt=""></p><p>情况 2：如下图所示，此时分割线左边的元素总数比右边多 1，但是第 一 个数组分割线左边第 1 个数 8 大于第 二 个数组分割线左边第 1 个数 7。说明，第 1 个数组左边的数多了，分割线要左移。</p><p><img src="https://i.loli.net/2021/11/01/TpRnHvheiKLZW1o.png" alt=""></p><p>就是在这种不断缩小搜索范围的方法中，定位我们要找的 <code>i</code> 是多少。</p><p>极端情况</p><p>这里要注意一个问题，那就是我们要在一个短的数组上搜索 i 。在搜索的过程中，我们会比较分割线左边和右边的数，即 nums[i]、 nums[i - 1]、 nums[j]、 nums[j - 1]，因此 这几个数的下标不能越界。</p><p><img src="https://i.loli.net/2021/11/01/fAvgj7uar2Ksq8X.png" alt=""></p><p>此时，分割线在第 2 个数组的左边没有值，会导致 nums2[j - 1] 的访问越界。因此我们必须在短的数组上搜索 i 。i 的定义是分割线的右边，而它的左边一定有值。这样就能保证，分割线在第 2 个数组的左右两边一定有元素，即分割线一定可以在第 2 个数组的中间切一刀。</p><p>即使我在短数组上搜索边界 <code>i</code> ，还真就可能遇到 <code>i</code> 或者 <code>j</code> 的左边或者右边取不到元素的情况，它们一定出现在退出循环的时候。</p><p><img src="https://i.loli.net/2021/11/01/fMULmj9KbnyeWG2.png" alt=""></p><p>最后，我们把关心的「边界线」两旁的 44 个数的极端情况都考虑一下：</p><ul><li>考虑nums1:</li><li><ul><li>当 i=0 时，对应上图右边，此时数组 nums1 在红线左边为空，可以设置 num1_left_max = 负无穷，这样在最终比较的时候，因为左边粉红色部分要选择出最大值，它一定不会被选中</li><li>当 i=m 时，对应上图左边，此时数组 nums1 在红线右边为空，可设置 num1_right_min = 正无穷，这样在最终比较的时候，因为右边蓝色部分要选择出最小值，它一定不会被选中，于是能兼容其它情况。</li></ul></li><li>数组nums2 同理</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">findMedianSortedArrays</span><span class="params">(<span class="keyword">int</span>[] nums1, <span class="keyword">int</span>[] nums2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(nums1.length &gt; nums2.length)&#123;</span><br><span class="line">            <span class="keyword">int</span>[] temp = nums1;</span><br><span class="line">            nums1 = nums2;</span><br><span class="line">            nums2 = temp;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> m = nums1.length;</span><br><span class="line">        <span class="keyword">int</span> n = nums2.length;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 分割线左边的所有元素需要满足的个数 m + (n-m+1)/2</span></span><br><span class="line">        <span class="keyword">int</span> totalLeft = (m+n+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 在nums1的区间[0,m]里查找恰当的分割线</span></span><br><span class="line">        <span class="comment">// 使得nums1[i-1] &lt;= nums2[j] &amp;&amp; nums2[j-1] &lt;= nums1[j]</span></span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = m;</span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="keyword">int</span> i = left + (right - left +<span class="number">1</span>) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">int</span> j = totalLeft - i;</span><br><span class="line">            <span class="keyword">if</span>(nums1[i-<span class="number">1</span>] &gt; nums2[j])&#123;</span><br><span class="line">                <span class="comment">// 下一轮搜索 [left, i-1]</span></span><br><span class="line">                right = i - <span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="comment">// 下一轮搜索 [i, right]</span></span><br><span class="line">                left = i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> i = left;</span><br><span class="line">        <span class="keyword">int</span> j = totalLeft - i;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> nums1LeftMax = i==<span class="number">0</span>? Integer.MIN_VALUE:nums1[i-<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span> nums1RightMin = i==m? Integer.MAX_VALUE:nums1[i];</span><br><span class="line">        <span class="keyword">int</span> nums2LeftMax = j == <span class="number">0</span> ? Integer.MIN_VALUE : nums2[j - <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span> nums2RightMin = j == n ? Integer.MAX_VALUE : nums2[j];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>( ((m+n) % <span class="number">2</span>) == <span class="number">1</span> )&#123;</span><br><span class="line">            <span class="keyword">return</span> Math.max(nums1LeftMax, nums2LeftMax);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> (<span class="keyword">double</span>)((Math.max(nums1LeftMax, nums2LeftMax) + Math.min(nums1RightMin, nums2RightMin))) /<span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;二分查找基本思想：减而治之&quot;&gt;&lt;a href=&quot;#二分查找基本思想：减而治之&quot; class=&quot;headerlink&quot; title=&quot;二分查找基本思想：减而治之&quot;&gt;&lt;/a&gt;二分查找基本思想：减而治之&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.n</summary>
      
    
    
    
    
    <category term="LeetCode" scheme="http://example.com/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks</title>
    <link href="http://example.com/2021/10/18/Adapting-BERT-for-Continual-Learning-of-a-Sequence-of-Aspect-Sentiment-Classification-Tasks/"/>
    <id>http://example.com/2021/10/18/Adapting-BERT-for-Continual-Learning-of-a-Sequence-of-Aspect-Sentiment-Classification-Tasks/</id>
    <published>2021-10-18T04:12:49.000Z</published>
    <updated>2021-10-18T10:34:35.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Adapting-BERT-for-Continual-Learning-of-a-Sequence-of-Aspect-Sentiment-Classification-Tasks"><a href="#Adapting-BERT-for-Continual-Learning-of-a-Sequence-of-Aspect-Sentiment-Classification-Tasks" class="headerlink" title="Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks"></a>Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks</h1><p>增量学习ASC任务序列的CL系统应解决以下两个问题：</p><ul><li>将从以前的任务中学到的知识转移到新的任务中，帮助它学习更好的模型</li><li>维护以前任务的模型性能，以便不会忘记它们</li></ul><p>针对这些问题，本文提出了一种新的基于胶囊网络的模型B-CL  (<em>BERT-based Continual Learning</em>) ，受《Parameter-efficient transfer learning for NLP》的Adapter Bert启发。</p><p>B-CL通过前向和后向知识转移显著提高了ASC在新任务和旧任务上的效果。</p><p>ASC任务定义如下：给定一个方面(例如，相机评论中的图像质量)和在特定领域(例如，相机)中包含该方面的意义，分类句子对该方面表示正面、负面还是中性(无意见)。</p><p>利用胶囊和动态路由 来识别与新任务相似的先前任务，并利用它们的共享知识来帮助新任务学习，并使用任务掩码来保护任务特定的知识，以避免遗忘(CF)。</p><h2 id="Adapter-BERT"><a href="#Adapter-BERT" class="headerlink" title="Adapter-BERT"></a>Adapter-BERT</h2><p>一个 adapter 是具有残差连接的2层全连接网络。只有adapter(黄框)和layer norm(绿色框)层是可训练的。其他模块(灰色框)被冻结。提出的的B-CL，用CLA代替适配器。CLA有两个子模块：知识共享模块(KSM)和任务特定模块(TSM)</p><p><img src="https://i.loli.net/2021/10/18/7tVlqd2pLezUja4.png" alt=""></p><p>在结束任务的训练期间，只训练适配器和规格化层，不改变任何其他BERT参数，这对CL是好的，因为微调BERT本身会导致严重的遗忘。</p><h2 id="Capsule-Network"><a href="#Capsule-Network" class="headerlink" title="Capsule Network"></a>Capsule Network</h2><p>与CNN不同的是，CapsNet用矢量胶囊取代了标量特征检测器，可以保留图像中的位置和厚度等额外信息。典型的CapsNet有两层胶囊层。</p><p>初级图层存储低级特征映射，类别层生成分类概率，每个胶囊对应一个类。它使用动态路由算法使每个较低级别的封装能够将其输出发送到类似的(或“agreed”，由点积计算的)较高级别封装。这是用来识别和分组相似任务及其共享功能或知识的关键属性。</p><p>值得注意的是，所提出的B-CL不采用整个胶囊网络，因为只对胶囊层和动态路由感兴趣，而对最大边际损失和分类器不感兴趣。</p><h2 id="Continual-Learning-Adapter-CLA"><a href="#Continual-Learning-Adapter-CLA" class="headerlink" title="Continual Learning Adapter (CLA)"></a>Continual Learning Adapter (CLA)</h2><p>B-CL的目标是：</p><ul><li><p>通过知识共享实现相关旧任务与新任务之间的知识转移；</p></li><li><p>通过防止新任务学习覆盖先前任务的特定任务知识来获得回避。</p></li></ul><p>CLA的体系结构如图所示</p><ul><li>知识共享模块(KSM)，用于从相似的先前任务和新任务中识别和利用可共享的知识</li><li>任务特定模块(TSM)，用于学习任务特定神经元并保护它们不被新任务更新。</li></ul><p><img src="https://z3.ax1x.com/2021/10/18/5UAeUS.png" alt=""></p><p>CLA接受两个input：</p><ul><li>来自transformer层内部前馈层的隐藏状态 $h^{(t)}$</li><li>task ID $t$</li></ul><p>output是隐藏状态，具有适合第t个任务的特征。</p><p>KSM 利用胶囊层（见下文）和动态路由对相似的任务和可共享的知识进行分组，</p><p>而 TSM 利用任务掩码 (TM) 来保护特定任务的神经元并让其他神经元自由。 这些自由的神经元稍后被TSM用于一项新的任务。由于TM是可微的，所以整个系统B-CL可以被端到端地训练。下面将详细介绍每个模块。</p><h3 id="Knowledge-Sharing-Module-KSM"><a href="#Knowledge-Sharing-Module-KSM" class="headerlink" title="Knowledge Sharing Module (KSM)"></a>Knowledge Sharing Module (KSM)</h3><p>KSM 将相似的任务和共享的知识（特征）分组到它们之间，以实现相似任务之间的知识转移。 这是通过两个胶囊层（任务胶囊层和知识共享胶囊层）和胶囊网络的动态路由算法实现的。</p><h4 id="Task-Capsule-Layer-TCL"><a href="#Task-Capsule-Layer-TCL" class="headerlink" title="Task Capsule Layer (TCL)"></a>Task Capsule Layer (TCL)</h4><p>TCL中的每个胶囊代表一个任务，TCL准备从每个任务派生低级特征。因此，对于每个新任务，TCL都会添加一个胶囊。</p><p>这种增量生长是有效且容易的，因为这些胶囊是离散的并且不共享参数。而且，每个胶囊只是一个具有少量参数的2层完全连接的网络。</p><p>$h^{(t)} \in R^{d_t\times d_e}$ 为CLA的输入，$d_t$ 是tokens数量，$d_e$是维度。</p><p>令到目前为止学习的任务集为 $T<em>{prev}$（在学习新任务 t 之前）和 $|T</em>{prev}|= n$</p><p>在 TCL 中，我们有 n+1 个不同的胶囊代表所有过去的 n 个学习任务以及新任务 t。</p><p>第 $i(i≤n+1)$ 个任务的封装为:</p><script type="math/tex; mode=display">p_i^{(t)} = f_i (h^{(t)})</script><p>其中 $f_i(\cdot) = MLP_i(\cdot)$ 代表为2层全连接层。</p><h4 id="Knowledge-Sharing-Capsule-Layer-KCL"><a href="#Knowledge-Sharing-Capsule-Layer-KCL" class="headerlink" title="Knowledge Sharing Capsule Layer (KCL)"></a>Knowledge Sharing Capsule Layer (KCL)</h4><p>KCL 中的每个知识共享胶囊都捕获那些具有相似特征或共享知识的任务（即它们的任务胶囊 ${p_i^{(t)}}_1^{n+1}$ ）。</p><p>这是通过动态路由算法自动实现的。 召回动态路由鼓励每个较低级别的胶囊（在案例中为任务胶囊）将其输出发送到类似（或“agreed”）的更高级别的胶囊（在我们的案例中为知识共享胶囊）。</p><p>本质上，相似的任务胶囊（具有许多共享特征）通过较高的系数（决定任务胶囊可以进入下一层的程度）“聚集”在一起，而不同的任务（具有很少的共享特征）则通过低系数。</p><p>这种聚类识别来自多个任务封装的共享特征或知识，并且有助于在相似任务之间向后转移。</p><p>KCL首先将每个任务胶囊 $p<em>i^{(t)}$ 变成临时特征 $u</em>{j|i}^{(t)}$：</p><script type="math/tex; mode=display">u_{j|i}^{(t)} = W_{ij} p_i^{(t)}</script><p>临时特征与权重 $c^{(t)}_{ij}$ 相加以获得知识共享胶囊 $s^{(t)}_j$ 中的初始值：</p><script type="math/tex; mode=display">s_j^{(t)} = \sum_i c_{ij}^{(t)} u_{j|i}^{(t)}</script><p>其中 $c<em>{ij}^{(t)}$ 是耦合系数加和为1。请注意，方程 1 中每个任务的任务胶囊映射到方程 3 中的知识共享胶囊，$c</em>{ij}^{(t)}$ 表示第 i 个任务的表示对第 j 个知识共享胶囊的信息量。</p><p>因此，知识共享胶囊可以表示不同的可共享知识。这确保仅使用与新任务显着或相似的任务胶囊，而忽略（并因此保护）其他任务胶囊以学习更一般的可共享知识。</p><p>在反向传播时，用较小梯度更新具有低 $c^{(t)}<em>{ij }$ 的相异任务，然而相似的任务有较高的$c^{(t)}</em>{ij }$被更新较大的梯度。这鼓励在类似任务之间向后转移。</p><p>Dynamic Routing</p><p>$c_{ij}^{(t )}$ 是由“Routing Softmax”计算的：</p><script type="math/tex; mode=display">c_{ij}^{(t)} = \frac{exp(b_{ij}^{(t)})}{ \sum_o exp(b_{io}^{(t)})}</script><p>其中每个 $b_{ij}$ 是对数先验概率，显示任务胶囊 $i$ 与知识共享胶囊 $j$ 的显著性或相似性。</p><p>它被初始化为0，表示它们之间在开始时没有显著联系。应用动态路由算法来更新 $b_{ij}$：</p><script type="math/tex; mode=display">b_{ij}^{(t)} \leftarrow b_{ij}^{(t)} + a_{ij}^{(t)}</script><p>其中 $a<em>{ij}$ 是协议系数，直观上，这一步倾向于聚合知识共享胶囊上的相似（或“agreed”）任务，具有更高的一致性系数 $a</em>{ij}^{(t)}$，因此具有更高的 logit $b^{(t)}<em>{ij}$ 或耦合系数 $c</em>{ij}^{(t )}$ 。协议系数的计算公式为 :</p><script type="math/tex; mode=display">a_{ij}^{(t)} = u_{j|i} ^{(t)} \cdot v_{j}^{(t)}</script><p>其中 $v_{j}^{(t)}$ 是规范化的表示形式，通过非线性 squash</p><script type="math/tex; mode=display">v_{j}^{(t)} = \frac{ ||s_j^{(t)}||^2 }{1+||s^{(t)}_j||} \frac{s_j^{(t)}}{||s_j^{(t)}||}</script><p>对于第一个任务 $s<em>j^{(t)} =  u</em>{j|i}^{(t)} $ , 其中 $v^{(t)}_{j}$ 归一化为[0，1]到表示知识共享胶囊 j 的激活概率。</p><h3 id="Task-Specific-Module-TSM"><a href="#Task-Specific-Module-TSM" class="headerlink" title="Task Specific Module (TSM)"></a>Task Specific Module (TSM)</h3><p>虽然知识共享对ASC很重要，但为以前的任务保存特定于任务的知识以防止遗忘(CF)也同样重要。</p><p>为此，使用任务掩码。具体地说，首先检测每个旧任务使用的神经元，然后在学习新任务时关闭或屏蔽所有使用过的神经元。</p><p><img src="https://z3.ax1x.com/2021/10/18/5UY0HO.png" alt=""></p><p>每项任务的两行对应于TSM中的 $k^{(t)}_0$ 和 $k^{(t)}_1$。在训练前的细胞中，0的细胞是需要保护(掩蔽)的神经元，那些没有编号的细胞是游离的神经元(未使用)。</p><p>在学习第一个任务（任务 0）后，获得了用橙色标记的有用神经元，每个神经元中都标有 1，作为学习未来任务的掩码。 在学习任务 1 中，那些对任务 0 有用的神经元被屏蔽（左侧的橙色神经元或细胞中的 0 为 0）。 该过程还学习了任务 1 的有用神经元，用 1 标记为绿色。 当任务 2 到达时，任务 0 和 1 的所有重要神经元都被屏蔽，即其掩码条目设置为 0（训练前的橙色和绿色）。 在训练任务 2 之后，我们看到任务 2 和任务 1 有一个对它们都很重要的共享神经元。 共享神经元以红色和绿色标记。</p><p>在训练后的细胞中，那些带有1的细胞显示了对当前任务重要的神经元，这些神经元被用作未来的mask。具有一种以上颜色的单元格表示它们由多个任务共享。这0个没有颜色的单元格不会被任何任务使用。</p><h3 id="Task-Masks"><a href="#Task-Masks" class="headerlink" title="Task Masks"></a>Task Masks</h3><p>给定知识共享胶囊 $s^{(t)}_j$，TSM 通过全连接的网络将它们映射到输入 $k_l^{(t)} $，其中 $l$ 是 TSM 中的第 $l$ 层。</p><p>在训练任务的分类器期间，为TSM中每个层 $l$ 的每个任务 $t$ 训练任务掩码（“软”二元掩码）$m_l^{(t)}$，指示该层中对任务重要的神经元。</p><p>在这里借用了hard attention想法，并利用任务ID embedding来训练任务掩码。</p><p>对于task ID t，其嵌入 $e^{(t)}_l$ 由网络的其他部分一起学习的可微确定性参数组成。</p><script type="math/tex; mode=display">m_l^{(t)} = \sigma(se_l^{(t)})</script><p>给定 TSM 中每一层的输出 $k^{(t)}_l$，按元素相乘 $k^{(t)}_l ⊗ m_l^{(t)}$ 。最后一层 $k^{(t)}$ 的屏蔽输出通过跳跃连接馈送到下一层BERT。</p><p>在学习任务 $t$ 之后，保存最终 $m^{(t)}_l$ 并将其添加到集合 ${m^{(t)}_l}$。</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>对于每个过去的任务 $i<em>{prev} \in T</em>{prev}$，其掩码 $m^{(i<em>{prev})}_l$ 指示该任务使用的需要保护的神经元。在学习任务 t 中，$m_l^{(i</em>{prev})}$ 用于将 TSM 中第 $l$ 层所有使用的神经元上的梯度 $g_l^{(t)}$ 设置为 0。</p><p>在修改梯度之前，我们首先通过所有先前任务的掩码，累积所有使用的神经元。 由于 $m_l^{(iprev)}$ 是二进制的，我们使用最大池化来实现累计：</p><script type="math/tex; mode=display">m_l^{(t_{ac})} = MaxPool(\{m_l^{i_{prev}}\})</script><p>$m<em>l^{(t</em>{ac})}$ 被应用于梯度：</p><script type="math/tex; mode=display">g_l^{(t)} = g_l^{(t)} \otimes (1- m_l^{(t_{ac})})</script><p>对应于 $m^{(t<em>{ac})}_l$ 中的 1 个条目的那些梯度设置为 0，而其他保持不变。 通过这种方式，旧任务中的神经元受到保护。 请注意，我们扩展（复制）向量  $m^{(t</em>{ac})}_l$  以匹配 $g^{(t)}_l$ 的维度。</p><p>虽然这个想法是直观的，但 $e^{(t)}<em>l $ 并不容易训练。为了使 $e^{(t)}_l $  的学习更容易和更稳定，应用了退火策略。也就是说，s 在训练期间退火，引入梯度流，并在测试期间设置 $s=s</em>{max}$。</p><p>往上三个等式 将单位阶跃函数近似为掩码，当 s → ∞ 时，$m^{(t)}_l \to {0,1}$。一个训练 epoch 开始时，所有神经元都是同等活跃的，在这个 epoch 内逐渐极化。</p><p>具体地说，s 按如下方式退火：</p><script type="math/tex; mode=display">s = \frac{1}{s_{max}} + (s_{max} - \frac{1}{s_{max}}) \frac{b-1}{B-1}</script><p>其中 b 是批次索引，B 是epoch中的批次总数。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p><img src="https://z3.ax1x.com/2021/10/18/5UOc7R.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Adapting-BERT-for-Continual-Learning-of-a-Sequence-of-Aspect-Sentiment-Classification-Tasks&quot;&gt;&lt;a href=&quot;#Adapting-BERT-for-Continual-L</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
  <entry>
    <title>只用一行代码可以提高模型表现吗？</title>
    <link href="http://example.com/2021/10/10/%E5%8F%AA%E7%94%A8%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%E5%8F%AF%E4%BB%A5%E6%8F%90%E9%AB%98%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%8E%B0%E5%90%97%EF%BC%9F/"/>
    <id>http://example.com/2021/10/10/%E5%8F%AA%E7%94%A8%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%E5%8F%AF%E4%BB%A5%E6%8F%90%E9%AB%98%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%8E%B0%E5%90%97%EF%BC%9F/</id>
    <published>2021-10-10T14:59:59.000Z</published>
    <updated>2021-10-20T14:48:43.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="只用一行代码能提高模型表现吗"><a href="#只用一行代码能提高模型表现吗" class="headerlink" title="只用一行代码能提高模型表现吗?"></a>只用一行代码能提高模型表现吗?</h1><p>一行代码能做什么，有的人能发顶会，而有的人…</p><p>相信大家在训练模型的时候都会遇到一个现象，训练集损失降到一定的值之后，验证集的损失就开始上升了，在实验中一般奇怪的是准确率还跟着上升。这是为什么？如下图所示：</p><p><img src="https://z3.ax1x.com/2021/10/09/5kiiNV.png" alt=""></p><p>先看图(a)，是一个正常的训练过程，对于阶段A，随着training loss的降低，test loss也会 跟着降低;</p><p>但是到阶段B后，我们继续在训练集上训练，会让test loss上升。我们通常认为这是过拟合了，因为泛化误差变大了。</p><p>图 (b) 是ICML2020上《Do We Need Zero Training Loss After Achieving Zero Training Error》提出的flooding方法。这是一种使训练损失在一个小常量附近浮动的方法，以防止训练损失趋近于零 (这也是flooding的约束假设)。</p><p>为什么要防止训练损失趋近于0呢？</p><p>如果我们在模型已经记住了训练数据，完全没有错误的情况下仍继续训练，训练损失可以很容易地变得(接近)零，特别是对于过度参数化的模型。我们的模型其实就是个函数拟合器，在训练集上拟合的太好就容易发生过拟合。</p><p>经过推导（下文），flooding其实也和正则化的一些方法一样，通过各种方式避免训练过多。正则化方法可以被认为是间接控制训练损失的方法，通过引入额外的约束假设。</p><p>这里科普一下花书对于正则化的官方定义：</p><blockquote><p>凡是可以减少泛化误差(过拟合) 而不是减少训练误差的方法——正则化方法。</p></blockquote><p>其实对抗训练从理论上也是一种正则化方法，而正则化其实也可以理解成我们在求解最优化问题中的约束条件。我们通常希望将模型约束到一个较为”平坦“的损失，能够使得模型鲁棒性、泛化性更好。</p><p>从svm的角度来思考这个问题。对于一个线性可分的二分类问题，有无数条分类面能将其分开，而svm是去挑选能满足“最大间隔”的分类器。从另一个角度来理解是，越平坦的损失，是不是能越尽可能的将不同类给分开，因为样本进行些许扰动，损失的变化不会太大，相当于进行细微扰动后的样本也不会被分类到另一类去。</p><h2 id="flooding-方法分析"><a href="#flooding-方法分析" class="headerlink" title="flooding 方法分析"></a>flooding 方法分析</h2><p>论文其实就一行代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">logits = model(x)</span><br><span class="line">loss = criterion(logits, y)</span><br><span class="line">loss = (loss - b).<span class="built_in">abs</span>() + b <span class="comment"># This is it!</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><p>泛洪是直接解决训练损失变为(接近)零的问题。当训练损失达到合理的小值时，泛洪故意阻止训练损失的进一步减少。</p><p>设原来的损失函数为 $\mathcal{L}(\theta)$ ，改为 $\tilde{\mathcal{L}}(\theta)$:</p><script type="math/tex; mode=display">\tilde{\mathcal{L}}(\theta) =  |\mathcal{L}(\theta) - b| + b</script><p>其中 b 是超参数阈值</p><p>当 $\mathcal{L}(\theta) &gt; b$ 时， $ \tilde{\mathcal{L}}(\theta) =\mathcal{L}(\theta)$, 这个时候和正常他梯度下降无异；</p><p>当$\mathcal{L}(\theta) &lt;b$ 时， $ \tilde{\mathcal{L}}= 2b - \mathcal{L}(\theta)$ 变成了梯度上升了。</p><blockquote><p>当training loss大于一个阈值（flood level）时，进行正常的梯度下降；当training loss低于阈值时，会反过来进行梯度上升，让training loss保持在一个阈值附近，让模型持续进行“random walk”，并期望模型能被优化到一个平坦的损失区域，这样发现test loss进行了double decent！一个简单的理解是，这和early stop类的方法类似，防止参数被优化到一个不好的极小值出不来。</p></blockquote><p>这里借用 <a href="https://kexue.fm/archives/7643">我们真的需要把训练集的损失降低到零吗？</a> 的推导</p><p>当损失函数达到 b 之后，训练流程大概就是在交替执行梯度下降和梯度上升。直观想的话，感觉一步上升一步下降，似乎刚好抵消了。事实真的如此吗？我们来算一下看看。假设先下降一步后上升一步，学习率为 $\epsilon$，那么：</p><script type="math/tex; mode=display">\theta_n = \theta_{n-1} - \epsilon g(\theta_{n-1}) \ ,\ \ \theta_{n+1} = \theta_n + \epsilon g(\theta_n)</script><p>其中 $g(\theta) = \nabla_{\theta} \mathcal{L}(\theta)$ , 现在有：</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}\theta_{n+1} =&\, \theta_{n-1} - \varepsilon g(\theta_{n-1}) + \varepsilon g\big(\theta_{n-1} - \varepsilon g(\theta_{n-1})\big)\\ \approx&\,\theta_{n-1} - \varepsilon g(\theta_{n-1}) + \varepsilon \big(g(\theta_{n-1}) - \varepsilon \nabla_{\theta} g(\theta_{n-1}) g(\theta_{n-1})\big)\\ =&\,\theta_{n-1} - \frac{\varepsilon^2}{2}\nabla_{\theta}\Vert g(\theta_{n-1})\Vert^2 \end{aligned}\end{equation}</script><p>近似那一步是使用了泰勒展式对损失函数进行近似展开，最终的结果就是相当于损失函数为梯度惩罚 $\Vert g(\theta)\Vert^2=\Vert\nabla<em>{\theta}\mathcal{L}(\theta)\Vert^2$、学习率为 $\frac{\varepsilon^2}{2}$ 的梯度下降。更妙的是，改为“先上升再下降”，其表达式依然是一样的（这不禁让我想起“先升价10%再降价10%”和“先降价10%再升价10%”的故事）。因此，平均而言，Flooding对损失函数的改动，相当于在保证了损失函数足够小之后去最小化 $\Vert\nabla</em>{\theta}\mathcal{L}(\theta)\Vert^2$，也就是推动参数往更平稳的区域走，这通常能提供提高泛化性能（更好地抵抗扰动），因此一定程度上就能解释Flooding其作用的原因了。</p><p>本质上来讲，这跟往参数里边加入随机扰动、对抗训练等也没什么差别，只不过这里是保证了损失足够小后再加扰动。读者可以参考<a href="https://kexue.fm/archives/7466">《泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练》</a>了解相关内容，也可以参考“圣经”《深度学习》第二部分第七章的“正则化”一节。</p><h2 id="关于-b-的选择"><a href="#关于-b-的选择" class="headerlink" title="关于 b 的选择"></a>关于 b 的选择</h2><p> b 的选择，原论文说 b 的选择是一个暴力迭代的过程，需要多次尝试</p><blockquote><p> The flood level is chosen from $b\in {0, 0.01,0.02,…,0.50}$</p></blockquote><p>脑洞：b 无非就是决定什么时候开始交替训练罢了，那如果我们从一开始就用不同的学习率进行交替训练呢？也就是自始自终都执行</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}&\theta_n = \theta_{n-1} - \varepsilon_1 g(\theta_{n-1})\\ &\theta_{n+1} = \theta_n + \varepsilon_2 g(\theta_n) \end{aligned}\end{equation}</script><p>其中 $\varepsilon_1&gt; \varepsilon_2$，这样我们就把 b 去掉了（引入了 $ \varepsilon_1, \varepsilon_2$ 的选择，天下没有免费的午餐）。重复上述近似展开，我们就得到</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned} \theta_{n+1} =& \, \theta_{n-1} - \varepsilon_1g(\theta_{n-1})+\varepsilon_2g(\theta_{n-1} - \varepsilon_1g(\theta_{n-1}))\\\approx&\, \theta_{n-1} - \varepsilon_1g(\theta_{n-1}) + \varepsilon_2(g(\theta_{n-1}) - \varepsilon_1\nabla_\theta g(\theta_{n-1})g(\theta_{n-1}))\\=&\, \theta_{n-1} - (\varepsilon_1 - \varepsilon_2) g(\theta_{n-1}) - \frac{\varepsilon_1\varepsilon_2}{2}\nabla_{\theta}\Vert g(\theta_{n-1})\Vert^2\\ =&\,\theta_{n-1} - (\varepsilon_1 - \varepsilon_2)\nabla_{\theta}\left[\mathcal{L}(\theta_{n-1}) + \frac{\varepsilon_1\varepsilon_2}{2(\varepsilon_1 - \varepsilon_2)}\Vert \nabla_{\theta}\mathcal{L}(\theta_{n-1})\Vert^2\right] \end{aligned}\end{equation}</script><p>这就相当于自始自终都在用学习率 $\varepsilon<em>1-\varepsilon_2$ 来优化损失函数 $\mathcal {L}(\theta) + \frac {\varepsilon_1\varepsilon_2}{2 (\varepsilon_1 - \varepsilon_2)}\Vert\nabla</em>{\theta}\mathcal {L}(\theta)\Vert^2$ 也就是说一开始就把梯度惩罚给加了进去，这样能提升模型的泛化性能吗？<a href="http://www.danielpovey.com/files/2017_interspeech_backstitch.pdf">《Backstitch: Counteracting Finite-sample Bias via Negative Steps》</a>里边指出这种做法在语音识别上是有效的，请读者自行测试甄别。</p><p>这种做法在这篇博客上做了尝试，可能验证loss会降的更低一点，但具体得分情况还得自己尝试。<a href="https://wmathor.com/index.php/archives/1551/">我们真的需要把训练集的损失降到零吗？</a></p><h2 id="实验测试"><a href="#实验测试" class="headerlink" title="实验测试"></a>实验测试</h2><p>在第五届达观杯竞赛中使用的BERT模型，进行了实验。原论文的实验配合Eearly Stop 和 Weight decay 一起使用效果较好。重要的要花时间去调的是b的取值，初始的b值一般设为 验证集loss开始上扬的值的一半。</p><p>在我的实验中发现，在预训练后的bert模型加上dice loss之后，验证集loss上扬的情况就不存在了。但是预训练后的bert加上cross entropy还是会上扬。而未经过预训练的bert无论是在dice loss还是cross entropy上都会上扬。分析背后的原因可能有二：</p><ul><li>预训练后的bert模型表现更加稳定，对数据有一定的认识。</li><li>cross entropy对每个样本都一视同仁，不管当前样本是简单还是复杂。当简单样本有很多时，模型训练就会被这些简单的样本占据，使得模型难以从复杂样本中学习，而dice loss一旦模型正确分类当前样本（刚刚过0.5），就会使模型更少关注它，而不是像交叉熵那样，鼓励模型迫近0或1这两个点。这就能有效避免模型训练受到简单样本的支配，同时也防止了过拟合。</li></ul><h3 id="无flooding的情况下"><a href="#无flooding的情况下" class="headerlink" title="无flooding的情况下"></a>无flooding的情况下</h3><p>预训练后的bert+dice loss 的情况如下图所示。</p><p><img src="https://z3.ax1x.com/2021/10/10/5Ah6FP.png" alt=""></p><p>预训练后的bert + cross entropy，依旧上扬但相比下一个图未经预训练bert的情况要好一些。</p><p><img src="https://z3.ax1x.com/2021/10/10/5ETvN9.png" alt=""></p><p>未经预训练的bert+cross entropy</p><p><img src="https://z3.ax1x.com/2021/10/10/5EIsUA.png" alt=""></p><h3 id="经过flooding后"><a href="#经过flooding后" class="headerlink" title="经过flooding后"></a>经过flooding后</h3><p>No_pretrain bert/ cross entropy/  flooding b=0.5/ weight decy=0.01/ early patience=12 相比于上图验证集的loss已经不在无止境的上扬了。在下图的15到20step之间train的loss也不是和上图一样一路走低，而是出现了波动，这和论文的预期一致。</p><p><img src="https://z3.ax1x.com/2021/10/10/5ECkIP.png" alt=""></p><p>未经预训练后的bert b=0.5  dice loss</p><p><img src="https://z3.ax1x.com/2021/10/10/5EIvb4.png" alt=""></p><p>未经预训练后的bert，b=1.0 ,cross entropy</p><p><img src="https://i.loli.net/2021/10/10/DfcyQP3n5oqFLze.png" alt=""></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>flooding确实可以缓解验证集损失上扬的现象，而且本质还是个正则化的功能。至于具体效果有多大，是好是坏还是要根据具体任务去调试b的取值。</p><p>不过我的实验可以证明  预训练后的bert 和 dice loss 确实是可以让模型避免出现类似过拟合的现象。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://wmathor.com/index.php/archives/1551/">我们真的需要把训练集的损失降到零吗？</a></p><p><a href="https://zhuanlan.zhihu.com/p/163676138">【论文】一行代码发一篇ICML？</a></p><p><a href="https://kexue.fm/archives/7643">我们真的需要把训练集的损失降低到零吗？</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;只用一行代码能提高模型表现吗&quot;&gt;&lt;a href=&quot;#只用一行代码能提高模型表现吗&quot; class=&quot;headerlink&quot; title=&quot;只用一行代码能提高模型表现吗?&quot;&gt;&lt;/a&gt;只用一行代码能提高模型表现吗?&lt;/h1&gt;&lt;p&gt;一行代码能做什么，有的人能发顶会，而有的</summary>
      
    
    
    
    
    <category term="ML&amp;DL" scheme="http://example.com/tags/ML-DL/"/>
    
  </entry>
  
  <entry>
    <title>79/130/200/733FloodFill/17/22/784字符串回溯</title>
    <link href="http://example.com/2021/10/10/79-130-200-733FloodFill-17-22-784%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%9B%9E%E6%BA%AF/"/>
    <id>http://example.com/2021/10/10/79-130-200-733FloodFill-17-22-784%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%9B%9E%E6%BA%AF/</id>
    <published>2021-10-10T01:37:48.000Z</published>
    <updated>2021-10-22T01:21:51.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="79-130-200-733FloodFill-17-22-784字符串回溯"><a href="#79-130-200-733FloodFill-17-22-784字符串回溯" class="headerlink" title="79/130/200/733FloodFill/17/22/784字符串回溯"></a>79/130/200/733FloodFill/17/22/784字符串回溯</h1><h2 id="Flood-Fill"><a href="#Flood-Fill" class="headerlink" title="Flood Fill"></a>Flood Fill</h2><p>提示：Flood 是「洪水」的意思，Flood Fill 直译是「泛洪填充」的意思，体现了洪水能够从一点开始，迅速填满当前位置附近的地势低的区域。类似的应用还有：PS 软件中的「点一下把这一片区域的颜色都替换掉」，扫雷游戏「点一下打开一大片没有雷的区域」。</p><p>下面这几个问题，思想不难，但是初学的时候代码很不容易写对，并且也很难调试。我们的建议是多写几遍，忘记了就再写一次，参考规范的编写实现（设置 visited 数组，设置方向数组，抽取私有方法），把代码写对。</p><h3 id="79-单词搜索"><a href="#79-单词搜索" class="headerlink" title="79. 单词搜索"></a><a href="https://leetcode-cn.com/problems/word-search/">79. 单词搜索</a></h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span>[][] DIRECTIONS = &#123;&#123;-<span class="number">1</span>,<span class="number">0</span>&#125;, &#123;<span class="number">0</span>, -<span class="number">1</span>&#125;, &#123;<span class="number">0</span>,<span class="number">1</span>&#125;, &#123;<span class="number">1</span>,<span class="number">0</span>&#125;&#125;;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> rows;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> cols;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> len;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span>[][] visited;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">char</span>[] charArray;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">char</span>[][] board;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">exist</span><span class="params">(<span class="keyword">char</span>[][] board, String word)</span> </span>&#123;</span><br><span class="line">        rows = board.length;</span><br><span class="line">        <span class="keyword">if</span> (rows==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cols = board[<span class="number">0</span>].length;</span><br><span class="line">        visited = <span class="keyword">new</span> <span class="keyword">boolean</span>[rows][cols];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.len = word.length();</span><br><span class="line">        <span class="keyword">this</span>.charArray = word.toCharArray();</span><br><span class="line">        <span class="keyword">this</span>.board = board;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;rows;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;cols;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(dfs(i, j, <span class="number">0</span>))&#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">inArea</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> x&gt;=<span class="number">0</span> &amp;&amp; x&lt;rows &amp;&amp; y&gt;=<span class="number">0</span> &amp;&amp; y&lt;cols;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y, <span class="keyword">int</span> begin)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(begin == len-<span class="number">1</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> board[x][y] == charArray[begin];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(board[x][y] == charArray[begin])&#123;</span><br><span class="line">            visited[x][y] = <span class="keyword">true</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span>[] direction:DIRECTIONS)&#123;</span><br><span class="line">                <span class="keyword">int</span> newX = x + direction[<span class="number">0</span>];</span><br><span class="line">                <span class="keyword">int</span> newY = y + direction[<span class="number">1</span>];</span><br><span class="line">                <span class="keyword">if</span>(inArea(newX,newY) &amp;&amp; !visited[newX][newY])&#123;</span><br><span class="line">                    <span class="keyword">if</span>(dfs(newX, newY, begin+<span class="number">1</span>))&#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            visited[x][y] = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>说明：</p><p>偏移量数组在二维平面内是经常使用的，可以把它的设置当做一个技巧，并且在这个问题中，偏移量数组内的 4 个偏移的顺序无关紧要；<br>说明：类似使用这个技巧的问题还有：「力扣」第 130 题：被围绕的区域、「力扣」第 200 题：岛屿数量。</p><p>对于这种搜索算法，我认为理解 DFS 和状态重置并不难，代码编写也相对固定，难在代码的编写和细节的处理，建议多次编写，自己多总结多思考，把自己遇到的坑记下。</p><h2 id="130-被围绕的区域"><a href="#130-被围绕的区域" class="headerlink" title="130. 被围绕的区域"></a><a href="https://leetcode-cn.com/problems/surrounded-regions/">130. 被围绕的区域</a></h2><h4 id="法一：深度优先遍历"><a href="#法一：深度优先遍历" class="headerlink" title="法一：深度优先遍历"></a>法一：深度优先遍历</h4><p>关键：与边界相连 $O$ 不能被替换成 $X$</p><p>具体步骤：</p><ul><li>第一步：把四周有 $O$ 的地方都替换成 $-$，在四周进行 floodfill 算法（染色）</li><li>第二步：再从头到尾遍历一遍，把$O$ 换成 $X$，把 $-$ 换成 $O$</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span>[][] DIRECTIONS = &#123;&#123;-<span class="number">1</span>,<span class="number">0</span>&#125;, &#123;<span class="number">0</span>, -<span class="number">1</span>&#125;, &#123;<span class="number">0</span>, <span class="number">1</span>&#125;, &#123;<span class="number">1</span>,<span class="number">0</span>&#125;&#125;;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> rows;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> cols;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">solve</span><span class="params">(<span class="keyword">char</span>[][] board)</span> </span>&#123;</span><br><span class="line">        rows = board.length;</span><br><span class="line">        <span class="keyword">if</span>(rows==<span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">        cols = board[<span class="number">0</span>].length;</span><br><span class="line">        <span class="keyword">if</span>(cols==<span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 第一步：把四周的 0 以及与 0 连通的 0 都设置成 -</span></span><br><span class="line">        <span class="comment">// 第一列和最后一列</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;rows; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(board[i][<span class="number">0</span>] == <span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">                dfs(i, <span class="number">0</span>, board);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(board[i][cols-<span class="number">1</span>] == <span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">                dfs(i, cols-<span class="number">1</span>, board);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 第一行和最后一行</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;cols;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(board[<span class="number">0</span>][i] == <span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">                dfs(<span class="number">0</span>, i, board);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(board[rows-<span class="number">1</span>][i] == <span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">                dfs(rows-<span class="number">1</span>, i, board);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 第二行：遍历一次棋盘</span></span><br><span class="line">        <span class="comment">// 1. 剩下的O就是被包围的O</span></span><br><span class="line">        <span class="comment">// 2. - 是原来不能被包围的O，恢复成O</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;rows;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;cols; j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(board[i][j] == <span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">                    board[i][j] = <span class="string">&#x27;X&#x27;</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span>(board[i][j] == <span class="string">&#x27;-&#x27;</span>)&#123;</span><br><span class="line">                    board[i][j] = <span class="string">&#x27;O&#x27;</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">inArea</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y, <span class="keyword">int</span> rows, <span class="keyword">int</span> cols)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> x&gt;=<span class="number">0</span> &amp;&amp; x&lt;rows &amp;&amp; y&gt;=<span class="number">0</span> &amp;&amp; y&lt;cols;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j, <span class="keyword">char</span>[][] board)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(inArea(i,j,rows,cols) &amp;&amp; board[i][j]==<span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">            board[i][j] = <span class="string">&#x27;-&#x27;</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> k=<span class="number">0</span>; k&lt;<span class="number">4</span>; k++)&#123;</span><br><span class="line">                <span class="keyword">int</span> newX = i + DIRECTIONS[k][<span class="number">0</span>];</span><br><span class="line">                <span class="keyword">int</span> newY = j + DIRECTIONS[k][<span class="number">1</span>];</span><br><span class="line">                dfs(newX, newY, board);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>时间复杂度：$O(rows * cols)$ ，其中 rows 和 cols 分别为矩阵的行数和列数， 深度优先遍历过程中，每一个单元格至多只会被标记一次</li><li>空间复杂度：$O(rows * cols)$ ，深度优先遍历最多使用的栈的开销为整个棋盘大小</li></ul><h3 id="法二：广度优先遍历"><a href="#法二：广度优先遍历" class="headerlink" title="法二：广度优先遍历"></a>法二：广度优先遍历</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span>[][] DIRECTIONS = &#123;&#123;-<span class="number">1</span>,<span class="number">0</span>&#125;, &#123;<span class="number">0</span>, -<span class="number">1</span>&#125;, &#123;<span class="number">0</span>, <span class="number">1</span>&#125;, &#123;<span class="number">1</span>,<span class="number">0</span>&#125;&#125;;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> rows;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> cols;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">solve</span><span class="params">(<span class="keyword">char</span>[][] board)</span> </span>&#123;</span><br><span class="line">        rows = board.length;</span><br><span class="line">        <span class="keyword">if</span>(rows==<span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">        cols = board[<span class="number">0</span>].length;</span><br><span class="line">        <span class="keyword">if</span>(cols==<span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 第一步：把四周的‘O’ 全部推入队列，通过广度优先遍历，把’O‘连通的地方全部编辑</span></span><br><span class="line">        Queue&lt;<span class="keyword">int</span> []&gt; queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;rows; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(board[i][<span class="number">0</span>] == <span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">                queue.offer(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;i, <span class="number">0</span>&#125;);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(board[i][cols-<span class="number">1</span>] == <span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">                queue.offer(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;i, cols-<span class="number">1</span>&#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;cols; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(board[<span class="number">0</span>][i] == <span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">                queue.offer(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">0</span>, i&#125;);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(board[rows-<span class="number">1</span>][i] == <span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">                queue.offer(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;rows-<span class="number">1</span>, i&#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(!queue.isEmpty())&#123;</span><br><span class="line">            <span class="keyword">int</span>[] top = queue.poll();</span><br><span class="line">            <span class="keyword">int</span> i = top[<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">int</span> j = top[<span class="number">1</span>];</span><br><span class="line">            board[i][j] = <span class="string">&#x27;-&#x27;</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span>[] direction : DIRECTIONS)&#123;</span><br><span class="line">                <span class="keyword">int</span> newX = i + direction[<span class="number">0</span>];</span><br><span class="line">                <span class="keyword">int</span> newY = j + direction[<span class="number">1</span>];</span><br><span class="line">                <span class="keyword">if</span>(inArea(newX, newY, rows, cols) &amp;&amp; board[newX][newY]==<span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">                    queue.offer(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;newX, newY&#125;);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 第 2 步：恢复</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; rows; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; cols; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (board[i][j] == <span class="string">&#x27;-&#x27;</span>) &#123;</span><br><span class="line">                    board[i][j] = <span class="string">&#x27;O&#x27;</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (board[i][j] == <span class="string">&#x27;O&#x27;</span>) &#123;</span><br><span class="line">                    board[i][j] = <span class="string">&#x27;X&#x27;</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">     <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">inArea</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y, <span class="keyword">int</span> rows, <span class="keyword">int</span> cols)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> x &gt;= <span class="number">0</span> &amp;&amp; x &lt; rows &amp;&amp; y &gt;= <span class="number">0</span> &amp;&amp; y &lt; cols;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="法三：并查集"><a href="#法三：并查集" class="headerlink" title="法三：并查集"></a>法三：并查集</h3><ul><li>把四周的 $O$ 都和一个虚拟节点合并起来</li><li>在内部，只看两个方向，把 $O$ 都合并起来</li><li>最后再扫一次数组，不和 虚拟节点 链接的 $O$都标记成 $X$</li></ul><p>并查集的写法容易受到 floorfill的影响， 用并查集的时候，其实只用每一行的右边和下面都看一下，只针对 $O$， 能合并就合并一下。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">UnionFind</span></span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">int</span>[] parent;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">UnionFind</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.parent = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++)&#123;</span><br><span class="line">                parent[i] = i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isConnected</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">            <span class="keyword">return</span> find(x) == find(y);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">            <span class="keyword">while</span>(x != parent[x])&#123;</span><br><span class="line">                parent[x] = parent[parent[x]];</span><br><span class="line">                x = parent[x];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> x;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">union</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">            <span class="keyword">int</span> xRoot = find(x);</span><br><span class="line">            <span class="keyword">int</span> yRoot = find(y);</span><br><span class="line">            <span class="keyword">if</span>(xRoot == yRoot)&#123;</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            parent[xRoot] = yRoot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">getIndex</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y, <span class="keyword">int</span> cols)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> x*cols +y;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">solve</span><span class="params">(<span class="keyword">char</span>[][] board)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> rows = board.length;</span><br><span class="line">        <span class="keyword">if</span>(rows == <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> cols = board[<span class="number">0</span>].length;</span><br><span class="line">        <span class="keyword">if</span>(cols == <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        UnionFind unionFind = <span class="keyword">new</span> UnionFind(rows * cols + <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">int</span> dummyNode = rows*cols;</span><br><span class="line">        <span class="comment">// 填写第一行和最后一行</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt; cols; j++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(board[<span class="number">0</span>][j] == <span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">                unionFind.union(getIndex(<span class="number">0</span>, j, cols), dummyNode);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(board[rows-<span class="number">1</span>][j] == <span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">                unionFind.union(getIndex(rows-<span class="number">1</span>, j ,cols), dummyNode);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 填写第 1 列和最后一列</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; rows - <span class="number">1</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (board[i][<span class="number">0</span>] == <span class="string">&#x27;O&#x27;</span>) &#123;</span><br><span class="line">                unionFind.union(getIndex(i, <span class="number">0</span>, cols), dummyNode);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (board[i][cols - <span class="number">1</span>] == <span class="string">&#x27;O&#x27;</span>) &#123;</span><br><span class="line">                unionFind.union(getIndex(i, cols - <span class="number">1</span>, cols), dummyNode);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span>[][] directions = <span class="keyword">new</span> <span class="keyword">int</span>[][]&#123;&#123;<span class="number">0</span>,<span class="number">1</span>&#125;,&#123;<span class="number">1</span>,<span class="number">0</span>&#125;&#125;;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;rows; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;cols;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(board[i][j] == <span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">                    <span class="keyword">for</span>(<span class="keyword">int</span>[] direction : directions)&#123;</span><br><span class="line">                        <span class="keyword">int</span> newX = i + direction[<span class="number">0</span>];</span><br><span class="line">                        <span class="keyword">int</span> newY = j + direction[<span class="number">1</span>];</span><br><span class="line">                        <span class="keyword">if</span>(newX &lt; rows &amp;&amp; newY&lt;cols &amp;&amp; board[newX][newY] ==<span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">                            unionFind.union(getIndex(i, j, cols), getIndex(newX, newY, cols));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; rows - <span class="number">1</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; cols - <span class="number">1</span>; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (board[i][j] == <span class="string">&#x27;O&#x27;</span>) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (!unionFind.isConnected(getIndex(i, j, cols), dummyNode)) &#123;</span><br><span class="line">                        board[i][j] = <span class="string">&#x27;X&#x27;</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="200-岛屿数量"><a href="#200-岛屿数量" class="headerlink" title="200. 岛屿数量"></a><a href="https://leetcode-cn.com/problems/number-of-islands/">200. 岛屿数量</a></h2><p>Flood法：从一个区域中提取若干个连通的点与其他相邻区域分开。</p><p>从一个点扩散开，找到与其连通的点，其实就是从一个点卡死，进行一次深度优先或广度优先遍历，发现一片连着的区域。把与之相连的所有的格子都标记上，视为发现了一个「岛屿」。</p><p>深度优先：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span>[][] visited;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> rows;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> cols;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span>[][] DIRECTIONS = &#123;&#123;-<span class="number">1</span>,<span class="number">0</span>&#125;, &#123;<span class="number">0</span>,-<span class="number">1</span>&#125;, &#123;<span class="number">1</span>,<span class="number">0</span>&#125;, &#123;<span class="number">0</span>,<span class="number">1</span>&#125;&#125;;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">char</span>[][] grid;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">numIslands</span><span class="params">(<span class="keyword">char</span>[][] grid)</span> </span>&#123;</span><br><span class="line">        rows = grid.length;</span><br><span class="line">        <span class="keyword">if</span>(rows==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        cols = grid[<span class="number">0</span>].length;</span><br><span class="line">        <span class="keyword">if</span>(cols==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        visited = <span class="keyword">new</span> <span class="keyword">boolean</span>[rows][cols];</span><br><span class="line">        <span class="keyword">this</span>.grid = grid;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; rows; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; cols; j++) &#123;</span><br><span class="line">                <span class="comment">// 如果是岛屿中的一个点，并且没有被访问过，就进行深度优先遍历</span></span><br><span class="line">                <span class="keyword">if</span> (!visited[i][j] &amp;&amp; grid[i][j] == <span class="string">&#x27;1&#x27;</span>) &#123;</span><br><span class="line">                    dfs(i, j);</span><br><span class="line">                    count++;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">inArea</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> x&gt;=<span class="number">0</span> &amp;&amp; x&lt;rows &amp;&amp; y&gt;=<span class="number">0</span> &amp;&amp; y&lt;cols;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j)</span></span>&#123;</span><br><span class="line">        visited[i][j] = <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">0</span>;k&lt;<span class="number">4</span>;k++)&#123;</span><br><span class="line">            <span class="keyword">int</span> newX = i + DIRECTIONS[k][<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">int</span> newY = j + DIRECTIONS[k][<span class="number">1</span>];</span><br><span class="line">            <span class="comment">// 如果不越界，还是陆地，没有被访问过</span></span><br><span class="line">            <span class="keyword">if</span> (inArea(newX, newY) &amp;&amp; grid[newX][newY] == <span class="string">&#x27;1&#x27;</span> &amp;&amp; !visited[newX][newY]) &#123;</span><br><span class="line">                dfs(newX, newY);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>广度优先：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.LinkedList;</span><br><span class="line"><span class="keyword">import</span> java.util.Queue;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span>[][] DIRECTIONS = &#123;&#123;-<span class="number">1</span>, <span class="number">0</span>&#125;, &#123;<span class="number">0</span>, -<span class="number">1</span>&#125;, &#123;<span class="number">1</span>, <span class="number">0</span>&#125;, &#123;<span class="number">0</span>, <span class="number">1</span>&#125;&#125;;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> rows;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> cols;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">char</span>[][] grid;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span>[][] visited;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">numIslands</span><span class="params">(<span class="keyword">char</span>[][] grid)</span> </span>&#123;</span><br><span class="line">        rows = grid.length;</span><br><span class="line">        <span class="keyword">if</span> (rows == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cols = grid[<span class="number">0</span>].length;</span><br><span class="line">        <span class="keyword">this</span>.grid = grid;</span><br><span class="line">        visited = <span class="keyword">new</span> <span class="keyword">boolean</span>[rows][cols];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; rows; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; cols; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (!visited[i][j] &amp;&amp; grid[i][j] == <span class="string">&#x27;1&#x27;</span>) &#123;</span><br><span class="line">                    bfs(i, j);</span><br><span class="line">                    count++;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">bfs</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">        Queue&lt;Integer&gt; queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">        queue.offer(i * cols + j);</span><br><span class="line">        <span class="comment">// 注意：这里要标记上已经访问过</span></span><br><span class="line">        visited[i][j] = <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">while</span> (!queue.isEmpty()) &#123;</span><br><span class="line">            <span class="keyword">int</span> cur = queue.poll();</span><br><span class="line">            <span class="keyword">int</span> curX = cur / cols;</span><br><span class="line">            <span class="keyword">int</span> curY = cur % cols;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; <span class="number">4</span>; k++) &#123;</span><br><span class="line">                <span class="keyword">int</span> newX = curX + DIRECTIONS[k][<span class="number">0</span>];</span><br><span class="line">                <span class="keyword">int</span> newY = curY + DIRECTIONS[k][<span class="number">1</span>];</span><br><span class="line">                <span class="keyword">if</span> (inArea(newX, newY) &amp;&amp; grid[newX][newY] == <span class="string">&#x27;1&#x27;</span> &amp;&amp; !visited[newX][newY]) &#123;</span><br><span class="line">                    queue.offer(newX * cols + newY);</span><br><span class="line">                    </span><br><span class="line">                    visited[newX][newY] = <span class="keyword">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">inArea</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> x &gt;= <span class="number">0</span> &amp;&amp; x &lt; rows &amp;&amp; y &gt;= <span class="number">0</span> &amp;&amp; y &lt; cols;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>并查集</p><p>关于连通性问题，并查集也是常用的数据结构</p><p>思路：并查集中维护连通分量的个数，在遍历的过程中：</p><ul><li>相邻的陆地（只需要向右看和向下看）合并，只要发生过合并，岛屿数量就减一</li><li>在遍历过程中，同时记录空地的数量</li><li>并查集中连通分量的个数 - 空地的个数 = 岛屿数</li></ul><h2 id="733-图像渲染"><a href="#733-图像渲染" class="headerlink" title="733. 图像渲染"></a><a href="https://leetcode-cn.com/problems/flood-fill/">733. 图像渲染</a></h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> m,n;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span>[][] directions = &#123;&#123;<span class="number">1</span>,<span class="number">0</span>&#125;,&#123;<span class="number">0</span>,<span class="number">1</span>&#125;,&#123;-<span class="number">1</span>,<span class="number">0</span>&#125;,&#123;<span class="number">0</span>,-<span class="number">1</span>&#125;&#125;;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[][] floodFill(<span class="keyword">int</span>[][] image, <span class="keyword">int</span> sr, <span class="keyword">int</span> sc, <span class="keyword">int</span> newColor) &#123;</span><br><span class="line">        m = image.length;</span><br><span class="line">        n = image[<span class="number">0</span>].length;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> currColor = image[sr][sc];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(currColor!=newColor)&#123;</span><br><span class="line">            dfs(sr, sc,currColor, newColor, image); </span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> image;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">inArea</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> x&gt;=<span class="number">0</span> &amp;&amp; x&lt;m &amp;&amp; y&gt;=<span class="number">0</span> &amp;&amp; y&lt;n;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> sr, <span class="keyword">int</span> sc, <span class="keyword">int</span> color, <span class="keyword">int</span> newColor, <span class="keyword">int</span>[][] image)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(image[sr][sc] == color)&#123;</span><br><span class="line">            image[sr][sc] = newColor;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span>[] direct : directions)&#123;</span><br><span class="line">                <span class="keyword">int</span> newx = sr + direct[<span class="number">0</span>];</span><br><span class="line">                <span class="keyword">int</span> newy = sc + direct[<span class="number">1</span>];</span><br><span class="line">                <span class="keyword">if</span>(inArea(newx, newy))&#123;</span><br><span class="line">                    dfs(newx,newy, color, newColor, image);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="17-电话号码的字母组合"><a href="#17-电话号码的字母组合" class="headerlink" title="17. 电话号码的字母组合"></a><a href="https://leetcode-cn.com/problems/letter-combinations-of-a-phone-number/">17. 电话号码的字母组合</a></h2><ul><li>由于字符追加到后面，是新创建一个对象，因此 <strong>没有显式回溯（状态重置）的过程 </strong>；</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;String&gt; <span class="title">letterCombinations</span><span class="params">(String digits)</span> </span>&#123;</span><br><span class="line">        String[] digitsMap =  &#123;<span class="string">&quot;abc&quot;</span>,<span class="string">&quot;def&quot;</span>,<span class="string">&quot;ghi&quot;</span>,<span class="string">&quot;jkl&quot;</span>,<span class="string">&quot;mno&quot;</span>,<span class="string">&quot;pqrs&quot;</span>,<span class="string">&quot;tuv&quot;</span>,<span class="string">&quot;wxyz&quot;</span>&#125;;</span><br><span class="line">        List&lt;String&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(digits.length() == <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> res;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        findCombinations(digits, digitsMap, <span class="number">0</span>, <span class="string">&quot;&quot;</span>, res);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">findCombinations</span><span class="params">(String digits, String[] digitsMap, <span class="keyword">int</span> start, String pre, List&lt;String&gt; res)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(start == digits.length())&#123;</span><br><span class="line">            res.add(pre);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        String nextStr = digitsMap[digits.charAt(start) - <span class="string">&#x27;2&#x27;</span>];</span><br><span class="line">        <span class="keyword">int</span> len = nextStr.length();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;len; i++)&#123;</span><br><span class="line">            findCombinations(digits, digitsMap, start+<span class="number">1</span>, pre+nextStr.charAt(i), res);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="784-字母大小写全排列"><a href="#784-字母大小写全排列" class="headerlink" title="784. 字母大小写全排列"></a><a href="https://leetcode-cn.com/problems/letter-case-permutation/">784. 字母大小写全排列</a></h2><p><img src="https://i.loli.net/2021/10/21/bfzBmC8V7c3gFhO.png" alt=""></p><p>大小写转换问题，使用异或运算转换。</p><p>ASCII表 A到Z，Z完了之后没有直接到a，中间间隔了6个字符</p><p><img src="https://i.loli.net/2021/10/21/5IeZNlnCD3J94kY.png" alt=""></p><p>发现大写字符与其对应的小写字符的ASCII的差为32，32这个值是 $2^5$ 可以表示为 $ 1&lt;&lt;5$</p><p>变换大小写这件事等价于：</p><ul><li>如果字符是小写字符，减去32得到大写字符</li><li>如果字符是大写字符，加上32得到小写字符</li></ul><p>而这两者合并起来，就是给这个字符做一次不进位的加法，即异或上 $1&lt;&lt;5$​</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;String&gt; <span class="title">letterCasePermutation</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = s.length();</span><br><span class="line">        List&lt;String&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(n==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> res;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">char</span>[] charArray = s.toCharArray();</span><br><span class="line">        dfs(charArray, <span class="number">0</span>, res);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">char</span>[] charArray, <span class="keyword">int</span> index, List&lt;String&gt; res)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(index == charArray.length)&#123;</span><br><span class="line">            res.add(<span class="keyword">new</span> String(charArray));</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        dfs(charArray, index+<span class="number">1</span>, res);</span><br><span class="line">        <span class="keyword">if</span>(Character.isLetter(charArray[index]))&#123;</span><br><span class="line">            charArray[index] ^= <span class="number">1</span>&lt;&lt;<span class="number">5</span>;</span><br><span class="line">            dfs(charArray, index+<span class="number">1</span>, res);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="22-括号生成"><a href="#22-括号生成" class="headerlink" title="22. 括号生成"></a><a href="https://leetcode-cn.com/problems/generate-parentheses/">22. 括号生成</a></h2><h3 id="方法一：深度优先遍历"><a href="#方法一：深度优先遍历" class="headerlink" title="方法一：深度优先遍历"></a>方法一：深度优先遍历</h3><p>我们以 <code>n = 2</code> 为例，画树形结构图。方法是 「做减法」。</p><p><img src="https://i.loli.net/2021/10/22/O2Gqza81NJeYDMZ.jpg" alt=""></p><p>画出图后可分析出的结论：</p><ul><li>当前左右括号都有大于0个可以使用的时候，才可以产生分支。</li><li>产生左分支的时候，只看当前是否还有左括号可以使用</li><li>产生右分支的时候，还收到左分支的限制，右边剩余可以使用的括号数量一定得严格大于左边剩余的数量的时候，才可以阐释分支。</li><li>在左边和右边剩余的括号数都为0时结算</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;String&gt; <span class="title">generateParenthesis</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(n==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> res;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        dfs(<span class="string">&quot;&quot;</span>,n , n, res);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">dfs</span><span class="params">(String curStr, <span class="keyword">int</span> left, <span class="keyword">int</span> right, List&lt;String&gt; res)</span></span>&#123;</span><br><span class="line">        <span class="comment">// 因为每一次尝试，都是使用新的字符串变量所有无须回溯</span></span><br><span class="line">        <span class="comment">// 在递归终止的时候，直接把它天道结果集即可</span></span><br><span class="line">        <span class="keyword">if</span>(left==<span class="number">0</span> &amp;&amp; right==<span class="number">0</span>)&#123;</span><br><span class="line">            res.add(curStr);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 剪枝 （左括号可以使用的个数严格大于右括号可以使用的个数，才剪枝，注意这个细节）</span></span><br><span class="line">        <span class="keyword">if</span>(left &gt; right)&#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(left&gt;<span class="number">0</span>)&#123;</span><br><span class="line">            dfs(curStr + <span class="string">&quot;(&quot;</span> , left-<span class="number">1</span>, right, res);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(right&gt;<span class="number">0</span>)&#123;</span><br><span class="line">            dfs(curStr + <span class="string">&quot;)&quot;</span>, left , right-<span class="number">1</span>, res);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;79-130-200-733FloodFill-17-22-784字符串回溯&quot;&gt;&lt;a href=&quot;#79-130-200-733FloodFill-17-22-784字符串回溯&quot; class=&quot;headerlink&quot; title=&quot;79/130/200/733Fl</summary>
      
    
    
    
    
    <category term="LeetCode" scheme="http://example.com/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>Meta-Learning Representations for Continual Learning</title>
    <link href="http://example.com/2021/10/09/Meta-Learning-Representations-for-Continual-Learning/"/>
    <id>http://example.com/2021/10/09/Meta-Learning-Representations-for-Continual-Learning/</id>
    <published>2021-10-09T07:11:17.000Z</published>
    <updated>2021-10-10T03:48:27.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Meta-Learning-Representations-for-Continual-Learning"><a href="#Meta-Learning-Representations-for-Continual-Learning" class="headerlink" title="Meta-Learning Representations for Continual Learning"></a>Meta-Learning Representations for Continual Learning</h1><p>持续学习的代理应该能够在现有知识的基础上快速学习新数据，同时最大限度地减少遗忘。</p><p>目前基于神经网络函数逼近器的智能系统 很容易遗忘，而且很少经过训练来促进未来的学习。这种糟糕行为的一个原因是，他们从没有为这两个目标明确训练的表征中学习。</p><p>本文提出了OML，它的目标是通过学习表征来直接最小化灾难性的干扰，加速未来的学习，并且在连续学习中对在线更新下的遗忘具有健壮性。</p><p>证明了学习自然稀疏表示是可能的，这对于在线更新更有效。此外，该算法是对现有的连续学习策略(如MER和GEM)的补充。</p><blockquote><p>有经验的程序员学习一门新的编程语言比以前从未编程的人要快得多，而且不需要忘记旧的语言来学习新的语言。</p></blockquote><p>在这项工作中，显式地学习一种持续学习的表示法，以避免干扰并促进未来的学习。设计一个元目标，它使用灾难性干扰作为训练信号，通过在线更新直接优化。目标是学习一种表示，以便模型在 meta-test 时使用的随机在线更新总体上提高其预测的准确性。</p><h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><p>Continual Learning Prediction (CLP)问题由无休止的样本流组成</p><script type="math/tex; mode=display">T = (X_1,Y_1) ,(X_2,Y_2) ,..., (X_t,Y_t) ...</script><p>随机向量 $Y<em>t$ 根据未知分布 $p(Y|X_t)$抽样。我们假设过程 $X_1,X_2,..,X_t,…$有一个边际分布 $\mu ：X \to [0,\infty)$，它反映了每个输入被观察到的频率。这种假设允许各种相关序列。例如，可以从潜在依赖于过去变量 $X</em>{t-1}$ 和 $X_{t-2}$ 的分布中采样 $X_t$。然而目标 $Y_t$ 仅依赖于 $X_t$ , 而不依赖于过去的 $X_i$。</p><p>定义 $S<em>k = (X</em>{j+1}, Y<em>{j+1}),(X</em>{j+2}, Y<em>{j+2}) ,…,(X</em>{j+k}, Y_{j+k}) $ 为从CLP问题 $T$ 中抽样的长度为 $k$ 的随机轨迹。</p><p>最后，$p(S_k|T)$ 给出了可以从问题 $T$ 中抽样的所有长度为 $k$ 的轨迹上的分布。</p><p>对于给定的CLP问题，我们的目标是学习一个函数 $f_{W,\theta}$ 它可以预测给定 $X_t$ 的 $Y_t$。更具体地说，设 $l:Y\times Y \to R$ 是将预测 $\hat y \in Y$ 和目标 $y$ 之间的损失定义为 $l(\hat y,y)$的函数。</p><p>如果假设输入 $X$ 与某个密度 $\mu$ 成正比： $X \to [0,\infty)$, 那么我们希望最小化目标：</p><script type="math/tex; mode=display">L_{CLP}(W,\theta) = E[l(f_{W,\theta}(X), Y)] = \int \left[ \int l(f_{W,\theta}(x), y) p(y|x) dy  \right] \mu(x) dx</script><p>其中 $W,\theta$ 代表一系列参数，是更新和最小化的目标。</p><p>为了最小化 $L_{CLP}$，我们将自己限制在通过从 $p(S_k|T)$ 采样的单个 $k$ 长度轨迹上的在线更新来学习。</p><p>这改变了标准 iid 设置中的学习问题——模型看到长度为 k 的相关样本的单一轨迹，而不是直接从 $p(x, y) = p(y|x)\mu(x)$ 中采样。当简单地为IID设置应用标准算法时，此修改可能会引起重大问题。相反，我们需要设计考虑这种相关性的算法。</p><p>这个公式可以表示各种连续问题。 一个例子是在线回归问题，例如在给定当前位置的情况下预测机器人的下一个空间位置； 另一个是现有的增量分类基准。 CLP 公式还允许依赖于最近 m 次观测的历史记录的目标 $Y<em>t$。 这可以通过将每个 $X_t$ 定义为最后 m 个观测值来获得。 $X_t$ 和 $X</em>{t-1}$ 之间的重叠不违反对相关输入序列的假设。 最后，强化学习中的预测问题——从一个状态预测策略的值——可以通过将输入 $X_t$ 视为状态和要采样的目标返回或引导目标来表示。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>端到端训练的神经网络在使用从 $p(S_k|T)$ 采样的单个轨迹来最小化CLP loss 方面不是有效的，原因有两个。</p><p>首先，它们的样本效率极低，需要多个epoch的训练才能收敛到合理的解决方案。</p><p>其次，当在线学习相关数据流时，他们会受到灾难性的干扰。</p><p>元学习可以有效地提高神经网络的样本效率。但元学习模型初始化，这种归纳偏差不足以解决灾难性的干扰问题。</p><p>经验发现，学习编码器比只学习初始化的性能要好得多，此外，元学习优化问题在学习编码器时表现更好(对超参数不敏感，收敛速度更快)。这种差异的一种解释是，当在高度相关的数据流上学习时，全局且贪婪的更新算法(例如梯度下降)将贪婪地改变神经网络的初始层相对于当前样本的权重。初始层中的这种变化将干扰模型的过去知识。因此，初始化不是增量学习的有效归纳偏差。另一方面，当学习编码器 $\phi_{\theta}$ 时，神经网络可以学习使得更新不那么全局的高度稀疏表示(因为连接到零的特征的权重保持不变)。</p><p>为了将神经网络应用于问题的求解，作者提出了一种元学习函数 $\phi_{\theta} (X)$  ——一种由 θ 参数化的深度表示学习网络(RLN)—从$X \to R^{d}$中学习。然后学习另一个来自 $R^d\to Y$的函数 $g_W$，称为预测学习网络(PLN)。</p><p>两个函数的组合为 $f<em>{W,\theta }(x) = g</em>{W}(\phi_{\theta} (X))$</p><p><img src="https://z3.ax1x.com/2021/10/10/5APRG8.png" alt=""></p><p>将 $\theta$ 视为通过最小化元目标学习的元参数，然后在元测试时固定。在学习 $\theta $ 之后，我们从 $R^d\to Y $ 学习 $g_W $，用于从单个轨迹 $S $ 使用单次传递全在线的SGD更新解决CLP问题。</p><p>对于元训练，假设由 $p(T)$ 给出的CLP问题上的分布。</p><p>OML的目标函数为 ：</p><script type="math/tex; mode=display">min_{W,\theta} \sum_{T_i \sim p(T)} OML(W,\theta) = \sum_{T_i\sim P(T)} \sum_{S_k^j\sim p(S_k|T_i)} [ L_{CLP_i} (U(W,\theta, S_k^j))]</script><p>其中 $S<em>k^j = (X</em>{j+1}^i,Y<em>{j+1}^i),(X</em>{j+2}^i, Y<em>{j+2}^i),…,(X</em>{j+k}^i,Y_{j+k}^i)$ , </p><p>$U(W<em>t, \theta, S_k^j) =(W</em>{t+k},\theta)$ 表示一个更新 $W_{t+k}$ 是k步SGD后的权重向量</p><p>U 中第 $j$ 步更新 在样本 $(X<em>{t+j}^i, Y</em>{t+j}^i)$ 使用参数  $W<em>{t+j-1}, \theta$ , 得到 $(W</em>{t+j},\theta)$ </p><p>MAML-Rep和OML目标可以分别实现为算法1和算法2，两者之间的主要区别以蓝色突出显示:</p><p><img src="https://z3.ax1x.com/2021/10/10/5AkLJP.png" alt=""></p><p>注意，MAML-Rep使用完整批数据 $S_k$ 进行 $l$ 次内部更新(其中 $l $ 是超参数)，而OML使用 $S_k$ 中的一个数据点进行一次更新。这使得OML可以考虑在线持续学习的影响，例如灾难性的遗忘。</p><p>OML 目标的目的是学习适合在线持续学习的表示。 为了说明什么将构成持续学习的有效表示，假设我们有三个输入集群，它们具有显着不同的 $p(Y |x)$，对应于 $p<em>1$、$p_2$ 和 $p_3$。 对于固定的二维表示 $\phi</em>{\theta} : X \to R^2$，我们可以考虑由线性模型给出的解 $W\in R^2$ 的线性模型，该模型为每个 $p_i$ 提供等效准确的解。</p><p>这三个过程在图2中的 $W\in  R^2$参数空间中描述为三条不同颜色的线。</p><p><img src="https://z3.ax1x.com/2021/10/10/5AZfkd.png" alt=""></p><p>对于目标是由三个不同的分布 $p_1(Y|x)$ 、$p_2(Y|x)$ 和 $p_3(Y|x)$生成的问题，研究了表示对连续学习的影响。</p><p>目的是通过对来自三个分布的样本进行在线学习，找到一个对所有三个分布都有效的参数向量 W。 对于两种不同的表示，这些流形及其交集可能看起来非常不同。 直觉是，当流形平行（允许正泛化）或正交（避免干扰）时，来自 W 的在线更新更有效。 产生这种流形的表示不太可能自然出现。 相反，我们必须明确地找到它。 通过考虑在线持续学习的影响，OML 目标针对这种表示进行了优化。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Meta-Learning-Representations-for-Continual-Learning&quot;&gt;&lt;a href=&quot;#Meta-Learning-Representations-for-Continual-Learning&quot; class=&quot;headerl</summary>
      
    
    
    
    
    <category term="context detection" scheme="http://example.com/tags/context-detection/"/>
    
  </entry>
  
</feed>
