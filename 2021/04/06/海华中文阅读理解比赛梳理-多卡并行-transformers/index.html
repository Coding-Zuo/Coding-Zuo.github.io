<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>海华中文阅读理解比赛梳理/多卡并行/transformers | Coding-Zuo</title><meta name="keywords" content="DataGame"><meta name="author" content="Coding-Zuo"><meta name="copyright" content="Coding-Zuo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="海华中文阅读理解比赛梳理文文言文古诗词现代诗词 1 字词解释 2 标点符号作用 3 句子解释 4 填空 5 选择正读音 6 推理总结 7 态度情感 8 外部知识 不需要先验知识的问题 如一个问题能够在文档中进行匹配，回答起来就几乎不需要先验知识需要先验知识的问題 1、关于语言的知识：需要词汇&#x2F;语法知识,例如:习语、谚语、否定、反义词、同义词语法转换 2、特定领域的知识：需要但不限于些事实上的知识，">
<meta property="og:type" content="article">
<meta property="og:title" content="海华中文阅读理解比赛梳理&#x2F;多卡并行&#x2F;transformers">
<meta property="og:url" content="http://example.com/2021/04/06/%E6%B5%B7%E5%8D%8E%E4%B8%AD%E6%96%87%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%AF%94%E8%B5%9B%E6%A2%B3%E7%90%86-%E5%A4%9A%E5%8D%A1%E5%B9%B6%E8%A1%8C-transformers/index.html">
<meta property="og:site_name" content="Coding-Zuo">
<meta property="og:description" content="海华中文阅读理解比赛梳理文文言文古诗词现代诗词 1 字词解释 2 标点符号作用 3 句子解释 4 填空 5 选择正读音 6 推理总结 7 态度情感 8 外部知识 不需要先验知识的问题 如一个问题能够在文档中进行匹配，回答起来就几乎不需要先验知识需要先验知识的问題 1、关于语言的知识：需要词汇&#x2F;语法知识,例如:习语、谚语、否定、反义词、同义词语法转换 2、特定领域的知识：需要但不限于些事实上的知识，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/04/06/BLiKPmD6Mwve1FQ.png">
<meta property="article:published_time" content="2021-04-06T12:28:26.000Z">
<meta property="article:modified_time" content="2021-05-01T03:47:02.000Z">
<meta property="article:author" content="Coding-Zuo">
<meta property="article:tag" content="DataGame">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/04/06/BLiKPmD6Mwve1FQ.png"><link rel="shortcut icon" href="https://i.loli.net/2021/03/22/reFlcYOnP3dSuJX.png"><link rel="canonical" href="http://example.com/2021/04/06/%E6%B5%B7%E5%8D%8E%E4%B8%AD%E6%96%87%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%AF%94%E8%B5%9B%E6%A2%B3%E7%90%86-%E5%A4%9A%E5%8D%A1%E5%B9%B6%E8%A1%8C-transformers/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.json","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-05-01 11:47:02'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/css/footer.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css"><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="Coding-Zuo" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="https://i.loli.net/2021/03/22/YP2oqk7lOAfceTD.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">116</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https:www.baidu.com"><i class="fa-fw fas fa-heart"></i><span> 我的简历</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E9%9F%B3%E4%B9%90"><i class="fa-fw /music/"></i><span> 0</span></a></li><li><a class="site-page child" href="/%E7%94%B5%E5%BD%B1"><i class="fa-fw /movies/"></i><span> 1</span></a></li><li><a class="site-page child" href="/%E7%85%A7%E7%89%87"><i class="fa-fw /Gallery/"></i><span> 2</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.loli.net/2021/04/06/BLiKPmD6Mwve1FQ.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Coding-Zuo</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https:www.baidu.com"><i class="fa-fw fas fa-heart"></i><span> 我的简历</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E9%9F%B3%E4%B9%90"><i class="fa-fw /music/"></i><span> 0</span></a></li><li><a class="site-page child" href="/%E7%94%B5%E5%BD%B1"><i class="fa-fw /movies/"></i><span> 1</span></a></li><li><a class="site-page child" href="/%E7%85%A7%E7%89%87"><i class="fa-fw /Gallery/"></i><span> 2</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">海华中文阅读理解比赛梳理/多卡并行/transformers</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-04-06T12:28:26.000Z" title="发表于 2021-04-06 20:28:26">2021-04-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-05-01T03:47:02.000Z" title="更新于 2021-05-01 11:47:02">2021-05-01</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="海华中文阅读理解比赛梳理/多卡并行/transformers"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="海华中文阅读理解比赛梳理"><a href="#海华中文阅读理解比赛梳理" class="headerlink" title="海华中文阅读理解比赛梳理"></a>海华中文阅读理解比赛梳理</h1><p>文文言文古诗词现代诗词</p>
<p>1 字词解释 2 标点符号作用 3 句子解释 4 填空 5 选择正读音 6 推理总结 7 态度情感 8 外部知识</p>
<p>不需要先验知识的问题</p>
<p>如一个问题能够在文档中进行匹配，回答起来就几乎不需要先验知识需要先验知识的问題</p>
<p>1、关于语言的知识：需要词汇/语法知识,例如:习语、谚语、否定、反义词、同义词语法转换</p>
<p>2、特定领域的知识：需要但不限于些事实上的知识，这些事实与特定领域的概念概念定义和属性，概念之间的关系</p>
<p>3、一般世界的知识：需要有关世界如何运作的一般知识，或者被称为常识。比如百科全书中的知识</p>
<p>这个赛题的难点是有些预训练语言模型没有学到的先验知识怎么学</p>
<h2 id="赛题概述"><a href="#赛题概述" class="headerlink" title="赛题概述"></a>赛题概述</h2><ul>
<li>train 训练集提供了6313条数据数据格式是和中小学生做的阅读题一样，一篇文章有两到三个问题每个问题有两到四个答案选项。</li>
<li>validation 验证集提供了1000条数据。</li>
</ul>
<p>原始单条数据格式如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;ID&quot;</span>: <span class="string">&quot;0001&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;Content&quot;</span>: <span class="string">&quot;春之怀古张晓风春天必然曾经是这样的：从绿意内敛的山头，一把雪再也撑不住了，噗嗤的一声，将冷面笑成花面，一首澌澌然的歌便从云端唱到山麓，从山麓唱到低低的荒村。。。。。很多省略。&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;Questions&quot;</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;Q_id&quot;</span>: <span class="string">&quot;000101&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;Question&quot;</span>: <span class="string">&quot;鸟又可以开始丈量天空了。”这句话的意思是   （   ）&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;Choices&quot;</span>: [</span><br><span class="line">          <span class="string">&quot;A．鸟又可以飞了。&quot;</span>,</span><br><span class="line">          <span class="string">&quot;B． 鸟又要远飞了。&quot;</span>,</span><br><span class="line">          <span class="string">&quot;C．鸟又可以筑巢了。&quot;</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">&quot;Answer&quot;</span>: <span class="string">&quot;A&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;Q_id&quot;</span>: <span class="string">&quot;000102&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;Question&quot;</span>: <span class="string">&quot;本文写景非常含蓄，请读一读找一找哪些不在作者的笔下有所描述&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;Choices&quot;</span>: [</span><br><span class="line">          <span class="string">&quot;A．冰雪融化&quot;</span>,</span><br><span class="line">          <span class="string">&quot;B． 蝴蝶在花间飞舞&quot;</span>,</span><br><span class="line">          <span class="string">&quot;C．白云在空中飘&quot;</span>,</span><br><span class="line">          <span class="string">&quot;D．小鸟在空中自由地飞&quot;</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">&quot;Answer&quot;</span>: <span class="string">&quot;C&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="EDA-与预处理"><a href="#EDA-与预处理" class="headerlink" title="EDA 与预处理"></a>EDA 与预处理</h2><p>将原始数据每个问题抽出来以 [文章- 问题 -答案] 作为一条数据。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;Question&quot;</span>: <span class="string">&quot;下列对这首诗的理解和赏析，不正确的一项是&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;Choices&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;A．作者写作此诗之时，皮日休正患病居家，闭门谢客，与外界不通音讯。&quot;</span>,</span><br><span class="line">        <span class="string">&quot;B．由于友人患病，原有的约会被暂时搁置，作者游春的诗篇也未能写出。&quot;</span>,</span><br><span class="line">        <span class="string">&quot;C．作者虽然身在书斋从事教学，但心中盼望能走进自然，领略美好春光。&quot;</span>,</span><br><span class="line">        <span class="string">&quot;D．尾联使用了关于沈约的典故，可以由此推测皮日休所患的疾病是目疾。&quot;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;Answer&quot;</span>: <span class="string">&quot;A&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;Q_id&quot;</span>: <span class="string">&quot;000101&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;Content&quot;</span>: <span class="string">&quot;奉和袭美抱疾杜门见寄次韵  陆龟蒙虽失春城醉上期，下帷裁遍未裁诗。因吟郢岸百亩蕙，欲采商崖三秀芝。栖野鹤笼宽使织，施山僧饭别教炊。但医沈约重瞳健，不怕江花不满枝。&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>训练集从6313变为15421条数据，相当于有15421个问题</p>
<p>验证集从1000变为2444条数据，相当于有2444个问题</p>
<p>接下来看看文章的长度如何？</p>
<p><img src="https://i.loli.net/2021/04/06/UXPvh1c6CIRY54J.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">count    15421.000000</span><br><span class="line">mean      1039.781272</span><br><span class="line">std        435.583878</span><br><span class="line">min         38.000000</span><br><span class="line">25%        744.000000</span><br><span class="line">50%       1067.000000</span><br><span class="line">75%       1251.000000</span><br><span class="line">max       3047.000000</span><br><span class="line">Name: content_len, dtype: float64</span><br><span class="line">count    2444.000000</span><br><span class="line">mean      927.508592</span><br><span class="line">std       481.552693</span><br><span class="line">min        40.000000</span><br><span class="line">25%       596.000000</span><br><span class="line">50%       938.000000</span><br><span class="line">75%      1179.500000</span><br><span class="line">max      3047.000000</span><br><span class="line">Name: content_len, dtype: float64</span><br></pre></td></tr></table></figure>
<p>发现content文章都非常长，绝大多数都超过了512。</p>
<p>使用预训练模型bert的话，如何训练很长的文章是是个提高的点。</p>
<p>我的想法是bert模型一个这个提高的点、看看最近比较火的Longformer怎么做，再用几个和长度无关的模型像lstm等最后做集成。</p>
<p><img src="https://i.loli.net/2021/04/06/gBUFiwHtaK195rq.png" alt=""></p>
<p>答案中选C的居多，点歌都选C。。。</p>
<p>在提供的测试集中有一个特别的地方，赛方给出了文章的类型。</p>
<p>00 现代文 11文言文 22 古诗词 33现代诗词</p>
<p><img src="https://i.loli.net/2021/04/06/ZoVXJGdhrf7wga5.png" alt=""></p>
<p>测试集还给了难度，使用想法：</p>
<p>可以训练一个模型预测文本的难度和类型，标注训练集，可能会有提升。</p>
<p>接下来将标签从ABCD转成0123</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_df[<span class="string">&#x27;label&#x27;</span>] = train_df[<span class="string">&#x27;Answer&#x27;</span>].apply(<span class="keyword">lambda</span> x:[<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;C&#x27;</span>,<span class="string">&#x27;D&#x27;</span>].index(x)) </span><br><span class="line"></span><br><span class="line">test_df[<span class="string">&#x27;label&#x27;</span>] = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h2 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h2><h3 id="分词器"><a href="#分词器" class="headerlink" title="分词器"></a>分词器</h3><p>采用transformers提供的bert分词器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;model&#x27;</span>) <span class="comment">#加载bert的分词器</span></span><br></pre></td></tr></table></figure>
<p>这里我试过如果要将bert改成roberta，分词器还是要采用BertTokenizer，如果用RobertaTokenizer会报错。</p>
<p>参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/121787628">关于transformers库中不同模型的Tokenizer</a></p>
<p><strong>由于中文的特殊性不太适合采用byte级别的编码，所以大部分开源的中文Roberta预训练模型仍然采用的是单字词表，所以直接使用<code>BertTokenizer</code>读取即可，</strong> 不需要使用<code>RobertaTokenizer</code>。</p>
<h3 id="模型部分"><a href="#模型部分" class="headerlink" title="模型部分"></a>模型部分</h3><p>BertForMultipleChoice <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/bert.html#bertformultiplechoice">https://huggingface.co/transformers/model_doc/bert.html#bertformultiplechoice</a></p>
<p>把每个问题和文章的不同选项拆开拼成一个输入。如下图第一行</p>
<p><img src="https://i.loli.net/2021/04/08/sgbWnyIqdT9hcrE.png" alt=""></p>
<p>baseline采用transformers提供的调包，封装好的BertForMultipleChoice (多项选择任务)，它的源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertForMultipleChoice</span>(<span class="params">BertPreTrainedModel</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(config)</span><br><span class="line"></span><br><span class="line">        self.bert = BertModel(config)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line">        self.classifier = nn.Linear(config.hidden_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        input_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        token_type_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        position_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        inputs_embeds=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        labels=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">       </span><br><span class="line">        num_choices = input_ids.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        input_ids = input_ids.view(-<span class="number">1</span>, input_ids.size(-<span class="number">1</span>))</span><br><span class="line">        attention_mask = attention_mask.view(-<span class="number">1</span>, attention_mask.size(-<span class="number">1</span>)) <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        token_type_ids = token_type_ids.view(-<span class="number">1</span>, token_type_ids.size(-<span class="number">1</span>)) <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        position_ids = position_ids.view(-<span class="number">1</span>, position_ids.size(-<span class="number">1</span>)) <span class="keyword">if</span> position_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">				<span class="comment"># 将bert三个输入展平 输入到bertmodel</span></span><br><span class="line">        outputs = self.bert(</span><br><span class="line">            input_ids,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">            position_ids=position_ids,</span><br><span class="line">            head_mask=head_mask,</span><br><span class="line">            inputs_embeds=inputs_embeds,</span><br><span class="line">        )</span><br><span class="line">				<span class="comment"># 隐层输出</span></span><br><span class="line">        <span class="comment"># last_hidden_state: [32=4*batch, seq_len,768]</span></span><br><span class="line">        <span class="comment"># pooler_ouput: [32=4*batch,768]</span></span><br><span class="line">        pooled_output = outputs[<span class="number">1</span>] <span class="comment"># CLS https://www.cnblogs.com/webbery/p/12167552.html</span></span><br><span class="line">        <span class="comment"># bert_output = outputs[0] # last_hidden</span></span><br><span class="line"></span><br><span class="line">        pooled_output = self.dropout(pooled_output)</span><br><span class="line">        logits = self.classifier(pooled_output)</span><br><span class="line">        reshaped_logits = logits.view(-<span class="number">1</span>, num_choices)</span><br><span class="line"></span><br><span class="line">        outputs = (reshaped_logits,) + outputs[<span class="number">2</span>:]  <span class="comment"># add hidden states and attention if they are here</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss_fct = CrossEntropyLoss()</span><br><span class="line">            loss = loss_fct(reshaped_logits, labels)</span><br><span class="line">            outputs = (loss,) + outputs</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs  <span class="comment"># (loss), reshaped_logits, (hidden_states), (attentions)</span></span><br></pre></td></tr></table></figure>
<p>做bert方面的模型扩展可以参考上面，其实就是BertModel加上了线性层。</p>
<h3 id="制造模型输入数据"><a href="#制造模型输入数据" class="headerlink" title="制造模型输入数据"></a>制造模型输入数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dataframe</span>):</span></span><br><span class="line">        self.df = dataframe</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.df)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span> </span><br><span class="line">      <span class="comment">#将一条数据从(文章,问题,4个选项)转成(文章,问题,选项1)、(文章,问题,选项2)...</span></span><br><span class="line">        label = self.df.label.values[idx]</span><br><span class="line">        question = self.df.Question.values[idx]</span><br><span class="line">        content = self.df.Content.values[idx]</span><br><span class="line">        choice = self.df.Choices.values[idx][<span class="number">2</span>:-<span class="number">2</span>].split(<span class="string">&#x27;\&#x27;, \&#x27;&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(choice) &lt; <span class="number">4</span>: <span class="comment">#如果选项不满四个，就补“不知道”</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>-<span class="built_in">len</span>(choice)):</span><br><span class="line">                choice.append(<span class="string">&#x27;D．不知道&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        content = [content <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(choice))]</span><br><span class="line">        pair = [question + <span class="string">&#x27; &#x27;</span> + i[<span class="number">2</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> choice]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> content, pair, label</span><br></pre></td></tr></table></figure>
<p>$61536 = 15325\times 4 + 71 \times3+ 25\times2 $</p>
<p>数据将变成61536条</p>
<p>如果用五折交叉验证： 训练集 49228 验证集12307</p>
<p>如果Using 8 dataloader workers every process</p>
<p>每个batch 8条数据的话  约等于每个epoch 训练集运行772次，验证集193次</p>
<p>(这个地方不知道算的对不对)</p>
<p>将数据做成bert需要的三种编码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span>(<span class="params">data</span>):</span> </span><br><span class="line">  <span class="comment"># 将文章问题选项拼在一起后，得到分词后的数字id，输出的size是(batch, n_choices, max_len)</span></span><br><span class="line">    input_ids, attention_mask, token_type_ids = [], [], []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> data:</span><br><span class="line">        text = tokenizer(x[<span class="number">1</span>],</span><br><span class="line">                         text_pair=x[<span class="number">0</span>],</span><br><span class="line">                         padding=<span class="string">&#x27;max_length&#x27;</span>,  <span class="comment"># 填充到使用参数max_length指定的最大长度，或者填充到模型的最大可接受输入长度(如果未提供该参数)。</span></span><br><span class="line">                         truncation=<span class="literal">True</span>,</span><br><span class="line">                         <span class="comment"># TRUE或‘LIMEST_FIRST’：截断到使用参数max_length指定的最大长度，或者截断到模型的最大可接受输入长度(如果没有提供该参数)。这将逐个令牌截断令牌，如果提供了一对序列(或一批对)，则从该对中最长的序列中删除一个令牌。</span></span><br><span class="line">                         max_length=Param[<span class="string">&#x27;max_len&#x27;</span>],</span><br><span class="line">                         return_tensors=<span class="string">&#x27;pt&#x27;</span>)  <span class="comment"># 返回pytorch tensor格式</span></span><br><span class="line">        input_ids.append(text[<span class="string">&#x27;input_ids&#x27;</span>].tolist())</span><br><span class="line">        attention_mask.append(text[<span class="string">&#x27;attention_mask&#x27;</span>].tolist())</span><br><span class="line">        token_type_ids.append(text[<span class="string">&#x27;token_type_ids&#x27;</span>].tolist())</span><br><span class="line">    input_ids = torch.tensor(input_ids)</span><br><span class="line">    attention_mask = torch.tensor(attention_mask)</span><br><span class="line">    token_type_ids = torch.tensor(token_type_ids)</span><br><span class="line">    label = torch.tensor([x[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data])</span><br><span class="line">    <span class="keyword">return</span> input_ids, attention_mask, token_type_ids, label</span><br></pre></td></tr></table></figure>
<p>DataLoader</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">train_set = utils.MyDataset(train)</span><br><span class="line">val_set = utils.MyDataset(val)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;单卡直接写&quot;&quot;&quot;</span></span><br><span class="line">train_loader = DataLoader(train_set, batch_size=CFG[<span class="string">&#x27;train_bs&#x27;</span>], collate_fn=collate_fn, shuffle=<span class="literal">True</span>, num_workers=CFG[<span class="string">&#x27;num_workers&#x27;</span>])</span><br><span class="line">val_loader = DataLoader(val_set, batch_size=CFG[<span class="string">&#x27;valid_bs&#x27;</span>], collate_fn=collate_fn, shuffle=<span class="literal">False</span>, num_workers=CFG[<span class="string">&#x27;num_workers&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;多卡写法&quot;&quot;&quot;</span></span><br><span class="line"> <span class="comment"># 给每个rank对应的进程分配训练的样本索引</span></span><br><span class="line">train_sampler = DistributedSampler(train_set)</span><br><span class="line">val_sampler = DistributedSampler(val_set)</span><br><span class="line"> <span class="comment"># 将样本索引每batch_size个元素组成一个list 验证集不用</span></span><br><span class="line">train_batch_sampler = torch.utils.data.BatchSampler(train_sampler, batch_size=args.batch_size, drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_set, batch_sampler=train_batch_sampler, pin_memory=<span class="literal">False</span>,</span><br><span class="line">                                  collate_fn=collate_fn, num_workers=<span class="number">2</span>)</span><br><span class="line">val_loader = DataLoader(val_set, batch_size=args.batch_size, sampler=val_sampler, pin_memory=<span class="literal">False</span>, collate_fn=collate_fn, num_workers=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>DistributedSampler/BatchSampler:</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_37400316/article/details/107210970">四Sampler源码</a></p>
<p><a target="_blank" rel="noopener" href="https://www.freesion.com/article/6505681767/">（TORCH.NN.PARALLEL.DISTRIBUTEDDATAPARALLEL）时，DISTRIBUTEDSAMPLER(DATASET)用法解释</a></p>
<p>Dataloader 中的 num_workers:</p>
<p>加快训练进程<br>为了加快训练过程，使用DataLoader类的num workers可选属性。<br>num workers属性告诉数据加载器实例要使用多少子进程来加载数据。默认情况下，num  workers值设置为0，值为0告诉加载程序在主进程内加载数据。<br>这意味着训练将在主进程中按顺序工作。在训练过程中使用了一个batch，并且需要另一个batch之后，从磁盘读取批数据。现在，如果我们有一个worker进程，我们可以利用机器多个核的。这意味着在主进程准备好进行另一批处理时，下一批处理已经可以加载并准备就绪。这就是加速的来源。批处理使用其他工作进程加载，并在内存中排队。</p>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><h4 id="优化配置"><a href="#优化配置" class="headerlink" title="优化配置"></a>优化配置</h4><p>多层不同学习率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fc_para = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">id</span>, model.module.classifier.parameters()))</span><br><span class="line">lstm_para = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">id</span>, model.module.lstm.parameters()))</span><br><span class="line">gru_para = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">id</span>, model.module.gru.parameters()))</span><br><span class="line">base_para = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: <span class="built_in">id</span>(p) <span class="keyword">not</span> <span class="keyword">in</span> fc_para, model.module.parameters())</span><br><span class="line">params = [&#123;<span class="string">&#x27;params&#x27;</span>: base_para&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;params&#x27;</span>: model.module.lstm.parameters(), <span class="string">&#x27;lr&#x27;</span>: args.other_lr&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;params&#x27;</span>: model.module.gru.parameters(), <span class="string">&#x27;lr&#x27;</span>: args.other_lr&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;params&#x27;</span>: model.module.classifier.parameters(), <span class="string">&#x27;lr&#x27;</span>: args.fc_lr&#125;]</span><br><span class="line">scaler = GradScaler() <span class="comment"># 有v100的话还可以开半精度</span></span><br><span class="line">optimizer = AdamW(model.module.parameters(), lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line"><span class="comment"># criterion = nn.CrossEntropyLoss().cuda(local_rank)</span></span><br><span class="line">criterion = utils.LabelSmoothingCrossEntropy().cuda(local_rank) <span class="comment"># 标签平滑</span></span><br></pre></td></tr></table></figure>
<h4 id="梯度累积"><a href="#梯度累积" class="headerlink" title="梯度累积"></a>梯度累积</h4><p>由于机器显存限制，不得不用梯度累积来达到目的batch数。</p>
<p>用多次小的 mini-batch 来模拟一个较大的 mini-batch，即：global_batch_size = batch_size*iter_size</p>
<p>batch size 和 learning rate 要等比例放大。但需要注意：特别大的 batch size 还需要再加上其他 trick 如 warmup 才能保证训练顺利（因为太大的初始 lr 很容易 train 出 nan）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">loss = criterion(output, y) / args.accum_iter</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ((step + <span class="number">1</span>) % args.accum_iter == <span class="number">0</span>) <span class="keyword">or</span> ((step + <span class="number">1</span>) == <span class="built_in">len</span>(train_loader)):</span><br><span class="line">    scaler.step(optimizer)</span><br><span class="line">    scaler.update()</span><br><span class="line">    scheduler.step()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>苏神:<a target="_blank" rel="noopener" href="https://kexue.fm/archives/6794"> 用时间换取效果：Keras梯度累积优化器</a></p>
<h4 id="loss计算与warmup"><a href="#loss计算与warmup" class="headerlink" title="loss计算与warmup"></a>loss计算与warmup</h4><p>warmup顾名思义就是热身，在刚刚开始训练时以很小的学习率进行训练，使得网络熟悉数据，随着训练的进行学习率慢慢变大，到了一定程度，以设置的初始学习率进行训练，接着过了一些inter后，学习率再慢慢变小；学习率变化：上升——平稳——下降；</p>
<p>warm up setp（一般等于epoch*inter_per_epoch），当step小于warm up setp时，学习率等于基础学习率×(当前step/warmup_step)，由于后者是一个小于1的数值，因此在整个warm up的过程中，学习率是一个递增的过程！当warm up结束后，学习率以基础学习率进行训练，再学习率开始递减</p>
<p>1、当网络非常容易nan时候，采用warm up进行训练，可使得网络正常训练；</p>
<p>2、如果训练集损失很低，准确率高，但测试集损失大，准确率低，可用warm up；具体可看：<a target="_blank" rel="noopener" href="https://blog.csdn.net/u011995719/article/details/77884728">https://blog.csdn.net/u011995719/article/details/77884728</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.zhujian.life/posts/f311f0.html">[LR Scheduler]warmup</a></p>
<h4 id="模型保存与加载"><a href="#模型保存与加载" class="headerlink" title="模型保存与加载"></a>模型保存与加载</h4><p>这里有个小地方要注意，因为多卡并行时model用DistributedDataParallel包装了，所以在save时不时直接的model.state_dict()，而是model.module.state_dict()。 这个问题当时困扰了我好久，模型保存完的都是没经过学习的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> val_acc &gt; best_acc:</span><br><span class="line">    best_acc = val_acc</span><br><span class="line">    print(<span class="string">&quot;best:&quot;</span>, best_acc)</span><br><span class="line">    <span class="keyword">if</span> distribute_utils.is_main_process():</span><br><span class="line">        torch.save(model.module.state_dict(),</span><br><span class="line">                   <span class="string">&#x27;spawn_adv_pgd_&#123;&#125;_fold_&#123;&#125;.pt&#x27;</span>.<span class="built_in">format</span>(args.model.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>], fold))</span><br></pre></td></tr></table></figure>
<h2 id="提升点"><a href="#提升点" class="headerlink" title="提升点"></a>提升点</h2><p>更长的文本（512、sliding window、xinet、longformer） </p>
<ul>
<li>滑动窗口把文章截成很多段然后取平均softmax</li>
<li>xlnet 不限制长度，时间长</li>
<li>longformer 4096 transformers有提供</li>
</ul>
<p>更好的模型（roberta、large、DUMA）</p>
<ul>
<li>DUMA bert上再加attention</li>
</ul>
<p>更多的数据（爬虫、C3）</p>
<ul>
<li>先训练C3中文的有提升 但新改了规则说不让用外部数据了。</li>
</ul>
<p>比赛复盘<a href="">海华阅读理解比赛复盘</a></p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><ol>
<li>五折交叉验证有的轮次收敛有的轮次不收敛</li>
</ol>
<p>数据shuffle过，要加warmup用cosine lr，学习率往小调从2e-5调到1e-5</p>
<p>还有一种情况是因为label不均衡造成的，每折数据不一样</p>
<ol>
<li>验证集loss和acc都上涨</li>
</ol>
<p>现象很常见，原因是过拟合或者训练验证数据分布不一致造成。就是在训练后期，预测的结果趋向于极端，使少数预测错的样本主导了loss，但同时少数样本不影响整体的验证acc情况。</p>
<h2 id="可能用到的外部数据"><a href="#可能用到的外部数据" class="headerlink" title="可能用到的外部数据"></a>可能用到的外部数据</h2><p>1、RACE dataset<br>2、SQuAD2.0 and CoQA dataset<br>3、ARC dataset<br>4、DREAM dataset<br>5、ChineseSquad，<a target="_blank" rel="noopener" href="https://github.com/zengjunjun/ChineseSquad">https://github.com/zengjunjun/ChineseSquad</a><br>6、cmrc2018，<a target="_blank" rel="noopener" href="https://github.com/ymcui/cmrc2018">https://github.com/ymcui/cmrc2018</a><br>7、c3 dataset<br>8、dureader dataset</p>
<p>1、 爬取中学语文阅读理解试题（全部选项、无标注） <a target="_blank" rel="noopener" href="https://github.com/sz128/ext_data_for_haihua_ai_mrc">https://github.com/sz128/ext_data_for_haihua_ai_mrc</a> （内含网盘下载链接）<br>2、C3数据：<a target="_blank" rel="noopener" href="https://github.com/nlpdata/c3">https://github.com/nlpdata/c3</a><br>3、 开源的中文预训练语言模型：</p>
<p>MacBERT (<a target="_blank" rel="noopener" href="https://github.com/ymcui/MacBERT">https://github.com/ymcui/MacBERT</a>)<br>Chinese-BERT-wwm（<a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-BERT-wwm）">https://github.com/ymcui/Chinese-BERT-wwm）</a><br>Chinese-ELECTRA（<a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-ELECTRA">https://github.com/ymcui/Chinese-ELECTRA</a>)<br>ALBERT-zh (<a target="_blank" rel="noopener" href="https://github.com/brightmart/albert_zh">https://github.com/brightmart/albert_zh</a>)<br>guwenBERT (<a target="_blank" rel="noopener" href="https://github.com/Ethan-yt/guwenbert">https://github.com/Ethan-yt/guwenbert</a>);</p>
</article><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://i.loli.net/2021/03/22/HzNFBbkvZ2QMOKV.jpg" target="_blank"><img class="post-qr-code-img" src="https://i.loli.net/2021/03/22/HzNFBbkvZ2QMOKV.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://i.loli.net/2021/03/22/wMGegPYTAXx9cJo.jpg" target="_blank"><img class="post-qr-code-img" src="https://i.loli.net/2021/03/22/wMGegPYTAXx9cJo.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/04/07/%E6%95%B0%E7%BB%84%E8%8C%83%E5%9B%B4%E5%86%85%E8%AE%A1%E6%95%B0/"><img class="prev-cover" src="https://i.loli.net/2021/04/08/b1iU6fhFVNnuWJx.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">数组范围内计数</div></div></a></div><div class="next-post pull-right"><a href="/2021/04/06/bfprt%E7%AE%97%E6%B3%95/"><img class="next-cover" src="https://i.loli.net/2021/04/06/lGSAiXsypmtaweM.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">bfprt算法</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/10/01/第五届达观杯——风险事件标签识别比赛复盘/" title="第五届达观杯——风险事件标签识别比赛复盘"><img class="cover" src="https://z3.ax1x.com/2021/10/01/47nQm9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-01</div><div class="title">第五届达观杯——风险事件标签识别比赛复盘</div></div></a></div><div><a href="/2021/05/01/海华阅读理解比赛复盘/" title="海华阅读理解比赛复盘"><img class="cover" src="https://i.loli.net/2021/05/01/dizZGI2DktTPlJj.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-01</div><div class="title">海华阅读理解比赛复盘</div></div></a></div><div><a href="/2021/04/15/Kaggle上传dataset的方法/" title="Kaggle上传dataset的方法"><img class="cover" src="https://i.loli.net/2021/04/15/p6FQI8tvTfmR3XE.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-15</div><div class="title">Kaggle上传dataset的方法</div></div></a></div><div><a href="/2021/03/27/Pytorch多GPU并行实例/" title="Pytorch多GPU并行实例"><img class="cover" src="https://i.loli.net/2021/03/27/bBWz3dgpP2AIHGJ.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-27</div><div class="title">Pytorch多GPU并行实例</div></div></a></div><div><a href="/2021/01/29/16080066587381/" title="天池赛题:天猫重复购学习笔记(我的EDA模板)"><img class="cover" src="https://images6.alphacoders.com/977/thumbbig-977917.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-01-29</div><div class="title">天池赛题:天猫重复购学习笔记(我的EDA模板)</div></div></a></div></div></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Coding-Zuo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2021/04/06/%E6%B5%B7%E5%8D%8E%E4%B8%AD%E6%96%87%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%AF%94%E8%B5%9B%E6%A2%B3%E7%90%86-%E5%A4%9A%E5%8D%A1%E5%B9%B6%E8%A1%8C-transformers/">http://example.com/2021/04/06/%E6%B5%B7%E5%8D%8E%E4%B8%AD%E6%96%87%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%AF%94%E8%B5%9B%E6%A2%B3%E7%90%86-%E5%A4%9A%E5%8D%A1%E5%B9%B6%E8%A1%8C-transformers/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Coding-Zuo</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/DataGame/">DataGame</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=null" async="async"></script></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B5%B7%E5%8D%8E%E4%B8%AD%E6%96%87%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E6%AF%94%E8%B5%9B%E6%A2%B3%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">海华中文阅读理解比赛梳理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B5%9B%E9%A2%98%E6%A6%82%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text">赛题概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EDA-%E4%B8%8E%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.2.</span> <span class="toc-text">EDA 与预处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Baseline"><span class="toc-number">1.3.</span> <span class="toc-text">Baseline</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-number">1.3.1.</span> <span class="toc-text">分词器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%83%A8%E5%88%86"><span class="toc-number">1.3.2.</span> <span class="toc-text">模型部分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%B6%E9%80%A0%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-number">1.3.3.</span> <span class="toc-text">制造模型输入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">1.3.4.</span> <span class="toc-text">训练过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E9%85%8D%E7%BD%AE"><span class="toc-number">1.3.4.1.</span> <span class="toc-text">优化配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF"><span class="toc-number">1.3.4.2.</span> <span class="toc-text">梯度累积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#loss%E8%AE%A1%E7%AE%97%E4%B8%8Ewarmup"><span class="toc-number">1.3.4.3.</span> <span class="toc-text">loss计算与warmup</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.3.4.4.</span> <span class="toc-text">模型保存与加载</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%90%E5%8D%87%E7%82%B9"><span class="toc-number">1.4.</span> <span class="toc-text">提升点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.5.</span> <span class="toc-text">遇到的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%AF%E8%83%BD%E7%94%A8%E5%88%B0%E7%9A%84%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE"><span class="toc-number">1.6.</span> <span class="toc-text">可能用到的外部数据</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Coding-Zuo</div><div class="footer_custom_text">Hi, welcome to my BLOG</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: 'd652112bf81876e00118',
      clientSecret: 'a5abed418c7cc2736af5b4d0cbd7ff97d460a5b3',
      repo: 'Coding-Zuo.github.io',
      owner: 'Coding-Zuo',
      admin: ['Coding-Zuo'],
      id: '8c97fed0c5bcc388b3bf5e4c553028cc',
      language: 'zh-CN',
      perPage: 10,
      distractionFreeMode: false,
      pagerDirection: 'last',
      createIssueManually: false,
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script src="/js/custom.js"></script><script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script src="//code.tidio.co/mak6nokafytw9mgrsuzglwzfxiy3fpdl.js" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>