<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Pattern Exploiting Training (PET) | Coding-Zuo</title><meta name="keywords" content="context detection"><meta name="author" content="Coding-Zuo"><meta name="copyright" content="Coding-Zuo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="Pattern Exploiting Training (PET)介绍PET范式，可用于半监督或无监督训练。 这篇主要关注两篇相同作者的文章： 《Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference》 《It’s Not Just Size That Matters：Sm">
<meta property="og:type" content="article">
<meta property="og:title" content="Pattern Exploiting Training (PET)">
<meta property="og:url" content="http://example.com/2021/10/28/Pattern-Exploiting-Training-PET/index.html">
<meta property="og:site_name" content="Coding-Zuo">
<meta property="og:description" content="Pattern Exploiting Training (PET)介绍PET范式，可用于半监督或无监督训练。 这篇主要关注两篇相同作者的文章： 《Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference》 《It’s Not Just Size That Matters：Sm">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/10/28/DjK6qxmT9cA7Ov3.png">
<meta property="article:published_time" content="2021-10-28T02:31:26.000Z">
<meta property="article:modified_time" content="2021-10-30T03:39:47.000Z">
<meta property="article:author" content="Coding-Zuo">
<meta property="article:tag" content="context detection">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/10/28/DjK6qxmT9cA7Ov3.png"><link rel="shortcut icon" href="https://i.loli.net/2021/03/22/reFlcYOnP3dSuJX.png"><link rel="canonical" href="http://example.com/2021/10/28/Pattern-Exploiting-Training-PET/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.json","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-10-30 11:39:47'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/css/footer.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css"><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="Coding-Zuo" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="https://i.loli.net/2021/03/22/YP2oqk7lOAfceTD.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">139</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https:www.baidu.com"><i class="fa-fw fas fa-heart"></i><span> 我的简历</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E9%9F%B3%E4%B9%90"><i class="fa-fw /music/"></i><span> 0</span></a></li><li><a class="site-page child" href="/%E7%94%B5%E5%BD%B1"><i class="fa-fw /movies/"></i><span> 1</span></a></li><li><a class="site-page child" href="/%E7%85%A7%E7%89%87"><i class="fa-fw /Gallery/"></i><span> 2</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.loli.net/2021/10/28/DjK6qxmT9cA7Ov3.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Coding-Zuo</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https:www.baidu.com"><i class="fa-fw fas fa-heart"></i><span> 我的简历</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E9%9F%B3%E4%B9%90"><i class="fa-fw /music/"></i><span> 0</span></a></li><li><a class="site-page child" href="/%E7%94%B5%E5%BD%B1"><i class="fa-fw /movies/"></i><span> 1</span></a></li><li><a class="site-page child" href="/%E7%85%A7%E7%89%87"><i class="fa-fw /Gallery/"></i><span> 2</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Pattern Exploiting Training (PET)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-10-28T02:31:26.000Z" title="发表于 2021-10-28 10:31:26">2021-10-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-10-30T03:39:47.000Z" title="更新于 2021-10-30 11:39:47">2021-10-30</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Pattern Exploiting Training (PET)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Pattern-Exploiting-Training-PET"><a href="#Pattern-Exploiting-Training-PET" class="headerlink" title="Pattern Exploiting Training (PET)"></a>Pattern Exploiting Training (PET)</h1><p>介绍PET范式，可用于半监督或无监督训练。</p>
<p>这篇主要关注两篇相同作者的文章：</p>
<p>《Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference》</p>
<p>《It’s Not Just Size That Matters：Small Language Models Are Also Few-Shot Learners》</p>
<p>首先看到一个问题比较好：<strong>BERT在预训练时学习到的知识或者说参数我们在fine-tunning的时候都有用到吗？</strong></p>
<p>答案是不是的。</p>
<p>BERT的预训练其中一个任务是MLM，就是去预测被 【MASK】掉的token，采用的是拿bert的最后一个encoder（base版本，就是第12层的encoder输出，下图左图蓝色框）作为输入，然后接全连接层，做一个全词表的softmax分类（这部分就是左图的红色框）。但在finetuing的时候，我们是把MLM任务的全连接层抛弃掉，在最后一层encoder后接的初始化层来做具体下游任务。</p>
<p><img src="https://i.loli.net/2021/10/28/c31HAsXB5QbPklt.png" alt=""></p>
<p>MLM目标是预测 输入时被挑选的15%的单词，所以在BERT的最后一层（如BERT-base版本就是第12层）的token的embedding后会接一个【embedding维度，词表大小】的全连接矩阵，做token的预测，这个全连接矩阵就是MLM层参数</p>
<p>问题是，<strong>能不能通过某些巧妙的设计，把MLM层学习到的参数也利用上？</strong></p>
<blockquote>
<p>注意，Prompt设计的这种完形填空和MLM任务是有区别的，二者虽然都是都是词分类，但是候选集不同，MLM的候选词是整个词库，prompt是verbalizer里的词。Prompt使用MLM层把其他的词给忽略掉。</p>
</blockquote>
<p>答案当然是可以的，请继续往下看。</p>
<p>现在举一个二分类的例子，输入一条汽车论坛的评论，输出这个评论是属于【积极】or【消极】。但问题是现在我每个类别只有10个labeled数据，1K条unlabeled数据。怎么训练model？</p>
<p>直接做有监督训练?样本量太少，会过拟合。应该优先采用半监督学习的方法，如UDA、MixText这种，而PET采用的是另外一种巧妙的设计思想。</p>
<p>对于”I love this movie”这句输入，可以在后面加上Prompt也就是Pattern：”the movie is <em>_</em>“，组成如下这样一句话：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">I love this movie, the movie is ___</span><br></pre></td></tr></table></figure>
<p>然后让预训练模型用表示情感的答案（例如”great”、”terrible”等）做完形填空，最后再将该答案转换为情感分类的标签。这里定义一个<strong>verblizer</strong>作为映射函数，把label【great】映射为+，把label【terrible】映射为- 。</p>
<p>这样一来，我们就可以通过构造合适的「模板」，控制模型的输出空间，从而训练一个完全无监督的预训练模型来解决各种各样的下游任务。<strong>BERT预训练时的MLM层的参数能利用上</strong>。而且，<strong>即使model没有进行fine tunning，这个model其实就会含有一定的准确率</strong>！</p>
<p>Pattern和verblizer，就是一个PVP（pattern-verbalizer pairs）。</p>
<h2 id="Prompt-Notation"><a href="#Prompt-Notation" class="headerlink" title="Prompt Notation"></a>Prompt Notation</h2><p>设 $M$ 是被mask的语言模型，$V$ 是它的词表，$[MASK]$ 也包含在词表中。令 $L$ 为目标分类任务 A 的一组标签。</p>
<p>我们把任务 A 的输入写成一串短语 $x=(s_1,…,s_k)$，其中 $s_i\in V$.</p>
<p>例如如果 $A$ 是文本推理（两个话虽然句子），$k=2$</p>
<p>我们将pattern定义为一个函数 $P$, 它以 $x$ 为输入，输出一个短语或句子 $P(x)\in V^*$, 其中正好包含一个MASK标记，也就是说，它的输出可以被看作是一个完形填空问题。</p>
<p>此外，将 verbalizer 定义为一个注入函数 $v:L\to V$, 它将每个标签映射到 $M$ 的词表中的一个词。</p>
<p>令 $p=(P,v)$  是 PVP（pattern-verbalizer pairs）</p>
<p>我们假设可以访问较小的训练集 $T$ 和 通常大得多的一组无标签数据 $D$ 。</p>
<p>对于每个恰好包含一个MASK标签和 $w\in V$ 的序列 $z\in V^*$, 用 $M(w|z)$ 表示语言模型在掩码位置赋予$w$ 的非标准化分数。给定某个输入 $x$，我们将标签 $l\in L$的得分定义为:</p>
<script type="math/tex; mode=display">
s_p(l|x) = M(v(l) | P(x))</script><p>并使用Softmax获得标签上的概率分布 :</p>
<script type="math/tex; mode=display">
q_p(l|x) = \frac{e^{s_p(l|x)}}{\sum_{l'\in L} e^{s_p(l'|x)}}</script><h2 id="Auxiliary-Language-Modeling"><a href="#Auxiliary-Language-Modeling" class="headerlink" title="Auxiliary Language Modeling"></a>Auxiliary Language Modeling</h2><p>只有几个训练示例可用，可能会发生灾难性的遗忘。</p>
<p>由于现在是用MLM做分类任务，所以可以引入无标注数据一起训练！</p>
<p>举个简单的例子，下图样例1是labeled数据，我们利用pattern把它改写后，对 __ 部分做完形填空预测（即MLM任务）。</p>
<p>样例2是一个unlabeled数据，我们就不对  __ 部分做预测，而是对被【MASK】做预测。这里的【MASK】可以采用BERT的方法，随机对句子的15%token进行【MASK】。</p>
<p><img src="https://i.loli.net/2021/10/28/daK2yEYcXFSoN9u.png" alt=""></p>
<p>训练时两个损失联合训练：</p>
<script type="math/tex; mode=display">
L = (1-\alpha) \cdot L_{CE} + \alpha \cdot L_{MLM}</script><p>由于 $L<em>{MLM} $ 通常比 $L</em>{CE}$ 大得多，在初步实验中，发现$α=10^{-4}$的值能给出良好的结果</p>
<p>这样做的好处是，能让model更适应于当前的任务，有点像<strong>在预训练模型上继续根据任务的domain和task继续做预训练，然后再做fine-tunning呢？</strong></p>
<h2 id="Combining-PVPs"><a href="#Combining-PVPs" class="headerlink" title="Combining PVPs"></a>Combining PVPs</h2><p>引入一个问题，<strong>怎么评价我们的pattern定义得好不好？</strong></p>
<p>我们可以造两个pattern，又可以造两个verblizer。其实一共有4个PVP。我们怎么衡量哪一个PVP训练完后在测试集上的效果最好？</p>
<p>答案是我们也不知道，因为<strong>我们不能站在上帝视角从一开头就选出最佳的PVP，同样由于是小样本学习，也没有足够的验证集让我们挑选最佳的PVP</strong>。既然如此，解决方式就是<strong>知识蒸馏</strong>。</p>
<p>具体的，我们用20个labeled数据训练4个PVP模型，然后拿这四个PVP模型对1K条unlabeled数据进行预测，预测的结果用下式进行平均。</p>
<script type="math/tex; mode=display">
s_M(l|x) = \frac{1}{Z} \sum_{p\in P} w(p) \cdot s_p(l|x)</script><p>其中 $Z$ 保持概率和为1， $s_p(l|x)$ 就是单个PVP模型对样本预测的概率分布，$w(p)$ 就是PVP的权重。</p>
<p>有uniform和weighted两种方式，uniform就是所有PVP的权重都为1，weighted就是把每个PVP的权重设置为它们在训练集上的准确率。最后还要对上式进行<strong>temperature=2</strong>的软化。</p>
<p>这就是在做知识的蒸馏。<strong>何谓知识的蒸馏？</strong>经过这样处理后，噪声减少了，利用多个PVP平均的思想把某些本来单个PVP预测偏差比较大的进行平均后修正。</p>
<p>这样子，利用训练好的PVPs所有1K条unlabeled数据打上soft label，再用这1K条打上软标签的数据进行传统的有监督训练，训练完的model应用于下游任务的model。</p>
<blockquote>
<p>注意哦，这里就可以用<strong>轻量的模型</strong>来做fine tuning了哦，因为从20条labeled数据扩充到1K条有带有soft label的数据，labeled数据量大大增加，这时候轻量级的模型也能取得不错的结果，而且轻量模型对轻量部署、高并发等场景更加友好。</p>
</blockquote>
<p>下图就是所有的流程，再总结一下步骤就是</p>
<p><img src="https://i.loli.net/2021/10/28/iyWJmp32kv8uQeU.png" alt=""></p>
<ul>
<li>第一步先定义PVPs，然后对每对PVP用labeled数据进行单独的训练，该步可以加入上面提到的Auxiliary Language Modeling一起训练</li>
<li>第二步：用训练好的PVPs，对unlabled数据进行预测，并知识蒸馏，得到大量的soft label；</li>
<li>第三步：用第二步得到的带有soft label的data，用传统的fine tuning方法训练model。</li>
</ul>
<h2 id="IPET"><a href="#IPET" class="headerlink" title="IPET"></a>IPET</h2><p>将所有单个模型的知识提炼到单个分类器C中意味着它们不能相互学习。由于一些 pattern 的表现(明显地)比其他模式差，因此最终模型的训练集 $T_C$可能包含许多标记错误的示例。</p>
<p>在每个PVP训练的过程中，互相之间是没有耦合的，就是没有互相交换信息，IPET的意思就是想通过迭代，不断扩充上面训练PVP的数据集。</p>
<p>这里简单举个例子，现在有20个labeled数据，1K个unlabeled数据，定义5个PVP，</p>
<p>第一轮，利用20个labeled数据分别训练PVP，第二轮，用第2~4个PVP来预测这1K unlabeled数据，然后选一些模型预测概率比较高的加入到第一个PVP的训练集上，同样用第1、3、4、5个PVP来训练这1K条，然后也将这部分加入到第2个PVP的训练集中，然后再训练一轮，训练后，重复，这样每一轮每个PVP的训练样本不断增多，而且PVP之间的信息也发生了交互。</p>
<p><img src="https://i.loli.net/2021/10/28/lzncwRWb5ovF93e.png" alt=""></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://i.loli.net/2021/10/28/faoicY2BVnuyI9l.png" alt=""></p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><h3 id="Combining-PVPs-1"><a href="#Combining-PVPs-1" class="headerlink" title="Combining PVPs"></a>Combining PVPs</h3><p>作者发现不同PVP之间可能有很大的性能差别，如下图min就是最差的PVP，max就是最好的PVP，可以观察到它们之间的差别就很大。但是又不能站在上帝视角从一开始就选择最好的PVP，所以办法就是做commind PVPs，即上面所提到的知识蒸馏，而且发现蒸馏后会比采用单个最好的PVP效果还要好，并且发现uniform和weighted两个方法效果差不多。</p>
<p><img src="https://i.loli.net/2021/10/28/UkDHrpN1St9q8hR.png" alt=""></p>
<h3 id="Auxiliary-Language-Modeling-1"><a href="#Auxiliary-Language-Modeling-1" class="headerlink" title="Auxiliary Language Modeling"></a>Auxiliary Language Modeling</h3><p>labeled数据越少，auxiliary task的提升效果越明显。</p>
<p><img src="https://i.loli.net/2021/10/28/JtsafNYc1BxU9V2.png" alt=""></p>
<h3 id="Iterative-PER"><a href="#Iterative-PER" class="headerlink" title="Iterative PER"></a>Iterative PER</h3><p>iPET的效果，因为iPET是迭代多轮，每一轮每个PVP的训练集都会增大，从图可以看到每一轮的模型效果都是越来越好的。</p>
<p><img src="https://i.loli.net/2021/10/28/hVICaP1rKk62fdZ.png" alt=""></p>
<h3 id="In-Domain-Pretraining"><a href="#In-Domain-Pretraining" class="headerlink" title="In-Domain Pretraining"></a>In-Domain Pretraining</h3><p>这里讨论了一个问题：PET效果比有监督训练好，是不是因为PET在大量无标签上打上软标签，扩大了有标签数据集？</p>
<p>然后作者做了一个实验，有监督训练时，先在所有数据集上进行继续预训练（这一步作者认为相当于把无标签数据也加进来了），然后再fine funing。实验结果表明，即使这样，有监督效果也离PET有一定距离。</p>
<p><img src="https://i.loli.net/2021/10/28/Ftm58XIxSDTbj9Z.png" alt=""></p>
<h2 id="It’s-Not-Just-Size-That-Matters：Small-Language-Models-Are-Also-Few-Shot-Learners"><a href="#It’s-Not-Just-Size-That-Matters：Small-Language-Models-Are-Also-Few-Shot-Learners" class="headerlink" title="It’s Not Just Size That Matters：Small Language Models Are Also Few-Shot Learners"></a>It’s Not Just Size That Matters：Small Language Models Are Also Few-Shot Learners</h2><p>这篇论文是上篇论文的延伸，其实没有太多新的工作，主要是下面提到的处理多个token的mask，这篇论文主要PK GPT3，不断diss GPT3有多少的不环保。</p>
<h2 id="PET-with-Multiple-Masks"><a href="#PET-with-Multiple-Masks" class="headerlink" title="PET with Multiple Masks"></a>PET with Multiple Masks</h2><p>PET要定义pattern和verblizer，还拿那汽车评论场景举例，我们能不能定义一个verbilzer，它把不同label映射到长度不一的token，如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">定义 Pattern: s。真__!  这里s代表原始输入。  </span><br><span class="line">定义Verbilzer,  v(积极)&#x3D;好,   V(消极)&#x3D;不好</span><br><span class="line">样例 x:   保养贵，配件贵，小毛病多，还有烧机油风险。(label为消极)</span><br><span class="line">pattern(x) &#x3D; 保养贵,配件贵,小毛病多,还有烧机油风险，真__!</span><br></pre></td></tr></table></figure>
<p>因为verbilzer把标签映射到长度不一致的token，那我们究竟定义长度为多少的下划线<em>_</em>，来让model进行完形填空。答案是用最长的那个，例如这里最长的是”不好”，长度为2，所以就挖空两个下划线来让模型做完形填空预测。</p>
<p>做Inference时，</p>
<ul>
<li>$p(\text{积极}|x) =$ 第一个下划线__ 模型预测到token为好的概率。</li>
<li>$p(\text{消极}|x)= $  就麻烦一些，先让模型对两个下划线，进行预测，看是第一个下划线预测token为不，还是第二个下划线预测token为好的概率高一些，把高的那个token先填上去，再重新预测剩下的。举个例子，假如模型预测第一个下划线token为不的概率是0.5，第二个下划线token为好的概率为0.4，即先把不填上第一个下划线，然后再用模型重新预测第二个token为好的概率，假如为0.8，即  $p(\text{消极}|x)= 0.5*0.8=0.4$  </li>
</ul>
<p>做train时，就不考虑这么细致了，具体的，取上面的例子为例，</p>
<ul>
<li>$p(\text{积极}|x) =$ 第一个下划线__，模型预测到token为好的概率，跟inference是一样的</li>
<li>$p(\text{消极}|x)= 0.5*0.4 = 0.2$ ，这里就不分成两步，一步KO，目的是一次前向计算就算完，避免训练过慢。  </li>
</ul>
<p>最后，采用的损失函数也跟第一篇的不一样，这里用的是hinge loss，详细的请看论文。</p>
<script type="math/tex; mode=display">
\sum_{y'\in Y_x} max(0; 1-log\hat q_p (y|x) + log\hat q_p(y'|x))</script><h3 id="Unlabeled-Data-Usage"><a href="#Unlabeled-Data-Usage" class="headerlink" title="Unlabeled Data Usage"></a>Unlabeled Data Usage</h3><p>还是下面这幅图，这里讨论了unlabeled数据的利用。</p>
<p>在PET利用到unlabeled数据的有三个地方：</p>
<ul>
<li>第一处：PET的第二步，用PVPs对unlabeled数据进行知识蒸馏，给数据打上soft label，然后第三步利用这些软标签训练一个模型；</li>
<li>第二处：PET的第一步，假如用的是iPET的话，每一个generation都会把部分的无标签数据打上标签，加入到PVP的训练集；</li>
<li>第三处：PET的第一步，假如采用的是Auxiliary Language Modelling辅助训练，也会引入无标签数据。</li>
</ul>
<p>首先，讨论上面的第一点，究竟能不能直接用PET训练的第一步的PVPs来做预测，这样就不用给unlabeled数据打软标签了（因为虽然说unlabeled数据比labled数据容易获得，但某些场景下unlabeled数据也有可能是拿不到的），答案是可以的，大家看下表的倒数两列，发现不用PET训练的第二、第三步，直接采用第一步训练好的PVPs来做下游应用的预测，效果也是OK的。</p>
<p><strong>只不过，这样做的话，你应用于下游任务的时候就是一堆PVP模型，而不是单一个模型了，这样对轻量部署不是很友好</strong>。</p>
<p><img src="https://i.loli.net/2021/10/28/SwGde9gKBF5t1xb.png" alt=""></p>
<p>还讨论了上面的第二处，发现iPET训练过程中，每一个generation从unlabeled数据中挑选部分加入到PVP的训练集，能让PVP收敛更快，减少不稳定性。</p>
<p><img src="https://i.loli.net/2021/10/28/SeqEI7XiB2rgCVl.png" alt=""></p>
<h3 id="Model-Type"><a href="#Model-Type" class="headerlink" title="Model Type"></a>Model Type</h3><p>不同预训练模型的影响，像BERT这种双向的语言模型会比GPT这种单向的要好，因为假如采用的是单向的语言模型，那么pattern的下划线__部分只能放在句子末尾进行预测。</p>
<p><img src="https://i.loli.net/2021/10/28/73phCBVNdwszAtf.png" alt=""></p>
</article><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://i.loli.net/2021/03/22/HzNFBbkvZ2QMOKV.jpg" target="_blank"><img class="post-qr-code-img" src="https://i.loli.net/2021/03/22/HzNFBbkvZ2QMOKV.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://i.loli.net/2021/03/22/wMGegPYTAXx9cJo.jpg" target="_blank"><img class="post-qr-code-img" src="https://i.loli.net/2021/03/22/wMGegPYTAXx9cJo.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/10/30/Learn-Continually-Generalize-Rapidly-Lifelong-Knowledge-Accumulation-for-Few-shot-Learning/"><img class="prev-cover" src="https://i.loli.net/2021/10/30/L9XTOjA4csqBRMt.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Learn Continually, Generalize Rapidly, Lifelong Knowledge Accumulation for Few-shot Learning</div></div></a></div><div class="next-post pull-right"><a href="/2021/10/23/Parameter-Efficient-Transfer-Learning-for-NLP/"><img class="next-cover" src="https://i.loli.net/2021/10/23/93P1rDfEm6LtgnK.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Parameter-Efficient Transfer Learning for NLP</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/05/03/Learning-to-Prompt-for-Continual-Learning/" title="Learning to Prompt for Continual Learning"><img class="cover" src="https://s1.ax1x.com/2022/05/03/Ok0tEj.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-03</div><div class="title">Learning to Prompt for Continual Learning</div></div></a></div><div><a href="/2022/02/21/ZeroPrompt-Scaling-Prompt-Based-Pretraining-to-1-000-Tasks-Improves-Zero-Shot-Generalization/" title="ZeroPrompt- Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization"><img class="cover" src="https://s4.ax1x.com/2022/02/21/HXcFxA.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-02-21</div><div class="title">ZeroPrompt- Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization</div></div></a></div><div><a href="/2022/02/10/Prompt-Guided-Few-Shot-Event-Detection/" title="Prompt-Guided Few-Shot Event Detection"><img class="cover" src="https://i.loli.net/2021/10/28/DjK6qxmT9cA7Ov3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-02-10</div><div class="title">Prompt-Guided Few-Shot Event Detection</div></div></a></div><div><a href="/2022/02/10/MetaPrompting-Learning-to-Learn-Better-Prompts/" title="MetaPrompting: Learning to Learn Better Prompts"><img class="cover" src="https://s4.ax1x.com/2022/01/22/7fJ3lV.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-02-10</div><div class="title">MetaPrompting: Learning to Learn Better Prompts</div></div></a></div><div><a href="/2022/01/04/Exploring-Low-dimensional-Intrinsic-Task-Subspace-via-Prompt-Tuning/" title="Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning"><img class="cover" src="https://i.loli.net/2021/08/29/7pyGPBNQZRmv3Tr.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-04</div><div class="title">Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning</div></div></a></div><div><a href="/2021/12/27/Unsupervised-Domain-Adaptation-of-a-Pretrained-Cross-Lingual-Language-Model/" title="Unsupervised Domain Adaptation of a Pretrained Cross-Lingual Language Model"><img class="cover" src="https://i.loli.net/2021/11/17/hMK8qF4Jne6sTYC.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-27</div><div class="title">Unsupervised Domain Adaptation of a Pretrained Cross-Lingual Language Model</div></div></a></div></div></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Coding-Zuo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2021/10/28/Pattern-Exploiting-Training-PET/">http://example.com/2021/10/28/Pattern-Exploiting-Training-PET/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Coding-Zuo</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/context-detection/">context detection</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=null" async="async"></script></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Pattern-Exploiting-Training-PET"><span class="toc-number">1.</span> <span class="toc-text">Pattern Exploiting Training (PET)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Prompt-Notation"><span class="toc-number">1.1.</span> <span class="toc-text">Prompt Notation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Auxiliary-Language-Modeling"><span class="toc-number">1.2.</span> <span class="toc-text">Auxiliary Language Modeling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Combining-PVPs"><span class="toc-number">1.3.</span> <span class="toc-text">Combining PVPs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#IPET"><span class="toc-number">1.4.</span> <span class="toc-text">IPET</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">1.5.</span> <span class="toc-text">实验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E6%9E%90"><span class="toc-number">1.6.</span> <span class="toc-text">分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Combining-PVPs-1"><span class="toc-number">1.6.1.</span> <span class="toc-text">Combining PVPs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Auxiliary-Language-Modeling-1"><span class="toc-number">1.6.2.</span> <span class="toc-text">Auxiliary Language Modeling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Iterative-PER"><span class="toc-number">1.6.3.</span> <span class="toc-text">Iterative PER</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#In-Domain-Pretraining"><span class="toc-number">1.6.4.</span> <span class="toc-text">In-Domain Pretraining</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#It%E2%80%99s-Not-Just-Size-That-Matters%EF%BC%9ASmall-Language-Models-Are-Also-Few-Shot-Learners"><span class="toc-number">1.7.</span> <span class="toc-text">It’s Not Just Size That Matters：Small Language Models Are Also Few-Shot Learners</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PET-with-Multiple-Masks"><span class="toc-number">1.8.</span> <span class="toc-text">PET with Multiple Masks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Unlabeled-Data-Usage"><span class="toc-number">1.8.1.</span> <span class="toc-text">Unlabeled Data Usage</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-Type"><span class="toc-number">1.8.2.</span> <span class="toc-text">Model Type</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Coding-Zuo</div><div class="footer_custom_text">Hi, welcome to my BLOG</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: 'd652112bf81876e00118',
      clientSecret: 'a5abed418c7cc2736af5b4d0cbd7ff97d460a5b3',
      repo: 'Coding-Zuo.github.io',
      owner: 'Coding-Zuo',
      admin: ['Coding-Zuo'],
      id: '717a3d2a4f1dd16a8a37554762534f2d',
      language: 'zh-CN',
      perPage: 10,
      distractionFreeMode: false,
      pagerDirection: 'last',
      createIssueManually: false,
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script src="/js/custom.js"></script><script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script src="//code.tidio.co/mak6nokafytw9mgrsuzglwzfxiy3fpdl.js" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>